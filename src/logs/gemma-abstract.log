Here's a rough estimate of the cost; it might not be accurate and we are not responsible for any cost incurred.
Each round costs 0.01, proposer spending 0.01, assigner spending 0.00. The final assignment costs 0.00.
Total cost is 0.03
Do you want to continue? (y/n)Loading model google/gemma-2-2b-it with vLLM  (tensor_parallel_size=1)...
WARNING 10-30 23:35:31 config.py:185] gemma2 has interleaved attention, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).
INFO 10-30 23:35:35 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='google/gemma-2-2b-it', speculative_config=None, tokenizer='google/gemma-2-2b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2-2b-it, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 10-30 23:35:37 model_runner.py:1056] Starting to load model google/gemma-2-2b-it...
INFO 10-30 23:35:38 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 10-30 23:35:40 model_runner.py:1067] Loading model weights took 4.8999 GB
INFO 10-30 23:35:44 gpu_executor.py:122] # GPU blocks: 22076, # CPU blocks: 2520
INFO 10-30 23:35:44 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 86.23x
INFO 10-30 23:35:46 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 10-30 23:35:46 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 10-30 23:36:05 model_runner.py:1523] Graph capturing finished in 19 secs.
WARNING 10-30 23:36:05 sampling_params.py:299] temperature 0.001 is less than 0.01, which may cause numerical errors nan or inf in tensors. We have maxed it out to 0.01.
Model loaded successfully.
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   Evolutionary Learning proceeds by evolving a population of classifiers, from
which it generally returns (with some notable exceptions) the single
best-of-run classifier as final result. In the meanwhile, Ensemble Learning,
one of the most efficient approaches in supervised Machine Learning for the
last decade, proceeds by building a population of diverse classifiers. Ensemble
Learning with Evolutionary Computation thus receives increasing attention. The
Evolutionary Ensemble Learning (EEL) approach presented in this paper features
two contributions. First, a new fitness function, inspired by co-evolution and
enforcing the classifier diversity, is presented. Further, a new selection
criterion based on the classification margin is proposed. This criterion is
used to extract the classifier ensemble from the final population only
(Off-line) or incrementally along evolution (On-line). Experiments on a set of
benchmark problems show that Off-line outperforms single-hypothesis
evolutionary learning and state-of-art Boosting and generates smaller
classifier ensembles.

1.   Cluster matching by permuting cluster labels is important in many clustering
contexts such as cluster validation and cluster ensemble techniques. The
classic approach is to minimize the euclidean distance between two cluster
solutions which induces inappropriate stability in certain settings. Therefore,
we present the truematch algorithm that introduces two improvements best
explained in the crisp case. First, instead of maximizing the trace of the
cluster crosstable, we propose to maximize a chi-square transformation of this
crosstable. Thus, the trace will not be dominated by the cells with the largest
counts but by the cells with the most non-random observations, taking into
account the marginals. Second, we suggest a probabilistic component in order to
break ties and to make the matching algorithm truly random on random data. The
truematch algorithm is designed as a building block of the truecluster
framework and scales in polynomial time. First simulation results confirm that
the truematch algorithm gives more consistent truecluster results for unequal
cluster sizes. Free R software is available.

2.   We consider the problem of finding an n-agent joint-policy for the optimal
finite-horizon control of a decentralized Pomdp (Dec-Pomdp). This is a problem
of very high complexity (NEXP-hard in n >= 2). In this paper, we propose a new
mathematical programming approach for the problem. Our approach is based on two
ideas: First, we represent each agent's policy in the sequence-form and not in
the tree-form, thereby obtaining a very compact representation of the set of
joint-policies. Second, using this compact representation, we solve this
problem as an instance of combinatorial optimization for which we formulate a
mixed integer linear program (MILP). The optimal solution of the MILP directly
yields an optimal joint-policy for the Dec-Pomdp. Computational experience
shows that formulating and solving the MILP requires significantly less time to
solve benchmark Dec-Pomdp problems than existing algorithms. For example, the
multi-agent tiger problem for horizon 4 is solved in 72 secs with the MILP
whereas existing algorithms require several hours to solve it.

3.   This paper proposes an approach to training rough set models using Bayesian
framework trained using Markov Chain Monte Carlo (MCMC) method. The prior
probabilities are constructed from the prior knowledge that good rough set
models have fewer rules. Markov Chain Monte Carlo sampling is conducted through
sampling in the rough set granule space and Metropolis algorithm is used as an
acceptance criteria. The proposed method is tested to estimate the risk of HIV
given demographic data. The results obtained shows that the proposed approach
is able to achieve an average accuracy of 58% with the accuracy varying up to
66%. In addition the Bayesian rough set give the probabilities of the estimated
HIV status as well as the linguistic rules describing how the demographic
parameters drive the risk of HIV.

4.   This paper proposes a neuro-rough model based on multi-layered perceptron and
rough set. The neuro-rough model is then tested on modelling the risk of HIV
from demographic data. The model is formulated using Bayesian framework and
trained using Monte Carlo method and Metropolis criterion. When the model was
tested to estimate the risk of HIV infection given the demographic data it was
found to give the accuracy of 62%. The proposed model is able to combine the
accuracy of the Bayesian MLP model and the transparency of Bayesian rough set
model.

5.   Noise, corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust,
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise.

6.   This paper presents a fault classification method which makes use of a
Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the
vibration signals of cylindrical shells. The calculation of Pseudomodal
Energies, for the purposes of condition monitoring, has previously been found
to be an accurate method of extracting features from vibration signals. This
calculation is therefore used to extract features from vibration signals
obtained from a diverse population of cylindrical shells. Some of the cylinders
in the population have faults in different substructures. The pseudomodal
energies calculated from the vibration signals are then used as inputs to a
neuro-fuzzy model. A leave-one-out cross-validation process is used to test the
performance of the model. It is found that the neuro-fuzzy model is able to
classify faults with an accuracy of 91.62%, which is higher than the previously
used multilayer perceptron.

7.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

8.   This paper describes a system capable of semi-automatically filling an XML
template from free texts in the clinical domain (practice guidelines). The XML
template includes semantic information not explicitly encoded in the text
(pairs of conditions and actions/recommendations). Therefore, there is a need
to compute the exact scope of conditions over text sequences expressing the
required actions. We present a system developed for this task. We show that it
yields good performance when applied to the analysis of French practice
guidelines.

9.   The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.

10.   The act of bluffing confounds game designers to this day. The very nature of
bluffing is even open for debate, adding further complication to the process of
creating intelligent virtual players that can bluff, and hence play,
realistically. Through the use of intelligent, learning agents, and carefully
designed agent outlooks, an agent can in fact learn to predict its opponents
reactions based not only on its own cards, but on the actions of those around
it. With this wider scope of understanding, an agent can in learn to bluff its
opponents, with the action representing not an illogical action, as bluffing is
often viewed, but rather as an act of maximising returns through an effective
statistical optimisation. By using a tee dee lambda learning algorithm to
continuously adapt neural network agent intelligence, agents have been shown to
be able to learn to bluff without outside prompting, and even to learn to call
each others bluffs in free, competitive play.

11.   Representing and reasoning about qualitative temporal information is an
essential part of many artificial intelligence tasks. Lots of models have been
proposed in the litterature for representing such temporal information. All
derive from a point-based or an interval-based framework. One fundamental
reasoning task that arises in applications of these frameworks is given by the
following scheme: given possibly indefinite and incomplete knowledge of the
binary relationships between some temporal objects, find the consistent
scenarii between all these objects. All these models require transitive tables
-- or similarly inference rules-- for solving such tasks. We have defined an
alternative model, S-languages - to represent qualitative temporal information,
based on the only two relations of \emph{precedence} and \emph{simultaneity}.
In this paper, we show how this model enables to avoid transitive tables or
inference rules to handle this kind of problem.

12.   The semiring-based constraint satisfaction problems (semiring CSPs), proposed
by Bistarelli, Montanari and Rossi \cite{BMR97}, is a very general framework of
soft constraints. In this paper we propose an abstraction scheme for soft
constraints that uses semiring homomorphism. To find optimal solutions of the
concrete problem, the idea is, first working in the abstract problem and
finding its optimal solutions, then using them to solve the concrete problem.
  In particular, we show that a mapping preserves optimal solutions if and only
if it is an order-reflecting semiring homomorphism. Moreover, for a semiring
homomorphism $\alpha$ and a problem $P$ over $S$, if $t$ is optimal in
$\alpha(P)$, then there is an optimal solution $\bar{t}$ of $P$ such that
$\bar{t}$ has the same value as $t$ in $\alpha(P)$.

13.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

14.   This paper presents bushing condition monitoring frameworks that use
multi-layer perceptrons (MLP), radial basis functions (RBF) and support vector
machines (SVM) classifiers. The first level of the framework determines if the
bushing is faulty or not while the second level determines the type of fault.
The diagnostic gases in the bushings are analyzed using the dissolve gas
analysis. MLP gives superior performance in terms of accuracy and training time
than SVM and RBF. In addition, an on-line bushing condition monitoring
approach, which is able to adapt to newly acquired data are introduced. This
approach is able to accommodate new classes that are introduced by incoming
data and is implemented using an incremental learning algorithm that uses MLP.
The testing results improved from 67.5% to 95.8% as new data were introduced
and the testing results improved from 60% to 95.3% as new conditions were
introduced. On average the confidence value of the framework on its decision
was 0.92.

15.   This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.

16.   Water plays a pivotal role in many physical processes, and most importantly
in sustaining human life, animal life and plant life. Water supply entities
therefore have the responsibility to supply clean and safe water at the rate
required by the consumer. It is therefore necessary to implement mechanisms and
systems that can be employed to predict both short-term and long-term water
demands. The increasingly growing field of computational intelligence
techniques has been proposed as an efficient tool in the modelling of dynamic
phenomena. The primary objective of this paper is to compare the efficiency of
two computational intelligence techniques in water demand forecasting. The
techniques under comparison are the Artificial Neural Networks (ANNs) and the
Support Vector Machines (SVMs). In this study it was observed that the ANNs
perform better than the SVMs. This performance is measured against the
generalisation ability of the two.

17.   This paper introduces a continuous model for Multi-cellular Developmental
Design. The cells are fixed on a 2D grid and exchange "chemicals" with their
neighbors during the growth process. The quantity of chemicals that a cell
produces, as well as the differentiation value of the cell in the phenotype,
are controlled by a Neural Network (the genotype) that takes as inputs the
chemicals produced by the neighboring cells at the previous time step. In the
proposed model, the number of iterations of the growth process is not
pre-determined, but emerges during evolution: only organisms for which the
growth process stabilizes give a phenotype (the stable state), others are
declared nonviable. The optimization of the controller is done using the NEAT
algorithm, that optimizes both the topology and the weights of the Neural
Networks. Though each cell only receives local information from its neighbors,
the experimental results of the proposed approach on the 'flags' problems (the
phenotype must match a given 2D pattern) are almost as good as those of a
direct regression approach using the same model with global information.
Moreover, the resulting multi-cellular organisms exhibit almost perfect
self-healing characteristics.

18.   Semantic network research has seen a resurgence from its early history in the
cognitive sciences with the inception of the Semantic Web initiative. The
Semantic Web effort has brought forth an array of technologies that support the
encoding, storage, and querying of the semantic network data structure at the
world stage. Currently, the popular conception of the Semantic Web is that of a
data modeling medium where real and conceptual entities are related in
semantically meaningful ways. However, new models have emerged that explicitly
encode procedural information within the semantic network substrate. With these
new technologies, the Semantic Web has evolved from a data modeling medium to a
computational medium. This article provides a classification of existing
computational modeling efforts and the requirements of supporting technologies
that will aid in the further growth of this burgeoning domain.

19.   When Kurt Goedel layed the foundations of theoretical computer science in
1931, he also introduced essential concepts of the theory of Artificial
Intelligence (AI). Although much of subsequent AI research has focused on
heuristics, which still play a major role in many practical AI applications, in
the new millennium AI theory has finally become a full-fledged formal science,
with important optimality results for embodied agents living in unknown
environments, obtained through a combination of theory a la Goedel and
probability theory. Here we look back at important milestones of AI history,
mention essential recent results, and speculate about what we may expect from
the next 25 years, emphasizing the significance of the ongoing dramatic
hardware speedups, and discussing Goedel-inspired, self-referential,
self-improving universal problem solvers.

20.   Gaussian mixture models (GMM) and support vector machines (SVM) are
introduced to classify faults in a population of cylindrical shells. The
proposed procedures are tested on a population of 20 cylindrical shells and
their performance is compared to the procedure, which uses multi-layer
perceptrons (MLP). The modal properties extracted from vibration data are used
to train the GMM, SVM and MLP. It is observed that the GMM produces 98%, SVM
produces 94% classification accuracy while the MLP produces 88% classification
rates.

21.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

22.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

23.   An ensemble based approach for dealing with missing data, without predicting
or imputing the missing values is proposed. This technique is suitable for
online operations of neural networks and as a result, is used for online
condition monitoring. The proposed technique is tested in both classification
and regression problems. An ensemble of Fuzzy-ARTMAPs is used for
classification whereas an ensemble of multi-layer perceptrons is used for the
regression problem. Results obtained using this ensemble-based technique are
compared to those obtained using a combination of auto-associative neural
networks and genetic algorithms and findings show that this method can perform
up to 9% better in regression problems. Another advantage of the proposed
technique is that it eliminates the need for finding the best estimate of the
data, and hence, saves time.

24.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.

25.   In this paper, we employ Probabilistic Neural Network (PNN) with image and
data processing techniques to implement a general purpose automated leaf
recognition algorithm. 12 leaf features are extracted and orthogonalized into 5
principal variables which consist the input vector of the PNN. The PNN is
trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater
than 90%. Compared with other approaches, our algorithm is an accurate
artificial intelligence approach which is fast in execution and easy in
implementation.

26.   Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of fault classification methods"
"has a topic of multi-cellular developmental design"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of evolutionary learning and ensemble methods"\n- "has a topic of decentralized control in multi-agent systems"\n- "has a topic of Bayesian methods in rough set models"\n- "has a topic of face recognition robustness"\n- "has a topic of condition monitoring in engineering"\n- "has a topic of quantum computation and artificial intelligence"\n- "has a topic of water demand forecasting using computational intelligence"\n- "has a topic of semantic web and computational modeling"\n- "has a topic of conflict prediction using machine learning"\n- "has a topic of automated leaf recognition algorithms"\n- "has a topic of natural language processing for semantic access"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

1.   This paper proposes an approach to training rough set models using Bayesian
framework trained using Markov Chain Monte Carlo (MCMC) method. The prior
probabilities are constructed from the prior knowledge that good rough set
models have fewer rules. Markov Chain Monte Carlo sampling is conducted through
sampling in the rough set granule space and Metropolis algorithm is used as an
acceptance criteria. The proposed method is tested to estimate the risk of HIV
given demographic data. The results obtained shows that the proposed approach
is able to achieve an average accuracy of 58% with the accuracy varying up to
66%. In addition the Bayesian rough set give the probabilities of the estimated
HIV status as well as the linguistic rules describing how the demographic
parameters drive the risk of HIV.

2.   In this paper, we employ Probabilistic Neural Network (PNN) with image and
data processing techniques to implement a general purpose automated leaf
recognition algorithm. 12 leaf features are extracted and orthogonalized into 5
principal variables which consist the input vector of the PNN. The PNN is
trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater
than 90%. Compared with other approaches, our algorithm is an accurate
artificial intelligence approach which is fast in execution and easy in
implementation.

3.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.

4.   Gaussian mixture models (GMM) and support vector machines (SVM) are
introduced to classify faults in a population of cylindrical shells. The
proposed procedures are tested on a population of 20 cylindrical shells and
their performance is compared to the procedure, which uses multi-layer
perceptrons (MLP). The modal properties extracted from vibration data are used
to train the GMM, SVM and MLP. It is observed that the GMM produces 98%, SVM
produces 94% classification accuracy while the MLP produces 88% classification
rates.

5.   Representing and reasoning about qualitative temporal information is an
essential part of many artificial intelligence tasks. Lots of models have been
proposed in the litterature for representing such temporal information. All
derive from a point-based or an interval-based framework. One fundamental
reasoning task that arises in applications of these frameworks is given by the
following scheme: given possibly indefinite and incomplete knowledge of the
binary relationships between some temporal objects, find the consistent
scenarii between all these objects. All these models require transitive tables
-- or similarly inference rules-- for solving such tasks. We have defined an
alternative model, S-languages - to represent qualitative temporal information,
based on the only two relations of \emph{precedence} and \emph{simultaneity}.
In this paper, we show how this model enables to avoid transitive tables or
inference rules to handle this kind of problem.

6.   Water plays a pivotal role in many physical processes, and most importantly
in sustaining human life, animal life and plant life. Water supply entities
therefore have the responsibility to supply clean and safe water at the rate
required by the consumer. It is therefore necessary to implement mechanisms and
systems that can be employed to predict both short-term and long-term water
demands. The increasingly growing field of computational intelligence
techniques has been proposed as an efficient tool in the modelling of dynamic
phenomena. The primary objective of this paper is to compare the efficiency of
two computational intelligence techniques in water demand forecasting. The
techniques under comparison are the Artificial Neural Networks (ANNs) and the
Support Vector Machines (SVMs). In this study it was observed that the ANNs
perform better than the SVMs. This performance is measured against the
generalisation ability of the two.

7.   This paper proposes a neuro-rough model based on multi-layered perceptron and
rough set. The neuro-rough model is then tested on modelling the risk of HIV
from demographic data. The model is formulated using Bayesian framework and
trained using Monte Carlo method and Metropolis criterion. When the model was
tested to estimate the risk of HIV infection given the demographic data it was
found to give the accuracy of 62%. The proposed model is able to combine the
accuracy of the Bayesian MLP model and the transparency of Bayesian rough set
model.

8.   The act of bluffing confounds game designers to this day. The very nature of
bluffing is even open for debate, adding further complication to the process of
creating intelligent virtual players that can bluff, and hence play,
realistically. Through the use of intelligent, learning agents, and carefully
designed agent outlooks, an agent can in fact learn to predict its opponents
reactions based not only on its own cards, but on the actions of those around
it. With this wider scope of understanding, an agent can in learn to bluff its
opponents, with the action representing not an illogical action, as bluffing is
often viewed, but rather as an act of maximising returns through an effective
statistical optimisation. By using a tee dee lambda learning algorithm to
continuously adapt neural network agent intelligence, agents have been shown to
be able to learn to bluff without outside prompting, and even to learn to call
each others bluffs in free, competitive play.

9.   When Kurt Goedel layed the foundations of theoretical computer science in
1931, he also introduced essential concepts of the theory of Artificial
Intelligence (AI). Although much of subsequent AI research has focused on
heuristics, which still play a major role in many practical AI applications, in
the new millennium AI theory has finally become a full-fledged formal science,
with important optimality results for embodied agents living in unknown
environments, obtained through a combination of theory a la Goedel and
probability theory. Here we look back at important milestones of AI history,
mention essential recent results, and speculate about what we may expect from
the next 25 years, emphasizing the significance of the ongoing dramatic
hardware speedups, and discussing Goedel-inspired, self-referential,
self-improving universal problem solvers.

10.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

11.   This paper introduces a continuous model for Multi-cellular Developmental
Design. The cells are fixed on a 2D grid and exchange "chemicals" with their
neighbors during the growth process. The quantity of chemicals that a cell
produces, as well as the differentiation value of the cell in the phenotype,
are controlled by a Neural Network (the genotype) that takes as inputs the
chemicals produced by the neighboring cells at the previous time step. In the
proposed model, the number of iterations of the growth process is not
pre-determined, but emerges during evolution: only organisms for which the
growth process stabilizes give a phenotype (the stable state), others are
declared nonviable. The optimization of the controller is done using the NEAT
algorithm, that optimizes both the topology and the weights of the Neural
Networks. Though each cell only receives local information from its neighbors,
the experimental results of the proposed approach on the 'flags' problems (the
phenotype must match a given 2D pattern) are almost as good as those of a
direct regression approach using the same model with global information.
Moreover, the resulting multi-cellular organisms exhibit almost perfect
self-healing characteristics.

12.   An ensemble based approach for dealing with missing data, without predicting
or imputing the missing values is proposed. This technique is suitable for
online operations of neural networks and as a result, is used for online
condition monitoring. The proposed technique is tested in both classification
and regression problems. An ensemble of Fuzzy-ARTMAPs is used for
classification whereas an ensemble of multi-layer perceptrons is used for the
regression problem. Results obtained using this ensemble-based technique are
compared to those obtained using a combination of auto-associative neural
networks and genetic algorithms and findings show that this method can perform
up to 9% better in regression problems. Another advantage of the proposed
technique is that it eliminates the need for finding the best estimate of the
data, and hence, saves time.

13.   Cluster matching by permuting cluster labels is important in many clustering
contexts such as cluster validation and cluster ensemble techniques. The
classic approach is to minimize the euclidean distance between two cluster
solutions which induces inappropriate stability in certain settings. Therefore,
we present the truematch algorithm that introduces two improvements best
explained in the crisp case. First, instead of maximizing the trace of the
cluster crosstable, we propose to maximize a chi-square transformation of this
crosstable. Thus, the trace will not be dominated by the cells with the largest
counts but by the cells with the most non-random observations, taking into
account the marginals. Second, we suggest a probabilistic component in order to
break ties and to make the matching algorithm truly random on random data. The
truematch algorithm is designed as a building block of the truecluster
framework and scales in polynomial time. First simulation results confirm that
the truematch algorithm gives more consistent truecluster results for unequal
cluster sizes. Free R software is available.

14.   The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.

15.   Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.

16.   Semantic network research has seen a resurgence from its early history in the
cognitive sciences with the inception of the Semantic Web initiative. The
Semantic Web effort has brought forth an array of technologies that support the
encoding, storage, and querying of the semantic network data structure at the
world stage. Currently, the popular conception of the Semantic Web is that of a
data modeling medium where real and conceptual entities are related in
semantically meaningful ways. However, new models have emerged that explicitly
encode procedural information within the semantic network substrate. With these
new technologies, the Semantic Web has evolved from a data modeling medium to a
computational medium. This article provides a classification of existing
computational modeling efforts and the requirements of supporting technologies
that will aid in the further growth of this burgeoning domain.

17.   This paper presents bushing condition monitoring frameworks that use
multi-layer perceptrons (MLP), radial basis functions (RBF) and support vector
machines (SVM) classifiers. The first level of the framework determines if the
bushing is faulty or not while the second level determines the type of fault.
The diagnostic gases in the bushings are analyzed using the dissolve gas
analysis. MLP gives superior performance in terms of accuracy and training time
than SVM and RBF. In addition, an on-line bushing condition monitoring
approach, which is able to adapt to newly acquired data are introduced. This
approach is able to accommodate new classes that are introduced by incoming
data and is implemented using an incremental learning algorithm that uses MLP.
The testing results improved from 67.5% to 95.8% as new data were introduced
and the testing results improved from 60% to 95.3% as new conditions were
introduced. On average the confidence value of the framework on its decision
was 0.92.

18.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

19.   The semiring-based constraint satisfaction problems (semiring CSPs), proposed
by Bistarelli, Montanari and Rossi \cite{BMR97}, is a very general framework of
soft constraints. In this paper we propose an abstraction scheme for soft
constraints that uses semiring homomorphism. To find optimal solutions of the
concrete problem, the idea is, first working in the abstract problem and
finding its optimal solutions, then using them to solve the concrete problem.
  In particular, we show that a mapping preserves optimal solutions if and only
if it is an order-reflecting semiring homomorphism. Moreover, for a semiring
homomorphism $\alpha$ and a problem $P$ over $S$, if $t$ is optimal in
$\alpha(P)$, then there is an optimal solution $\bar{t}$ of $P$ such that
$\bar{t}$ has the same value as $t$ in $\alpha(P)$.

20.   Evolutionary Learning proceeds by evolving a population of classifiers, from
which it generally returns (with some notable exceptions) the single
best-of-run classifier as final result. In the meanwhile, Ensemble Learning,
one of the most efficient approaches in supervised Machine Learning for the
last decade, proceeds by building a population of diverse classifiers. Ensemble
Learning with Evolutionary Computation thus receives increasing attention. The
Evolutionary Ensemble Learning (EEL) approach presented in this paper features
two contributions. First, a new fitness function, inspired by co-evolution and
enforcing the classifier diversity, is presented. Further, a new selection
criterion based on the classification margin is proposed. This criterion is
used to extract the classifier ensemble from the final population only
(Off-line) or incrementally along evolution (On-line). Experiments on a set of
benchmark problems show that Off-line outperforms single-hypothesis
evolutionary learning and state-of-art Boosting and generates smaller
classifier ensembles.

21.   We consider the problem of finding an n-agent joint-policy for the optimal
finite-horizon control of a decentralized Pomdp (Dec-Pomdp). This is a problem
of very high complexity (NEXP-hard in n >= 2). In this paper, we propose a new
mathematical programming approach for the problem. Our approach is based on two
ideas: First, we represent each agent's policy in the sequence-form and not in
the tree-form, thereby obtaining a very compact representation of the set of
joint-policies. Second, using this compact representation, we solve this
problem as an instance of combinatorial optimization for which we formulate a
mixed integer linear program (MILP). The optimal solution of the MILP directly
yields an optimal joint-policy for the Dec-Pomdp. Computational experience
shows that formulating and solving the MILP requires significantly less time to
solve benchmark Dec-Pomdp problems than existing algorithms. For example, the
multi-agent tiger problem for horizon 4 is solved in 72 secs with the MILP
whereas existing algorithms require several hours to solve it.

22.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

23.   This paper presents a fault classification method which makes use of a
Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the
vibration signals of cylindrical shells. The calculation of Pseudomodal
Energies, for the purposes of condition monitoring, has previously been found
to be an accurate method of extracting features from vibration signals. This
calculation is therefore used to extract features from vibration signals
obtained from a diverse population of cylindrical shells. Some of the cylinders
in the population have faults in different substructures. The pseudomodal
energies calculated from the vibration signals are then used as inputs to a
neuro-fuzzy model. A leave-one-out cross-validation process is used to test the
performance of the model. It is found that the neuro-fuzzy model is able to
classify faults with an accuracy of 91.62%, which is higher than the previously
used multilayer perceptron.

24.   This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.

25.   This paper describes a system capable of semi-automatically filling an XML
template from free texts in the clinical domain (practice guidelines). The XML
template includes semantic information not explicitly encoded in the text
(pairs of conditions and actions/recommendations). Therefore, there is a need
to compute the exact scope of conditions over text sequences expressing the
required actions. We present a system developed for this task. We show that it
yields good performance when applied to the analysis of French practice
guidelines.

26.   Noise, corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust,
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"- "has a topic of evolutionary robotics"
"has a topic of condition monitoring frameworks"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of protein family detection"\n- "has a topic of rough set models and Bayesian frameworks"\n- "has a topic of automated leaf recognition algorithms"\n- "has a topic of predicting militarized interstate disputes"\n- "has a topic of fault classification in cylindrical shells"\n- "has a topic of qualitative temporal information representation"\n- "has a topic of water demand forecasting"\n- "has a topic of multi-cellular developmental design"\n- "has a topic of missing data handling in neural networks"\n- "has a topic of evolutionary learning and ensemble methods"\n- "has a topic of quantum computation and artificial intelligence"\n- "has a topic of XML template filling in clinical guidelines"\n- "has a topic of face recognition robustness"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   The semiring-based constraint satisfaction problems (semiring CSPs), proposed
by Bistarelli, Montanari and Rossi \cite{BMR97}, is a very general framework of
soft constraints. In this paper we propose an abstraction scheme for soft
constraints that uses semiring homomorphism. To find optimal solutions of the
concrete problem, the idea is, first working in the abstract problem and
finding its optimal solutions, then using them to solve the concrete problem.
  In particular, we show that a mapping preserves optimal solutions if and only
if it is an order-reflecting semiring homomorphism. Moreover, for a semiring
homomorphism $\alpha$ and a problem $P$ over $S$, if $t$ is optimal in
$\alpha(P)$, then there is an optimal solution $\bar{t}$ of $P$ such that
$\bar{t}$ has the same value as $t$ in $\alpha(P)$.

1.   Cluster matching by permuting cluster labels is important in many clustering
contexts such as cluster validation and cluster ensemble techniques. The
classic approach is to minimize the euclidean distance between two cluster
solutions which induces inappropriate stability in certain settings. Therefore,
we present the truematch algorithm that introduces two improvements best
explained in the crisp case. First, instead of maximizing the trace of the
cluster crosstable, we propose to maximize a chi-square transformation of this
crosstable. Thus, the trace will not be dominated by the cells with the largest
counts but by the cells with the most non-random observations, taking into
account the marginals. Second, we suggest a probabilistic component in order to
break ties and to make the matching algorithm truly random on random data. The
truematch algorithm is designed as a building block of the truecluster
framework and scales in polynomial time. First simulation results confirm that
the truematch algorithm gives more consistent truecluster results for unequal
cluster sizes. Free R software is available.

2.   This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.

3.   Evolutionary Learning proceeds by evolving a population of classifiers, from
which it generally returns (with some notable exceptions) the single
best-of-run classifier as final result. In the meanwhile, Ensemble Learning,
one of the most efficient approaches in supervised Machine Learning for the
last decade, proceeds by building a population of diverse classifiers. Ensemble
Learning with Evolutionary Computation thus receives increasing attention. The
Evolutionary Ensemble Learning (EEL) approach presented in this paper features
two contributions. First, a new fitness function, inspired by co-evolution and
enforcing the classifier diversity, is presented. Further, a new selection
criterion based on the classification margin is proposed. This criterion is
used to extract the classifier ensemble from the final population only
(Off-line) or incrementally along evolution (On-line). Experiments on a set of
benchmark problems show that Off-line outperforms single-hypothesis
evolutionary learning and state-of-art Boosting and generates smaller
classifier ensembles.

4.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.

5.   Semantic network research has seen a resurgence from its early history in the
cognitive sciences with the inception of the Semantic Web initiative. The
Semantic Web effort has brought forth an array of technologies that support the
encoding, storage, and querying of the semantic network data structure at the
world stage. Currently, the popular conception of the Semantic Web is that of a
data modeling medium where real and conceptual entities are related in
semantically meaningful ways. However, new models have emerged that explicitly
encode procedural information within the semantic network substrate. With these
new technologies, the Semantic Web has evolved from a data modeling medium to a
computational medium. This article provides a classification of existing
computational modeling efforts and the requirements of supporting technologies
that will aid in the further growth of this burgeoning domain.

6.   This paper presents a fault classification method which makes use of a
Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the
vibration signals of cylindrical shells. The calculation of Pseudomodal
Energies, for the purposes of condition monitoring, has previously been found
to be an accurate method of extracting features from vibration signals. This
calculation is therefore used to extract features from vibration signals
obtained from a diverse population of cylindrical shells. Some of the cylinders
in the population have faults in different substructures. The pseudomodal
energies calculated from the vibration signals are then used as inputs to a
neuro-fuzzy model. A leave-one-out cross-validation process is used to test the
performance of the model. It is found that the neuro-fuzzy model is able to
classify faults with an accuracy of 91.62%, which is higher than the previously
used multilayer perceptron.

7.   In this paper, we employ Probabilistic Neural Network (PNN) with image and
data processing techniques to implement a general purpose automated leaf
recognition algorithm. 12 leaf features are extracted and orthogonalized into 5
principal variables which consist the input vector of the PNN. The PNN is
trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater
than 90%. Compared with other approaches, our algorithm is an accurate
artificial intelligence approach which is fast in execution and easy in
implementation.

8.   This paper proposes an approach to training rough set models using Bayesian
framework trained using Markov Chain Monte Carlo (MCMC) method. The prior
probabilities are constructed from the prior knowledge that good rough set
models have fewer rules. Markov Chain Monte Carlo sampling is conducted through
sampling in the rough set granule space and Metropolis algorithm is used as an
acceptance criteria. The proposed method is tested to estimate the risk of HIV
given demographic data. The results obtained shows that the proposed approach
is able to achieve an average accuracy of 58% with the accuracy varying up to
66%. In addition the Bayesian rough set give the probabilities of the estimated
HIV status as well as the linguistic rules describing how the demographic
parameters drive the risk of HIV.

9.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

10.   This paper describes a system capable of semi-automatically filling an XML
template from free texts in the clinical domain (practice guidelines). The XML
template includes semantic information not explicitly encoded in the text
(pairs of conditions and actions/recommendations). Therefore, there is a need
to compute the exact scope of conditions over text sequences expressing the
required actions. We present a system developed for this task. We show that it
yields good performance when applied to the analysis of French practice
guidelines.

11.   This paper proposes a neuro-rough model based on multi-layered perceptron and
rough set. The neuro-rough model is then tested on modelling the risk of HIV
from demographic data. The model is formulated using Bayesian framework and
trained using Monte Carlo method and Metropolis criterion. When the model was
tested to estimate the risk of HIV infection given the demographic data it was
found to give the accuracy of 62%. The proposed model is able to combine the
accuracy of the Bayesian MLP model and the transparency of Bayesian rough set
model.

12.   This paper presents bushing condition monitoring frameworks that use
multi-layer perceptrons (MLP), radial basis functions (RBF) and support vector
machines (SVM) classifiers. The first level of the framework determines if the
bushing is faulty or not while the second level determines the type of fault.
The diagnostic gases in the bushings are analyzed using the dissolve gas
analysis. MLP gives superior performance in terms of accuracy and training time
than SVM and RBF. In addition, an on-line bushing condition monitoring
approach, which is able to adapt to newly acquired data are introduced. This
approach is able to accommodate new classes that are introduced by incoming
data and is implemented using an incremental learning algorithm that uses MLP.
The testing results improved from 67.5% to 95.8% as new data were introduced
and the testing results improved from 60% to 95.3% as new conditions were
introduced. On average the confidence value of the framework on its decision
was 0.92.

13.   Noise, corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust,
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise.

14.   This paper introduces a continuous model for Multi-cellular Developmental
Design. The cells are fixed on a 2D grid and exchange "chemicals" with their
neighbors during the growth process. The quantity of chemicals that a cell
produces, as well as the differentiation value of the cell in the phenotype,
are controlled by a Neural Network (the genotype) that takes as inputs the
chemicals produced by the neighboring cells at the previous time step. In the
proposed model, the number of iterations of the growth process is not
pre-determined, but emerges during evolution: only organisms for which the
growth process stabilizes give a phenotype (the stable state), others are
declared nonviable. The optimization of the controller is done using the NEAT
algorithm, that optimizes both the topology and the weights of the Neural
Networks. Though each cell only receives local information from its neighbors,
the experimental results of the proposed approach on the 'flags' problems (the
phenotype must match a given 2D pattern) are almost as good as those of a
direct regression approach using the same model with global information.
Moreover, the resulting multi-cellular organisms exhibit almost perfect
self-healing characteristics.

15.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

16.   When Kurt Goedel layed the foundations of theoretical computer science in
1931, he also introduced essential concepts of the theory of Artificial
Intelligence (AI). Although much of subsequent AI research has focused on
heuristics, which still play a major role in many practical AI applications, in
the new millennium AI theory has finally become a full-fledged formal science,
with important optimality results for embodied agents living in unknown
environments, obtained through a combination of theory a la Goedel and
probability theory. Here we look back at important milestones of AI history,
mention essential recent results, and speculate about what we may expect from
the next 25 years, emphasizing the significance of the ongoing dramatic
hardware speedups, and discussing Goedel-inspired, self-referential,
self-improving universal problem solvers.

17.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

18.   Water plays a pivotal role in many physical processes, and most importantly
in sustaining human life, animal life and plant life. Water supply entities
therefore have the responsibility to supply clean and safe water at the rate
required by the consumer. It is therefore necessary to implement mechanisms and
systems that can be employed to predict both short-term and long-term water
demands. The increasingly growing field of computational intelligence
techniques has been proposed as an efficient tool in the modelling of dynamic
phenomena. The primary objective of this paper is to compare the efficiency of
two computational intelligence techniques in water demand forecasting. The
techniques under comparison are the Artificial Neural Networks (ANNs) and the
Support Vector Machines (SVMs). In this study it was observed that the ANNs
perform better than the SVMs. This performance is measured against the
generalisation ability of the two.

19.   An ensemble based approach for dealing with missing data, without predicting
or imputing the missing values is proposed. This technique is suitable for
online operations of neural networks and as a result, is used for online
condition monitoring. The proposed technique is tested in both classification
and regression problems. An ensemble of Fuzzy-ARTMAPs is used for
classification whereas an ensemble of multi-layer perceptrons is used for the
regression problem. Results obtained using this ensemble-based technique are
compared to those obtained using a combination of auto-associative neural
networks and genetic algorithms and findings show that this method can perform
up to 9% better in regression problems. Another advantage of the proposed
technique is that it eliminates the need for finding the best estimate of the
data, and hence, saves time.

20.   The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.

21.   Gaussian mixture models (GMM) and support vector machines (SVM) are
introduced to classify faults in a population of cylindrical shells. The
proposed procedures are tested on a population of 20 cylindrical shells and
their performance is compared to the procedure, which uses multi-layer
perceptrons (MLP). The modal properties extracted from vibration data are used
to train the GMM, SVM and MLP. It is observed that the GMM produces 98%, SVM
produces 94% classification accuracy while the MLP produces 88% classification
rates.

22.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

23.   Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.

24.   Representing and reasoning about qualitative temporal information is an
essential part of many artificial intelligence tasks. Lots of models have been
proposed in the litterature for representing such temporal information. All
derive from a point-based or an interval-based framework. One fundamental
reasoning task that arises in applications of these frameworks is given by the
following scheme: given possibly indefinite and incomplete knowledge of the
binary relationships between some temporal objects, find the consistent
scenarii between all these objects. All these models require transitive tables
-- or similarly inference rules-- for solving such tasks. We have defined an
alternative model, S-languages - to represent qualitative temporal information,
based on the only two relations of \emph{precedence} and \emph{simultaneity}.
In this paper, we show how this model enables to avoid transitive tables or
inference rules to handle this kind of problem.

25.   We consider the problem of finding an n-agent joint-policy for the optimal
finite-horizon control of a decentralized Pomdp (Dec-Pomdp). This is a problem
of very high complexity (NEXP-hard in n >= 2). In this paper, we propose a new
mathematical programming approach for the problem. Our approach is based on two
ideas: First, we represent each agent's policy in the sequence-form and not in
the tree-form, thereby obtaining a very compact representation of the set of
joint-policies. Second, using this compact representation, we solve this
problem as an instance of combinatorial optimization for which we formulate a
mixed integer linear program (MILP). The optimal solution of the MILP directly
yields an optimal joint-policy for the Dec-Pomdp. Computational experience
shows that formulating and solving the MILP requires significantly less time to
solve benchmark Dec-Pomdp problems than existing algorithms. For example, the
multi-agent tiger problem for horizon 4 is solved in 72 secs with the MILP
whereas existing algorithms require several hours to solve it.

26.   The act of bluffing confounds game designers to this day. The very nature of
bluffing is even open for debate, adding further complication to the process of
creating intelligent virtual players that can bluff, and hence play,
realistically. Through the use of intelligent, learning agents, and carefully
designed agent outlooks, an agent can in fact learn to predict its opponents
reactions based not only on its own cards, but on the actions of those around
it. With this wider scope of understanding, an agent can in learn to bluff its
opponents, with the action representing not an illogical action, as bluffing is
often viewed, but rather as an act of maximising returns through an effective
statistical optimisation. By using a tee dee lambda learning algorithm to
continuously adapt neural network agent intelligence, agents have been shown to
be able to learn to bluff without outside prompting, and even to learn to call
each others bluffs in free, competitive play.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of fault classification methods"
"has a topic of protein family detection"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of soft constraints and optimization"\n- "has a topic of clustering algorithms and techniques"\n- "has a topic of quantum computation and artificial intelligence"\n- "has a topic of evolutionary learning and ensemble methods"\n- "has a topic of conflict prediction and machine learning"\n- "has a topic of semantic web and natural language processing"\n- "has a topic of fault diagnosis and monitoring"\n- "has a topic of automated leaf recognition and classification"\n- "has a topic of Bayesian methods in risk estimation"\n- "has a topic of structural response prediction in earthquakes"\n- "has a topic of water demand forecasting using computational intelligence"\n- "has a topic of missing data handling in neural networks"\n- "has a topic of symbolic controllers in robotics"\n- "has a topic of qualitative temporal reasoning in AI"\n- "has a topic of decentralized control in multi-agent systems"\n- "has a topic of bluffing strategies in game design"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   The semiring-based constraint satisfaction problems (semiring CSPs), proposed
by Bistarelli, Montanari and Rossi \cite{BMR97}, is a very general framework of
soft constraints. In this paper we propose an abstraction scheme for soft
constraints that uses semiring homomorphism. To find optimal solutions of the
concrete problem, the idea is, first working in the abstract problem and
finding its optimal solutions, then using them to solve the concrete problem.
  In particular, we show that a mapping preserves optimal solutions if and only
if it is an order-reflecting semiring homomorphism. Moreover, for a semiring
homomorphism $\alpha$ and a problem $P$ over $S$, if $t$ is optimal in
$\alpha(P)$, then there is an optimal solution $\bar{t}$ of $P$ such that
$\bar{t}$ has the same value as $t$ in $\alpha(P)$.

1.   Water plays a pivotal role in many physical processes, and most importantly
in sustaining human life, animal life and plant life. Water supply entities
therefore have the responsibility to supply clean and safe water at the rate
required by the consumer. It is therefore necessary to implement mechanisms and
systems that can be employed to predict both short-term and long-term water
demands. The increasingly growing field of computational intelligence
techniques has been proposed as an efficient tool in the modelling of dynamic
phenomena. The primary objective of this paper is to compare the efficiency of
two computational intelligence techniques in water demand forecasting. The
techniques under comparison are the Artificial Neural Networks (ANNs) and the
Support Vector Machines (SVMs). In this study it was observed that the ANNs
perform better than the SVMs. This performance is measured against the
generalisation ability of the two.

2.   Noise, corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust,
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise.

3.   Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.

4.   The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.

5.   This paper presents a fault classification method which makes use of a
Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the
vibration signals of cylindrical shells. The calculation of Pseudomodal
Energies, for the purposes of condition monitoring, has previously been found
to be an accurate method of extracting features from vibration signals. This
calculation is therefore used to extract features from vibration signals
obtained from a diverse population of cylindrical shells. Some of the cylinders
in the population have faults in different substructures. The pseudomodal
energies calculated from the vibration signals are then used as inputs to a
neuro-fuzzy model. A leave-one-out cross-validation process is used to test the
performance of the model. It is found that the neuro-fuzzy model is able to
classify faults with an accuracy of 91.62%, which is higher than the previously
used multilayer perceptron.

6.   Evolutionary Learning proceeds by evolving a population of classifiers, from
which it generally returns (with some notable exceptions) the single
best-of-run classifier as final result. In the meanwhile, Ensemble Learning,
one of the most efficient approaches in supervised Machine Learning for the
last decade, proceeds by building a population of diverse classifiers. Ensemble
Learning with Evolutionary Computation thus receives increasing attention. The
Evolutionary Ensemble Learning (EEL) approach presented in this paper features
two contributions. First, a new fitness function, inspired by co-evolution and
enforcing the classifier diversity, is presented. Further, a new selection
criterion based on the classification margin is proposed. This criterion is
used to extract the classifier ensemble from the final population only
(Off-line) or incrementally along evolution (On-line). Experiments on a set of
benchmark problems show that Off-line outperforms single-hypothesis
evolutionary learning and state-of-art Boosting and generates smaller
classifier ensembles.

7.   This paper proposes an approach to training rough set models using Bayesian
framework trained using Markov Chain Monte Carlo (MCMC) method. The prior
probabilities are constructed from the prior knowledge that good rough set
models have fewer rules. Markov Chain Monte Carlo sampling is conducted through
sampling in the rough set granule space and Metropolis algorithm is used as an
acceptance criteria. The proposed method is tested to estimate the risk of HIV
given demographic data. The results obtained shows that the proposed approach
is able to achieve an average accuracy of 58% with the accuracy varying up to
66%. In addition the Bayesian rough set give the probabilities of the estimated
HIV status as well as the linguistic rules describing how the demographic
parameters drive the risk of HIV.

8.   Semantic network research has seen a resurgence from its early history in the
cognitive sciences with the inception of the Semantic Web initiative. The
Semantic Web effort has brought forth an array of technologies that support the
encoding, storage, and querying of the semantic network data structure at the
world stage. Currently, the popular conception of the Semantic Web is that of a
data modeling medium where real and conceptual entities are related in
semantically meaningful ways. However, new models have emerged that explicitly
encode procedural information within the semantic network substrate. With these
new technologies, the Semantic Web has evolved from a data modeling medium to a
computational medium. This article provides a classification of existing
computational modeling efforts and the requirements of supporting technologies
that will aid in the further growth of this burgeoning domain.

9.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

10.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

11.   In this paper, we employ Probabilistic Neural Network (PNN) with image and
data processing techniques to implement a general purpose automated leaf
recognition algorithm. 12 leaf features are extracted and orthogonalized into 5
principal variables which consist the input vector of the PNN. The PNN is
trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater
than 90%. Compared with other approaches, our algorithm is an accurate
artificial intelligence approach which is fast in execution and easy in
implementation.

12.   This paper presents bushing condition monitoring frameworks that use
multi-layer perceptrons (MLP), radial basis functions (RBF) and support vector
machines (SVM) classifiers. The first level of the framework determines if the
bushing is faulty or not while the second level determines the type of fault.
The diagnostic gases in the bushings are analyzed using the dissolve gas
analysis. MLP gives superior performance in terms of accuracy and training time
than SVM and RBF. In addition, an on-line bushing condition monitoring
approach, which is able to adapt to newly acquired data are introduced. This
approach is able to accommodate new classes that are introduced by incoming
data and is implemented using an incremental learning algorithm that uses MLP.
The testing results improved from 67.5% to 95.8% as new data were introduced
and the testing results improved from 60% to 95.3% as new conditions were
introduced. On average the confidence value of the framework on its decision
was 0.92.

13.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

14.   This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.

15.   We consider the problem of finding an n-agent joint-policy for the optimal
finite-horizon control of a decentralized Pomdp (Dec-Pomdp). This is a problem
of very high complexity (NEXP-hard in n >= 2). In this paper, we propose a new
mathematical programming approach for the problem. Our approach is based on two
ideas: First, we represent each agent's policy in the sequence-form and not in
the tree-form, thereby obtaining a very compact representation of the set of
joint-policies. Second, using this compact representation, we solve this
problem as an instance of combinatorial optimization for which we formulate a
mixed integer linear program (MILP). The optimal solution of the MILP directly
yields an optimal joint-policy for the Dec-Pomdp. Computational experience
shows that formulating and solving the MILP requires significantly less time to
solve benchmark Dec-Pomdp problems than existing algorithms. For example, the
multi-agent tiger problem for horizon 4 is solved in 72 secs with the MILP
whereas existing algorithms require several hours to solve it.

16.   This paper proposes a neuro-rough model based on multi-layered perceptron and
rough set. The neuro-rough model is then tested on modelling the risk of HIV
from demographic data. The model is formulated using Bayesian framework and
trained using Monte Carlo method and Metropolis criterion. When the model was
tested to estimate the risk of HIV infection given the demographic data it was
found to give the accuracy of 62%. The proposed model is able to combine the
accuracy of the Bayesian MLP model and the transparency of Bayesian rough set
model.

17.   An ensemble based approach for dealing with missing data, without predicting
or imputing the missing values is proposed. This technique is suitable for
online operations of neural networks and as a result, is used for online
condition monitoring. The proposed technique is tested in both classification
and regression problems. An ensemble of Fuzzy-ARTMAPs is used for
classification whereas an ensemble of multi-layer perceptrons is used for the
regression problem. Results obtained using this ensemble-based technique are
compared to those obtained using a combination of auto-associative neural
networks and genetic algorithms and findings show that this method can perform
up to 9% better in regression problems. Another advantage of the proposed
technique is that it eliminates the need for finding the best estimate of the
data, and hence, saves time.

18.   Gaussian mixture models (GMM) and support vector machines (SVM) are
introduced to classify faults in a population of cylindrical shells. The
proposed procedures are tested on a population of 20 cylindrical shells and
their performance is compared to the procedure, which uses multi-layer
perceptrons (MLP). The modal properties extracted from vibration data are used
to train the GMM, SVM and MLP. It is observed that the GMM produces 98%, SVM
produces 94% classification accuracy while the MLP produces 88% classification
rates.

19.   This paper describes a system capable of semi-automatically filling an XML
template from free texts in the clinical domain (practice guidelines). The XML
template includes semantic information not explicitly encoded in the text
(pairs of conditions and actions/recommendations). Therefore, there is a need
to compute the exact scope of conditions over text sequences expressing the
required actions. We present a system developed for this task. We show that it
yields good performance when applied to the analysis of French practice
guidelines.

20.   The act of bluffing confounds game designers to this day. The very nature of
bluffing is even open for debate, adding further complication to the process of
creating intelligent virtual players that can bluff, and hence play,
realistically. Through the use of intelligent, learning agents, and carefully
designed agent outlooks, an agent can in fact learn to predict its opponents
reactions based not only on its own cards, but on the actions of those around
it. With this wider scope of understanding, an agent can in learn to bluff its
opponents, with the action representing not an illogical action, as bluffing is
often viewed, but rather as an act of maximising returns through an effective
statistical optimisation. By using a tee dee lambda learning algorithm to
continuously adapt neural network agent intelligence, agents have been shown to
be able to learn to bluff without outside prompting, and even to learn to call
each others bluffs in free, competitive play.

21.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

22.   This paper introduces a continuous model for Multi-cellular Developmental
Design. The cells are fixed on a 2D grid and exchange "chemicals" with their
neighbors during the growth process. The quantity of chemicals that a cell
produces, as well as the differentiation value of the cell in the phenotype,
are controlled by a Neural Network (the genotype) that takes as inputs the
chemicals produced by the neighboring cells at the previous time step. In the
proposed model, the number of iterations of the growth process is not
pre-determined, but emerges during evolution: only organisms for which the
growth process stabilizes give a phenotype (the stable state), others are
declared nonviable. The optimization of the controller is done using the NEAT
algorithm, that optimizes both the topology and the weights of the Neural
Networks. Though each cell only receives local information from its neighbors,
the experimental results of the proposed approach on the 'flags' problems (the
phenotype must match a given 2D pattern) are almost as good as those of a
direct regression approach using the same model with global information.
Moreover, the resulting multi-cellular organisms exhibit almost perfect
self-healing characteristics.

23.   Cluster matching by permuting cluster labels is important in many clustering
contexts such as cluster validation and cluster ensemble techniques. The
classic approach is to minimize the euclidean distance between two cluster
solutions which induces inappropriate stability in certain settings. Therefore,
we present the truematch algorithm that introduces two improvements best
explained in the crisp case. First, instead of maximizing the trace of the
cluster crosstable, we propose to maximize a chi-square transformation of this
crosstable. Thus, the trace will not be dominated by the cells with the largest
counts but by the cells with the most non-random observations, taking into
account the marginals. Second, we suggest a probabilistic component in order to
break ties and to make the matching algorithm truly random on random data. The
truematch algorithm is designed as a building block of the truecluster
framework and scales in polynomial time. First simulation results confirm that
the truematch algorithm gives more consistent truecluster results for unequal
cluster sizes. Free R software is available.

24.   When Kurt Goedel layed the foundations of theoretical computer science in
1931, he also introduced essential concepts of the theory of Artificial
Intelligence (AI). Although much of subsequent AI research has focused on
heuristics, which still play a major role in many practical AI applications, in
the new millennium AI theory has finally become a full-fledged formal science,
with important optimality results for embodied agents living in unknown
environments, obtained through a combination of theory a la Goedel and
probability theory. Here we look back at important milestones of AI history,
mention essential recent results, and speculate about what we may expect from
the next 25 years, emphasizing the significance of the ongoing dramatic
hardware speedups, and discussing Goedel-inspired, self-referential,
self-improving universal problem solvers.

25.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.

26.   Representing and reasoning about qualitative temporal information is an
essential part of many artificial intelligence tasks. Lots of models have been
proposed in the litterature for representing such temporal information. All
derive from a point-based or an interval-based framework. One fundamental
reasoning task that arises in applications of these frameworks is given by the
following scheme: given possibly indefinite and incomplete knowledge of the
binary relationships between some temporal objects, find the consistent
scenarii between all these objects. All these models require transitive tables
-- or similarly inference rules-- for solving such tasks. We have defined an
alternative model, S-languages - to represent qualitative temporal information,
based on the only two relations of \emph{precedence} and \emph{simultaneity}.
In this paper, we show how this model enables to avoid transitive tables or
inference rules to handle this kind of problem.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"- "has a topic of evolutionary robotics"
"has a topic of condition monitoring frameworks"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of computational intelligence techniques"\n- "has a topic of neural network classifiers"\n- "has a topic of semantic web technologies"\n- "has a topic of fault classification methods"\n- "has a topic of ensemble learning approaches"\n- "has a topic of Bayesian frameworks in modeling"\n- "has a topic of multi-agent systems"\n- "has a topic of qualitative temporal reasoning"\n- "has a topic of artificial intelligence history"\n- "has a topic of water demand forecasting"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   Semantic network research has seen a resurgence from its early history in the
cognitive sciences with the inception of the Semantic Web initiative. The
Semantic Web effort has brought forth an array of technologies that support the
encoding, storage, and querying of the semantic network data structure at the
world stage. Currently, the popular conception of the Semantic Web is that of a
data modeling medium where real and conceptual entities are related in
semantically meaningful ways. However, new models have emerged that explicitly
encode procedural information within the semantic network substrate. With these
new technologies, the Semantic Web has evolved from a data modeling medium to a
computational medium. This article provides a classification of existing
computational modeling efforts and the requirements of supporting technologies
that will aid in the further growth of this burgeoning domain.

1.   This paper proposes a neuro-rough model based on multi-layered perceptron and
rough set. The neuro-rough model is then tested on modelling the risk of HIV
from demographic data. The model is formulated using Bayesian framework and
trained using Monte Carlo method and Metropolis criterion. When the model was
tested to estimate the risk of HIV infection given the demographic data it was
found to give the accuracy of 62%. The proposed model is able to combine the
accuracy of the Bayesian MLP model and the transparency of Bayesian rough set
model.

2.   Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.

3.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.

4.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

5.   This paper describes a system capable of semi-automatically filling an XML
template from free texts in the clinical domain (practice guidelines). The XML
template includes semantic information not explicitly encoded in the text
(pairs of conditions and actions/recommendations). Therefore, there is a need
to compute the exact scope of conditions over text sequences expressing the
required actions. We present a system developed for this task. We show that it
yields good performance when applied to the analysis of French practice
guidelines.

6.   This paper presents bushing condition monitoring frameworks that use
multi-layer perceptrons (MLP), radial basis functions (RBF) and support vector
machines (SVM) classifiers. The first level of the framework determines if the
bushing is faulty or not while the second level determines the type of fault.
The diagnostic gases in the bushings are analyzed using the dissolve gas
analysis. MLP gives superior performance in terms of accuracy and training time
than SVM and RBF. In addition, an on-line bushing condition monitoring
approach, which is able to adapt to newly acquired data are introduced. This
approach is able to accommodate new classes that are introduced by incoming
data and is implemented using an incremental learning algorithm that uses MLP.
The testing results improved from 67.5% to 95.8% as new data were introduced
and the testing results improved from 60% to 95.3% as new conditions were
introduced. On average the confidence value of the framework on its decision
was 0.92.

7.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

8.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

9.   The semiring-based constraint satisfaction problems (semiring CSPs), proposed
by Bistarelli, Montanari and Rossi \cite{BMR97}, is a very general framework of
soft constraints. In this paper we propose an abstraction scheme for soft
constraints that uses semiring homomorphism. To find optimal solutions of the
concrete problem, the idea is, first working in the abstract problem and
finding its optimal solutions, then using them to solve the concrete problem.
  In particular, we show that a mapping preserves optimal solutions if and only
if it is an order-reflecting semiring homomorphism. Moreover, for a semiring
homomorphism $\alpha$ and a problem $P$ over $S$, if $t$ is optimal in
$\alpha(P)$, then there is an optimal solution $\bar{t}$ of $P$ such that
$\bar{t}$ has the same value as $t$ in $\alpha(P)$.

10.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

11.   Gaussian mixture models (GMM) and support vector machines (SVM) are
introduced to classify faults in a population of cylindrical shells. The
proposed procedures are tested on a population of 20 cylindrical shells and
their performance is compared to the procedure, which uses multi-layer
perceptrons (MLP). The modal properties extracted from vibration data are used
to train the GMM, SVM and MLP. It is observed that the GMM produces 98%, SVM
produces 94% classification accuracy while the MLP produces 88% classification
rates.

12.   Representing and reasoning about qualitative temporal information is an
essential part of many artificial intelligence tasks. Lots of models have been
proposed in the litterature for representing such temporal information. All
derive from a point-based or an interval-based framework. One fundamental
reasoning task that arises in applications of these frameworks is given by the
following scheme: given possibly indefinite and incomplete knowledge of the
binary relationships between some temporal objects, find the consistent
scenarii between all these objects. All these models require transitive tables
-- or similarly inference rules-- for solving such tasks. We have defined an
alternative model, S-languages - to represent qualitative temporal information,
based on the only two relations of \emph{precedence} and \emph{simultaneity}.
In this paper, we show how this model enables to avoid transitive tables or
inference rules to handle this kind of problem.

13.   This paper presents a fault classification method which makes use of a
Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the
vibration signals of cylindrical shells. The calculation of Pseudomodal
Energies, for the purposes of condition monitoring, has previously been found
to be an accurate method of extracting features from vibration signals. This
calculation is therefore used to extract features from vibration signals
obtained from a diverse population of cylindrical shells. Some of the cylinders
in the population have faults in different substructures. The pseudomodal
energies calculated from the vibration signals are then used as inputs to a
neuro-fuzzy model. A leave-one-out cross-validation process is used to test the
performance of the model. It is found that the neuro-fuzzy model is able to
classify faults with an accuracy of 91.62%, which is higher than the previously
used multilayer perceptron.

14.   This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.

15.   Noise, corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust,
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise.

16.   The act of bluffing confounds game designers to this day. The very nature of
bluffing is even open for debate, adding further complication to the process of
creating intelligent virtual players that can bluff, and hence play,
realistically. Through the use of intelligent, learning agents, and carefully
designed agent outlooks, an agent can in fact learn to predict its opponents
reactions based not only on its own cards, but on the actions of those around
it. With this wider scope of understanding, an agent can in learn to bluff its
opponents, with the action representing not an illogical action, as bluffing is
often viewed, but rather as an act of maximising returns through an effective
statistical optimisation. By using a tee dee lambda learning algorithm to
continuously adapt neural network agent intelligence, agents have been shown to
be able to learn to bluff without outside prompting, and even to learn to call
each others bluffs in free, competitive play.

17.   In this paper, we employ Probabilistic Neural Network (PNN) with image and
data processing techniques to implement a general purpose automated leaf
recognition algorithm. 12 leaf features are extracted and orthogonalized into 5
principal variables which consist the input vector of the PNN. The PNN is
trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater
than 90%. Compared with other approaches, our algorithm is an accurate
artificial intelligence approach which is fast in execution and easy in
implementation.

18.   Cluster matching by permuting cluster labels is important in many clustering
contexts such as cluster validation and cluster ensemble techniques. The
classic approach is to minimize the euclidean distance between two cluster
solutions which induces inappropriate stability in certain settings. Therefore,
we present the truematch algorithm that introduces two improvements best
explained in the crisp case. First, instead of maximizing the trace of the
cluster crosstable, we propose to maximize a chi-square transformation of this
crosstable. Thus, the trace will not be dominated by the cells with the largest
counts but by the cells with the most non-random observations, taking into
account the marginals. Second, we suggest a probabilistic component in order to
break ties and to make the matching algorithm truly random on random data. The
truematch algorithm is designed as a building block of the truecluster
framework and scales in polynomial time. First simulation results confirm that
the truematch algorithm gives more consistent truecluster results for unequal
cluster sizes. Free R software is available.

19.   We consider the problem of finding an n-agent joint-policy for the optimal
finite-horizon control of a decentralized Pomdp (Dec-Pomdp). This is a problem
of very high complexity (NEXP-hard in n >= 2). In this paper, we propose a new
mathematical programming approach for the problem. Our approach is based on two
ideas: First, we represent each agent's policy in the sequence-form and not in
the tree-form, thereby obtaining a very compact representation of the set of
joint-policies. Second, using this compact representation, we solve this
problem as an instance of combinatorial optimization for which we formulate a
mixed integer linear program (MILP). The optimal solution of the MILP directly
yields an optimal joint-policy for the Dec-Pomdp. Computational experience
shows that formulating and solving the MILP requires significantly less time to
solve benchmark Dec-Pomdp problems than existing algorithms. For example, the
multi-agent tiger problem for horizon 4 is solved in 72 secs with the MILP
whereas existing algorithms require several hours to solve it.

20.   When Kurt Goedel layed the foundations of theoretical computer science in
1931, he also introduced essential concepts of the theory of Artificial
Intelligence (AI). Although much of subsequent AI research has focused on
heuristics, which still play a major role in many practical AI applications, in
the new millennium AI theory has finally become a full-fledged formal science,
with important optimality results for embodied agents living in unknown
environments, obtained through a combination of theory a la Goedel and
probability theory. Here we look back at important milestones of AI history,
mention essential recent results, and speculate about what we may expect from
the next 25 years, emphasizing the significance of the ongoing dramatic
hardware speedups, and discussing Goedel-inspired, self-referential,
self-improving universal problem solvers.

21.   This paper introduces a continuous model for Multi-cellular Developmental
Design. The cells are fixed on a 2D grid and exchange "chemicals" with their
neighbors during the growth process. The quantity of chemicals that a cell
produces, as well as the differentiation value of the cell in the phenotype,
are controlled by a Neural Network (the genotype) that takes as inputs the
chemicals produced by the neighboring cells at the previous time step. In the
proposed model, the number of iterations of the growth process is not
pre-determined, but emerges during evolution: only organisms for which the
growth process stabilizes give a phenotype (the stable state), others are
declared nonviable. The optimization of the controller is done using the NEAT
algorithm, that optimizes both the topology and the weights of the Neural
Networks. Though each cell only receives local information from its neighbors,
the experimental results of the proposed approach on the 'flags' problems (the
phenotype must match a given 2D pattern) are almost as good as those of a
direct regression approach using the same model with global information.
Moreover, the resulting multi-cellular organisms exhibit almost perfect
self-healing characteristics.

22.   Water plays a pivotal role in many physical processes, and most importantly
in sustaining human life, animal life and plant life. Water supply entities
therefore have the responsibility to supply clean and safe water at the rate
required by the consumer. It is therefore necessary to implement mechanisms and
systems that can be employed to predict both short-term and long-term water
demands. The increasingly growing field of computational intelligence
techniques has been proposed as an efficient tool in the modelling of dynamic
phenomena. The primary objective of this paper is to compare the efficiency of
two computational intelligence techniques in water demand forecasting. The
techniques under comparison are the Artificial Neural Networks (ANNs) and the
Support Vector Machines (SVMs). In this study it was observed that the ANNs
perform better than the SVMs. This performance is measured against the
generalisation ability of the two.

23.   The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.

24.   Evolutionary Learning proceeds by evolving a population of classifiers, from
which it generally returns (with some notable exceptions) the single
best-of-run classifier as final result. In the meanwhile, Ensemble Learning,
one of the most efficient approaches in supervised Machine Learning for the
last decade, proceeds by building a population of diverse classifiers. Ensemble
Learning with Evolutionary Computation thus receives increasing attention. The
Evolutionary Ensemble Learning (EEL) approach presented in this paper features
two contributions. First, a new fitness function, inspired by co-evolution and
enforcing the classifier diversity, is presented. Further, a new selection
criterion based on the classification margin is proposed. This criterion is
used to extract the classifier ensemble from the final population only
(Off-line) or incrementally along evolution (On-line). Experiments on a set of
benchmark problems show that Off-line outperforms single-hypothesis
evolutionary learning and state-of-art Boosting and generates smaller
classifier ensembles.

25.   An ensemble based approach for dealing with missing data, without predicting
or imputing the missing values is proposed. This technique is suitable for
online operations of neural networks and as a result, is used for online
condition monitoring. The proposed technique is tested in both classification
and regression problems. An ensemble of Fuzzy-ARTMAPs is used for
classification whereas an ensemble of multi-layer perceptrons is used for the
regression problem. Results obtained using this ensemble-based technique are
compared to those obtained using a combination of auto-associative neural
networks and genetic algorithms and findings show that this method can perform
up to 9% better in regression problems. Another advantage of the proposed
technique is that it eliminates the need for finding the best estimate of the
data, and hence, saves time.

26.   This paper proposes an approach to training rough set models using Bayesian
framework trained using Markov Chain Monte Carlo (MCMC) method. The prior
probabilities are constructed from the prior knowledge that good rough set
models have fewer rules. Markov Chain Monte Carlo sampling is conducted through
sampling in the rough set granule space and Metropolis algorithm is used as an
acceptance criteria. The proposed method is tested to estimate the risk of HIV
given demographic data. The results obtained shows that the proposed approach
is able to achieve an average accuracy of 58% with the accuracy varying up to
66%. In addition the Bayesian rough set give the probabilities of the estimated
HIV status as well as the linguistic rules describing how the demographic
parameters drive the risk of HIV.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of hiv risk modeling"
"has a topic of decentralized control systems"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of semantic web technologies"\n- "has a topic of artificial intelligence in healthcare"\n- "has a topic of fault classification methods"\n- "has a topic of evolutionary robotics"\n- "has a topic of water demand forecasting"\n- "has a topic of quantum computation applications"\n- "has a topic of neural network performance evaluation"\n- "has a topic of multi-agent systems"\n- "has a topic of computational intelligence techniques"\n- "has a topic of natural language processing in information retrieval"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   In this paper, we employ Probabilistic Neural Network (PNN) with image and
data processing techniques to implement a general purpose automated leaf
recognition algorithm. 12 leaf features are extracted and orthogonalized into 5
principal variables which consist the input vector of the PNN. The PNN is
trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater
than 90%. Compared with other approaches, our algorithm is an accurate
artificial intelligence approach which is fast in execution and easy in
implementation.

1.   Cluster matching by permuting cluster labels is important in many clustering
contexts such as cluster validation and cluster ensemble techniques. The
classic approach is to minimize the euclidean distance between two cluster
solutions which induces inappropriate stability in certain settings. Therefore,
we present the truematch algorithm that introduces two improvements best
explained in the crisp case. First, instead of maximizing the trace of the
cluster crosstable, we propose to maximize a chi-square transformation of this
crosstable. Thus, the trace will not be dominated by the cells with the largest
counts but by the cells with the most non-random observations, taking into
account the marginals. Second, we suggest a probabilistic component in order to
break ties and to make the matching algorithm truly random on random data. The
truematch algorithm is designed as a building block of the truecluster
framework and scales in polynomial time. First simulation results confirm that
the truematch algorithm gives more consistent truecluster results for unequal
cluster sizes. Free R software is available.

2.   This paper proposes an approach to training rough set models using Bayesian
framework trained using Markov Chain Monte Carlo (MCMC) method. The prior
probabilities are constructed from the prior knowledge that good rough set
models have fewer rules. Markov Chain Monte Carlo sampling is conducted through
sampling in the rough set granule space and Metropolis algorithm is used as an
acceptance criteria. The proposed method is tested to estimate the risk of HIV
given demographic data. The results obtained shows that the proposed approach
is able to achieve an average accuracy of 58% with the accuracy varying up to
66%. In addition the Bayesian rough set give the probabilities of the estimated
HIV status as well as the linguistic rules describing how the demographic
parameters drive the risk of HIV.

3.   This paper presents a fault classification method which makes use of a
Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the
vibration signals of cylindrical shells. The calculation of Pseudomodal
Energies, for the purposes of condition monitoring, has previously been found
to be an accurate method of extracting features from vibration signals. This
calculation is therefore used to extract features from vibration signals
obtained from a diverse population of cylindrical shells. Some of the cylinders
in the population have faults in different substructures. The pseudomodal
energies calculated from the vibration signals are then used as inputs to a
neuro-fuzzy model. A leave-one-out cross-validation process is used to test the
performance of the model. It is found that the neuro-fuzzy model is able to
classify faults with an accuracy of 91.62%, which is higher than the previously
used multilayer perceptron.

4.   This paper proposes a neuro-rough model based on multi-layered perceptron and
rough set. The neuro-rough model is then tested on modelling the risk of HIV
from demographic data. The model is formulated using Bayesian framework and
trained using Monte Carlo method and Metropolis criterion. When the model was
tested to estimate the risk of HIV infection given the demographic data it was
found to give the accuracy of 62%. The proposed model is able to combine the
accuracy of the Bayesian MLP model and the transparency of Bayesian rough set
model.

5.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

6.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

7.   Evolutionary Learning proceeds by evolving a population of classifiers, from
which it generally returns (with some notable exceptions) the single
best-of-run classifier as final result. In the meanwhile, Ensemble Learning,
one of the most efficient approaches in supervised Machine Learning for the
last decade, proceeds by building a population of diverse classifiers. Ensemble
Learning with Evolutionary Computation thus receives increasing attention. The
Evolutionary Ensemble Learning (EEL) approach presented in this paper features
two contributions. First, a new fitness function, inspired by co-evolution and
enforcing the classifier diversity, is presented. Further, a new selection
criterion based on the classification margin is proposed. This criterion is
used to extract the classifier ensemble from the final population only
(Off-line) or incrementally along evolution (On-line). Experiments on a set of
benchmark problems show that Off-line outperforms single-hypothesis
evolutionary learning and state-of-art Boosting and generates smaller
classifier ensembles.

8.   When Kurt Goedel layed the foundations of theoretical computer science in
1931, he also introduced essential concepts of the theory of Artificial
Intelligence (AI). Although much of subsequent AI research has focused on
heuristics, which still play a major role in many practical AI applications, in
the new millennium AI theory has finally become a full-fledged formal science,
with important optimality results for embodied agents living in unknown
environments, obtained through a combination of theory a la Goedel and
probability theory. Here we look back at important milestones of AI history,
mention essential recent results, and speculate about what we may expect from
the next 25 years, emphasizing the significance of the ongoing dramatic
hardware speedups, and discussing Goedel-inspired, self-referential,
self-improving universal problem solvers.

9.   Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.

10.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.

11.   An ensemble based approach for dealing with missing data, without predicting
or imputing the missing values is proposed. This technique is suitable for
online operations of neural networks and as a result, is used for online
condition monitoring. The proposed technique is tested in both classification
and regression problems. An ensemble of Fuzzy-ARTMAPs is used for
classification whereas an ensemble of multi-layer perceptrons is used for the
regression problem. Results obtained using this ensemble-based technique are
compared to those obtained using a combination of auto-associative neural
networks and genetic algorithms and findings show that this method can perform
up to 9% better in regression problems. Another advantage of the proposed
technique is that it eliminates the need for finding the best estimate of the
data, and hence, saves time.

12.   The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.

13.   This paper describes a system capable of semi-automatically filling an XML
template from free texts in the clinical domain (practice guidelines). The XML
template includes semantic information not explicitly encoded in the text
(pairs of conditions and actions/recommendations). Therefore, there is a need
to compute the exact scope of conditions over text sequences expressing the
required actions. We present a system developed for this task. We show that it
yields good performance when applied to the analysis of French practice
guidelines.

14.   Semantic network research has seen a resurgence from its early history in the
cognitive sciences with the inception of the Semantic Web initiative. The
Semantic Web effort has brought forth an array of technologies that support the
encoding, storage, and querying of the semantic network data structure at the
world stage. Currently, the popular conception of the Semantic Web is that of a
data modeling medium where real and conceptual entities are related in
semantically meaningful ways. However, new models have emerged that explicitly
encode procedural information within the semantic network substrate. With these
new technologies, the Semantic Web has evolved from a data modeling medium to a
computational medium. This article provides a classification of existing
computational modeling efforts and the requirements of supporting technologies
that will aid in the further growth of this burgeoning domain.

15.   The semiring-based constraint satisfaction problems (semiring CSPs), proposed
by Bistarelli, Montanari and Rossi \cite{BMR97}, is a very general framework of
soft constraints. In this paper we propose an abstraction scheme for soft
constraints that uses semiring homomorphism. To find optimal solutions of the
concrete problem, the idea is, first working in the abstract problem and
finding its optimal solutions, then using them to solve the concrete problem.
  In particular, we show that a mapping preserves optimal solutions if and only
if it is an order-reflecting semiring homomorphism. Moreover, for a semiring
homomorphism $\alpha$ and a problem $P$ over $S$, if $t$ is optimal in
$\alpha(P)$, then there is an optimal solution $\bar{t}$ of $P$ such that
$\bar{t}$ has the same value as $t$ in $\alpha(P)$.

16.   Noise, corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust,
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise.

17.   The act of bluffing confounds game designers to this day. The very nature of
bluffing is even open for debate, adding further complication to the process of
creating intelligent virtual players that can bluff, and hence play,
realistically. Through the use of intelligent, learning agents, and carefully
designed agent outlooks, an agent can in fact learn to predict its opponents
reactions based not only on its own cards, but on the actions of those around
it. With this wider scope of understanding, an agent can in learn to bluff its
opponents, with the action representing not an illogical action, as bluffing is
often viewed, but rather as an act of maximising returns through an effective
statistical optimisation. By using a tee dee lambda learning algorithm to
continuously adapt neural network agent intelligence, agents have been shown to
be able to learn to bluff without outside prompting, and even to learn to call
each others bluffs in free, competitive play.

18.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

19.   Representing and reasoning about qualitative temporal information is an
essential part of many artificial intelligence tasks. Lots of models have been
proposed in the litterature for representing such temporal information. All
derive from a point-based or an interval-based framework. One fundamental
reasoning task that arises in applications of these frameworks is given by the
following scheme: given possibly indefinite and incomplete knowledge of the
binary relationships between some temporal objects, find the consistent
scenarii between all these objects. All these models require transitive tables
-- or similarly inference rules-- for solving such tasks. We have defined an
alternative model, S-languages - to represent qualitative temporal information,
based on the only two relations of \emph{precedence} and \emph{simultaneity}.
In this paper, we show how this model enables to avoid transitive tables or
inference rules to handle this kind of problem.

20.   This paper presents bushing condition monitoring frameworks that use
multi-layer perceptrons (MLP), radial basis functions (RBF) and support vector
machines (SVM) classifiers. The first level of the framework determines if the
bushing is faulty or not while the second level determines the type of fault.
The diagnostic gases in the bushings are analyzed using the dissolve gas
analysis. MLP gives superior performance in terms of accuracy and training time
than SVM and RBF. In addition, an on-line bushing condition monitoring
approach, which is able to adapt to newly acquired data are introduced. This
approach is able to accommodate new classes that are introduced by incoming
data and is implemented using an incremental learning algorithm that uses MLP.
The testing results improved from 67.5% to 95.8% as new data were introduced
and the testing results improved from 60% to 95.3% as new conditions were
introduced. On average the confidence value of the framework on its decision
was 0.92.

21.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

22.   Gaussian mixture models (GMM) and support vector machines (SVM) are
introduced to classify faults in a population of cylindrical shells. The
proposed procedures are tested on a population of 20 cylindrical shells and
their performance is compared to the procedure, which uses multi-layer
perceptrons (MLP). The modal properties extracted from vibration data are used
to train the GMM, SVM and MLP. It is observed that the GMM produces 98%, SVM
produces 94% classification accuracy while the MLP produces 88% classification
rates.

23.   We consider the problem of finding an n-agent joint-policy for the optimal
finite-horizon control of a decentralized Pomdp (Dec-Pomdp). This is a problem
of very high complexity (NEXP-hard in n >= 2). In this paper, we propose a new
mathematical programming approach for the problem. Our approach is based on two
ideas: First, we represent each agent's policy in the sequence-form and not in
the tree-form, thereby obtaining a very compact representation of the set of
joint-policies. Second, using this compact representation, we solve this
problem as an instance of combinatorial optimization for which we formulate a
mixed integer linear program (MILP). The optimal solution of the MILP directly
yields an optimal joint-policy for the Dec-Pomdp. Computational experience
shows that formulating and solving the MILP requires significantly less time to
solve benchmark Dec-Pomdp problems than existing algorithms. For example, the
multi-agent tiger problem for horizon 4 is solved in 72 secs with the MILP
whereas existing algorithms require several hours to solve it.

24.   This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.

25.   This paper introduces a continuous model for Multi-cellular Developmental
Design. The cells are fixed on a 2D grid and exchange "chemicals" with their
neighbors during the growth process. The quantity of chemicals that a cell
produces, as well as the differentiation value of the cell in the phenotype,
are controlled by a Neural Network (the genotype) that takes as inputs the
chemicals produced by the neighboring cells at the previous time step. In the
proposed model, the number of iterations of the growth process is not
pre-determined, but emerges during evolution: only organisms for which the
growth process stabilizes give a phenotype (the stable state), others are
declared nonviable. The optimization of the controller is done using the NEAT
algorithm, that optimizes both the topology and the weights of the Neural
Networks. Though each cell only receives local information from its neighbors,
the experimental results of the proposed approach on the 'flags' problems (the
phenotype must match a given 2D pattern) are almost as good as those of a
direct regression approach using the same model with global information.
Moreover, the resulting multi-cellular organisms exhibit almost perfect
self-healing characteristics.

26.   Water plays a pivotal role in many physical processes, and most importantly
in sustaining human life, animal life and plant life. Water supply entities
therefore have the responsibility to supply clean and safe water at the rate
required by the consumer. It is therefore necessary to implement mechanisms and
systems that can be employed to predict both short-term and long-term water
demands. The increasingly growing field of computational intelligence
techniques has been proposed as an efficient tool in the modelling of dynamic
phenomena. The primary objective of this paper is to compare the efficiency of
two computational intelligence techniques in water demand forecasting. The
techniques under comparison are the Artificial Neural Networks (ANNs) and the
Support Vector Machines (SVMs). In this study it was observed that the ANNs
perform better than the SVMs. This performance is measured against the
generalisation ability of the two.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of multi-cellular developmental design"
"has a topic of fault classification methods"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of automated leaf recognition algorithms"\n- "has a topic of clustering techniques and algorithms"\n- "has a topic of Bayesian frameworks in machine learning"\n- "has a topic of fault diagnosis and classification"\n- "has a topic of evolutionary learning and ensemble methods"\n- "has a topic of semantic web and natural language processing"\n- "has a topic of conflict prediction using machine learning"\n- "has a topic of handling missing data in neural networks"\n- "has a topic of symbolic controllers in robotics"\n- "has a topic of XML template filling in clinical guidelines"\n- "has a topic of qualitative temporal reasoning"\n- "has a topic of condition monitoring in engineering"\n- "has a topic of earthquake response prediction using neural networks"\n- "has a topic of quantum computation and artificial intelligence"\n- "has a topic of water demand forecasting using computational intelligence"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   An ensemble based approach for dealing with missing data, without predicting
or imputing the missing values is proposed. This technique is suitable for
online operations of neural networks and as a result, is used for online
condition monitoring. The proposed technique is tested in both classification
and regression problems. An ensemble of Fuzzy-ARTMAPs is used for
classification whereas an ensemble of multi-layer perceptrons is used for the
regression problem. Results obtained using this ensemble-based technique are
compared to those obtained using a combination of auto-associative neural
networks and genetic algorithms and findings show that this method can perform
up to 9% better in regression problems. Another advantage of the proposed
technique is that it eliminates the need for finding the best estimate of the
data, and hence, saves time.

1.   Evolutionary Learning proceeds by evolving a population of classifiers, from
which it generally returns (with some notable exceptions) the single
best-of-run classifier as final result. In the meanwhile, Ensemble Learning,
one of the most efficient approaches in supervised Machine Learning for the
last decade, proceeds by building a population of diverse classifiers. Ensemble
Learning with Evolutionary Computation thus receives increasing attention. The
Evolutionary Ensemble Learning (EEL) approach presented in this paper features
two contributions. First, a new fitness function, inspired by co-evolution and
enforcing the classifier diversity, is presented. Further, a new selection
criterion based on the classification margin is proposed. This criterion is
used to extract the classifier ensemble from the final population only
(Off-line) or incrementally along evolution (On-line). Experiments on a set of
benchmark problems show that Off-line outperforms single-hypothesis
evolutionary learning and state-of-art Boosting and generates smaller
classifier ensembles.

2.   The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.

3.   This paper describes a system capable of semi-automatically filling an XML
template from free texts in the clinical domain (practice guidelines). The XML
template includes semantic information not explicitly encoded in the text
(pairs of conditions and actions/recommendations). Therefore, there is a need
to compute the exact scope of conditions over text sequences expressing the
required actions. We present a system developed for this task. We show that it
yields good performance when applied to the analysis of French practice
guidelines.

4.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

5.   This paper proposes an approach to training rough set models using Bayesian
framework trained using Markov Chain Monte Carlo (MCMC) method. The prior
probabilities are constructed from the prior knowledge that good rough set
models have fewer rules. Markov Chain Monte Carlo sampling is conducted through
sampling in the rough set granule space and Metropolis algorithm is used as an
acceptance criteria. The proposed method is tested to estimate the risk of HIV
given demographic data. The results obtained shows that the proposed approach
is able to achieve an average accuracy of 58% with the accuracy varying up to
66%. In addition the Bayesian rough set give the probabilities of the estimated
HIV status as well as the linguistic rules describing how the demographic
parameters drive the risk of HIV.

6.   This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.

7.   Noise, corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust,
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise.

8.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.

9.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

10.   This paper presents bushing condition monitoring frameworks that use
multi-layer perceptrons (MLP), radial basis functions (RBF) and support vector
machines (SVM) classifiers. The first level of the framework determines if the
bushing is faulty or not while the second level determines the type of fault.
The diagnostic gases in the bushings are analyzed using the dissolve gas
analysis. MLP gives superior performance in terms of accuracy and training time
than SVM and RBF. In addition, an on-line bushing condition monitoring
approach, which is able to adapt to newly acquired data are introduced. This
approach is able to accommodate new classes that are introduced by incoming
data and is implemented using an incremental learning algorithm that uses MLP.
The testing results improved from 67.5% to 95.8% as new data were introduced
and the testing results improved from 60% to 95.3% as new conditions were
introduced. On average the confidence value of the framework on its decision
was 0.92.

11.   The act of bluffing confounds game designers to this day. The very nature of
bluffing is even open for debate, adding further complication to the process of
creating intelligent virtual players that can bluff, and hence play,
realistically. Through the use of intelligent, learning agents, and carefully
designed agent outlooks, an agent can in fact learn to predict its opponents
reactions based not only on its own cards, but on the actions of those around
it. With this wider scope of understanding, an agent can in learn to bluff its
opponents, with the action representing not an illogical action, as bluffing is
often viewed, but rather as an act of maximising returns through an effective
statistical optimisation. By using a tee dee lambda learning algorithm to
continuously adapt neural network agent intelligence, agents have been shown to
be able to learn to bluff without outside prompting, and even to learn to call
each others bluffs in free, competitive play.

12.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

13.   Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.

14.   This paper presents a fault classification method which makes use of a
Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the
vibration signals of cylindrical shells. The calculation of Pseudomodal
Energies, for the purposes of condition monitoring, has previously been found
to be an accurate method of extracting features from vibration signals. This
calculation is therefore used to extract features from vibration signals
obtained from a diverse population of cylindrical shells. Some of the cylinders
in the population have faults in different substructures. The pseudomodal
energies calculated from the vibration signals are then used as inputs to a
neuro-fuzzy model. A leave-one-out cross-validation process is used to test the
performance of the model. It is found that the neuro-fuzzy model is able to
classify faults with an accuracy of 91.62%, which is higher than the previously
used multilayer perceptron.

15.   We consider the problem of finding an n-agent joint-policy for the optimal
finite-horizon control of a decentralized Pomdp (Dec-Pomdp). This is a problem
of very high complexity (NEXP-hard in n >= 2). In this paper, we propose a new
mathematical programming approach for the problem. Our approach is based on two
ideas: First, we represent each agent's policy in the sequence-form and not in
the tree-form, thereby obtaining a very compact representation of the set of
joint-policies. Second, using this compact representation, we solve this
problem as an instance of combinatorial optimization for which we formulate a
mixed integer linear program (MILP). The optimal solution of the MILP directly
yields an optimal joint-policy for the Dec-Pomdp. Computational experience
shows that formulating and solving the MILP requires significantly less time to
solve benchmark Dec-Pomdp problems than existing algorithms. For example, the
multi-agent tiger problem for horizon 4 is solved in 72 secs with the MILP
whereas existing algorithms require several hours to solve it.

16.   This paper introduces a continuous model for Multi-cellular Developmental
Design. The cells are fixed on a 2D grid and exchange "chemicals" with their
neighbors during the growth process. The quantity of chemicals that a cell
produces, as well as the differentiation value of the cell in the phenotype,
are controlled by a Neural Network (the genotype) that takes as inputs the
chemicals produced by the neighboring cells at the previous time step. In the
proposed model, the number of iterations of the growth process is not
pre-determined, but emerges during evolution: only organisms for which the
growth process stabilizes give a phenotype (the stable state), others are
declared nonviable. The optimization of the controller is done using the NEAT
algorithm, that optimizes both the topology and the weights of the Neural
Networks. Though each cell only receives local information from its neighbors,
the experimental results of the proposed approach on the 'flags' problems (the
phenotype must match a given 2D pattern) are almost as good as those of a
direct regression approach using the same model with global information.
Moreover, the resulting multi-cellular organisms exhibit almost perfect
self-healing characteristics.

17.   Semantic network research has seen a resurgence from its early history in the
cognitive sciences with the inception of the Semantic Web initiative. The
Semantic Web effort has brought forth an array of technologies that support the
encoding, storage, and querying of the semantic network data structure at the
world stage. Currently, the popular conception of the Semantic Web is that of a
data modeling medium where real and conceptual entities are related in
semantically meaningful ways. However, new models have emerged that explicitly
encode procedural information within the semantic network substrate. With these
new technologies, the Semantic Web has evolved from a data modeling medium to a
computational medium. This article provides a classification of existing
computational modeling efforts and the requirements of supporting technologies
that will aid in the further growth of this burgeoning domain.

18.   When Kurt Goedel layed the foundations of theoretical computer science in
1931, he also introduced essential concepts of the theory of Artificial
Intelligence (AI). Although much of subsequent AI research has focused on
heuristics, which still play a major role in many practical AI applications, in
the new millennium AI theory has finally become a full-fledged formal science,
with important optimality results for embodied agents living in unknown
environments, obtained through a combination of theory a la Goedel and
probability theory. Here we look back at important milestones of AI history,
mention essential recent results, and speculate about what we may expect from
the next 25 years, emphasizing the significance of the ongoing dramatic
hardware speedups, and discussing Goedel-inspired, self-referential,
self-improving universal problem solvers.

19.   Cluster matching by permuting cluster labels is important in many clustering
contexts such as cluster validation and cluster ensemble techniques. The
classic approach is to minimize the euclidean distance between two cluster
solutions which induces inappropriate stability in certain settings. Therefore,
we present the truematch algorithm that introduces two improvements best
explained in the crisp case. First, instead of maximizing the trace of the
cluster crosstable, we propose to maximize a chi-square transformation of this
crosstable. Thus, the trace will not be dominated by the cells with the largest
counts but by the cells with the most non-random observations, taking into
account the marginals. Second, we suggest a probabilistic component in order to
break ties and to make the matching algorithm truly random on random data. The
truematch algorithm is designed as a building block of the truecluster
framework and scales in polynomial time. First simulation results confirm that
the truematch algorithm gives more consistent truecluster results for unequal
cluster sizes. Free R software is available.

20.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

21.   In this paper, we employ Probabilistic Neural Network (PNN) with image and
data processing techniques to implement a general purpose automated leaf
recognition algorithm. 12 leaf features are extracted and orthogonalized into 5
principal variables which consist the input vector of the PNN. The PNN is
trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater
than 90%. Compared with other approaches, our algorithm is an accurate
artificial intelligence approach which is fast in execution and easy in
implementation.

22.   The semiring-based constraint satisfaction problems (semiring CSPs), proposed
by Bistarelli, Montanari and Rossi \cite{BMR97}, is a very general framework of
soft constraints. In this paper we propose an abstraction scheme for soft
constraints that uses semiring homomorphism. To find optimal solutions of the
concrete problem, the idea is, first working in the abstract problem and
finding its optimal solutions, then using them to solve the concrete problem.
  In particular, we show that a mapping preserves optimal solutions if and only
if it is an order-reflecting semiring homomorphism. Moreover, for a semiring
homomorphism $\alpha$ and a problem $P$ over $S$, if $t$ is optimal in
$\alpha(P)$, then there is an optimal solution $\bar{t}$ of $P$ such that
$\bar{t}$ has the same value as $t$ in $\alpha(P)$.

23.   This paper proposes a neuro-rough model based on multi-layered perceptron and
rough set. The neuro-rough model is then tested on modelling the risk of HIV
from demographic data. The model is formulated using Bayesian framework and
trained using Monte Carlo method and Metropolis criterion. When the model was
tested to estimate the risk of HIV infection given the demographic data it was
found to give the accuracy of 62%. The proposed model is able to combine the
accuracy of the Bayesian MLP model and the transparency of Bayesian rough set
model.

24.   Representing and reasoning about qualitative temporal information is an
essential part of many artificial intelligence tasks. Lots of models have been
proposed in the litterature for representing such temporal information. All
derive from a point-based or an interval-based framework. One fundamental
reasoning task that arises in applications of these frameworks is given by the
following scheme: given possibly indefinite and incomplete knowledge of the
binary relationships between some temporal objects, find the consistent
scenarii between all these objects. All these models require transitive tables
-- or similarly inference rules-- for solving such tasks. We have defined an
alternative model, S-languages - to represent qualitative temporal information,
based on the only two relations of \emph{precedence} and \emph{simultaneity}.
In this paper, we show how this model enables to avoid transitive tables or
inference rules to handle this kind of problem.

25.   Gaussian mixture models (GMM) and support vector machines (SVM) are
introduced to classify faults in a population of cylindrical shells. The
proposed procedures are tested on a population of 20 cylindrical shells and
their performance is compared to the procedure, which uses multi-layer
perceptrons (MLP). The modal properties extracted from vibration data are used
to train the GMM, SVM and MLP. It is observed that the GMM produces 98%, SVM
produces 94% classification accuracy while the MLP produces 88% classification
rates.

26.   Water plays a pivotal role in many physical processes, and most importantly
in sustaining human life, animal life and plant life. Water supply entities
therefore have the responsibility to supply clean and safe water at the rate
required by the consumer. It is therefore necessary to implement mechanisms and
systems that can be employed to predict both short-term and long-term water
demands. The increasingly growing field of computational intelligence
techniques has been proposed as an efficient tool in the modelling of dynamic
phenomena. The primary objective of this paper is to compare the efficiency of
two computational intelligence techniques in water demand forecasting. The
techniques under comparison are the Artificial Neural Networks (ANNs) and the
Support Vector Machines (SVMs). In this study it was observed that the ANNs
perform better than the SVMs. This performance is measured against the
generalisation ability of the two.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of condition monitoring frameworks"
"- "has a topic of evolutionary robotics"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of ensemble learning techniques"\n- "has a topic of artificial intelligence applications"\n- "has a topic of fault classification methods"\n- "has a topic of water demand forecasting"\n- "has a topic of neural network models"\n- "has a topic of semantic web technologies"\n- "has a topic of qualitative temporal reasoning"\n- "has a topic of protein family detection"\n- "has a topic of game theory and bluffing"\n- "has a topic of computational intelligence techniques"']
50 descriptions proposed in total. Here are the descriptions:
- "has a topic of soft constraints and optimization
has a topic of conflict prediction using machine learning
- "has a topic of semantic web technologies
has a topic of qualitative temporal reasoning
has a topic of automated leaf recognition and classification
has a topic of Bayesian frameworks in modeling
has a topic of face recognition robustness
has a topic of Bayesian frameworks in machine learning
has a topic of multi-agent systems
- "has a topic of automated leaf recognition algorithms
has a topic of evolutionary robotics
has a topic of clustering techniques and algorithms
has a topic of quantum computation applications
has a topic of water demand forecasting
has a topic of quantum computation and artificial intelligence
has a topic of fault classification in cylindrical shells
has a topic of evolutionary learning and ensemble methods
has a topic of multi-cellular developmental design
has a topic of condition monitoring in engineering
- "has a topic of ensemble learning techniques
has a topic of water demand forecasting using computational intelligence
has a topic of HIV risk modeling
- "has a topic of computational intelligence techniques
has a topic of fault diagnosis and classification
has a topic of Bayesian methods in rough set models
has a topic of decentralized control in multi-agent systems
has a topic of protein family detection
has a topic of artificial intelligence in healthcare
has a topic of neural network classifiers
has a topic of neural network models
has a topic of semantic web technologies
- "has a topic of evolutionary robotics
has a topic of semantic web and natural language processing
has a topic of clustering algorithms and techniques
has a topic of ensemble learning approaches
has a topic of artificial intelligence history
has a topic of decentralized control systems
has a topic of qualitative temporal information representation
- "has a topic of evolutionary learning and ensemble methods
has a topic of automated leaf recognition algorithms
has a topic of handling missing data in neural networks
has a topic of conflict prediction and machine learning
- "has a topic of protein family detection
has a topic of condition monitoring frameworks
has a topic of semantic web and computational modeling
has a topic of neural network performance evaluation
has a topic of predicting militarized interstate disputes
has a topic of fault classification methods
has a topic of fault diagnosis and monitoring
has a topic of artificial intelligence applications
First prompt example:
You are tasked with classifying whether a TEXT satisfies a PROPERTY. You need to respond with Yes or No, and when uncertain, always respond with No.

<example>
<input>
Text: I like apples.
Property: is talking about fruits
</input>
<output>
Yes
</output>
</example>

Now, solve this new task:
Text:   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

Property: - "has a topic of soft constraints and optimization

Processing 50000 prompts with vLLM's automatic batching...
Stage Assign assign
Assigned all recall: 0.82
Assigned all precision: 0.23780487804878048
Assigned description: - "has a topic of soft constraints and optimization. recall: 0.213 precision: 0.11267605633802817
Assigned description: has a topic of conflict prediction using machine learning. recall: 0.013 precision: 0.0
Assigned description: - "has a topic of semantic web technologies. recall: 0.042 precision: 0.0
Assigned description: has a topic of qualitative temporal reasoning. recall: 0.062 precision: 0.04838709677419355
Assigned description: has a topic of automated leaf recognition and classification. recall: 0.001 precision: 0.0
Assigned description: has a topic of Bayesian frameworks in modeling. recall: 0.125 precision: 0.048
Assigned description: has a topic of face recognition robustness. recall: 0.001 precision: 0.0
Assigned description: has a topic of Bayesian frameworks in machine learning. recall: 0.104 precision: 0.0
Assigned description: has a topic of multi-agent systems. recall: 0.069 precision: 0.08695652173913043
Assigned description: - "has a topic of automated leaf recognition algorithms. recall: 0.001 precision: 0.0
Assigned description: has a topic of evolutionary robotics. recall: 0.013 precision: 0.0
Assigned description: has a topic of clustering techniques and algorithms. recall: 0.039 precision: 0.05128205128205128
Assigned description: has a topic of quantum computation applications. recall: 0.002 precision: 0.0
Assigned description: has a topic of water demand forecasting. recall: 0.001 precision: 0.0
Assigned description: has a topic of quantum computation and artificial intelligence. recall: 0.005 precision: 0.0
Assigned description: has a topic of fault classification in cylindrical shells. recall: 0.002 precision: 0.0
Assigned description: has a topic of evolutionary learning and ensemble methods. recall: 0.032 precision: 0.0
Assigned description: has a topic of multi-cellular developmental design. recall: 0.001 precision: 0.0
Assigned description: has a topic of condition monitoring in engineering. recall: 0.014 precision: 0.07142857142857142
Assigned description: - "has a topic of ensemble learning techniques. recall: 0.039 precision: 0.02564102564102564
Assigned description: has a topic of water demand forecasting using computational intelligence. recall: 0.001 precision: 0.0
Assigned description: has a topic of HIV risk modeling. recall: 0.002 precision: 0.0
Assigned description: - "has a topic of computational intelligence techniques. recall: 0.624 precision: 0.16025641025641027
Assigned description: has a topic of fault diagnosis and classification. recall: 0.024 precision: 0.0
Assigned description: has a topic of Bayesian methods in rough set models. recall: 0.012 precision: 0.0
Assigned description: has a topic of decentralized control in multi-agent systems. recall: 0.034 precision: 0.029411764705882353
Assigned description: has a topic of protein family detection. recall: 0.001 precision: 0.0
Assigned description: has a topic of artificial intelligence in healthcare. recall: 0.028 precision: 0.0
Assigned description: has a topic of neural network classifiers. recall: 0.031 precision: 0.0
Assigned description: has a topic of neural network models. recall: 0.046 precision: 0.021739130434782608
Assigned description: has a topic of semantic web technologies. recall: 0.046 precision: 0.0
Assigned description: - "has a topic of evolutionary robotics. recall: 0.013 precision: 0.0
Assigned description: has a topic of semantic web and natural language processing. recall: 0.066 precision: 0.015151515151515152
Assigned description: has a topic of clustering algorithms and techniques. recall: 0.034 precision: 0.0
Assigned description: has a topic of ensemble learning approaches. recall: 0.048 precision: 0.0
Assigned description: has a topic of artificial intelligence history. recall: 0.009 precision: 0.1111111111111111
Assigned description: has a topic of decentralized control systems. recall: 0.023 precision: 0.0
Assigned description: has a topic of qualitative temporal information representation. recall: 0.101 precision: 0.0594059405940594
Assigned description: - "has a topic of evolutionary learning and ensemble methods. recall: 0.028 precision: 0.0
Assigned description: has a topic of automated leaf recognition algorithms. recall: 0.001 precision: 0.0
Assigned description: has a topic of handling missing data in neural networks. recall: 0.002 precision: 0.5
Assigned description: has a topic of conflict prediction and machine learning. recall: 0.027 precision: 0.1111111111111111
Assigned description: - "has a topic of protein family detection. recall: 0.001 precision: 0.0
Assigned description: has a topic of condition monitoring frameworks. recall: 0.022 precision: 0.0
Assigned description: has a topic of semantic web and computational modeling. recall: 0.12 precision: 0.175
Assigned description: has a topic of neural network performance evaluation. recall: 0.018 precision: 0.0
Assigned description: has a topic of predicting militarized interstate disputes. recall: 0.001 precision: 0.0
Assigned description: has a topic of fault classification methods. recall: 0.014 precision: 0.0
Assigned description: has a topic of fault diagnosis and monitoring. recall: 0.029 precision: 0.0
Assigned description: has a topic of artificial intelligence applications. recall: 0.406 precision: 0.04187192118226601
Drop 22 because it cover 624 from 1000
Dropping 1 descriptions because they are too popular or too unpopular:
Stage select select
Unmatched count: 419
Description: has a topic of Bayesian frameworks in modeling. Count: 125
Description: has a topic of clustering techniques and algorithms. Count: 34
Description: - "has a topic of protein family detection. Count: 1
Description: has a topic of semantic web and computational modeling. Count: 111
Description: has a topic of predicting militarized interstate disputes. Count: 1
Description: has a topic of artificial intelligence applications. Count: 309
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   This paper introduces the concept of fitness cloud as an alternative way to
visualize and analyze search spaces than given by the geographic notion of
fitness landscape. It is argued that the fitness cloud concept overcomes
several deficiencies of the landscape representation. Our analysis is based on
the correlation between fitness of solutions and fitnesses of nearest solutions
according to some neighboring. We focus on the behavior of local search
heuristics, such as hill climber, on the well-known NK fitness landscape. In
both cases the fitness vs. fitness correlation is shown to be related to the
epistatic parameter K.

1.   This paper describes application of rough set theory, on the analysis of
hydrocyclone operation. In this manner, using Self Organizing Map (SOM) as
preprocessing step, best crisp granules of data are obtained. Then, using a
combining of SOM and rough set theory (RST)-called SORST-, the dominant rules
on the information table, obtained from laboratory tests, are extracted. Based
on these rules, an approximate estimation on decision attribute is fulfilled.
Finally, a brief comparison of this method with the SOM-NFIS system (briefly
SONFIS) is highlighted.

2.   When implementing a propagator for a constraint, one must decide about
variants: When implementing min, should one also implement max? Should one
implement linear equations both with and without coefficients? Constraint
variants are ubiquitous: implementing them requires considerable (if not
prohibitive) effort and decreases maintainability, but will deliver better
performance.
  This paper shows how to use variable views, previously introduced for an
implementation architecture, to derive perfect propagator variants. A model for
views and derived propagators is introduced. Derived propagators are proved to
be indeed perfect in that they inherit essential properties such as correctness
and domain and bounds consistency. Techniques for systematically deriving
propagators such as transformation, generalization, specialization, and
channeling are developed for several variable domains. We evaluate the massive
impact of derived propagators. Without derived propagators, Gecode would
require 140000 rather than 40000 lines of code for propagators.

3.   This paper presents a fault classification method which makes use of a
Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the
vibration signals of cylindrical shells. The calculation of Pseudomodal
Energies, for the purposes of condition monitoring, has previously been found
to be an accurate method of extracting features from vibration signals. This
calculation is therefore used to extract features from vibration signals
obtained from a diverse population of cylindrical shells. Some of the cylinders
in the population have faults in different substructures. The pseudomodal
energies calculated from the vibration signals are then used as inputs to a
neuro-fuzzy model. A leave-one-out cross-validation process is used to test the
performance of the model. It is found that the neuro-fuzzy model is able to
classify faults with an accuracy of 91.62%, which is higher than the previously
used multilayer perceptron.

4.   An empty spot refers to an empty hard-to-fill space which can be found in the
records of the social interaction, and is the clue to the persons in the
underlying social network who do not appear in the records. This contribution
addresses a problem to predict relevant empty spots in social interaction.
Homogeneous and inhomogeneous networks are studied as a model underlying the
social interaction. A heuristic predictor function approach is presented as a
new method to address the problem. Simulation experiment is demonstrated over a
homogeneous network. A test data in the form of baskets is generated from the
simulated communication. Precision to predict the empty spots is calculated to
demonstrate the performance of the presented approach.

5.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

6.   This paper has been withdrawn.

7.   This philosophical paper explores the relation between modern scientific
simulations and the future of the universe. We argue that a simulation of an
entire universe will result from future scientific activity. This requires us
to tackle the challenge of simulating open-ended evolution at all levels in a
single simulation. The simulation should encompass not only biological
evolution, but also physical evolution (a level below) and cultural evolution
(a level above). The simulation would allow us to probe what would happen if we
would "replay the tape of the universe" with the same or different laws and
initial conditions. We also distinguish between real-world and artificial-world
modelling. Assuming that intelligent life could indeed simulate an entire
universe, this leads to two tentative hypotheses. Some authors have argued that
we may already be in a simulation run by an intelligent entity. Or, if such a
simulation could be made real, this would lead to the production of a new
universe. This last direction is argued with a careful speculative
philosophical approach, emphasizing the imperative to find a solution to the
heat death problem in cosmology. The reader is invited to consult Annex 1 for
an overview of the logical structure of this paper. -- Keywords: far future,
future of science, ALife, simulation, realization, cosmology, heat death,
fine-tuning, physical eschatology, cosmological natural selection, cosmological
artificial selection, artificial cosmogenesis, selfish biocosm hypothesis,
meduso-anthropic principle, developmental singularity hypothesis, role of
intelligent life.

8.   Approximately more than 90% of all coal production in Iranian underground
mines is derived directly longwall mining method. Out of seam dilution is one
of the essential problems in these mines. Therefore the dilution can impose the
additional cost of mining and milling. As a result, recognition of the
effective parameters on the dilution has a remarkable role in industry. In this
way, this paper has analyzed the influence of 13 parameters (attributed
variables) versus the decision attribute (dilution value), so that using two
approximate reasoning methods, namely Rough Set Theory (RST) and Self
Organizing Neuro- Fuzzy Inference System (SONFIS) the best rules on our
collected data sets has been extracted. The other benefit of later methods is
to predict new unknown cases. So, the reduced sets (reducts) by RST have been
obtained. Therefore the emerged results by utilizing mentioned methods shows
that the high sensitive variables are thickness of layer, length of stope, rate
of advance, number of miners, type of advancing.

9.   Although the definition and measurement of intelligence is clearly of
fundamental importance to the field of artificial intelligence, no general
survey of definitions and tests of machine intelligence exists. Indeed few
researchers are even aware of alternatives to the Turing test and its many
derivatives. In this paper we fill this gap by providing a short survey of the
many tests of machine intelligence that have been proposed.

10.   This paper addresses a method to analyze the covert social network foundation
hidden behind the terrorism disaster. It is to solve a node discovery problem,
which means to discover a node, which functions relevantly in a social network,
but escaped from monitoring on the presence and mutual relationship of nodes.
The method aims at integrating the expert investigator's prior understanding,
insight on the terrorists' social network nature derived from the complex graph
theory, and computational data processing. The social network responsible for
the 9/11 attack in 2001 is used to execute simulation experiment to evaluate
the performance of the method.

11.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

12.   Methods to solve a node discovery problem for a social network are presented.
Covert nodes refer to the nodes which are not observable directly. They
transmit the influence and affect the resulting collaborative activities among
the persons in a social network, but do not appear in the surveillance logs
which record the participants of the collaborative activities. Discovering the
covert nodes is identifying the suspicious logs where the covert nodes would
appear if the covert nodes became overt. The performance of the methods is
demonstrated with a test dataset generated from computationally synthesized
networks and a real organization.

13.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

14.   In this paper, I present a method to solve a node discovery problem in a
networked organization. Covert nodes refer to the nodes which are not
observable directly. They affect social interactions, but do not appear in the
surveillance logs which record the participants of the social interactions.
Discovering the covert nodes is defined as identifying the suspicious logs
where the covert nodes would appear if the covert nodes became overt. A
mathematical model is developed for the maximal likelihood estimation of the
network behind the social interactions and for the identification of the
suspicious logs. Precision, recall, and F measure characteristics are
demonstrated with the dataset generated from a real organization and the
computationally synthesized datasets. The performance is close to the
theoretical limit for any covert nodes in the networks of any topologies and
sizes if the ratio of the number of observation to the number of possible
communication patterns is large.

15.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

16.   We consider hexagonal cellular automata with immediate cell neighbourhood and
three cell-states. Every cell calculates its next state depending on the
integral representation of states in its neighbourhood, i.e. how many
neighbours are in each one state. We employ evolutionary algorithms to breed
local transition functions that support mobile localizations (gliders), and
characterize sets of the functions selected in terms of quasi-chemical systems.
Analysis of the set of functions evolved allows to speculate that mobile
localizations are likely to emerge in the quasi-chemical systems with limited
diffusion of one reagent, a small number of molecules is required for
amplification of travelling localizations, and reactions leading to stationary
localizations involve relatively equal amount of quasi-chemical species.
Techniques developed can be applied in cascading signals in nature-inspired
spatially extended computing devices, and phenomenological studies and
classification of non-linear discrete systems.

17.   Most definitions of ontology, viewed as a "specification of a
conceptualization", agree on the fact that if an ontology can take different
forms, it necessarily includes a vocabulary of terms and some specification of
their meaning in relation to the domain's conceptualization. And as domain
knowledge is mainly conveyed through scientific and technical texts, we can
hope to extract some useful information from them for building ontology. But is
it as simple as this? In this article we shall see that the lexical structure,
i.e. the network of words linked by linguistic relationships, does not
necessarily match the domain conceptualization. We have to bear in mind that
writing documents is the concern of textual linguistics, of which one of the
principles is the incompleteness of text, whereas building ontology - viewed as
task-independent knowledge - is concerned with conceptualization based on
formal and not natural languages. Nevertheless, the famous Sapir and Whorf
hypothesis, concerning the interdependence of thought and language, is also
applicable to formal languages. This means that the way an ontology is built
and a concept is defined depends directly on the formal language which is used;
and the results will not be the same. The introduction of the notion of
ontoterminology allows to take into account epistemological principles for
formal ontology building.

18.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

19.   This paper has been withdrawn by the author due to extremely unscientific
errors.

20.   This theoretical work defines the measure of autocorrelation of evolvability
in the context of neutral fitness landscape. This measure has been studied on
the classical MAX-SAT problem. This work highlight a new characteristic of
neutral fitness landscapes which allows to design new adapted metaheuristic.

21.   The semiring-based constraint satisfaction problems (semiring CSPs), proposed
by Bistarelli, Montanari and Rossi \cite{BMR97}, is a very general framework of
soft constraints. In this paper we propose an abstraction scheme for soft
constraints that uses semiring homomorphism. To find optimal solutions of the
concrete problem, the idea is, first working in the abstract problem and
finding its optimal solutions, then using them to solve the concrete problem.
  In particular, we show that a mapping preserves optimal solutions if and only
if it is an order-reflecting semiring homomorphism. Moreover, for a semiring
homomorphism $\alpha$ and a problem $P$ over $S$, if $t$ is optimal in
$\alpha(P)$, then there is an optimal solution $\bar{t}$ of $P$ such that
$\bar{t}$ has the same value as $t$ in $\alpha(P)$.

22.   The investigation of the terrorist attack is a time-critical task. The
investigators have a limited time window to diagnose the organizational
background of the terrorists, to run down and arrest the wire-pullers, and to
take an action to prevent or eradicate the terrorist attack. The intuitive
interface to visualize the intelligence data set stimulates the investigators'
experience and knowledge, and aids them in decision-making for an immediately
effective action. This paper presents a computational method to analyze the
intelligence data set on the collective actions of the perpetrators of the
attack, and to visualize it into the form of a social network diagram which
predicts the positions where the wire-pullers conceals themselves.

23.   We describe decomposition during search (DDS), an integration of And/Or tree
search into propagation-based constraint solvers. The presented search
algorithm dynamically decomposes sub-problems of a constraint satisfaction
problem into independent partial problems, avoiding redundant work.
  The paper discusses how DDS interacts with key features that make
propagation-based solvers successful: constraint propagation, especially for
global constraints, and dynamic search heuristics.
  We have implemented DDS for the Gecode constraint programming library. Two
applications, solution counting in graph coloring and protein structure
prediction, exemplify the benefits of DDS in practice.

24.   A fundamental problem in artificial intelligence is that nobody really knows
what intelligence is. The problem is especially acute when we need to consider
artificial systems which are significantly different to humans. In this paper
we approach this problem in the following way: We take a number of well known
informal definitions of human intelligence that have been given by experts, and
extract their essential features. These are then mathematically formalised to
produce a general measure of intelligence for arbitrary machines. We believe
that this equation formally captures the concept of machine intelligence in the
broadest reasonable sense. We then show how this formal definition is related
to the theory of universal optimal learning agents. Finally, we survey the many
other tests and definitions of intelligence that have been proposed for
machines.

25.   A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.

26.   Knowledge-based economy forces companies in the nation to group together as a
cluster in order to maintain their competitiveness in the world market. The
cluster development relies on two key success factors which are knowledge
sharing and collaboration between the actors in the cluster. Thus, our study
tries to propose knowledge management system to support knowledge management
activities within the cluster. To achieve the objectives of this study,
ontology takes a very important role in knowledge management process in various
ways; such as building reusable and faster knowledge-bases, better way for
representing the knowledge explicitly. However, creating and representing
ontology create difficulties to organization due to the ambiguity and
unstructured of source of knowledge. Therefore, the objectives of this paper
are to propose the methodology to create and represent ontology for the
organization development by using knowledge engineering approach. The
handicraft cluster in Thailand is used as a case study to illustrate our
proposed methodology.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of terrorism and social network dynamics"
"has a topic of ontology and conceptualization in knowledge representation"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of fitness landscapes and optimization algorithms"\n- "has a topic of fuzzy logic and decision-making systems"\n- "has a topic of intelligence definitions and machine learning"\n- "has a topic of social network analysis and covert operations"\n- "has a topic of constraint satisfaction problems and algorithm design"\n- "has a topic of knowledge management and organizational development"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   The semiring-based constraint satisfaction problems (semiring CSPs), proposed
by Bistarelli, Montanari and Rossi \cite{BMR97}, is a very general framework of
soft constraints. In this paper we propose an abstraction scheme for soft
constraints that uses semiring homomorphism. To find optimal solutions of the
concrete problem, the idea is, first working in the abstract problem and
finding its optimal solutions, then using them to solve the concrete problem.
  In particular, we show that a mapping preserves optimal solutions if and only
if it is an order-reflecting semiring homomorphism. Moreover, for a semiring
homomorphism $\alpha$ and a problem $P$ over $S$, if $t$ is optimal in
$\alpha(P)$, then there is an optimal solution $\bar{t}$ of $P$ such that
$\bar{t}$ has the same value as $t$ in $\alpha(P)$.

1.   This paper presents a fault classification method which makes use of a
Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the
vibration signals of cylindrical shells. The calculation of Pseudomodal
Energies, for the purposes of condition monitoring, has previously been found
to be an accurate method of extracting features from vibration signals. This
calculation is therefore used to extract features from vibration signals
obtained from a diverse population of cylindrical shells. Some of the cylinders
in the population have faults in different substructures. The pseudomodal
energies calculated from the vibration signals are then used as inputs to a
neuro-fuzzy model. A leave-one-out cross-validation process is used to test the
performance of the model. It is found that the neuro-fuzzy model is able to
classify faults with an accuracy of 91.62%, which is higher than the previously
used multilayer perceptron.

2.   A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.

3.   A fundamental problem in artificial intelligence is that nobody really knows
what intelligence is. The problem is especially acute when we need to consider
artificial systems which are significantly different to humans. In this paper
we approach this problem in the following way: We take a number of well known
informal definitions of human intelligence that have been given by experts, and
extract their essential features. These are then mathematically formalised to
produce a general measure of intelligence for arbitrary machines. We believe
that this equation formally captures the concept of machine intelligence in the
broadest reasonable sense. We then show how this formal definition is related
to the theory of universal optimal learning agents. Finally, we survey the many
other tests and definitions of intelligence that have been proposed for
machines.

4.   This theoretical work defines the measure of autocorrelation of evolvability
in the context of neutral fitness landscape. This measure has been studied on
the classical MAX-SAT problem. This work highlight a new characteristic of
neutral fitness landscapes which allows to design new adapted metaheuristic.

5.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

6.   We consider hexagonal cellular automata with immediate cell neighbourhood and
three cell-states. Every cell calculates its next state depending on the
integral representation of states in its neighbourhood, i.e. how many
neighbours are in each one state. We employ evolutionary algorithms to breed
local transition functions that support mobile localizations (gliders), and
characterize sets of the functions selected in terms of quasi-chemical systems.
Analysis of the set of functions evolved allows to speculate that mobile
localizations are likely to emerge in the quasi-chemical systems with limited
diffusion of one reagent, a small number of molecules is required for
amplification of travelling localizations, and reactions leading to stationary
localizations involve relatively equal amount of quasi-chemical species.
Techniques developed can be applied in cascading signals in nature-inspired
spatially extended computing devices, and phenomenological studies and
classification of non-linear discrete systems.

7.   An empty spot refers to an empty hard-to-fill space which can be found in the
records of the social interaction, and is the clue to the persons in the
underlying social network who do not appear in the records. This contribution
addresses a problem to predict relevant empty spots in social interaction.
Homogeneous and inhomogeneous networks are studied as a model underlying the
social interaction. A heuristic predictor function approach is presented as a
new method to address the problem. Simulation experiment is demonstrated over a
homogeneous network. A test data in the form of baskets is generated from the
simulated communication. Precision to predict the empty spots is calculated to
demonstrate the performance of the presented approach.

8.   This paper addresses a method to analyze the covert social network foundation
hidden behind the terrorism disaster. It is to solve a node discovery problem,
which means to discover a node, which functions relevantly in a social network,
but escaped from monitoring on the presence and mutual relationship of nodes.
The method aims at integrating the expert investigator's prior understanding,
insight on the terrorists' social network nature derived from the complex graph
theory, and computational data processing. The social network responsible for
the 9/11 attack in 2001 is used to execute simulation experiment to evaluate
the performance of the method.

9.   This paper has been withdrawn by the author due to extremely unscientific
errors.

10.   The investigation of the terrorist attack is a time-critical task. The
investigators have a limited time window to diagnose the organizational
background of the terrorists, to run down and arrest the wire-pullers, and to
take an action to prevent or eradicate the terrorist attack. The intuitive
interface to visualize the intelligence data set stimulates the investigators'
experience and knowledge, and aids them in decision-making for an immediately
effective action. This paper presents a computational method to analyze the
intelligence data set on the collective actions of the perpetrators of the
attack, and to visualize it into the form of a social network diagram which
predicts the positions where the wire-pullers conceals themselves.

11.   Most definitions of ontology, viewed as a "specification of a
conceptualization", agree on the fact that if an ontology can take different
forms, it necessarily includes a vocabulary of terms and some specification of
their meaning in relation to the domain's conceptualization. And as domain
knowledge is mainly conveyed through scientific and technical texts, we can
hope to extract some useful information from them for building ontology. But is
it as simple as this? In this article we shall see that the lexical structure,
i.e. the network of words linked by linguistic relationships, does not
necessarily match the domain conceptualization. We have to bear in mind that
writing documents is the concern of textual linguistics, of which one of the
principles is the incompleteness of text, whereas building ontology - viewed as
task-independent knowledge - is concerned with conceptualization based on
formal and not natural languages. Nevertheless, the famous Sapir and Whorf
hypothesis, concerning the interdependence of thought and language, is also
applicable to formal languages. This means that the way an ontology is built
and a concept is defined depends directly on the formal language which is used;
and the results will not be the same. The introduction of the notion of
ontoterminology allows to take into account epistemological principles for
formal ontology building.

12.   We describe decomposition during search (DDS), an integration of And/Or tree
search into propagation-based constraint solvers. The presented search
algorithm dynamically decomposes sub-problems of a constraint satisfaction
problem into independent partial problems, avoiding redundant work.
  The paper discusses how DDS interacts with key features that make
propagation-based solvers successful: constraint propagation, especially for
global constraints, and dynamic search heuristics.
  We have implemented DDS for the Gecode constraint programming library. Two
applications, solution counting in graph coloring and protein structure
prediction, exemplify the benefits of DDS in practice.

13.   This paper describes application of rough set theory, on the analysis of
hydrocyclone operation. In this manner, using Self Organizing Map (SOM) as
preprocessing step, best crisp granules of data are obtained. Then, using a
combining of SOM and rough set theory (RST)-called SORST-, the dominant rules
on the information table, obtained from laboratory tests, are extracted. Based
on these rules, an approximate estimation on decision attribute is fulfilled.
Finally, a brief comparison of this method with the SOM-NFIS system (briefly
SONFIS) is highlighted.

14.   Although the definition and measurement of intelligence is clearly of
fundamental importance to the field of artificial intelligence, no general
survey of definitions and tests of machine intelligence exists. Indeed few
researchers are even aware of alternatives to the Turing test and its many
derivatives. In this paper we fill this gap by providing a short survey of the
many tests of machine intelligence that have been proposed.

15.   Knowledge-based economy forces companies in the nation to group together as a
cluster in order to maintain their competitiveness in the world market. The
cluster development relies on two key success factors which are knowledge
sharing and collaboration between the actors in the cluster. Thus, our study
tries to propose knowledge management system to support knowledge management
activities within the cluster. To achieve the objectives of this study,
ontology takes a very important role in knowledge management process in various
ways; such as building reusable and faster knowledge-bases, better way for
representing the knowledge explicitly. However, creating and representing
ontology create difficulties to organization due to the ambiguity and
unstructured of source of knowledge. Therefore, the objectives of this paper
are to propose the methodology to create and represent ontology for the
organization development by using knowledge engineering approach. The
handicraft cluster in Thailand is used as a case study to illustrate our
proposed methodology.

16.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

17.   In this paper, I present a method to solve a node discovery problem in a
networked organization. Covert nodes refer to the nodes which are not
observable directly. They affect social interactions, but do not appear in the
surveillance logs which record the participants of the social interactions.
Discovering the covert nodes is defined as identifying the suspicious logs
where the covert nodes would appear if the covert nodes became overt. A
mathematical model is developed for the maximal likelihood estimation of the
network behind the social interactions and for the identification of the
suspicious logs. Precision, recall, and F measure characteristics are
demonstrated with the dataset generated from a real organization and the
computationally synthesized datasets. The performance is close to the
theoretical limit for any covert nodes in the networks of any topologies and
sizes if the ratio of the number of observation to the number of possible
communication patterns is large.

18.   This philosophical paper explores the relation between modern scientific
simulations and the future of the universe. We argue that a simulation of an
entire universe will result from future scientific activity. This requires us
to tackle the challenge of simulating open-ended evolution at all levels in a
single simulation. The simulation should encompass not only biological
evolution, but also physical evolution (a level below) and cultural evolution
(a level above). The simulation would allow us to probe what would happen if we
would "replay the tape of the universe" with the same or different laws and
initial conditions. We also distinguish between real-world and artificial-world
modelling. Assuming that intelligent life could indeed simulate an entire
universe, this leads to two tentative hypotheses. Some authors have argued that
we may already be in a simulation run by an intelligent entity. Or, if such a
simulation could be made real, this would lead to the production of a new
universe. This last direction is argued with a careful speculative
philosophical approach, emphasizing the imperative to find a solution to the
heat death problem in cosmology. The reader is invited to consult Annex 1 for
an overview of the logical structure of this paper. -- Keywords: far future,
future of science, ALife, simulation, realization, cosmology, heat death,
fine-tuning, physical eschatology, cosmological natural selection, cosmological
artificial selection, artificial cosmogenesis, selfish biocosm hypothesis,
meduso-anthropic principle, developmental singularity hypothesis, role of
intelligent life.

19.   Methods to solve a node discovery problem for a social network are presented.
Covert nodes refer to the nodes which are not observable directly. They
transmit the influence and affect the resulting collaborative activities among
the persons in a social network, but do not appear in the surveillance logs
which record the participants of the collaborative activities. Discovering the
covert nodes is identifying the suspicious logs where the covert nodes would
appear if the covert nodes became overt. The performance of the methods is
demonstrated with a test dataset generated from computationally synthesized
networks and a real organization.

20.   This paper introduces the concept of fitness cloud as an alternative way to
visualize and analyze search spaces than given by the geographic notion of
fitness landscape. It is argued that the fitness cloud concept overcomes
several deficiencies of the landscape representation. Our analysis is based on
the correlation between fitness of solutions and fitnesses of nearest solutions
according to some neighboring. We focus on the behavior of local search
heuristics, such as hill climber, on the well-known NK fitness landscape. In
both cases the fitness vs. fitness correlation is shown to be related to the
epistatic parameter K.

21.   When implementing a propagator for a constraint, one must decide about
variants: When implementing min, should one also implement max? Should one
implement linear equations both with and without coefficients? Constraint
variants are ubiquitous: implementing them requires considerable (if not
prohibitive) effort and decreases maintainability, but will deliver better
performance.
  This paper shows how to use variable views, previously introduced for an
implementation architecture, to derive perfect propagator variants. A model for
views and derived propagators is introduced. Derived propagators are proved to
be indeed perfect in that they inherit essential properties such as correctness
and domain and bounds consistency. Techniques for systematically deriving
propagators such as transformation, generalization, specialization, and
channeling are developed for several variable domains. We evaluate the massive
impact of derived propagators. Without derived propagators, Gecode would
require 140000 rather than 40000 lines of code for propagators.

22.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

23.   This paper has been withdrawn.

24.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

25.   Approximately more than 90% of all coal production in Iranian underground
mines is derived directly longwall mining method. Out of seam dilution is one
of the essential problems in these mines. Therefore the dilution can impose the
additional cost of mining and milling. As a result, recognition of the
effective parameters on the dilution has a remarkable role in industry. In this
way, this paper has analyzed the influence of 13 parameters (attributed
variables) versus the decision attribute (dilution value), so that using two
approximate reasoning methods, namely Rough Set Theory (RST) and Self
Organizing Neuro- Fuzzy Inference System (SONFIS) the best rules on our
collected data sets has been extracted. The other benefit of later methods is
to predict new unknown cases. So, the reduced sets (reducts) by RST have been
obtained. Therefore the emerged results by utilizing mentioned methods shows
that the high sensitive variables are thickness of layer, length of stope, rate
of advance, number of miners, type of advancing.

26.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"- "has a topic of machine intelligence definitions and tests"
"has a topic of evolutionary algorithms and cellular automata"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of soft constraints and optimization"\n- "has a topic of fault classification in engineering"\n- "has a topic of causality and default theories"\n- "has a topic of definitions and measures of intelligence"\n- "has a topic of fuzzy logic and stability assessment"\n- "has a topic of social network analysis and covert nodes"\n- "has a topic of knowledge management and ontology"\n- "has a topic of computational methods in terrorism analysis"\n- "has a topic of fitness landscapes and search spaces"\n- "has a topic of constraint satisfaction problem solving"\n- "has a topic of unconscious preferences and visualization methods"\n- "has a topic of mining methods and dilution analysis"\n- "has a topic of informal definitions of intelligence"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   Methods to solve a node discovery problem for a social network are presented.
Covert nodes refer to the nodes which are not observable directly. They
transmit the influence and affect the resulting collaborative activities among
the persons in a social network, but do not appear in the surveillance logs
which record the participants of the collaborative activities. Discovering the
covert nodes is identifying the suspicious logs where the covert nodes would
appear if the covert nodes became overt. The performance of the methods is
demonstrated with a test dataset generated from computationally synthesized
networks and a real organization.

1.   This paper describes application of rough set theory, on the analysis of
hydrocyclone operation. In this manner, using Self Organizing Map (SOM) as
preprocessing step, best crisp granules of data are obtained. Then, using a
combining of SOM and rough set theory (RST)-called SORST-, the dominant rules
on the information table, obtained from laboratory tests, are extracted. Based
on these rules, an approximate estimation on decision attribute is fulfilled.
Finally, a brief comparison of this method with the SOM-NFIS system (briefly
SONFIS) is highlighted.

2.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

3.   This paper introduces the concept of fitness cloud as an alternative way to
visualize and analyze search spaces than given by the geographic notion of
fitness landscape. It is argued that the fitness cloud concept overcomes
several deficiencies of the landscape representation. Our analysis is based on
the correlation between fitness of solutions and fitnesses of nearest solutions
according to some neighboring. We focus on the behavior of local search
heuristics, such as hill climber, on the well-known NK fitness landscape. In
both cases the fitness vs. fitness correlation is shown to be related to the
epistatic parameter K.

4.   A fundamental problem in artificial intelligence is that nobody really knows
what intelligence is. The problem is especially acute when we need to consider
artificial systems which are significantly different to humans. In this paper
we approach this problem in the following way: We take a number of well known
informal definitions of human intelligence that have been given by experts, and
extract their essential features. These are then mathematically formalised to
produce a general measure of intelligence for arbitrary machines. We believe
that this equation formally captures the concept of machine intelligence in the
broadest reasonable sense. We then show how this formal definition is related
to the theory of universal optimal learning agents. Finally, we survey the many
other tests and definitions of intelligence that have been proposed for
machines.

5.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

6.   This paper has been withdrawn.

7.   A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.

8.   This paper presents a fault classification method which makes use of a
Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the
vibration signals of cylindrical shells. The calculation of Pseudomodal
Energies, for the purposes of condition monitoring, has previously been found
to be an accurate method of extracting features from vibration signals. This
calculation is therefore used to extract features from vibration signals
obtained from a diverse population of cylindrical shells. Some of the cylinders
in the population have faults in different substructures. The pseudomodal
energies calculated from the vibration signals are then used as inputs to a
neuro-fuzzy model. A leave-one-out cross-validation process is used to test the
performance of the model. It is found that the neuro-fuzzy model is able to
classify faults with an accuracy of 91.62%, which is higher than the previously
used multilayer perceptron.

9.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

10.   Approximately more than 90% of all coal production in Iranian underground
mines is derived directly longwall mining method. Out of seam dilution is one
of the essential problems in these mines. Therefore the dilution can impose the
additional cost of mining and milling. As a result, recognition of the
effective parameters on the dilution has a remarkable role in industry. In this
way, this paper has analyzed the influence of 13 parameters (attributed
variables) versus the decision attribute (dilution value), so that using two
approximate reasoning methods, namely Rough Set Theory (RST) and Self
Organizing Neuro- Fuzzy Inference System (SONFIS) the best rules on our
collected data sets has been extracted. The other benefit of later methods is
to predict new unknown cases. So, the reduced sets (reducts) by RST have been
obtained. Therefore the emerged results by utilizing mentioned methods shows
that the high sensitive variables are thickness of layer, length of stope, rate
of advance, number of miners, type of advancing.

11.   An empty spot refers to an empty hard-to-fill space which can be found in the
records of the social interaction, and is the clue to the persons in the
underlying social network who do not appear in the records. This contribution
addresses a problem to predict relevant empty spots in social interaction.
Homogeneous and inhomogeneous networks are studied as a model underlying the
social interaction. A heuristic predictor function approach is presented as a
new method to address the problem. Simulation experiment is demonstrated over a
homogeneous network. A test data in the form of baskets is generated from the
simulated communication. Precision to predict the empty spots is calculated to
demonstrate the performance of the presented approach.

12.   In this paper, I present a method to solve a node discovery problem in a
networked organization. Covert nodes refer to the nodes which are not
observable directly. They affect social interactions, but do not appear in the
surveillance logs which record the participants of the social interactions.
Discovering the covert nodes is defined as identifying the suspicious logs
where the covert nodes would appear if the covert nodes became overt. A
mathematical model is developed for the maximal likelihood estimation of the
network behind the social interactions and for the identification of the
suspicious logs. Precision, recall, and F measure characteristics are
demonstrated with the dataset generated from a real organization and the
computationally synthesized datasets. The performance is close to the
theoretical limit for any covert nodes in the networks of any topologies and
sizes if the ratio of the number of observation to the number of possible
communication patterns is large.

13.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

14.   Most definitions of ontology, viewed as a "specification of a
conceptualization", agree on the fact that if an ontology can take different
forms, it necessarily includes a vocabulary of terms and some specification of
their meaning in relation to the domain's conceptualization. And as domain
knowledge is mainly conveyed through scientific and technical texts, we can
hope to extract some useful information from them for building ontology. But is
it as simple as this? In this article we shall see that the lexical structure,
i.e. the network of words linked by linguistic relationships, does not
necessarily match the domain conceptualization. We have to bear in mind that
writing documents is the concern of textual linguistics, of which one of the
principles is the incompleteness of text, whereas building ontology - viewed as
task-independent knowledge - is concerned with conceptualization based on
formal and not natural languages. Nevertheless, the famous Sapir and Whorf
hypothesis, concerning the interdependence of thought and language, is also
applicable to formal languages. This means that the way an ontology is built
and a concept is defined depends directly on the formal language which is used;
and the results will not be the same. The introduction of the notion of
ontoterminology allows to take into account epistemological principles for
formal ontology building.

15.   The investigation of the terrorist attack is a time-critical task. The
investigators have a limited time window to diagnose the organizational
background of the terrorists, to run down and arrest the wire-pullers, and to
take an action to prevent or eradicate the terrorist attack. The intuitive
interface to visualize the intelligence data set stimulates the investigators'
experience and knowledge, and aids them in decision-making for an immediately
effective action. This paper presents a computational method to analyze the
intelligence data set on the collective actions of the perpetrators of the
attack, and to visualize it into the form of a social network diagram which
predicts the positions where the wire-pullers conceals themselves.

16.   This paper has been withdrawn by the author due to extremely unscientific
errors.

17.   This philosophical paper explores the relation between modern scientific
simulations and the future of the universe. We argue that a simulation of an
entire universe will result from future scientific activity. This requires us
to tackle the challenge of simulating open-ended evolution at all levels in a
single simulation. The simulation should encompass not only biological
evolution, but also physical evolution (a level below) and cultural evolution
(a level above). The simulation would allow us to probe what would happen if we
would "replay the tape of the universe" with the same or different laws and
initial conditions. We also distinguish between real-world and artificial-world
modelling. Assuming that intelligent life could indeed simulate an entire
universe, this leads to two tentative hypotheses. Some authors have argued that
we may already be in a simulation run by an intelligent entity. Or, if such a
simulation could be made real, this would lead to the production of a new
universe. This last direction is argued with a careful speculative
philosophical approach, emphasizing the imperative to find a solution to the
heat death problem in cosmology. The reader is invited to consult Annex 1 for
an overview of the logical structure of this paper. -- Keywords: far future,
future of science, ALife, simulation, realization, cosmology, heat death,
fine-tuning, physical eschatology, cosmological natural selection, cosmological
artificial selection, artificial cosmogenesis, selfish biocosm hypothesis,
meduso-anthropic principle, developmental singularity hypothesis, role of
intelligent life.

18.   We consider hexagonal cellular automata with immediate cell neighbourhood and
three cell-states. Every cell calculates its next state depending on the
integral representation of states in its neighbourhood, i.e. how many
neighbours are in each one state. We employ evolutionary algorithms to breed
local transition functions that support mobile localizations (gliders), and
characterize sets of the functions selected in terms of quasi-chemical systems.
Analysis of the set of functions evolved allows to speculate that mobile
localizations are likely to emerge in the quasi-chemical systems with limited
diffusion of one reagent, a small number of molecules is required for
amplification of travelling localizations, and reactions leading to stationary
localizations involve relatively equal amount of quasi-chemical species.
Techniques developed can be applied in cascading signals in nature-inspired
spatially extended computing devices, and phenomenological studies and
classification of non-linear discrete systems.

19.   We describe decomposition during search (DDS), an integration of And/Or tree
search into propagation-based constraint solvers. The presented search
algorithm dynamically decomposes sub-problems of a constraint satisfaction
problem into independent partial problems, avoiding redundant work.
  The paper discusses how DDS interacts with key features that make
propagation-based solvers successful: constraint propagation, especially for
global constraints, and dynamic search heuristics.
  We have implemented DDS for the Gecode constraint programming library. Two
applications, solution counting in graph coloring and protein structure
prediction, exemplify the benefits of DDS in practice.

20.   Although the definition and measurement of intelligence is clearly of
fundamental importance to the field of artificial intelligence, no general
survey of definitions and tests of machine intelligence exists. Indeed few
researchers are even aware of alternatives to the Turing test and its many
derivatives. In this paper we fill this gap by providing a short survey of the
many tests of machine intelligence that have been proposed.

21.   This theoretical work defines the measure of autocorrelation of evolvability
in the context of neutral fitness landscape. This measure has been studied on
the classical MAX-SAT problem. This work highlight a new characteristic of
neutral fitness landscapes which allows to design new adapted metaheuristic.

22.   The semiring-based constraint satisfaction problems (semiring CSPs), proposed
by Bistarelli, Montanari and Rossi \cite{BMR97}, is a very general framework of
soft constraints. In this paper we propose an abstraction scheme for soft
constraints that uses semiring homomorphism. To find optimal solutions of the
concrete problem, the idea is, first working in the abstract problem and
finding its optimal solutions, then using them to solve the concrete problem.
  In particular, we show that a mapping preserves optimal solutions if and only
if it is an order-reflecting semiring homomorphism. Moreover, for a semiring
homomorphism $\alpha$ and a problem $P$ over $S$, if $t$ is optimal in
$\alpha(P)$, then there is an optimal solution $\bar{t}$ of $P$ such that
$\bar{t}$ has the same value as $t$ in $\alpha(P)$.

23.   Knowledge-based economy forces companies in the nation to group together as a
cluster in order to maintain their competitiveness in the world market. The
cluster development relies on two key success factors which are knowledge
sharing and collaboration between the actors in the cluster. Thus, our study
tries to propose knowledge management system to support knowledge management
activities within the cluster. To achieve the objectives of this study,
ontology takes a very important role in knowledge management process in various
ways; such as building reusable and faster knowledge-bases, better way for
representing the knowledge explicitly. However, creating and representing
ontology create difficulties to organization due to the ambiguity and
unstructured of source of knowledge. Therefore, the objectives of this paper
are to propose the methodology to create and represent ontology for the
organization development by using knowledge engineering approach. The
handicraft cluster in Thailand is used as a case study to illustrate our
proposed methodology.

24.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

25.   When implementing a propagator for a constraint, one must decide about
variants: When implementing min, should one also implement max? Should one
implement linear equations both with and without coefficients? Constraint
variants are ubiquitous: implementing them requires considerable (if not
prohibitive) effort and decreases maintainability, but will deliver better
performance.
  This paper shows how to use variable views, previously introduced for an
implementation architecture, to derive perfect propagator variants. A model for
views and derived propagators is introduced. Derived propagators are proved to
be indeed perfect in that they inherit essential properties such as correctness
and domain and bounds consistency. Techniques for systematically deriving
propagators such as transformation, generalization, specialization, and
channeling are developed for several variable domains. We evaluate the massive
impact of derived propagators. Without derived propagators, Gecode would
require 140000 rather than 40000 lines of code for propagators.

26.   This paper addresses a method to analyze the covert social network foundation
hidden behind the terrorism disaster. It is to solve a node discovery problem,
which means to discover a node, which functions relevantly in a social network,
but escaped from monitoring on the presence and mutual relationship of nodes.
The method aims at integrating the expert investigator's prior understanding,
insight on the terrorists' social network nature derived from the complex graph
theory, and computational data processing. The social network responsible for
the 9/11 attack in 2001 is used to execute simulation experiment to evaluate
the performance of the method.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of terrorism and social network dynamics"
"has a topic of social network analysis and covert nodes"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of covert node discovery in social networks"\n- "has a topic of artificial intelligence and intelligence measurement"\n- "has a topic of fuzzy logic and stability assessment in engineering"\n- "has a topic of knowledge management and ontology development"\n- "has a topic of computational methods in social network analysis"\n- "has a topic of constraint satisfaction problems and optimization techniques"\n- "has a topic of hydrocyclone operation analysis using rough set theory"\n- "has a topic of terrorism analysis and intelligence visualization"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   Methods to solve a node discovery problem for a social network are presented.
Covert nodes refer to the nodes which are not observable directly. They
transmit the influence and affect the resulting collaborative activities among
the persons in a social network, but do not appear in the surveillance logs
which record the participants of the collaborative activities. Discovering the
covert nodes is identifying the suspicious logs where the covert nodes would
appear if the covert nodes became overt. The performance of the methods is
demonstrated with a test dataset generated from computationally synthesized
networks and a real organization.

1.   We consider hexagonal cellular automata with immediate cell neighbourhood and
three cell-states. Every cell calculates its next state depending on the
integral representation of states in its neighbourhood, i.e. how many
neighbours are in each one state. We employ evolutionary algorithms to breed
local transition functions that support mobile localizations (gliders), and
characterize sets of the functions selected in terms of quasi-chemical systems.
Analysis of the set of functions evolved allows to speculate that mobile
localizations are likely to emerge in the quasi-chemical systems with limited
diffusion of one reagent, a small number of molecules is required for
amplification of travelling localizations, and reactions leading to stationary
localizations involve relatively equal amount of quasi-chemical species.
Techniques developed can be applied in cascading signals in nature-inspired
spatially extended computing devices, and phenomenological studies and
classification of non-linear discrete systems.

2.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

3.   Knowledge-based economy forces companies in the nation to group together as a
cluster in order to maintain their competitiveness in the world market. The
cluster development relies on two key success factors which are knowledge
sharing and collaboration between the actors in the cluster. Thus, our study
tries to propose knowledge management system to support knowledge management
activities within the cluster. To achieve the objectives of this study,
ontology takes a very important role in knowledge management process in various
ways; such as building reusable and faster knowledge-bases, better way for
representing the knowledge explicitly. However, creating and representing
ontology create difficulties to organization due to the ambiguity and
unstructured of source of knowledge. Therefore, the objectives of this paper
are to propose the methodology to create and represent ontology for the
organization development by using knowledge engineering approach. The
handicraft cluster in Thailand is used as a case study to illustrate our
proposed methodology.

4.   Although the definition and measurement of intelligence is clearly of
fundamental importance to the field of artificial intelligence, no general
survey of definitions and tests of machine intelligence exists. Indeed few
researchers are even aware of alternatives to the Turing test and its many
derivatives. In this paper we fill this gap by providing a short survey of the
many tests of machine intelligence that have been proposed.

5.   This paper has been withdrawn.

6.   This paper introduces the concept of fitness cloud as an alternative way to
visualize and analyze search spaces than given by the geographic notion of
fitness landscape. It is argued that the fitness cloud concept overcomes
several deficiencies of the landscape representation. Our analysis is based on
the correlation between fitness of solutions and fitnesses of nearest solutions
according to some neighboring. We focus on the behavior of local search
heuristics, such as hill climber, on the well-known NK fitness landscape. In
both cases the fitness vs. fitness correlation is shown to be related to the
epistatic parameter K.

7.   This paper presents a fault classification method which makes use of a
Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the
vibration signals of cylindrical shells. The calculation of Pseudomodal
Energies, for the purposes of condition monitoring, has previously been found
to be an accurate method of extracting features from vibration signals. This
calculation is therefore used to extract features from vibration signals
obtained from a diverse population of cylindrical shells. Some of the cylinders
in the population have faults in different substructures. The pseudomodal
energies calculated from the vibration signals are then used as inputs to a
neuro-fuzzy model. A leave-one-out cross-validation process is used to test the
performance of the model. It is found that the neuro-fuzzy model is able to
classify faults with an accuracy of 91.62%, which is higher than the previously
used multilayer perceptron.

8.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

9.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

10.   The investigation of the terrorist attack is a time-critical task. The
investigators have a limited time window to diagnose the organizational
background of the terrorists, to run down and arrest the wire-pullers, and to
take an action to prevent or eradicate the terrorist attack. The intuitive
interface to visualize the intelligence data set stimulates the investigators'
experience and knowledge, and aids them in decision-making for an immediately
effective action. This paper presents a computational method to analyze the
intelligence data set on the collective actions of the perpetrators of the
attack, and to visualize it into the form of a social network diagram which
predicts the positions where the wire-pullers conceals themselves.

11.   A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.

12.   In this paper, I present a method to solve a node discovery problem in a
networked organization. Covert nodes refer to the nodes which are not
observable directly. They affect social interactions, but do not appear in the
surveillance logs which record the participants of the social interactions.
Discovering the covert nodes is defined as identifying the suspicious logs
where the covert nodes would appear if the covert nodes became overt. A
mathematical model is developed for the maximal likelihood estimation of the
network behind the social interactions and for the identification of the
suspicious logs. Precision, recall, and F measure characteristics are
demonstrated with the dataset generated from a real organization and the
computationally synthesized datasets. The performance is close to the
theoretical limit for any covert nodes in the networks of any topologies and
sizes if the ratio of the number of observation to the number of possible
communication patterns is large.

13.   This philosophical paper explores the relation between modern scientific
simulations and the future of the universe. We argue that a simulation of an
entire universe will result from future scientific activity. This requires us
to tackle the challenge of simulating open-ended evolution at all levels in a
single simulation. The simulation should encompass not only biological
evolution, but also physical evolution (a level below) and cultural evolution
(a level above). The simulation would allow us to probe what would happen if we
would "replay the tape of the universe" with the same or different laws and
initial conditions. We also distinguish between real-world and artificial-world
modelling. Assuming that intelligent life could indeed simulate an entire
universe, this leads to two tentative hypotheses. Some authors have argued that
we may already be in a simulation run by an intelligent entity. Or, if such a
simulation could be made real, this would lead to the production of a new
universe. This last direction is argued with a careful speculative
philosophical approach, emphasizing the imperative to find a solution to the
heat death problem in cosmology. The reader is invited to consult Annex 1 for
an overview of the logical structure of this paper. -- Keywords: far future,
future of science, ALife, simulation, realization, cosmology, heat death,
fine-tuning, physical eschatology, cosmological natural selection, cosmological
artificial selection, artificial cosmogenesis, selfish biocosm hypothesis,
meduso-anthropic principle, developmental singularity hypothesis, role of
intelligent life.

14.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

15.   When implementing a propagator for a constraint, one must decide about
variants: When implementing min, should one also implement max? Should one
implement linear equations both with and without coefficients? Constraint
variants are ubiquitous: implementing them requires considerable (if not
prohibitive) effort and decreases maintainability, but will deliver better
performance.
  This paper shows how to use variable views, previously introduced for an
implementation architecture, to derive perfect propagator variants. A model for
views and derived propagators is introduced. Derived propagators are proved to
be indeed perfect in that they inherit essential properties such as correctness
and domain and bounds consistency. Techniques for systematically deriving
propagators such as transformation, generalization, specialization, and
channeling are developed for several variable domains. We evaluate the massive
impact of derived propagators. Without derived propagators, Gecode would
require 140000 rather than 40000 lines of code for propagators.

16.   An empty spot refers to an empty hard-to-fill space which can be found in the
records of the social interaction, and is the clue to the persons in the
underlying social network who do not appear in the records. This contribution
addresses a problem to predict relevant empty spots in social interaction.
Homogeneous and inhomogeneous networks are studied as a model underlying the
social interaction. A heuristic predictor function approach is presented as a
new method to address the problem. Simulation experiment is demonstrated over a
homogeneous network. A test data in the form of baskets is generated from the
simulated communication. Precision to predict the empty spots is calculated to
demonstrate the performance of the presented approach.

17.   We describe decomposition during search (DDS), an integration of And/Or tree
search into propagation-based constraint solvers. The presented search
algorithm dynamically decomposes sub-problems of a constraint satisfaction
problem into independent partial problems, avoiding redundant work.
  The paper discusses how DDS interacts with key features that make
propagation-based solvers successful: constraint propagation, especially for
global constraints, and dynamic search heuristics.
  We have implemented DDS for the Gecode constraint programming library. Two
applications, solution counting in graph coloring and protein structure
prediction, exemplify the benefits of DDS in practice.

18.   This theoretical work defines the measure of autocorrelation of evolvability
in the context of neutral fitness landscape. This measure has been studied on
the classical MAX-SAT problem. This work highlight a new characteristic of
neutral fitness landscapes which allows to design new adapted metaheuristic.

19.   Approximately more than 90% of all coal production in Iranian underground
mines is derived directly longwall mining method. Out of seam dilution is one
of the essential problems in these mines. Therefore the dilution can impose the
additional cost of mining and milling. As a result, recognition of the
effective parameters on the dilution has a remarkable role in industry. In this
way, this paper has analyzed the influence of 13 parameters (attributed
variables) versus the decision attribute (dilution value), so that using two
approximate reasoning methods, namely Rough Set Theory (RST) and Self
Organizing Neuro- Fuzzy Inference System (SONFIS) the best rules on our
collected data sets has been extracted. The other benefit of later methods is
to predict new unknown cases. So, the reduced sets (reducts) by RST have been
obtained. Therefore the emerged results by utilizing mentioned methods shows
that the high sensitive variables are thickness of layer, length of stope, rate
of advance, number of miners, type of advancing.

20.   This paper addresses a method to analyze the covert social network foundation
hidden behind the terrorism disaster. It is to solve a node discovery problem,
which means to discover a node, which functions relevantly in a social network,
but escaped from monitoring on the presence and mutual relationship of nodes.
The method aims at integrating the expert investigator's prior understanding,
insight on the terrorists' social network nature derived from the complex graph
theory, and computational data processing. The social network responsible for
the 9/11 attack in 2001 is used to execute simulation experiment to evaluate
the performance of the method.

21.   The semiring-based constraint satisfaction problems (semiring CSPs), proposed
by Bistarelli, Montanari and Rossi \cite{BMR97}, is a very general framework of
soft constraints. In this paper we propose an abstraction scheme for soft
constraints that uses semiring homomorphism. To find optimal solutions of the
concrete problem, the idea is, first working in the abstract problem and
finding its optimal solutions, then using them to solve the concrete problem.
  In particular, we show that a mapping preserves optimal solutions if and only
if it is an order-reflecting semiring homomorphism. Moreover, for a semiring
homomorphism $\alpha$ and a problem $P$ over $S$, if $t$ is optimal in
$\alpha(P)$, then there is an optimal solution $\bar{t}$ of $P$ such that
$\bar{t}$ has the same value as $t$ in $\alpha(P)$.

22.   Most definitions of ontology, viewed as a "specification of a
conceptualization", agree on the fact that if an ontology can take different
forms, it necessarily includes a vocabulary of terms and some specification of
their meaning in relation to the domain's conceptualization. And as domain
knowledge is mainly conveyed through scientific and technical texts, we can
hope to extract some useful information from them for building ontology. But is
it as simple as this? In this article we shall see that the lexical structure,
i.e. the network of words linked by linguistic relationships, does not
necessarily match the domain conceptualization. We have to bear in mind that
writing documents is the concern of textual linguistics, of which one of the
principles is the incompleteness of text, whereas building ontology - viewed as
task-independent knowledge - is concerned with conceptualization based on
formal and not natural languages. Nevertheless, the famous Sapir and Whorf
hypothesis, concerning the interdependence of thought and language, is also
applicable to formal languages. This means that the way an ontology is built
and a concept is defined depends directly on the formal language which is used;
and the results will not be the same. The introduction of the notion of
ontoterminology allows to take into account epistemological principles for
formal ontology building.

23.   This paper describes application of rough set theory, on the analysis of
hydrocyclone operation. In this manner, using Self Organizing Map (SOM) as
preprocessing step, best crisp granules of data are obtained. Then, using a
combining of SOM and rough set theory (RST)-called SORST-, the dominant rules
on the information table, obtained from laboratory tests, are extracted. Based
on these rules, an approximate estimation on decision attribute is fulfilled.
Finally, a brief comparison of this method with the SOM-NFIS system (briefly
SONFIS) is highlighted.

24.   This paper has been withdrawn by the author due to extremely unscientific
errors.

25.   A fundamental problem in artificial intelligence is that nobody really knows
what intelligence is. The problem is especially acute when we need to consider
artificial systems which are significantly different to humans. In this paper
we approach this problem in the following way: We take a number of well known
informal definitions of human intelligence that have been given by experts, and
extract their essential features. These are then mathematically formalised to
produce a general measure of intelligence for arbitrary machines. We believe
that this equation formally captures the concept of machine intelligence in the
broadest reasonable sense. We then show how this formal definition is related
to the theory of universal optimal learning agents. Finally, we survey the many
other tests and definitions of intelligence that have been proposed for
machines.

26.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"- "has a topic of machine intelligence definitions and tests"
"has a topic of evolutionary algorithms and cellular automata"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of covert node discovery in social networks"\n- "has a topic of definitions and measurements of intelligence"\n- "has a topic of knowledge management systems in organizational clusters"\n- "has a topic of fault classification using neuro-fuzzy models"\n- "has a topic of constraint satisfaction problems and propagators"\n- "has a topic of simulation and modeling of complex systems"\n- "has a topic of rough set theory applications in data analysis"\n- "has a topic of fuzzy logic in stability assessment of underground openings"\n- "has a topic of terrorist network analysis and intelligence visualization"\n- "has a topic of semiring-based soft constraints in optimization"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

1.   An empty spot refers to an empty hard-to-fill space which can be found in the
records of the social interaction, and is the clue to the persons in the
underlying social network who do not appear in the records. This contribution
addresses a problem to predict relevant empty spots in social interaction.
Homogeneous and inhomogeneous networks are studied as a model underlying the
social interaction. A heuristic predictor function approach is presented as a
new method to address the problem. Simulation experiment is demonstrated over a
homogeneous network. A test data in the form of baskets is generated from the
simulated communication. Precision to predict the empty spots is calculated to
demonstrate the performance of the presented approach.

2.   Knowledge-based economy forces companies in the nation to group together as a
cluster in order to maintain their competitiveness in the world market. The
cluster development relies on two key success factors which are knowledge
sharing and collaboration between the actors in the cluster. Thus, our study
tries to propose knowledge management system to support knowledge management
activities within the cluster. To achieve the objectives of this study,
ontology takes a very important role in knowledge management process in various
ways; such as building reusable and faster knowledge-bases, better way for
representing the knowledge explicitly. However, creating and representing
ontology create difficulties to organization due to the ambiguity and
unstructured of source of knowledge. Therefore, the objectives of this paper
are to propose the methodology to create and represent ontology for the
organization development by using knowledge engineering approach. The
handicraft cluster in Thailand is used as a case study to illustrate our
proposed methodology.

3.   A fundamental problem in artificial intelligence is that nobody really knows
what intelligence is. The problem is especially acute when we need to consider
artificial systems which are significantly different to humans. In this paper
we approach this problem in the following way: We take a number of well known
informal definitions of human intelligence that have been given by experts, and
extract their essential features. These are then mathematically formalised to
produce a general measure of intelligence for arbitrary machines. We believe
that this equation formally captures the concept of machine intelligence in the
broadest reasonable sense. We then show how this formal definition is related
to the theory of universal optimal learning agents. Finally, we survey the many
other tests and definitions of intelligence that have been proposed for
machines.

4.   The investigation of the terrorist attack is a time-critical task. The
investigators have a limited time window to diagnose the organizational
background of the terrorists, to run down and arrest the wire-pullers, and to
take an action to prevent or eradicate the terrorist attack. The intuitive
interface to visualize the intelligence data set stimulates the investigators'
experience and knowledge, and aids them in decision-making for an immediately
effective action. This paper presents a computational method to analyze the
intelligence data set on the collective actions of the perpetrators of the
attack, and to visualize it into the form of a social network diagram which
predicts the positions where the wire-pullers conceals themselves.

5.   Approximately more than 90% of all coal production in Iranian underground
mines is derived directly longwall mining method. Out of seam dilution is one
of the essential problems in these mines. Therefore the dilution can impose the
additional cost of mining and milling. As a result, recognition of the
effective parameters on the dilution has a remarkable role in industry. In this
way, this paper has analyzed the influence of 13 parameters (attributed
variables) versus the decision attribute (dilution value), so that using two
approximate reasoning methods, namely Rough Set Theory (RST) and Self
Organizing Neuro- Fuzzy Inference System (SONFIS) the best rules on our
collected data sets has been extracted. The other benefit of later methods is
to predict new unknown cases. So, the reduced sets (reducts) by RST have been
obtained. Therefore the emerged results by utilizing mentioned methods shows
that the high sensitive variables are thickness of layer, length of stope, rate
of advance, number of miners, type of advancing.

6.   In this paper, I present a method to solve a node discovery problem in a
networked organization. Covert nodes refer to the nodes which are not
observable directly. They affect social interactions, but do not appear in the
surveillance logs which record the participants of the social interactions.
Discovering the covert nodes is defined as identifying the suspicious logs
where the covert nodes would appear if the covert nodes became overt. A
mathematical model is developed for the maximal likelihood estimation of the
network behind the social interactions and for the identification of the
suspicious logs. Precision, recall, and F measure characteristics are
demonstrated with the dataset generated from a real organization and the
computationally synthesized datasets. The performance is close to the
theoretical limit for any covert nodes in the networks of any topologies and
sizes if the ratio of the number of observation to the number of possible
communication patterns is large.

7.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

8.   This philosophical paper explores the relation between modern scientific
simulations and the future of the universe. We argue that a simulation of an
entire universe will result from future scientific activity. This requires us
to tackle the challenge of simulating open-ended evolution at all levels in a
single simulation. The simulation should encompass not only biological
evolution, but also physical evolution (a level below) and cultural evolution
(a level above). The simulation would allow us to probe what would happen if we
would "replay the tape of the universe" with the same or different laws and
initial conditions. We also distinguish between real-world and artificial-world
modelling. Assuming that intelligent life could indeed simulate an entire
universe, this leads to two tentative hypotheses. Some authors have argued that
we may already be in a simulation run by an intelligent entity. Or, if such a
simulation could be made real, this would lead to the production of a new
universe. This last direction is argued with a careful speculative
philosophical approach, emphasizing the imperative to find a solution to the
heat death problem in cosmology. The reader is invited to consult Annex 1 for
an overview of the logical structure of this paper. -- Keywords: far future,
future of science, ALife, simulation, realization, cosmology, heat death,
fine-tuning, physical eschatology, cosmological natural selection, cosmological
artificial selection, artificial cosmogenesis, selfish biocosm hypothesis,
meduso-anthropic principle, developmental singularity hypothesis, role of
intelligent life.

9.   Methods to solve a node discovery problem for a social network are presented.
Covert nodes refer to the nodes which are not observable directly. They
transmit the influence and affect the resulting collaborative activities among
the persons in a social network, but do not appear in the surveillance logs
which record the participants of the collaborative activities. Discovering the
covert nodes is identifying the suspicious logs where the covert nodes would
appear if the covert nodes became overt. The performance of the methods is
demonstrated with a test dataset generated from computationally synthesized
networks and a real organization.

10.   The semiring-based constraint satisfaction problems (semiring CSPs), proposed
by Bistarelli, Montanari and Rossi \cite{BMR97}, is a very general framework of
soft constraints. In this paper we propose an abstraction scheme for soft
constraints that uses semiring homomorphism. To find optimal solutions of the
concrete problem, the idea is, first working in the abstract problem and
finding its optimal solutions, then using them to solve the concrete problem.
  In particular, we show that a mapping preserves optimal solutions if and only
if it is an order-reflecting semiring homomorphism. Moreover, for a semiring
homomorphism $\alpha$ and a problem $P$ over $S$, if $t$ is optimal in
$\alpha(P)$, then there is an optimal solution $\bar{t}$ of $P$ such that
$\bar{t}$ has the same value as $t$ in $\alpha(P)$.

11.   This theoretical work defines the measure of autocorrelation of evolvability
in the context of neutral fitness landscape. This measure has been studied on
the classical MAX-SAT problem. This work highlight a new characteristic of
neutral fitness landscapes which allows to design new adapted metaheuristic.

12.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

13.   This paper has been withdrawn.

14.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

15.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

16.   This paper addresses a method to analyze the covert social network foundation
hidden behind the terrorism disaster. It is to solve a node discovery problem,
which means to discover a node, which functions relevantly in a social network,
but escaped from monitoring on the presence and mutual relationship of nodes.
The method aims at integrating the expert investigator's prior understanding,
insight on the terrorists' social network nature derived from the complex graph
theory, and computational data processing. The social network responsible for
the 9/11 attack in 2001 is used to execute simulation experiment to evaluate
the performance of the method.

17.   A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.

18.   This paper describes application of rough set theory, on the analysis of
hydrocyclone operation. In this manner, using Self Organizing Map (SOM) as
preprocessing step, best crisp granules of data are obtained. Then, using a
combining of SOM and rough set theory (RST)-called SORST-, the dominant rules
on the information table, obtained from laboratory tests, are extracted. Based
on these rules, an approximate estimation on decision attribute is fulfilled.
Finally, a brief comparison of this method with the SOM-NFIS system (briefly
SONFIS) is highlighted.

19.   When implementing a propagator for a constraint, one must decide about
variants: When implementing min, should one also implement max? Should one
implement linear equations both with and without coefficients? Constraint
variants are ubiquitous: implementing them requires considerable (if not
prohibitive) effort and decreases maintainability, but will deliver better
performance.
  This paper shows how to use variable views, previously introduced for an
implementation architecture, to derive perfect propagator variants. A model for
views and derived propagators is introduced. Derived propagators are proved to
be indeed perfect in that they inherit essential properties such as correctness
and domain and bounds consistency. Techniques for systematically deriving
propagators such as transformation, generalization, specialization, and
channeling are developed for several variable domains. We evaluate the massive
impact of derived propagators. Without derived propagators, Gecode would
require 140000 rather than 40000 lines of code for propagators.

20.   This paper has been withdrawn by the author due to extremely unscientific
errors.

21.   Most definitions of ontology, viewed as a "specification of a
conceptualization", agree on the fact that if an ontology can take different
forms, it necessarily includes a vocabulary of terms and some specification of
their meaning in relation to the domain's conceptualization. And as domain
knowledge is mainly conveyed through scientific and technical texts, we can
hope to extract some useful information from them for building ontology. But is
it as simple as this? In this article we shall see that the lexical structure,
i.e. the network of words linked by linguistic relationships, does not
necessarily match the domain conceptualization. We have to bear in mind that
writing documents is the concern of textual linguistics, of which one of the
principles is the incompleteness of text, whereas building ontology - viewed as
task-independent knowledge - is concerned with conceptualization based on
formal and not natural languages. Nevertheless, the famous Sapir and Whorf
hypothesis, concerning the interdependence of thought and language, is also
applicable to formal languages. This means that the way an ontology is built
and a concept is defined depends directly on the formal language which is used;
and the results will not be the same. The introduction of the notion of
ontoterminology allows to take into account epistemological principles for
formal ontology building.

22.   We consider hexagonal cellular automata with immediate cell neighbourhood and
three cell-states. Every cell calculates its next state depending on the
integral representation of states in its neighbourhood, i.e. how many
neighbours are in each one state. We employ evolutionary algorithms to breed
local transition functions that support mobile localizations (gliders), and
characterize sets of the functions selected in terms of quasi-chemical systems.
Analysis of the set of functions evolved allows to speculate that mobile
localizations are likely to emerge in the quasi-chemical systems with limited
diffusion of one reagent, a small number of molecules is required for
amplification of travelling localizations, and reactions leading to stationary
localizations involve relatively equal amount of quasi-chemical species.
Techniques developed can be applied in cascading signals in nature-inspired
spatially extended computing devices, and phenomenological studies and
classification of non-linear discrete systems.

23.   Although the definition and measurement of intelligence is clearly of
fundamental importance to the field of artificial intelligence, no general
survey of definitions and tests of machine intelligence exists. Indeed few
researchers are even aware of alternatives to the Turing test and its many
derivatives. In this paper we fill this gap by providing a short survey of the
many tests of machine intelligence that have been proposed.

24.   This paper introduces the concept of fitness cloud as an alternative way to
visualize and analyze search spaces than given by the geographic notion of
fitness landscape. It is argued that the fitness cloud concept overcomes
several deficiencies of the landscape representation. Our analysis is based on
the correlation between fitness of solutions and fitnesses of nearest solutions
according to some neighboring. We focus on the behavior of local search
heuristics, such as hill climber, on the well-known NK fitness landscape. In
both cases the fitness vs. fitness correlation is shown to be related to the
epistatic parameter K.

25.   We describe decomposition during search (DDS), an integration of And/Or tree
search into propagation-based constraint solvers. The presented search
algorithm dynamically decomposes sub-problems of a constraint satisfaction
problem into independent partial problems, avoiding redundant work.
  The paper discusses how DDS interacts with key features that make
propagation-based solvers successful: constraint propagation, especially for
global constraints, and dynamic search heuristics.
  We have implemented DDS for the Gecode constraint programming library. Two
applications, solution counting in graph coloring and protein structure
prediction, exemplify the benefits of DDS in practice.

26.   This paper presents a fault classification method which makes use of a
Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the
vibration signals of cylindrical shells. The calculation of Pseudomodal
Energies, for the purposes of condition monitoring, has previously been found
to be an accurate method of extracting features from vibration signals. This
calculation is therefore used to extract features from vibration signals
obtained from a diverse population of cylindrical shells. Some of the cylinders
in the population have faults in different substructures. The pseudomodal
energies calculated from the vibration signals are then used as inputs to a
neuro-fuzzy model. A leave-one-out cross-validation process is used to test the
performance of the model. It is found that the neuro-fuzzy model is able to
classify faults with an accuracy of 91.62%, which is higher than the previously
used multilayer perceptron.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of fuzzy logic and fault classification"
"has a topic of constraint satisfaction problems and optimization"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of social network analysis and covert nodes"\n- "has a topic of knowledge management and ontology development"\n- "has a topic of artificial intelligence and definitions of intelligence"\n- "has a topic of computational methods in terrorism investigation"\n- "has a topic of mining methods and operational efficiency"\n- "has a topic of evolutionary algorithms and cellular automata"\n- "has a topic of simulation and philosophical implications"\n- "has a topic of fuzzy block theory and stability assessment"\n- "has a topic of constraint programming and search algorithms"\n- "has a topic of hydrocyclone operation and rough set theory"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.

1.   This paper describes application of rough set theory, on the analysis of
hydrocyclone operation. In this manner, using Self Organizing Map (SOM) as
preprocessing step, best crisp granules of data are obtained. Then, using a
combining of SOM and rough set theory (RST)-called SORST-, the dominant rules
on the information table, obtained from laboratory tests, are extracted. Based
on these rules, an approximate estimation on decision attribute is fulfilled.
Finally, a brief comparison of this method with the SOM-NFIS system (briefly
SONFIS) is highlighted.

2.   This paper presents a fault classification method which makes use of a
Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the
vibration signals of cylindrical shells. The calculation of Pseudomodal
Energies, for the purposes of condition monitoring, has previously been found
to be an accurate method of extracting features from vibration signals. This
calculation is therefore used to extract features from vibration signals
obtained from a diverse population of cylindrical shells. Some of the cylinders
in the population have faults in different substructures. The pseudomodal
energies calculated from the vibration signals are then used as inputs to a
neuro-fuzzy model. A leave-one-out cross-validation process is used to test the
performance of the model. It is found that the neuro-fuzzy model is able to
classify faults with an accuracy of 91.62%, which is higher than the previously
used multilayer perceptron.

3.   This paper has been withdrawn.

4.   An empty spot refers to an empty hard-to-fill space which can be found in the
records of the social interaction, and is the clue to the persons in the
underlying social network who do not appear in the records. This contribution
addresses a problem to predict relevant empty spots in social interaction.
Homogeneous and inhomogeneous networks are studied as a model underlying the
social interaction. A heuristic predictor function approach is presented as a
new method to address the problem. Simulation experiment is demonstrated over a
homogeneous network. A test data in the form of baskets is generated from the
simulated communication. Precision to predict the empty spots is calculated to
demonstrate the performance of the presented approach.

5.   The semiring-based constraint satisfaction problems (semiring CSPs), proposed
by Bistarelli, Montanari and Rossi \cite{BMR97}, is a very general framework of
soft constraints. In this paper we propose an abstraction scheme for soft
constraints that uses semiring homomorphism. To find optimal solutions of the
concrete problem, the idea is, first working in the abstract problem and
finding its optimal solutions, then using them to solve the concrete problem.
  In particular, we show that a mapping preserves optimal solutions if and only
if it is an order-reflecting semiring homomorphism. Moreover, for a semiring
homomorphism $\alpha$ and a problem $P$ over $S$, if $t$ is optimal in
$\alpha(P)$, then there is an optimal solution $\bar{t}$ of $P$ such that
$\bar{t}$ has the same value as $t$ in $\alpha(P)$.

6.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

7.   This paper introduces the concept of fitness cloud as an alternative way to
visualize and analyze search spaces than given by the geographic notion of
fitness landscape. It is argued that the fitness cloud concept overcomes
several deficiencies of the landscape representation. Our analysis is based on
the correlation between fitness of solutions and fitnesses of nearest solutions
according to some neighboring. We focus on the behavior of local search
heuristics, such as hill climber, on the well-known NK fitness landscape. In
both cases the fitness vs. fitness correlation is shown to be related to the
epistatic parameter K.

8.   This paper has been withdrawn by the author due to extremely unscientific
errors.

9.   Knowledge-based economy forces companies in the nation to group together as a
cluster in order to maintain their competitiveness in the world market. The
cluster development relies on two key success factors which are knowledge
sharing and collaboration between the actors in the cluster. Thus, our study
tries to propose knowledge management system to support knowledge management
activities within the cluster. To achieve the objectives of this study,
ontology takes a very important role in knowledge management process in various
ways; such as building reusable and faster knowledge-bases, better way for
representing the knowledge explicitly. However, creating and representing
ontology create difficulties to organization due to the ambiguity and
unstructured of source of knowledge. Therefore, the objectives of this paper
are to propose the methodology to create and represent ontology for the
organization development by using knowledge engineering approach. The
handicraft cluster in Thailand is used as a case study to illustrate our
proposed methodology.

10.   A fundamental problem in artificial intelligence is that nobody really knows
what intelligence is. The problem is especially acute when we need to consider
artificial systems which are significantly different to humans. In this paper
we approach this problem in the following way: We take a number of well known
informal definitions of human intelligence that have been given by experts, and
extract their essential features. These are then mathematically formalised to
produce a general measure of intelligence for arbitrary machines. We believe
that this equation formally captures the concept of machine intelligence in the
broadest reasonable sense. We then show how this formal definition is related
to the theory of universal optimal learning agents. Finally, we survey the many
other tests and definitions of intelligence that have been proposed for
machines.

11.   We describe decomposition during search (DDS), an integration of And/Or tree
search into propagation-based constraint solvers. The presented search
algorithm dynamically decomposes sub-problems of a constraint satisfaction
problem into independent partial problems, avoiding redundant work.
  The paper discusses how DDS interacts with key features that make
propagation-based solvers successful: constraint propagation, especially for
global constraints, and dynamic search heuristics.
  We have implemented DDS for the Gecode constraint programming library. Two
applications, solution counting in graph coloring and protein structure
prediction, exemplify the benefits of DDS in practice.

12.   Although the definition and measurement of intelligence is clearly of
fundamental importance to the field of artificial intelligence, no general
survey of definitions and tests of machine intelligence exists. Indeed few
researchers are even aware of alternatives to the Turing test and its many
derivatives. In this paper we fill this gap by providing a short survey of the
many tests of machine intelligence that have been proposed.

13.   Approximately more than 90% of all coal production in Iranian underground
mines is derived directly longwall mining method. Out of seam dilution is one
of the essential problems in these mines. Therefore the dilution can impose the
additional cost of mining and milling. As a result, recognition of the
effective parameters on the dilution has a remarkable role in industry. In this
way, this paper has analyzed the influence of 13 parameters (attributed
variables) versus the decision attribute (dilution value), so that using two
approximate reasoning methods, namely Rough Set Theory (RST) and Self
Organizing Neuro- Fuzzy Inference System (SONFIS) the best rules on our
collected data sets has been extracted. The other benefit of later methods is
to predict new unknown cases. So, the reduced sets (reducts) by RST have been
obtained. Therefore the emerged results by utilizing mentioned methods shows
that the high sensitive variables are thickness of layer, length of stope, rate
of advance, number of miners, type of advancing.

14.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

15.   Methods to solve a node discovery problem for a social network are presented.
Covert nodes refer to the nodes which are not observable directly. They
transmit the influence and affect the resulting collaborative activities among
the persons in a social network, but do not appear in the surveillance logs
which record the participants of the collaborative activities. Discovering the
covert nodes is identifying the suspicious logs where the covert nodes would
appear if the covert nodes became overt. The performance of the methods is
demonstrated with a test dataset generated from computationally synthesized
networks and a real organization.

16.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

17.   This paper addresses a method to analyze the covert social network foundation
hidden behind the terrorism disaster. It is to solve a node discovery problem,
which means to discover a node, which functions relevantly in a social network,
but escaped from monitoring on the presence and mutual relationship of nodes.
The method aims at integrating the expert investigator's prior understanding,
insight on the terrorists' social network nature derived from the complex graph
theory, and computational data processing. The social network responsible for
the 9/11 attack in 2001 is used to execute simulation experiment to evaluate
the performance of the method.

18.   The investigation of the terrorist attack is a time-critical task. The
investigators have a limited time window to diagnose the organizational
background of the terrorists, to run down and arrest the wire-pullers, and to
take an action to prevent or eradicate the terrorist attack. The intuitive
interface to visualize the intelligence data set stimulates the investigators'
experience and knowledge, and aids them in decision-making for an immediately
effective action. This paper presents a computational method to analyze the
intelligence data set on the collective actions of the perpetrators of the
attack, and to visualize it into the form of a social network diagram which
predicts the positions where the wire-pullers conceals themselves.

19.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

20.   In this paper, I present a method to solve a node discovery problem in a
networked organization. Covert nodes refer to the nodes which are not
observable directly. They affect social interactions, but do not appear in the
surveillance logs which record the participants of the social interactions.
Discovering the covert nodes is defined as identifying the suspicious logs
where the covert nodes would appear if the covert nodes became overt. A
mathematical model is developed for the maximal likelihood estimation of the
network behind the social interactions and for the identification of the
suspicious logs. Precision, recall, and F measure characteristics are
demonstrated with the dataset generated from a real organization and the
computationally synthesized datasets. The performance is close to the
theoretical limit for any covert nodes in the networks of any topologies and
sizes if the ratio of the number of observation to the number of possible
communication patterns is large.

21.   This philosophical paper explores the relation between modern scientific
simulations and the future of the universe. We argue that a simulation of an
entire universe will result from future scientific activity. This requires us
to tackle the challenge of simulating open-ended evolution at all levels in a
single simulation. The simulation should encompass not only biological
evolution, but also physical evolution (a level below) and cultural evolution
(a level above). The simulation would allow us to probe what would happen if we
would "replay the tape of the universe" with the same or different laws and
initial conditions. We also distinguish between real-world and artificial-world
modelling. Assuming that intelligent life could indeed simulate an entire
universe, this leads to two tentative hypotheses. Some authors have argued that
we may already be in a simulation run by an intelligent entity. Or, if such a
simulation could be made real, this would lead to the production of a new
universe. This last direction is argued with a careful speculative
philosophical approach, emphasizing the imperative to find a solution to the
heat death problem in cosmology. The reader is invited to consult Annex 1 for
an overview of the logical structure of this paper. -- Keywords: far future,
future of science, ALife, simulation, realization, cosmology, heat death,
fine-tuning, physical eschatology, cosmological natural selection, cosmological
artificial selection, artificial cosmogenesis, selfish biocosm hypothesis,
meduso-anthropic principle, developmental singularity hypothesis, role of
intelligent life.

22.   This theoretical work defines the measure of autocorrelation of evolvability
in the context of neutral fitness landscape. This measure has been studied on
the classical MAX-SAT problem. This work highlight a new characteristic of
neutral fitness landscapes which allows to design new adapted metaheuristic.

23.   When implementing a propagator for a constraint, one must decide about
variants: When implementing min, should one also implement max? Should one
implement linear equations both with and without coefficients? Constraint
variants are ubiquitous: implementing them requires considerable (if not
prohibitive) effort and decreases maintainability, but will deliver better
performance.
  This paper shows how to use variable views, previously introduced for an
implementation architecture, to derive perfect propagator variants. A model for
views and derived propagators is introduced. Derived propagators are proved to
be indeed perfect in that they inherit essential properties such as correctness
and domain and bounds consistency. Techniques for systematically deriving
propagators such as transformation, generalization, specialization, and
channeling are developed for several variable domains. We evaluate the massive
impact of derived propagators. Without derived propagators, Gecode would
require 140000 rather than 40000 lines of code for propagators.

24.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

25.   Most definitions of ontology, viewed as a "specification of a
conceptualization", agree on the fact that if an ontology can take different
forms, it necessarily includes a vocabulary of terms and some specification of
their meaning in relation to the domain's conceptualization. And as domain
knowledge is mainly conveyed through scientific and technical texts, we can
hope to extract some useful information from them for building ontology. But is
it as simple as this? In this article we shall see that the lexical structure,
i.e. the network of words linked by linguistic relationships, does not
necessarily match the domain conceptualization. We have to bear in mind that
writing documents is the concern of textual linguistics, of which one of the
principles is the incompleteness of text, whereas building ontology - viewed as
task-independent knowledge - is concerned with conceptualization based on
formal and not natural languages. Nevertheless, the famous Sapir and Whorf
hypothesis, concerning the interdependence of thought and language, is also
applicable to formal languages. This means that the way an ontology is built
and a concept is defined depends directly on the formal language which is used;
and the results will not be the same. The introduction of the notion of
ontoterminology allows to take into account epistemological principles for
formal ontology building.

26.   We consider hexagonal cellular automata with immediate cell neighbourhood and
three cell-states. Every cell calculates its next state depending on the
integral representation of states in its neighbourhood, i.e. how many
neighbours are in each one state. We employ evolutionary algorithms to breed
local transition functions that support mobile localizations (gliders), and
characterize sets of the functions selected in terms of quasi-chemical systems.
Analysis of the set of functions evolved allows to speculate that mobile
localizations are likely to emerge in the quasi-chemical systems with limited
diffusion of one reagent, a small number of molecules is required for
amplification of travelling localizations, and reactions leading to stationary
localizations involve relatively equal amount of quasi-chemical species.
Techniques developed can be applied in cascading signals in nature-inspired
spatially extended computing devices, and phenomenological studies and
classification of non-linear discrete systems.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of ontology and conceptualization in knowledge representation"
"has a topic of terrorism and social network dynamics"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of causality and definitions in artificial intelligence"\n- "has a topic of fault classification and condition monitoring in engineering"\n- "has a topic of social network analysis and covert node discovery"\n- "has a topic of knowledge management and ontology development"\n- "has a topic of intelligence measurement and definitions in AI"\n- "has a topic of fuzzy logic and stability assessment in mining"\n- "has a topic of evolutionary algorithms and cellular automata"\n- "has a topic of simulation and philosophical implications of intelligence"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   We describe decomposition during search (DDS), an integration of And/Or tree
search into propagation-based constraint solvers. The presented search
algorithm dynamically decomposes sub-problems of a constraint satisfaction
problem into independent partial problems, avoiding redundant work.
  The paper discusses how DDS interacts with key features that make
propagation-based solvers successful: constraint propagation, especially for
global constraints, and dynamic search heuristics.
  We have implemented DDS for the Gecode constraint programming library. Two
applications, solution counting in graph coloring and protein structure
prediction, exemplify the benefits of DDS in practice.

1.   This paper introduces the concept of fitness cloud as an alternative way to
visualize and analyze search spaces than given by the geographic notion of
fitness landscape. It is argued that the fitness cloud concept overcomes
several deficiencies of the landscape representation. Our analysis is based on
the correlation between fitness of solutions and fitnesses of nearest solutions
according to some neighboring. We focus on the behavior of local search
heuristics, such as hill climber, on the well-known NK fitness landscape. In
both cases the fitness vs. fitness correlation is shown to be related to the
epistatic parameter K.

2.   Although the definition and measurement of intelligence is clearly of
fundamental importance to the field of artificial intelligence, no general
survey of definitions and tests of machine intelligence exists. Indeed few
researchers are even aware of alternatives to the Turing test and its many
derivatives. In this paper we fill this gap by providing a short survey of the
many tests of machine intelligence that have been proposed.

3.   Approximately more than 90% of all coal production in Iranian underground
mines is derived directly longwall mining method. Out of seam dilution is one
of the essential problems in these mines. Therefore the dilution can impose the
additional cost of mining and milling. As a result, recognition of the
effective parameters on the dilution has a remarkable role in industry. In this
way, this paper has analyzed the influence of 13 parameters (attributed
variables) versus the decision attribute (dilution value), so that using two
approximate reasoning methods, namely Rough Set Theory (RST) and Self
Organizing Neuro- Fuzzy Inference System (SONFIS) the best rules on our
collected data sets has been extracted. The other benefit of later methods is
to predict new unknown cases. So, the reduced sets (reducts) by RST have been
obtained. Therefore the emerged results by utilizing mentioned methods shows
that the high sensitive variables are thickness of layer, length of stope, rate
of advance, number of miners, type of advancing.

4.   The semiring-based constraint satisfaction problems (semiring CSPs), proposed
by Bistarelli, Montanari and Rossi \cite{BMR97}, is a very general framework of
soft constraints. In this paper we propose an abstraction scheme for soft
constraints that uses semiring homomorphism. To find optimal solutions of the
concrete problem, the idea is, first working in the abstract problem and
finding its optimal solutions, then using them to solve the concrete problem.
  In particular, we show that a mapping preserves optimal solutions if and only
if it is an order-reflecting semiring homomorphism. Moreover, for a semiring
homomorphism $\alpha$ and a problem $P$ over $S$, if $t$ is optimal in
$\alpha(P)$, then there is an optimal solution $\bar{t}$ of $P$ such that
$\bar{t}$ has the same value as $t$ in $\alpha(P)$.

5.   This paper presents a fault classification method which makes use of a
Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the
vibration signals of cylindrical shells. The calculation of Pseudomodal
Energies, for the purposes of condition monitoring, has previously been found
to be an accurate method of extracting features from vibration signals. This
calculation is therefore used to extract features from vibration signals
obtained from a diverse population of cylindrical shells. Some of the cylinders
in the population have faults in different substructures. The pseudomodal
energies calculated from the vibration signals are then used as inputs to a
neuro-fuzzy model. A leave-one-out cross-validation process is used to test the
performance of the model. It is found that the neuro-fuzzy model is able to
classify faults with an accuracy of 91.62%, which is higher than the previously
used multilayer perceptron.

6.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

7.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

8.   A fundamental problem in artificial intelligence is that nobody really knows
what intelligence is. The problem is especially acute when we need to consider
artificial systems which are significantly different to humans. In this paper
we approach this problem in the following way: We take a number of well known
informal definitions of human intelligence that have been given by experts, and
extract their essential features. These are then mathematically formalised to
produce a general measure of intelligence for arbitrary machines. We believe
that this equation formally captures the concept of machine intelligence in the
broadest reasonable sense. We then show how this formal definition is related
to the theory of universal optimal learning agents. Finally, we survey the many
other tests and definitions of intelligence that have been proposed for
machines.

9.   The investigation of the terrorist attack is a time-critical task. The
investigators have a limited time window to diagnose the organizational
background of the terrorists, to run down and arrest the wire-pullers, and to
take an action to prevent or eradicate the terrorist attack. The intuitive
interface to visualize the intelligence data set stimulates the investigators'
experience and knowledge, and aids them in decision-making for an immediately
effective action. This paper presents a computational method to analyze the
intelligence data set on the collective actions of the perpetrators of the
attack, and to visualize it into the form of a social network diagram which
predicts the positions where the wire-pullers conceals themselves.

10.   In this paper, I present a method to solve a node discovery problem in a
networked organization. Covert nodes refer to the nodes which are not
observable directly. They affect social interactions, but do not appear in the
surveillance logs which record the participants of the social interactions.
Discovering the covert nodes is defined as identifying the suspicious logs
where the covert nodes would appear if the covert nodes became overt. A
mathematical model is developed for the maximal likelihood estimation of the
network behind the social interactions and for the identification of the
suspicious logs. Precision, recall, and F measure characteristics are
demonstrated with the dataset generated from a real organization and the
computationally synthesized datasets. The performance is close to the
theoretical limit for any covert nodes in the networks of any topologies and
sizes if the ratio of the number of observation to the number of possible
communication patterns is large.

11.   This paper addresses a method to analyze the covert social network foundation
hidden behind the terrorism disaster. It is to solve a node discovery problem,
which means to discover a node, which functions relevantly in a social network,
but escaped from monitoring on the presence and mutual relationship of nodes.
The method aims at integrating the expert investigator's prior understanding,
insight on the terrorists' social network nature derived from the complex graph
theory, and computational data processing. The social network responsible for
the 9/11 attack in 2001 is used to execute simulation experiment to evaluate
the performance of the method.

12.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

13.   Knowledge-based economy forces companies in the nation to group together as a
cluster in order to maintain their competitiveness in the world market. The
cluster development relies on two key success factors which are knowledge
sharing and collaboration between the actors in the cluster. Thus, our study
tries to propose knowledge management system to support knowledge management
activities within the cluster. To achieve the objectives of this study,
ontology takes a very important role in knowledge management process in various
ways; such as building reusable and faster knowledge-bases, better way for
representing the knowledge explicitly. However, creating and representing
ontology create difficulties to organization due to the ambiguity and
unstructured of source of knowledge. Therefore, the objectives of this paper
are to propose the methodology to create and represent ontology for the
organization development by using knowledge engineering approach. The
handicraft cluster in Thailand is used as a case study to illustrate our
proposed methodology.

14.   This paper has been withdrawn.

15.   When implementing a propagator for a constraint, one must decide about
variants: When implementing min, should one also implement max? Should one
implement linear equations both with and without coefficients? Constraint
variants are ubiquitous: implementing them requires considerable (if not
prohibitive) effort and decreases maintainability, but will deliver better
performance.
  This paper shows how to use variable views, previously introduced for an
implementation architecture, to derive perfect propagator variants. A model for
views and derived propagators is introduced. Derived propagators are proved to
be indeed perfect in that they inherit essential properties such as correctness
and domain and bounds consistency. Techniques for systematically deriving
propagators such as transformation, generalization, specialization, and
channeling are developed for several variable domains. We evaluate the massive
impact of derived propagators. Without derived propagators, Gecode would
require 140000 rather than 40000 lines of code for propagators.

16.   Most definitions of ontology, viewed as a "specification of a
conceptualization", agree on the fact that if an ontology can take different
forms, it necessarily includes a vocabulary of terms and some specification of
their meaning in relation to the domain's conceptualization. And as domain
knowledge is mainly conveyed through scientific and technical texts, we can
hope to extract some useful information from them for building ontology. But is
it as simple as this? In this article we shall see that the lexical structure,
i.e. the network of words linked by linguistic relationships, does not
necessarily match the domain conceptualization. We have to bear in mind that
writing documents is the concern of textual linguistics, of which one of the
principles is the incompleteness of text, whereas building ontology - viewed as
task-independent knowledge - is concerned with conceptualization based on
formal and not natural languages. Nevertheless, the famous Sapir and Whorf
hypothesis, concerning the interdependence of thought and language, is also
applicable to formal languages. This means that the way an ontology is built
and a concept is defined depends directly on the formal language which is used;
and the results will not be the same. The introduction of the notion of
ontoterminology allows to take into account epistemological principles for
formal ontology building.

17.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

18.   This paper has been withdrawn by the author due to extremely unscientific
errors.

19.   This paper describes application of rough set theory, on the analysis of
hydrocyclone operation. In this manner, using Self Organizing Map (SOM) as
preprocessing step, best crisp granules of data are obtained. Then, using a
combining of SOM and rough set theory (RST)-called SORST-, the dominant rules
on the information table, obtained from laboratory tests, are extracted. Based
on these rules, an approximate estimation on decision attribute is fulfilled.
Finally, a brief comparison of this method with the SOM-NFIS system (briefly
SONFIS) is highlighted.

20.   This philosophical paper explores the relation between modern scientific
simulations and the future of the universe. We argue that a simulation of an
entire universe will result from future scientific activity. This requires us
to tackle the challenge of simulating open-ended evolution at all levels in a
single simulation. The simulation should encompass not only biological
evolution, but also physical evolution (a level below) and cultural evolution
(a level above). The simulation would allow us to probe what would happen if we
would "replay the tape of the universe" with the same or different laws and
initial conditions. We also distinguish between real-world and artificial-world
modelling. Assuming that intelligent life could indeed simulate an entire
universe, this leads to two tentative hypotheses. Some authors have argued that
we may already be in a simulation run by an intelligent entity. Or, if such a
simulation could be made real, this would lead to the production of a new
universe. This last direction is argued with a careful speculative
philosophical approach, emphasizing the imperative to find a solution to the
heat death problem in cosmology. The reader is invited to consult Annex 1 for
an overview of the logical structure of this paper. -- Keywords: far future,
future of science, ALife, simulation, realization, cosmology, heat death,
fine-tuning, physical eschatology, cosmological natural selection, cosmological
artificial selection, artificial cosmogenesis, selfish biocosm hypothesis,
meduso-anthropic principle, developmental singularity hypothesis, role of
intelligent life.

21.   A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.

22.   Methods to solve a node discovery problem for a social network are presented.
Covert nodes refer to the nodes which are not observable directly. They
transmit the influence and affect the resulting collaborative activities among
the persons in a social network, but do not appear in the surveillance logs
which record the participants of the collaborative activities. Discovering the
covert nodes is identifying the suspicious logs where the covert nodes would
appear if the covert nodes became overt. The performance of the methods is
demonstrated with a test dataset generated from computationally synthesized
networks and a real organization.

23.   An empty spot refers to an empty hard-to-fill space which can be found in the
records of the social interaction, and is the clue to the persons in the
underlying social network who do not appear in the records. This contribution
addresses a problem to predict relevant empty spots in social interaction.
Homogeneous and inhomogeneous networks are studied as a model underlying the
social interaction. A heuristic predictor function approach is presented as a
new method to address the problem. Simulation experiment is demonstrated over a
homogeneous network. A test data in the form of baskets is generated from the
simulated communication. Precision to predict the empty spots is calculated to
demonstrate the performance of the presented approach.

24.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

25.   This theoretical work defines the measure of autocorrelation of evolvability
in the context of neutral fitness landscape. This measure has been studied on
the classical MAX-SAT problem. This work highlight a new characteristic of
neutral fitness landscapes which allows to design new adapted metaheuristic.

26.   We consider hexagonal cellular automata with immediate cell neighbourhood and
three cell-states. Every cell calculates its next state depending on the
integral representation of states in its neighbourhood, i.e. how many
neighbours are in each one state. We employ evolutionary algorithms to breed
local transition functions that support mobile localizations (gliders), and
characterize sets of the functions selected in terms of quasi-chemical systems.
Analysis of the set of functions evolved allows to speculate that mobile
localizations are likely to emerge in the quasi-chemical systems with limited
diffusion of one reagent, a small number of molecules is required for
amplification of travelling localizations, and reactions leading to stationary
localizations involve relatively equal amount of quasi-chemical species.
Techniques developed can be applied in cascading signals in nature-inspired
spatially extended computing devices, and phenomenological studies and
classification of non-linear discrete systems.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of evolutionary algorithms and cellular automata"
"- "has a topic of machine intelligence definitions and tests"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of constraint satisfaction problems and algorithms"\n- "has a topic of fitness landscapes and optimization"\n- "has a topic of machine intelligence and its definitions"\n- "has a topic of social network analysis and covert nodes"\n- "has a topic of knowledge management and ontology development"\n- "has a topic of fuzzy logic and its applications"\n- "has a topic of terrorism analysis and intelligence visualization"\n- "has a topic of evolutionary theory and simulations"\n- "has a topic of causal inference and definitions of causality"']
99 descriptions proposed in total. Here are the descriptions:
- "has a topic of soft constraints and optimization
has a topic of conflict prediction using machine learning
- "has a topic of semantic web technologies
has a topic of qualitative temporal reasoning
has a topic of automated leaf recognition and classification
has a topic of Bayesian frameworks in modeling
has a topic of face recognition robustness
has a topic of Bayesian frameworks in machine learning
has a topic of multi-agent systems
- "has a topic of automated leaf recognition algorithms
has a topic of evolutionary robotics
has a topic of clustering techniques and algorithms
has a topic of quantum computation applications
has a topic of water demand forecasting
has a topic of quantum computation and artificial intelligence
has a topic of fault classification in cylindrical shells
has a topic of evolutionary learning and ensemble methods
has a topic of multi-cellular developmental design
has a topic of condition monitoring in engineering
- "has a topic of ensemble learning techniques
has a topic of water demand forecasting using computational intelligence
has a topic of HIV risk modeling
- "has a topic of computational intelligence techniques
has a topic of fault diagnosis and classification
has a topic of Bayesian methods in rough set models
has a topic of decentralized control in multi-agent systems
has a topic of protein family detection
has a topic of artificial intelligence in healthcare
has a topic of neural network classifiers
has a topic of neural network models
has a topic of semantic web technologies
- "has a topic of evolutionary robotics
has a topic of semantic web and natural language processing
has a topic of clustering algorithms and techniques
has a topic of ensemble learning approaches
has a topic of artificial intelligence history
has a topic of decentralized control systems
has a topic of qualitative temporal information representation
- "has a topic of evolutionary learning and ensemble methods
has a topic of automated leaf recognition algorithms
has a topic of handling missing data in neural networks
has a topic of conflict prediction and machine learning
- "has a topic of protein family detection
has a topic of condition monitoring frameworks
has a topic of semantic web and computational modeling
has a topic of neural network performance evaluation
has a topic of predicting militarized interstate disputes
has a topic of fault classification methods
has a topic of fault diagnosis and monitoring
has a topic of artificial intelligence applications
has a topic of fault classification and condition monitoring in engineering
has a topic of ontology and conceptualization in knowledge representation
- "has a topic of constraint satisfaction problems and algorithms
has a topic of social network analysis and covert nodes
has a topic of knowledge management and ontology development
- "has a topic of machine intelligence definitions and tests
has a topic of artificial intelligence and definitions of intelligence
has a topic of computational methods in social network analysis
has a topic of simulation and philosophical implications of intelligence
has a topic of knowledge management and ontology
has a topic of constraint satisfaction problems and algorithm design
has a topic of machine intelligence and its definitions
has a topic of terrorism analysis and intelligence visualization
has a topic of evolutionary algorithms and cellular automata
has a topic of evolutionary theory and simulations
- "has a topic of fitness landscapes and optimization algorithms
has a topic of constraint satisfaction problems and propagators
- "has a topic of causality and definitions in artificial intelligence
has a topic of scientific simulations and philosophical implications
has a topic of fuzzy logic in stability assessment of underground openings
has a topic of causality and default theories
has a topic of knowledge management systems in organizational clusters
- "has a topic of covert node discovery in social networks
- "has a topic of social network analysis and covert nodes
has a topic of definitions and measures of intelligence
has a topic of hydrocyclone operation analysis using rough set theory
has a topic of mining methods and operational efficiency
has a topic of terrorism and social network dynamics
has a topic of fuzzy logic and stability assessment in engineering
has a topic of computational methods in terrorism investigation
has a topic of simulation and philosophical implications
has a topic of social network analysis and covert node discovery
has a topic of fuzzy logic and stability assessment
has a topic of intelligence measurement and definitions in AI
has a topic of fuzzy logic and decision-making systems
has a topic of intelligence definitions and machine learning
has a topic of rough set theory applications in data analysis
has a topic of artificial intelligence and intelligence measurement
has a topic of fuzzy logic and fault classification
has a topic of constraint satisfaction problems and optimization
has a topic of fault classification using neuro-fuzzy models
has a topic of fitness landscapes and optimization
has a topic of constraint satisfaction problems and optimization techniques
has a topic of definitions and measurements of intelligence
has a topic of computational methods in terrorism analysis
has a topic of social network analysis and covert operations
has a topic of simulation and modeling of complex systems
has a topic of fuzzy block theory and stability assessment
has a topic of fuzzy logic and its applications
First prompt example:
You are tasked with classifying whether a TEXT satisfies a PROPERTY. You need to respond with Yes or No, and when uncertain, always respond with No.

<example>
<input>
Text: I like apples.
Property: is talking about fruits
</input>
<output>
Yes
</output>
</example>

Now, solve this new task:
Text:   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

Property: has a topic of fault classification and condition monitoring in engineering

Processing 49000 prompts with vLLM's automatic batching...
Stage Assign assign
Assigned all recall: 0.964
Assigned all precision: 0.06431535269709543
Assigned description: - "has a topic of soft constraints and optimization. recall: 0.213 precision: 0.014084507042253521
Assigned description: has a topic of conflict prediction using machine learning. recall: 0.013 precision: 0.0
Assigned description: - "has a topic of semantic web technologies. recall: 0.042 precision: 0.0
Assigned description: has a topic of qualitative temporal reasoning. recall: 0.062 precision: 0.0
Assigned description: has a topic of automated leaf recognition and classification. recall: 0.001 precision: 0.0
Assigned description: has a topic of Bayesian frameworks in modeling. recall: 0.125 precision: 0.008
Assigned description: has a topic of face recognition robustness. recall: 0.001 precision: 0.0
Assigned description: has a topic of Bayesian frameworks in machine learning. recall: 0.104 precision: 0.0
Assigned description: has a topic of multi-agent systems. recall: 0.069 precision: 0.014492753623188406
Assigned description: - "has a topic of automated leaf recognition algorithms. recall: 0.001 precision: 0.0
Assigned description: has a topic of evolutionary robotics. recall: 0.013 precision: 0.0
Assigned description: has a topic of clustering techniques and algorithms. recall: 0.039 precision: 0.02564102564102564
Assigned description: has a topic of quantum computation applications. recall: 0.002 precision: 0.0
Assigned description: has a topic of water demand forecasting. recall: 0.001 precision: 0.0
Assigned description: has a topic of quantum computation and artificial intelligence. recall: 0.005 precision: 0.0
Assigned description: has a topic of fault classification in cylindrical shells. recall: 0.002 precision: 0.0
Assigned description: has a topic of evolutionary learning and ensemble methods. recall: 0.032 precision: 0.0
Assigned description: has a topic of multi-cellular developmental design. recall: 0.001 precision: 0.0
Assigned description: has a topic of condition monitoring in engineering. recall: 0.014 precision: 0.0
Assigned description: - "has a topic of ensemble learning techniques. recall: 0.039 precision: 0.02564102564102564
Assigned description: has a topic of water demand forecasting using computational intelligence. recall: 0.001 precision: 0.0
Assigned description: has a topic of HIV risk modeling. recall: 0.002 precision: 0.0
Assigned description: - "has a topic of computational intelligence techniques. recall: 0.624 precision: 0.022435897435897436
Assigned description: has a topic of fault diagnosis and classification. recall: 0.024 precision: 0.0
Assigned description: has a topic of Bayesian methods in rough set models. recall: 0.012 precision: 0.0
Assigned description: has a topic of decentralized control in multi-agent systems. recall: 0.034 precision: 0.0
Assigned description: has a topic of protein family detection. recall: 0.001 precision: 0.0
Assigned description: has a topic of artificial intelligence in healthcare. recall: 0.028 precision: 0.0
Assigned description: has a topic of neural network classifiers. recall: 0.031 precision: 0.0
Assigned description: has a topic of neural network models. recall: 0.046 precision: 0.0
Assigned description: has a topic of semantic web technologies. recall: 0.046 precision: 0.0
Assigned description: - "has a topic of evolutionary robotics. recall: 0.013 precision: 0.0
Assigned description: has a topic of semantic web and natural language processing. recall: 0.066 precision: 0.0
Assigned description: has a topic of clustering algorithms and techniques. recall: 0.034 precision: 0.0
Assigned description: has a topic of ensemble learning approaches. recall: 0.048 precision: 0.0
Assigned description: has a topic of artificial intelligence history. recall: 0.009 precision: 0.0
Assigned description: has a topic of decentralized control systems. recall: 0.023 precision: 0.0
Assigned description: has a topic of qualitative temporal information representation. recall: 0.101 precision: 0.009900990099009901
Assigned description: - "has a topic of evolutionary learning and ensemble methods. recall: 0.028 precision: 0.0
Assigned description: has a topic of automated leaf recognition algorithms. recall: 0.001 precision: 0.0
Assigned description: has a topic of handling missing data in neural networks. recall: 0.002 precision: 0.5
Assigned description: has a topic of conflict prediction and machine learning. recall: 0.027 precision: 0.037037037037037035
Assigned description: - "has a topic of protein family detection. recall: 0.001 precision: 0.0
Assigned description: has a topic of condition monitoring frameworks. recall: 0.022 precision: 0.0
Assigned description: has a topic of semantic web and computational modeling. recall: 0.12 precision: 0.008333333333333333
Assigned description: has a topic of neural network performance evaluation. recall: 0.018 precision: 0.0
Assigned description: has a topic of predicting militarized interstate disputes. recall: 0.001 precision: 0.0
Assigned description: has a topic of fault classification methods. recall: 0.014 precision: 0.0
Assigned description: has a topic of fault diagnosis and monitoring. recall: 0.029 precision: 0.0
Assigned description: has a topic of artificial intelligence applications. recall: 0.406 precision: 0.0024630541871921183
Assigned description: has a topic of fault classification and condition monitoring in engineering. recall: 0.011 precision: 0.0
Assigned description: has a topic of ontology and conceptualization in knowledge representation. recall: 0.278 precision: 0.046762589928057555
Assigned description: - "has a topic of constraint satisfaction problems and algorithms. recall: 0.412 precision: 0.0
Assigned description: has a topic of social network analysis and covert nodes. recall: 0.015 precision: 0.0
Assigned description: has a topic of knowledge management and ontology development. recall: 0.098 precision: 0.0
Assigned description: - "has a topic of machine intelligence definitions and tests. recall: 0.044 precision: 0.0
Assigned description: has a topic of artificial intelligence and definitions of intelligence. recall: 0.141 precision: 0.0
Assigned description: has a topic of computational methods in social network analysis. recall: 0.026 precision: 0.0
Assigned description: has a topic of simulation and philosophical implications of intelligence. recall: 0.038 precision: 0.0
Assigned description: has a topic of knowledge management and ontology. recall: 0.11 precision: 0.00909090909090909
Assigned description: has a topic of constraint satisfaction problems and algorithm design. recall: 0.475 precision: 0.00631578947368421
Assigned description: has a topic of machine intelligence and its definitions. recall: 0.186 precision: 0.0
Assigned description: has a topic of terrorism analysis and intelligence visualization. recall: 0.005 precision: 0.0
Assigned description: has a topic of evolutionary algorithms and cellular automata. recall: 0.005 precision: 0.0
Assigned description: has a topic of evolutionary theory and simulations. recall: 0.04 precision: 0.0
Assigned description: - "has a topic of fitness landscapes and optimization algorithms. recall: 0.043 precision: 0.0
Assigned description: has a topic of constraint satisfaction problems and propagators. recall: 0.172 precision: 0.0
Assigned description: - "has a topic of causality and definitions in artificial intelligence. recall: 0.122 precision: 0.0
Assigned description: has a topic of scientific simulations and philosophical implications. recall: 0.145 precision: 0.013793103448275862
Assigned description: has a topic of fuzzy logic in stability assessment of underground openings. recall: 0.001 precision: 0.0
Assigned description: has a topic of causality and default theories. recall: 0.048 precision: 0.08333333333333333
Assigned description: has a topic of knowledge management systems in organizational clusters. recall: 0.013 precision: 0.0
Assigned description: - "has a topic of covert node discovery in social networks. recall: 0.005 precision: 0.0
Assigned description: - "has a topic of social network analysis and covert nodes. recall: 0.014 precision: 0.0
Assigned description: has a topic of definitions and measures of intelligence. recall: 0.024 precision: 0.0
Assigned description: has a topic of hydrocyclone operation analysis using rough set theory. recall: 0.002 precision: 0.0
Assigned description: has a topic of mining methods and operational efficiency. recall: 0.107 precision: 0.028037383177570093
Assigned description: has a topic of terrorism and social network dynamics. recall: 0.002 precision: 0.0
Assigned description: has a topic of fuzzy logic and stability assessment in engineering. recall: 0.016 precision: 0.0
Assigned description: has a topic of computational methods in terrorism investigation. recall: 0.005 precision: 0.0
Assigned description: has a topic of simulation and philosophical implications. recall: 0.08 precision: 0.0
Assigned description: has a topic of social network analysis and covert node discovery. recall: 0.013 precision: 0.0
Assigned description: has a topic of fuzzy logic and stability assessment. recall: 0.049 precision: 0.0
Assigned description: has a topic of intelligence measurement and definitions in AI. recall: 0.049 precision: 0.0
Assigned description: has a topic of fuzzy logic and decision-making systems. recall: 0.102 precision: 0.00980392156862745
Assigned description: has a topic of intelligence definitions and machine learning. recall: 0.067 precision: 0.0
Assigned description: has a topic of rough set theory applications in data analysis. recall: 0.019 precision: 0.15789473684210525
Assigned description: has a topic of artificial intelligence and intelligence measurement. recall: 0.384 precision: 0.0078125
Assigned description: has a topic of fuzzy logic and fault classification. recall: 0.005 precision: 0.0
Assigned description: has a topic of constraint satisfaction problems and optimization. recall: 0.359 precision: 0.0
Assigned description: has a topic of fault classification using neuro-fuzzy models. recall: 0.004 precision: 0.0
Assigned description: has a topic of fitness landscapes and optimization. recall: 0.048 precision: 0.0
Assigned description: has a topic of constraint satisfaction problems and optimization techniques. recall: 0.388 precision: 0.0
Assigned description: has a topic of definitions and measurements of intelligence. recall: 0.016 precision: 0.0
Assigned description: has a topic of computational methods in terrorism analysis. recall: 0.006 precision: 0.0
Assigned description: has a topic of social network analysis and covert operations. recall: 0.004 precision: 0.0
Assigned description: has a topic of simulation and modeling of complex systems. recall: 0.289 precision: 0.010380622837370242
Assigned description: has a topic of fuzzy block theory and stability assessment. recall: 0.002 precision: 0.0
Assigned description: has a topic of fuzzy logic and its applications. recall: 0.073 precision: 0.0
Drop 22 because it cover 624 from 1000
Dropping 1 descriptions because they are too popular or too unpopular:
Stage select select
Unmatched count: 243
Description: has a topic of knowledge management and ontology development. Count: 98
Description: has a topic of causality and default theories. Count: 43
Description: - "has a topic of covert node discovery in social networks. Count: 5
Description: has a topic of rough set theory applications in data analysis. Count: 19
Description: has a topic of artificial intelligence and intelligence measurement. Count: 349
Description: has a topic of constraint satisfaction problems and optimization techniques. Count: 243
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

1.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

2.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

3.   An ensemble based approach for dealing with missing data, without predicting
or imputing the missing values is proposed. This technique is suitable for
online operations of neural networks and as a result, is used for online
condition monitoring. The proposed technique is tested in both classification
and regression problems. An ensemble of Fuzzy-ARTMAPs is used for
classification whereas an ensemble of multi-layer perceptrons is used for the
regression problem. Results obtained using this ensemble-based technique are
compared to those obtained using a combination of auto-associative neural
networks and genetic algorithms and findings show that this method can perform
up to 9% better in regression problems. Another advantage of the proposed
technique is that it eliminates the need for finding the best estimate of the
data, and hence, saves time.

4.   We consider hexagonal cellular automata with immediate cell neighbourhood and
three cell-states. Every cell calculates its next state depending on the
integral representation of states in its neighbourhood, i.e. how many
neighbours are in each one state. We employ evolutionary algorithms to breed
local transition functions that support mobile localizations (gliders), and
characterize sets of the functions selected in terms of quasi-chemical systems.
Analysis of the set of functions evolved allows to speculate that mobile
localizations are likely to emerge in the quasi-chemical systems with limited
diffusion of one reagent, a small number of molecules is required for
amplification of travelling localizations, and reactions leading to stationary
localizations involve relatively equal amount of quasi-chemical species.
Techniques developed can be applied in cascading signals in nature-inspired
spatially extended computing devices, and phenomenological studies and
classification of non-linear discrete systems.

5.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.

6.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

7.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

8.   We extend Knuth's 16 Boolean binary logic operators to fuzzy logic and
neutrosophic logic binary operators. Then we generalize them to n-ary fuzzy
logic and neutrosophic logic operators using the smarandache codification of
the Venn diagram and a defined vector neutrosophic law. In such way, new
operators in neutrosophic logic/set/probability are built.

9.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

10.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

11.   The paper describes the proposition and application of a local search
metaheuristic for multi-objective optimization problems. It is based on two
main principles of heuristic search, intensification through variable
neighborhoods, and diversification through perturbations and successive
iterations in favorable regions of the search space. The concept is
successfully tested on permutation flow shop scheduling problems under multiple
objectives. While the obtained results are encouraging in terms of their
quality, another positive attribute of the approach is its' simplicity as it
does require the setting of only very few parameters. The implementation of the
Pareto Iterated Local Search metaheuristic is based on the MOOPPS computer
system of local search heuristics for multi-objective scheduling which has been
awarded the European Academic Software Award 2002 in Ronneby, Sweden
(http://www.easa-award.net/, http://www.bth.se/llab/easa_2002.nsf)

12.   This paper deals with enriched qualitative belief functions for reasoning
under uncertainty and for combining information expressed in natural language
through linguistic labels. In this work, two possible enrichments (quantitative
and/or qualitative) of linguistic labels are considered and operators
(addition, multiplication, division, etc) for dealing with them are proposed
and explained. We denote them $qe$-operators, $qe$ standing for
"qualitative-enriched" operators. These operators can be seen as a direct
extension of the classical qualitative operators ($q$-operators) proposed
recently in the Dezert-Smarandache Theory of plausible and paradoxist reasoning
(DSmT). $q$-operators are also justified in details in this paper. The
quantitative enrichment of linguistic label is a numerical supporting degree in
$[0,\infty)$, while the qualitative enrichment takes its values in a finite
ordered set of linguistic values. Quantitative enrichment is less precise than
qualitative enrichment, but it is expected more close with what human experts
can easily provide when expressing linguistic labels with supporting degrees.
Two simple examples are given to show how the fusion of qualitative-enriched
belief assignments can be done.

13.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

14.   This paper has been withdrawn by the author due to extremely unscientific
errors.

15.   The textured images' classification assumes to consider the images in terms
of area with the same texture. In uncertain environment, it could be better to
take an imprecise decision or to reject the area corresponding to an unlearning
class. Moreover, on the areas that are the classification units, we can have
more than one texture. These considerations allows us to develop a belief
decision model permitting to reject an area as unlearning and to decide on
unions and intersections of learning classes. The proposed approach finds all
its justification in an application of seabed characterization from sonar
images, which contributes to an illustration.

16.   This paper has been withdrawn.

17.   This paper describes application of information granulation theory, on the
analysis of hydrocyclone perforamance. In this manner, using a combining of
Self Organizing Map (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and
fuzzy granules are obtained(briefly called SONFIS). Balancing of crisp granules
and sub fuzzy granules, within non fuzzy information (initial granulation), is
rendered in an open-close iteration. Using two criteria, "simplicity of rules
"and "adaptive threoshold error level", stability of algorithm is guaranteed.
Validation of the proposed method, on the data set of the hydrocyclone is
rendered.

18.   In this chapter, we propose a new practical codification of the elements of
the Venn diagram in order to easily manipulate the focal elements. In order to
reduce the complexity, the eventual constraints must be integrated in the
codification at the beginning. Hence, we only consider a reduced hyper power
set $D_r^\Theta$ that can be $2^\Theta$ or $D^\Theta$. We describe all the
steps of a general belief function framework. The step of decision is
particularly studied, indeed, when we can decide on intersections of the
singletons of the discernment space no actual decision functions are easily to
use. Hence, two approaches are proposed, an extension of previous one and an
approach based on the specificity of the elements on which to decide. The
principal goal of this chapter is to provide practical codes of a general
belief function framework for the researchers and users needing the belief
function theory.

19.   Data-stream clustering is an ever-expanding subdomain of knowledge
extraction. Most of the past and present research effort aims at efficient
scaling up for the huge data repositories. Our approach focuses on qualitative
improvement, mainly for "weak signals" detection and precise tracking of
topical evolutions in the framework of information watch - though scalability
is intrinsically guaranteed in a possibly distributed implementation. Our
GERMEN algorithm exhaustively picks up the whole set of density peaks of the
data at time t, by identifying the local perturbations induced by the current
document vector, such as changing cluster borders, or new/vanishing clusters.
Optimality yields from the uniqueness 1) of the density landscape for any value
of our zoom parameter, 2) of the cluster allocation operated by our border
propagation rule. This results in a rigorous independence from the data
presentation ranking or any initialization parameter. We present here as a
first step the only assessment of a static view resulting from one year of the
CNRS/INIST Pascal database in the field of geotechnics.

20.   Cluster matching by permuting cluster labels is important in many clustering
contexts such as cluster validation and cluster ensemble techniques. The
classic approach is to minimize the euclidean distance between two cluster
solutions which induces inappropriate stability in certain settings. Therefore,
we present the truematch algorithm that introduces two improvements best
explained in the crisp case. First, instead of maximizing the trace of the
cluster crosstable, we propose to maximize a chi-square transformation of this
crosstable. Thus, the trace will not be dominated by the cells with the largest
counts but by the cells with the most non-random observations, taking into
account the marginals. Second, we suggest a probabilistic component in order to
break ties and to make the matching algorithm truly random on random data. The
truematch algorithm is designed as a building block of the truecluster
framework and scales in polynomial time. First simulation results confirm that
the truematch algorithm gives more consistent truecluster results for unequal
cluster sizes. Free R software is available.

21.   Noise, corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust,
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise.

22.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.

23.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

24.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

25.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

26.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of structural analysis using neural networks"
"has a topic of clustering algorithms"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of artificial neural networks in earthquake response prediction"\n- "has a topic of information fusion and conflict redistribution"\n- "has a topic of linguistic evidence and unithood measurement"\n- "has a topic of ensemble methods for missing data in neural networks"\n- "has a topic of narrative analysis in film scripts"\n- "has a topic of fuzzy logic and neutrosophic operators"\n- "has a topic of qualitative belief functions and reasoning under uncertainty"\n- "has a topic of multi-objective optimization in scheduling problems"\n- "has a topic of data-stream clustering and weak signals detection"\n- "has a topic of face recognition robustness using neural networks"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   Noise, corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust,
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise.

1.   An ensemble based approach for dealing with missing data, without predicting
or imputing the missing values is proposed. This technique is suitable for
online operations of neural networks and as a result, is used for online
condition monitoring. The proposed technique is tested in both classification
and regression problems. An ensemble of Fuzzy-ARTMAPs is used for
classification whereas an ensemble of multi-layer perceptrons is used for the
regression problem. Results obtained using this ensemble-based technique are
compared to those obtained using a combination of auto-associative neural
networks and genetic algorithms and findings show that this method can perform
up to 9% better in regression problems. Another advantage of the proposed
technique is that it eliminates the need for finding the best estimate of the
data, and hence, saves time.

2.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

3.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

4.   Cluster matching by permuting cluster labels is important in many clustering
contexts such as cluster validation and cluster ensemble techniques. The
classic approach is to minimize the euclidean distance between two cluster
solutions which induces inappropriate stability in certain settings. Therefore,
we present the truematch algorithm that introduces two improvements best
explained in the crisp case. First, instead of maximizing the trace of the
cluster crosstable, we propose to maximize a chi-square transformation of this
crosstable. Thus, the trace will not be dominated by the cells with the largest
counts but by the cells with the most non-random observations, taking into
account the marginals. Second, we suggest a probabilistic component in order to
break ties and to make the matching algorithm truly random on random data. The
truematch algorithm is designed as a building block of the truecluster
framework and scales in polynomial time. First simulation results confirm that
the truematch algorithm gives more consistent truecluster results for unequal
cluster sizes. Free R software is available.

5.   The paper describes the proposition and application of a local search
metaheuristic for multi-objective optimization problems. It is based on two
main principles of heuristic search, intensification through variable
neighborhoods, and diversification through perturbations and successive
iterations in favorable regions of the search space. The concept is
successfully tested on permutation flow shop scheduling problems under multiple
objectives. While the obtained results are encouraging in terms of their
quality, another positive attribute of the approach is its' simplicity as it
does require the setting of only very few parameters. The implementation of the
Pareto Iterated Local Search metaheuristic is based on the MOOPPS computer
system of local search heuristics for multi-objective scheduling which has been
awarded the European Academic Software Award 2002 in Ronneby, Sweden
(http://www.easa-award.net/, http://www.bth.se/llab/easa_2002.nsf)

6.   This paper has been withdrawn.

7.   We consider hexagonal cellular automata with immediate cell neighbourhood and
three cell-states. Every cell calculates its next state depending on the
integral representation of states in its neighbourhood, i.e. how many
neighbours are in each one state. We employ evolutionary algorithms to breed
local transition functions that support mobile localizations (gliders), and
characterize sets of the functions selected in terms of quasi-chemical systems.
Analysis of the set of functions evolved allows to speculate that mobile
localizations are likely to emerge in the quasi-chemical systems with limited
diffusion of one reagent, a small number of molecules is required for
amplification of travelling localizations, and reactions leading to stationary
localizations involve relatively equal amount of quasi-chemical species.
Techniques developed can be applied in cascading signals in nature-inspired
spatially extended computing devices, and phenomenological studies and
classification of non-linear discrete systems.

8.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

9.   Data-stream clustering is an ever-expanding subdomain of knowledge
extraction. Most of the past and present research effort aims at efficient
scaling up for the huge data repositories. Our approach focuses on qualitative
improvement, mainly for "weak signals" detection and precise tracking of
topical evolutions in the framework of information watch - though scalability
is intrinsically guaranteed in a possibly distributed implementation. Our
GERMEN algorithm exhaustively picks up the whole set of density peaks of the
data at time t, by identifying the local perturbations induced by the current
document vector, such as changing cluster borders, or new/vanishing clusters.
Optimality yields from the uniqueness 1) of the density landscape for any value
of our zoom parameter, 2) of the cluster allocation operated by our border
propagation rule. This results in a rigorous independence from the data
presentation ranking or any initialization parameter. We present here as a
first step the only assessment of a static view resulting from one year of the
CNRS/INIST Pascal database in the field of geotechnics.

10.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.

11.   This paper describes application of information granulation theory, on the
analysis of hydrocyclone perforamance. In this manner, using a combining of
Self Organizing Map (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and
fuzzy granules are obtained(briefly called SONFIS). Balancing of crisp granules
and sub fuzzy granules, within non fuzzy information (initial granulation), is
rendered in an open-close iteration. Using two criteria, "simplicity of rules
"and "adaptive threoshold error level", stability of algorithm is guaranteed.
Validation of the proposed method, on the data set of the hydrocyclone is
rendered.

12.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

13.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

14.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

15.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

16.   In this chapter, we propose a new practical codification of the elements of
the Venn diagram in order to easily manipulate the focal elements. In order to
reduce the complexity, the eventual constraints must be integrated in the
codification at the beginning. Hence, we only consider a reduced hyper power
set $D_r^\Theta$ that can be $2^\Theta$ or $D^\Theta$. We describe all the
steps of a general belief function framework. The step of decision is
particularly studied, indeed, when we can decide on intersections of the
singletons of the discernment space no actual decision functions are easily to
use. Hence, two approaches are proposed, an extension of previous one and an
approach based on the specificity of the elements on which to decide. The
principal goal of this chapter is to provide practical codes of a general
belief function framework for the researchers and users needing the belief
function theory.

17.   This paper has been withdrawn by the author due to extremely unscientific
errors.

18.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

19.   This paper deals with enriched qualitative belief functions for reasoning
under uncertainty and for combining information expressed in natural language
through linguistic labels. In this work, two possible enrichments (quantitative
and/or qualitative) of linguistic labels are considered and operators
(addition, multiplication, division, etc) for dealing with them are proposed
and explained. We denote them $qe$-operators, $qe$ standing for
"qualitative-enriched" operators. These operators can be seen as a direct
extension of the classical qualitative operators ($q$-operators) proposed
recently in the Dezert-Smarandache Theory of plausible and paradoxist reasoning
(DSmT). $q$-operators are also justified in details in this paper. The
quantitative enrichment of linguistic label is a numerical supporting degree in
$[0,\infty)$, while the qualitative enrichment takes its values in a finite
ordered set of linguistic values. Quantitative enrichment is less precise than
qualitative enrichment, but it is expected more close with what human experts
can easily provide when expressing linguistic labels with supporting degrees.
Two simple examples are given to show how the fusion of qualitative-enriched
belief assignments can be done.

20.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

21.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

22.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

23.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

24.   The textured images' classification assumes to consider the images in terms
of area with the same texture. In uncertain environment, it could be better to
take an imprecise decision or to reject the area corresponding to an unlearning
class. Moreover, on the areas that are the classification units, we can have
more than one texture. These considerations allows us to develop a belief
decision model permitting to reject an area as unlearning and to decide on
unions and intersections of learning classes. The proposed approach finds all
its justification in an application of seabed characterization from sonar
images, which contributes to an illustration.

25.   We extend Knuth's 16 Boolean binary logic operators to fuzzy logic and
neutrosophic logic binary operators. Then we generalize them to n-ary fuzzy
logic and neutrosophic logic operators using the smarandache codification of
the Venn diagram and a defined vector neutrosophic law. In such way, new
operators in neutrosophic logic/set/probability are built.

26.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"- "has a topic of fuzzy logic applications"
"has a topic of multi-objective optimization"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of face recognition systems"\n- "has a topic of data mining challenges"\n- "has a topic of clustering algorithms"\n- "has a topic of information fusion techniques"\n- "has a topic of qualitative analysis in narratives"\n- "has a topic of belief function theory"\n- "has a topic of structural protein analysis"\n- "has a topic of conflict prediction in international relations"\n- "has a topic of hydrocyclone performance analysis"\n- "has a topic of neural network applications in engineering"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   This paper deals with enriched qualitative belief functions for reasoning
under uncertainty and for combining information expressed in natural language
through linguistic labels. In this work, two possible enrichments (quantitative
and/or qualitative) of linguistic labels are considered and operators
(addition, multiplication, division, etc) for dealing with them are proposed
and explained. We denote them $qe$-operators, $qe$ standing for
"qualitative-enriched" operators. These operators can be seen as a direct
extension of the classical qualitative operators ($q$-operators) proposed
recently in the Dezert-Smarandache Theory of plausible and paradoxist reasoning
(DSmT). $q$-operators are also justified in details in this paper. The
quantitative enrichment of linguistic label is a numerical supporting degree in
$[0,\infty)$, while the qualitative enrichment takes its values in a finite
ordered set of linguistic values. Quantitative enrichment is less precise than
qualitative enrichment, but it is expected more close with what human experts
can easily provide when expressing linguistic labels with supporting degrees.
Two simple examples are given to show how the fusion of qualitative-enriched
belief assignments can be done.

1.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

2.   The textured images' classification assumes to consider the images in terms
of area with the same texture. In uncertain environment, it could be better to
take an imprecise decision or to reject the area corresponding to an unlearning
class. Moreover, on the areas that are the classification units, we can have
more than one texture. These considerations allows us to develop a belief
decision model permitting to reject an area as unlearning and to decide on
unions and intersections of learning classes. The proposed approach finds all
its justification in an application of seabed characterization from sonar
images, which contributes to an illustration.

3.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

4.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

5.   In this chapter, we propose a new practical codification of the elements of
the Venn diagram in order to easily manipulate the focal elements. In order to
reduce the complexity, the eventual constraints must be integrated in the
codification at the beginning. Hence, we only consider a reduced hyper power
set $D_r^\Theta$ that can be $2^\Theta$ or $D^\Theta$. We describe all the
steps of a general belief function framework. The step of decision is
particularly studied, indeed, when we can decide on intersections of the
singletons of the discernment space no actual decision functions are easily to
use. Hence, two approaches are proposed, an extension of previous one and an
approach based on the specificity of the elements on which to decide. The
principal goal of this chapter is to provide practical codes of a general
belief function framework for the researchers and users needing the belief
function theory.

6.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

7.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

8.   An ensemble based approach for dealing with missing data, without predicting
or imputing the missing values is proposed. This technique is suitable for
online operations of neural networks and as a result, is used for online
condition monitoring. The proposed technique is tested in both classification
and regression problems. An ensemble of Fuzzy-ARTMAPs is used for
classification whereas an ensemble of multi-layer perceptrons is used for the
regression problem. Results obtained using this ensemble-based technique are
compared to those obtained using a combination of auto-associative neural
networks and genetic algorithms and findings show that this method can perform
up to 9% better in regression problems. Another advantage of the proposed
technique is that it eliminates the need for finding the best estimate of the
data, and hence, saves time.

9.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

10.   We extend Knuth's 16 Boolean binary logic operators to fuzzy logic and
neutrosophic logic binary operators. Then we generalize them to n-ary fuzzy
logic and neutrosophic logic operators using the smarandache codification of
the Venn diagram and a defined vector neutrosophic law. In such way, new
operators in neutrosophic logic/set/probability are built.

11.   We consider hexagonal cellular automata with immediate cell neighbourhood and
three cell-states. Every cell calculates its next state depending on the
integral representation of states in its neighbourhood, i.e. how many
neighbours are in each one state. We employ evolutionary algorithms to breed
local transition functions that support mobile localizations (gliders), and
characterize sets of the functions selected in terms of quasi-chemical systems.
Analysis of the set of functions evolved allows to speculate that mobile
localizations are likely to emerge in the quasi-chemical systems with limited
diffusion of one reagent, a small number of molecules is required for
amplification of travelling localizations, and reactions leading to stationary
localizations involve relatively equal amount of quasi-chemical species.
Techniques developed can be applied in cascading signals in nature-inspired
spatially extended computing devices, and phenomenological studies and
classification of non-linear discrete systems.

12.   This paper has been withdrawn by the author due to extremely unscientific
errors.

13.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.

14.   This paper describes application of information granulation theory, on the
analysis of hydrocyclone perforamance. In this manner, using a combining of
Self Organizing Map (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and
fuzzy granules are obtained(briefly called SONFIS). Balancing of crisp granules
and sub fuzzy granules, within non fuzzy information (initial granulation), is
rendered in an open-close iteration. Using two criteria, "simplicity of rules
"and "adaptive threoshold error level", stability of algorithm is guaranteed.
Validation of the proposed method, on the data set of the hydrocyclone is
rendered.

15.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.

16.   Data-stream clustering is an ever-expanding subdomain of knowledge
extraction. Most of the past and present research effort aims at efficient
scaling up for the huge data repositories. Our approach focuses on qualitative
improvement, mainly for "weak signals" detection and precise tracking of
topical evolutions in the framework of information watch - though scalability
is intrinsically guaranteed in a possibly distributed implementation. Our
GERMEN algorithm exhaustively picks up the whole set of density peaks of the
data at time t, by identifying the local perturbations induced by the current
document vector, such as changing cluster borders, or new/vanishing clusters.
Optimality yields from the uniqueness 1) of the density landscape for any value
of our zoom parameter, 2) of the cluster allocation operated by our border
propagation rule. This results in a rigorous independence from the data
presentation ranking or any initialization parameter. We present here as a
first step the only assessment of a static view resulting from one year of the
CNRS/INIST Pascal database in the field of geotechnics.

17.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

18.   This paper has been withdrawn.

19.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

20.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

21.   Cluster matching by permuting cluster labels is important in many clustering
contexts such as cluster validation and cluster ensemble techniques. The
classic approach is to minimize the euclidean distance between two cluster
solutions which induces inappropriate stability in certain settings. Therefore,
we present the truematch algorithm that introduces two improvements best
explained in the crisp case. First, instead of maximizing the trace of the
cluster crosstable, we propose to maximize a chi-square transformation of this
crosstable. Thus, the trace will not be dominated by the cells with the largest
counts but by the cells with the most non-random observations, taking into
account the marginals. Second, we suggest a probabilistic component in order to
break ties and to make the matching algorithm truly random on random data. The
truematch algorithm is designed as a building block of the truecluster
framework and scales in polynomial time. First simulation results confirm that
the truematch algorithm gives more consistent truecluster results for unequal
cluster sizes. Free R software is available.

22.   Noise, corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust,
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise.

23.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

24.   The paper describes the proposition and application of a local search
metaheuristic for multi-objective optimization problems. It is based on two
main principles of heuristic search, intensification through variable
neighborhoods, and diversification through perturbations and successive
iterations in favorable regions of the search space. The concept is
successfully tested on permutation flow shop scheduling problems under multiple
objectives. While the obtained results are encouraging in terms of their
quality, another positive attribute of the approach is its' simplicity as it
does require the setting of only very few parameters. The implementation of the
Pareto Iterated Local Search metaheuristic is based on the MOOPPS computer
system of local search heuristics for multi-objective scheduling which has been
awarded the European Academic Software Award 2002 in Ronneby, Sweden
(http://www.easa-award.net/, http://www.bth.se/llab/easa_2002.nsf)

25.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

26.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of structural analysis using neural networks"
"has a topic of belief function theory"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of belief function theory and information fusion"\n- "has a topic of neural networks in earthquake response prediction"\n- "has a topic of qualitative analysis in narrative structures"\n- "has a topic of fuzzy logic and optimization techniques"\n- "has a topic of data mining and clustering algorithms"\n- "has a topic of protein family detection using hidden Markov models"\n- "has a topic of unithood measurement in linguistic evidence"\n- "has a topic of hydrocyclone performance analysis using fuzzy theory"\n- "has a topic of conflict prediction in international relations"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   This paper deals with enriched qualitative belief functions for reasoning
under uncertainty and for combining information expressed in natural language
through linguistic labels. In this work, two possible enrichments (quantitative
and/or qualitative) of linguistic labels are considered and operators
(addition, multiplication, division, etc) for dealing with them are proposed
and explained. We denote them $qe$-operators, $qe$ standing for
"qualitative-enriched" operators. These operators can be seen as a direct
extension of the classical qualitative operators ($q$-operators) proposed
recently in the Dezert-Smarandache Theory of plausible and paradoxist reasoning
(DSmT). $q$-operators are also justified in details in this paper. The
quantitative enrichment of linguistic label is a numerical supporting degree in
$[0,\infty)$, while the qualitative enrichment takes its values in a finite
ordered set of linguistic values. Quantitative enrichment is less precise than
qualitative enrichment, but it is expected more close with what human experts
can easily provide when expressing linguistic labels with supporting degrees.
Two simple examples are given to show how the fusion of qualitative-enriched
belief assignments can be done.

1.   This paper has been withdrawn.

2.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.

3.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

4.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

5.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

6.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

7.   An ensemble based approach for dealing with missing data, without predicting
or imputing the missing values is proposed. This technique is suitable for
online operations of neural networks and as a result, is used for online
condition monitoring. The proposed technique is tested in both classification
and regression problems. An ensemble of Fuzzy-ARTMAPs is used for
classification whereas an ensemble of multi-layer perceptrons is used for the
regression problem. Results obtained using this ensemble-based technique are
compared to those obtained using a combination of auto-associative neural
networks and genetic algorithms and findings show that this method can perform
up to 9% better in regression problems. Another advantage of the proposed
technique is that it eliminates the need for finding the best estimate of the
data, and hence, saves time.

8.   In this chapter, we propose a new practical codification of the elements of
the Venn diagram in order to easily manipulate the focal elements. In order to
reduce the complexity, the eventual constraints must be integrated in the
codification at the beginning. Hence, we only consider a reduced hyper power
set $D_r^\Theta$ that can be $2^\Theta$ or $D^\Theta$. We describe all the
steps of a general belief function framework. The step of decision is
particularly studied, indeed, when we can decide on intersections of the
singletons of the discernment space no actual decision functions are easily to
use. Hence, two approaches are proposed, an extension of previous one and an
approach based on the specificity of the elements on which to decide. The
principal goal of this chapter is to provide practical codes of a general
belief function framework for the researchers and users needing the belief
function theory.

9.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

10.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.

11.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

12.   This paper has been withdrawn by the author due to extremely unscientific
errors.

13.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

14.   The textured images' classification assumes to consider the images in terms
of area with the same texture. In uncertain environment, it could be better to
take an imprecise decision or to reject the area corresponding to an unlearning
class. Moreover, on the areas that are the classification units, we can have
more than one texture. These considerations allows us to develop a belief
decision model permitting to reject an area as unlearning and to decide on
unions and intersections of learning classes. The proposed approach finds all
its justification in an application of seabed characterization from sonar
images, which contributes to an illustration.

15.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

16.   We consider hexagonal cellular automata with immediate cell neighbourhood and
three cell-states. Every cell calculates its next state depending on the
integral representation of states in its neighbourhood, i.e. how many
neighbours are in each one state. We employ evolutionary algorithms to breed
local transition functions that support mobile localizations (gliders), and
characterize sets of the functions selected in terms of quasi-chemical systems.
Analysis of the set of functions evolved allows to speculate that mobile
localizations are likely to emerge in the quasi-chemical systems with limited
diffusion of one reagent, a small number of molecules is required for
amplification of travelling localizations, and reactions leading to stationary
localizations involve relatively equal amount of quasi-chemical species.
Techniques developed can be applied in cascading signals in nature-inspired
spatially extended computing devices, and phenomenological studies and
classification of non-linear discrete systems.

17.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

18.   Cluster matching by permuting cluster labels is important in many clustering
contexts such as cluster validation and cluster ensemble techniques. The
classic approach is to minimize the euclidean distance between two cluster
solutions which induces inappropriate stability in certain settings. Therefore,
we present the truematch algorithm that introduces two improvements best
explained in the crisp case. First, instead of maximizing the trace of the
cluster crosstable, we propose to maximize a chi-square transformation of this
crosstable. Thus, the trace will not be dominated by the cells with the largest
counts but by the cells with the most non-random observations, taking into
account the marginals. Second, we suggest a probabilistic component in order to
break ties and to make the matching algorithm truly random on random data. The
truematch algorithm is designed as a building block of the truecluster
framework and scales in polynomial time. First simulation results confirm that
the truematch algorithm gives more consistent truecluster results for unequal
cluster sizes. Free R software is available.

19.   We extend Knuth's 16 Boolean binary logic operators to fuzzy logic and
neutrosophic logic binary operators. Then we generalize them to n-ary fuzzy
logic and neutrosophic logic operators using the smarandache codification of
the Venn diagram and a defined vector neutrosophic law. In such way, new
operators in neutrosophic logic/set/probability are built.

20.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

21.   Noise, corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust,
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise.

22.   This paper describes application of information granulation theory, on the
analysis of hydrocyclone perforamance. In this manner, using a combining of
Self Organizing Map (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and
fuzzy granules are obtained(briefly called SONFIS). Balancing of crisp granules
and sub fuzzy granules, within non fuzzy information (initial granulation), is
rendered in an open-close iteration. Using two criteria, "simplicity of rules
"and "adaptive threoshold error level", stability of algorithm is guaranteed.
Validation of the proposed method, on the data set of the hydrocyclone is
rendered.

23.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

24.   Data-stream clustering is an ever-expanding subdomain of knowledge
extraction. Most of the past and present research effort aims at efficient
scaling up for the huge data repositories. Our approach focuses on qualitative
improvement, mainly for "weak signals" detection and precise tracking of
topical evolutions in the framework of information watch - though scalability
is intrinsically guaranteed in a possibly distributed implementation. Our
GERMEN algorithm exhaustively picks up the whole set of density peaks of the
data at time t, by identifying the local perturbations induced by the current
document vector, such as changing cluster borders, or new/vanishing clusters.
Optimality yields from the uniqueness 1) of the density landscape for any value
of our zoom parameter, 2) of the cluster allocation operated by our border
propagation rule. This results in a rigorous independence from the data
presentation ranking or any initialization parameter. We present here as a
first step the only assessment of a static view resulting from one year of the
CNRS/INIST Pascal database in the field of geotechnics.

25.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

26.   The paper describes the proposition and application of a local search
metaheuristic for multi-objective optimization problems. It is based on two
main principles of heuristic search, intensification through variable
neighborhoods, and diversification through perturbations and successive
iterations in favorable regions of the search space. The concept is
successfully tested on permutation flow shop scheduling problems under multiple
objectives. While the obtained results are encouraging in terms of their
quality, another positive attribute of the approach is its' simplicity as it
does require the setting of only very few parameters. The implementation of the
Pareto Iterated Local Search metaheuristic is based on the MOOPPS computer
system of local search heuristics for multi-objective scheduling which has been
awarded the European Academic Software Award 2002 in Ronneby, Sweden
(http://www.easa-award.net/, http://www.bth.se/llab/easa_2002.nsf)


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"- "has a topic of fuzzy logic applications"
"has a topic of multi-objective optimization"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of belief function theory"\n- "has a topic of neural network applications"\n- "has a topic of data mining techniques"\n- "has a topic of clustering algorithms"\n- "has a topic of narrative analysis in film"\n- "has a topic of conflict prediction in international relations"\n- "has a topic of fuzzy decision-making models"\n- "has a topic of information fusion methods"\n- "has a topic of hydrocyclone performance analysis"\n- "has a topic of protein family detection using hidden Markov models"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   In this chapter, we propose a new practical codification of the elements of
the Venn diagram in order to easily manipulate the focal elements. In order to
reduce the complexity, the eventual constraints must be integrated in the
codification at the beginning. Hence, we only consider a reduced hyper power
set $D_r^\Theta$ that can be $2^\Theta$ or $D^\Theta$. We describe all the
steps of a general belief function framework. The step of decision is
particularly studied, indeed, when we can decide on intersections of the
singletons of the discernment space no actual decision functions are easily to
use. Hence, two approaches are proposed, an extension of previous one and an
approach based on the specificity of the elements on which to decide. The
principal goal of this chapter is to provide practical codes of a general
belief function framework for the researchers and users needing the belief
function theory.

1.   We consider hexagonal cellular automata with immediate cell neighbourhood and
three cell-states. Every cell calculates its next state depending on the
integral representation of states in its neighbourhood, i.e. how many
neighbours are in each one state. We employ evolutionary algorithms to breed
local transition functions that support mobile localizations (gliders), and
characterize sets of the functions selected in terms of quasi-chemical systems.
Analysis of the set of functions evolved allows to speculate that mobile
localizations are likely to emerge in the quasi-chemical systems with limited
diffusion of one reagent, a small number of molecules is required for
amplification of travelling localizations, and reactions leading to stationary
localizations involve relatively equal amount of quasi-chemical species.
Techniques developed can be applied in cascading signals in nature-inspired
spatially extended computing devices, and phenomenological studies and
classification of non-linear discrete systems.

2.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

3.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

4.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.

5.   We extend Knuth's 16 Boolean binary logic operators to fuzzy logic and
neutrosophic logic binary operators. Then we generalize them to n-ary fuzzy
logic and neutrosophic logic operators using the smarandache codification of
the Venn diagram and a defined vector neutrosophic law. In such way, new
operators in neutrosophic logic/set/probability are built.

6.   This paper has been withdrawn by the author due to extremely unscientific
errors.

7.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

8.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

9.   This paper deals with enriched qualitative belief functions for reasoning
under uncertainty and for combining information expressed in natural language
through linguistic labels. In this work, two possible enrichments (quantitative
and/or qualitative) of linguistic labels are considered and operators
(addition, multiplication, division, etc) for dealing with them are proposed
and explained. We denote them $qe$-operators, $qe$ standing for
"qualitative-enriched" operators. These operators can be seen as a direct
extension of the classical qualitative operators ($q$-operators) proposed
recently in the Dezert-Smarandache Theory of plausible and paradoxist reasoning
(DSmT). $q$-operators are also justified in details in this paper. The
quantitative enrichment of linguistic label is a numerical supporting degree in
$[0,\infty)$, while the qualitative enrichment takes its values in a finite
ordered set of linguistic values. Quantitative enrichment is less precise than
qualitative enrichment, but it is expected more close with what human experts
can easily provide when expressing linguistic labels with supporting degrees.
Two simple examples are given to show how the fusion of qualitative-enriched
belief assignments can be done.

10.   Noise, corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust,
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise.

11.   Cluster matching by permuting cluster labels is important in many clustering
contexts such as cluster validation and cluster ensemble techniques. The
classic approach is to minimize the euclidean distance between two cluster
solutions which induces inappropriate stability in certain settings. Therefore,
we present the truematch algorithm that introduces two improvements best
explained in the crisp case. First, instead of maximizing the trace of the
cluster crosstable, we propose to maximize a chi-square transformation of this
crosstable. Thus, the trace will not be dominated by the cells with the largest
counts but by the cells with the most non-random observations, taking into
account the marginals. Second, we suggest a probabilistic component in order to
break ties and to make the matching algorithm truly random on random data. The
truematch algorithm is designed as a building block of the truecluster
framework and scales in polynomial time. First simulation results confirm that
the truematch algorithm gives more consistent truecluster results for unequal
cluster sizes. Free R software is available.

12.   The paper describes the proposition and application of a local search
metaheuristic for multi-objective optimization problems. It is based on two
main principles of heuristic search, intensification through variable
neighborhoods, and diversification through perturbations and successive
iterations in favorable regions of the search space. The concept is
successfully tested on permutation flow shop scheduling problems under multiple
objectives. While the obtained results are encouraging in terms of their
quality, another positive attribute of the approach is its' simplicity as it
does require the setting of only very few parameters. The implementation of the
Pareto Iterated Local Search metaheuristic is based on the MOOPPS computer
system of local search heuristics for multi-objective scheduling which has been
awarded the European Academic Software Award 2002 in Ronneby, Sweden
(http://www.easa-award.net/, http://www.bth.se/llab/easa_2002.nsf)

13.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

14.   The textured images' classification assumes to consider the images in terms
of area with the same texture. In uncertain environment, it could be better to
take an imprecise decision or to reject the area corresponding to an unlearning
class. Moreover, on the areas that are the classification units, we can have
more than one texture. These considerations allows us to develop a belief
decision model permitting to reject an area as unlearning and to decide on
unions and intersections of learning classes. The proposed approach finds all
its justification in an application of seabed characterization from sonar
images, which contributes to an illustration.

15.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.

16.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

17.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

18.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

19.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

20.   Data-stream clustering is an ever-expanding subdomain of knowledge
extraction. Most of the past and present research effort aims at efficient
scaling up for the huge data repositories. Our approach focuses on qualitative
improvement, mainly for "weak signals" detection and precise tracking of
topical evolutions in the framework of information watch - though scalability
is intrinsically guaranteed in a possibly distributed implementation. Our
GERMEN algorithm exhaustively picks up the whole set of density peaks of the
data at time t, by identifying the local perturbations induced by the current
document vector, such as changing cluster borders, or new/vanishing clusters.
Optimality yields from the uniqueness 1) of the density landscape for any value
of our zoom parameter, 2) of the cluster allocation operated by our border
propagation rule. This results in a rigorous independence from the data
presentation ranking or any initialization parameter. We present here as a
first step the only assessment of a static view resulting from one year of the
CNRS/INIST Pascal database in the field of geotechnics.

21.   This paper describes application of information granulation theory, on the
analysis of hydrocyclone perforamance. In this manner, using a combining of
Self Organizing Map (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and
fuzzy granules are obtained(briefly called SONFIS). Balancing of crisp granules
and sub fuzzy granules, within non fuzzy information (initial granulation), is
rendered in an open-close iteration. Using two criteria, "simplicity of rules
"and "adaptive threoshold error level", stability of algorithm is guaranteed.
Validation of the proposed method, on the data set of the hydrocyclone is
rendered.

22.   This paper has been withdrawn.

23.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

24.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

25.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

26.   An ensemble based approach for dealing with missing data, without predicting
or imputing the missing values is proposed. This technique is suitable for
online operations of neural networks and as a result, is used for online
condition monitoring. The proposed technique is tested in both classification
and regression problems. An ensemble of Fuzzy-ARTMAPs is used for
classification whereas an ensemble of multi-layer perceptrons is used for the
regression problem. Results obtained using this ensemble-based technique are
compared to those obtained using a combination of auto-associative neural
networks and genetic algorithms and findings show that this method can perform
up to 9% better in regression problems. Another advantage of the proposed
technique is that it eliminates the need for finding the best estimate of the
data, and hence, saves time.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of narrative analysis in film scripts"
"has a topic of information fusion techniques"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of belief function theory"\n- "has a topic of clustering algorithms"\n- "has a topic of fuzzy logic and its applications"\n- "has a topic of protein family detection using hidden Markov models"\n- "has a topic of optimization techniques in scheduling"\n- "has a topic of qualitative analysis in narrative structures"\n- "has a topic of neural networks in earthquake response prediction"\n- "has a topic of multi-objective optimization methods"\n- "has a topic of term extraction and linguistic evidence"\n- "has a topic of data mining and dynamic clustering"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

1.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

2.   An ensemble based approach for dealing with missing data, without predicting
or imputing the missing values is proposed. This technique is suitable for
online operations of neural networks and as a result, is used for online
condition monitoring. The proposed technique is tested in both classification
and regression problems. An ensemble of Fuzzy-ARTMAPs is used for
classification whereas an ensemble of multi-layer perceptrons is used for the
regression problem. Results obtained using this ensemble-based technique are
compared to those obtained using a combination of auto-associative neural
networks and genetic algorithms and findings show that this method can perform
up to 9% better in regression problems. Another advantage of the proposed
technique is that it eliminates the need for finding the best estimate of the
data, and hence, saves time.

3.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

4.   We consider hexagonal cellular automata with immediate cell neighbourhood and
three cell-states. Every cell calculates its next state depending on the
integral representation of states in its neighbourhood, i.e. how many
neighbours are in each one state. We employ evolutionary algorithms to breed
local transition functions that support mobile localizations (gliders), and
characterize sets of the functions selected in terms of quasi-chemical systems.
Analysis of the set of functions evolved allows to speculate that mobile
localizations are likely to emerge in the quasi-chemical systems with limited
diffusion of one reagent, a small number of molecules is required for
amplification of travelling localizations, and reactions leading to stationary
localizations involve relatively equal amount of quasi-chemical species.
Techniques developed can be applied in cascading signals in nature-inspired
spatially extended computing devices, and phenomenological studies and
classification of non-linear discrete systems.

5.   Noise, corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust,
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise.

6.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

7.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

8.   Data-stream clustering is an ever-expanding subdomain of knowledge
extraction. Most of the past and present research effort aims at efficient
scaling up for the huge data repositories. Our approach focuses on qualitative
improvement, mainly for "weak signals" detection and precise tracking of
topical evolutions in the framework of information watch - though scalability
is intrinsically guaranteed in a possibly distributed implementation. Our
GERMEN algorithm exhaustively picks up the whole set of density peaks of the
data at time t, by identifying the local perturbations induced by the current
document vector, such as changing cluster borders, or new/vanishing clusters.
Optimality yields from the uniqueness 1) of the density landscape for any value
of our zoom parameter, 2) of the cluster allocation operated by our border
propagation rule. This results in a rigorous independence from the data
presentation ranking or any initialization parameter. We present here as a
first step the only assessment of a static view resulting from one year of the
CNRS/INIST Pascal database in the field of geotechnics.

9.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

10.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

11.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

12.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

13.   We extend Knuth's 16 Boolean binary logic operators to fuzzy logic and
neutrosophic logic binary operators. Then we generalize them to n-ary fuzzy
logic and neutrosophic logic operators using the smarandache codification of
the Venn diagram and a defined vector neutrosophic law. In such way, new
operators in neutrosophic logic/set/probability are built.

14.   In this chapter, we propose a new practical codification of the elements of
the Venn diagram in order to easily manipulate the focal elements. In order to
reduce the complexity, the eventual constraints must be integrated in the
codification at the beginning. Hence, we only consider a reduced hyper power
set $D_r^\Theta$ that can be $2^\Theta$ or $D^\Theta$. We describe all the
steps of a general belief function framework. The step of decision is
particularly studied, indeed, when we can decide on intersections of the
singletons of the discernment space no actual decision functions are easily to
use. Hence, two approaches are proposed, an extension of previous one and an
approach based on the specificity of the elements on which to decide. The
principal goal of this chapter is to provide practical codes of a general
belief function framework for the researchers and users needing the belief
function theory.

15.   This paper deals with enriched qualitative belief functions for reasoning
under uncertainty and for combining information expressed in natural language
through linguistic labels. In this work, two possible enrichments (quantitative
and/or qualitative) of linguistic labels are considered and operators
(addition, multiplication, division, etc) for dealing with them are proposed
and explained. We denote them $qe$-operators, $qe$ standing for
"qualitative-enriched" operators. These operators can be seen as a direct
extension of the classical qualitative operators ($q$-operators) proposed
recently in the Dezert-Smarandache Theory of plausible and paradoxist reasoning
(DSmT). $q$-operators are also justified in details in this paper. The
quantitative enrichment of linguistic label is a numerical supporting degree in
$[0,\infty)$, while the qualitative enrichment takes its values in a finite
ordered set of linguistic values. Quantitative enrichment is less precise than
qualitative enrichment, but it is expected more close with what human experts
can easily provide when expressing linguistic labels with supporting degrees.
Two simple examples are given to show how the fusion of qualitative-enriched
belief assignments can be done.

16.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.

17.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

18.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.

19.   The paper describes the proposition and application of a local search
metaheuristic for multi-objective optimization problems. It is based on two
main principles of heuristic search, intensification through variable
neighborhoods, and diversification through perturbations and successive
iterations in favorable regions of the search space. The concept is
successfully tested on permutation flow shop scheduling problems under multiple
objectives. While the obtained results are encouraging in terms of their
quality, another positive attribute of the approach is its' simplicity as it
does require the setting of only very few parameters. The implementation of the
Pareto Iterated Local Search metaheuristic is based on the MOOPPS computer
system of local search heuristics for multi-objective scheduling which has been
awarded the European Academic Software Award 2002 in Ronneby, Sweden
(http://www.easa-award.net/, http://www.bth.se/llab/easa_2002.nsf)

20.   This paper has been withdrawn by the author due to extremely unscientific
errors.

21.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

22.   Cluster matching by permuting cluster labels is important in many clustering
contexts such as cluster validation and cluster ensemble techniques. The
classic approach is to minimize the euclidean distance between two cluster
solutions which induces inappropriate stability in certain settings. Therefore,
we present the truematch algorithm that introduces two improvements best
explained in the crisp case. First, instead of maximizing the trace of the
cluster crosstable, we propose to maximize a chi-square transformation of this
crosstable. Thus, the trace will not be dominated by the cells with the largest
counts but by the cells with the most non-random observations, taking into
account the marginals. Second, we suggest a probabilistic component in order to
break ties and to make the matching algorithm truly random on random data. The
truematch algorithm is designed as a building block of the truecluster
framework and scales in polynomial time. First simulation results confirm that
the truematch algorithm gives more consistent truecluster results for unequal
cluster sizes. Free R software is available.

23.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

24.   The textured images' classification assumes to consider the images in terms
of area with the same texture. In uncertain environment, it could be better to
take an imprecise decision or to reject the area corresponding to an unlearning
class. Moreover, on the areas that are the classification units, we can have
more than one texture. These considerations allows us to develop a belief
decision model permitting to reject an area as unlearning and to decide on
unions and intersections of learning classes. The proposed approach finds all
its justification in an application of seabed characterization from sonar
images, which contributes to an illustration.

25.   This paper describes application of information granulation theory, on the
analysis of hydrocyclone perforamance. In this manner, using a combining of
Self Organizing Map (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and
fuzzy granules are obtained(briefly called SONFIS). Balancing of crisp granules
and sub fuzzy granules, within non fuzzy information (initial granulation), is
rendered in an open-close iteration. Using two criteria, "simplicity of rules
"and "adaptive threoshold error level", stability of algorithm is guaranteed.
Validation of the proposed method, on the data set of the hydrocyclone is
rendered.

26.   This paper has been withdrawn.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of clustering algorithms"
"has a topic of structural analysis using neural networks"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of data mining challenges"\n- "has a topic of information fusion techniques"\n- "has a topic of missing data handling in neural networks"\n- "has a topic of belief function theory"\n- "has a topic of qualitative analysis in narrative structures"\n- "has a topic of optimization algorithms for multi-objective problems"\n- "has a topic of face recognition robustness"\n- "has a topic of protein family detection using hidden Markov models"\n- "has a topic of fuzzy logic applications"\n- "has a topic of hydrocyclone performance analysis"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

1.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

2.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

3.   We extend Knuth's 16 Boolean binary logic operators to fuzzy logic and
neutrosophic logic binary operators. Then we generalize them to n-ary fuzzy
logic and neutrosophic logic operators using the smarandache codification of
the Venn diagram and a defined vector neutrosophic law. In such way, new
operators in neutrosophic logic/set/probability are built.

4.   Noise, corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust,
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise.

5.   An ensemble based approach for dealing with missing data, without predicting
or imputing the missing values is proposed. This technique is suitable for
online operations of neural networks and as a result, is used for online
condition monitoring. The proposed technique is tested in both classification
and regression problems. An ensemble of Fuzzy-ARTMAPs is used for
classification whereas an ensemble of multi-layer perceptrons is used for the
regression problem. Results obtained using this ensemble-based technique are
compared to those obtained using a combination of auto-associative neural
networks and genetic algorithms and findings show that this method can perform
up to 9% better in regression problems. Another advantage of the proposed
technique is that it eliminates the need for finding the best estimate of the
data, and hence, saves time.

6.   The textured images' classification assumes to consider the images in terms
of area with the same texture. In uncertain environment, it could be better to
take an imprecise decision or to reject the area corresponding to an unlearning
class. Moreover, on the areas that are the classification units, we can have
more than one texture. These considerations allows us to develop a belief
decision model permitting to reject an area as unlearning and to decide on
unions and intersections of learning classes. The proposed approach finds all
its justification in an application of seabed characterization from sonar
images, which contributes to an illustration.

7.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.

8.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

9.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.

10.   This paper has been withdrawn by the author due to extremely unscientific
errors.

11.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

12.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

13.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

14.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

15.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

16.   This paper describes application of information granulation theory, on the
analysis of hydrocyclone perforamance. In this manner, using a combining of
Self Organizing Map (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and
fuzzy granules are obtained(briefly called SONFIS). Balancing of crisp granules
and sub fuzzy granules, within non fuzzy information (initial granulation), is
rendered in an open-close iteration. Using two criteria, "simplicity of rules
"and "adaptive threoshold error level", stability of algorithm is guaranteed.
Validation of the proposed method, on the data set of the hydrocyclone is
rendered.

17.   In this chapter, we propose a new practical codification of the elements of
the Venn diagram in order to easily manipulate the focal elements. In order to
reduce the complexity, the eventual constraints must be integrated in the
codification at the beginning. Hence, we only consider a reduced hyper power
set $D_r^\Theta$ that can be $2^\Theta$ or $D^\Theta$. We describe all the
steps of a general belief function framework. The step of decision is
particularly studied, indeed, when we can decide on intersections of the
singletons of the discernment space no actual decision functions are easily to
use. Hence, two approaches are proposed, an extension of previous one and an
approach based on the specificity of the elements on which to decide. The
principal goal of this chapter is to provide practical codes of a general
belief function framework for the researchers and users needing the belief
function theory.

18.   Data-stream clustering is an ever-expanding subdomain of knowledge
extraction. Most of the past and present research effort aims at efficient
scaling up for the huge data repositories. Our approach focuses on qualitative
improvement, mainly for "weak signals" detection and precise tracking of
topical evolutions in the framework of information watch - though scalability
is intrinsically guaranteed in a possibly distributed implementation. Our
GERMEN algorithm exhaustively picks up the whole set of density peaks of the
data at time t, by identifying the local perturbations induced by the current
document vector, such as changing cluster borders, or new/vanishing clusters.
Optimality yields from the uniqueness 1) of the density landscape for any value
of our zoom parameter, 2) of the cluster allocation operated by our border
propagation rule. This results in a rigorous independence from the data
presentation ranking or any initialization parameter. We present here as a
first step the only assessment of a static view resulting from one year of the
CNRS/INIST Pascal database in the field of geotechnics.

19.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

20.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

21.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

22.   This paper deals with enriched qualitative belief functions for reasoning
under uncertainty and for combining information expressed in natural language
through linguistic labels. In this work, two possible enrichments (quantitative
and/or qualitative) of linguistic labels are considered and operators
(addition, multiplication, division, etc) for dealing with them are proposed
and explained. We denote them $qe$-operators, $qe$ standing for
"qualitative-enriched" operators. These operators can be seen as a direct
extension of the classical qualitative operators ($q$-operators) proposed
recently in the Dezert-Smarandache Theory of plausible and paradoxist reasoning
(DSmT). $q$-operators are also justified in details in this paper. The
quantitative enrichment of linguistic label is a numerical supporting degree in
$[0,\infty)$, while the qualitative enrichment takes its values in a finite
ordered set of linguistic values. Quantitative enrichment is less precise than
qualitative enrichment, but it is expected more close with what human experts
can easily provide when expressing linguistic labels with supporting degrees.
Two simple examples are given to show how the fusion of qualitative-enriched
belief assignments can be done.

23.   We consider hexagonal cellular automata with immediate cell neighbourhood and
three cell-states. Every cell calculates its next state depending on the
integral representation of states in its neighbourhood, i.e. how many
neighbours are in each one state. We employ evolutionary algorithms to breed
local transition functions that support mobile localizations (gliders), and
characterize sets of the functions selected in terms of quasi-chemical systems.
Analysis of the set of functions evolved allows to speculate that mobile
localizations are likely to emerge in the quasi-chemical systems with limited
diffusion of one reagent, a small number of molecules is required for
amplification of travelling localizations, and reactions leading to stationary
localizations involve relatively equal amount of quasi-chemical species.
Techniques developed can be applied in cascading signals in nature-inspired
spatially extended computing devices, and phenomenological studies and
classification of non-linear discrete systems.

24.   The paper describes the proposition and application of a local search
metaheuristic for multi-objective optimization problems. It is based on two
main principles of heuristic search, intensification through variable
neighborhoods, and diversification through perturbations and successive
iterations in favorable regions of the search space. The concept is
successfully tested on permutation flow shop scheduling problems under multiple
objectives. While the obtained results are encouraging in terms of their
quality, another positive attribute of the approach is its' simplicity as it
does require the setting of only very few parameters. The implementation of the
Pareto Iterated Local Search metaheuristic is based on the MOOPPS computer
system of local search heuristics for multi-objective scheduling which has been
awarded the European Academic Software Award 2002 in Ronneby, Sweden
(http://www.easa-award.net/, http://www.bth.se/llab/easa_2002.nsf)

25.   Cluster matching by permuting cluster labels is important in many clustering
contexts such as cluster validation and cluster ensemble techniques. The
classic approach is to minimize the euclidean distance between two cluster
solutions which induces inappropriate stability in certain settings. Therefore,
we present the truematch algorithm that introduces two improvements best
explained in the crisp case. First, instead of maximizing the trace of the
cluster crosstable, we propose to maximize a chi-square transformation of this
crosstable. Thus, the trace will not be dominated by the cells with the largest
counts but by the cells with the most non-random observations, taking into
account the marginals. Second, we suggest a probabilistic component in order to
break ties and to make the matching algorithm truly random on random data. The
truematch algorithm is designed as a building block of the truecluster
framework and scales in polynomial time. First simulation results confirm that
the truematch algorithm gives more consistent truecluster results for unequal
cluster sizes. Free R software is available.

26.   This paper has been withdrawn.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of multi-objective optimization"
"- "has a topic of fuzzy logic applications"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of neural networks in structural engineering"\n- "has a topic of fuzzy logic and decision-making"\n- "has a topic of belief function theory and information fusion"\n- "has a topic of qualitative analysis in narrative structures"\n- "has a topic of data mining and clustering algorithms"\n- "has a topic of optimization techniques in scheduling"\n- "has a topic of conflict prediction in international relations"\n- "has a topic of image processing and recognition systems"\n- "has a topic of statistical measures in term extraction"\n- "has a topic of hydrocyclone performance analysis"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   This paper deals with enriched qualitative belief functions for reasoning
under uncertainty and for combining information expressed in natural language
through linguistic labels. In this work, two possible enrichments (quantitative
and/or qualitative) of linguistic labels are considered and operators
(addition, multiplication, division, etc) for dealing with them are proposed
and explained. We denote them $qe$-operators, $qe$ standing for
"qualitative-enriched" operators. These operators can be seen as a direct
extension of the classical qualitative operators ($q$-operators) proposed
recently in the Dezert-Smarandache Theory of plausible and paradoxist reasoning
(DSmT). $q$-operators are also justified in details in this paper. The
quantitative enrichment of linguistic label is a numerical supporting degree in
$[0,\infty)$, while the qualitative enrichment takes its values in a finite
ordered set of linguistic values. Quantitative enrichment is less precise than
qualitative enrichment, but it is expected more close with what human experts
can easily provide when expressing linguistic labels with supporting degrees.
Two simple examples are given to show how the fusion of qualitative-enriched
belief assignments can be done.

1.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

2.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

3.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

4.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

5.   Noise, corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust,
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise.

6.   An ensemble based approach for dealing with missing data, without predicting
or imputing the missing values is proposed. This technique is suitable for
online operations of neural networks and as a result, is used for online
condition monitoring. The proposed technique is tested in both classification
and regression problems. An ensemble of Fuzzy-ARTMAPs is used for
classification whereas an ensemble of multi-layer perceptrons is used for the
regression problem. Results obtained using this ensemble-based technique are
compared to those obtained using a combination of auto-associative neural
networks and genetic algorithms and findings show that this method can perform
up to 9% better in regression problems. Another advantage of the proposed
technique is that it eliminates the need for finding the best estimate of the
data, and hence, saves time.

7.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

8.   Data-stream clustering is an ever-expanding subdomain of knowledge
extraction. Most of the past and present research effort aims at efficient
scaling up for the huge data repositories. Our approach focuses on qualitative
improvement, mainly for "weak signals" detection and precise tracking of
topical evolutions in the framework of information watch - though scalability
is intrinsically guaranteed in a possibly distributed implementation. Our
GERMEN algorithm exhaustively picks up the whole set of density peaks of the
data at time t, by identifying the local perturbations induced by the current
document vector, such as changing cluster borders, or new/vanishing clusters.
Optimality yields from the uniqueness 1) of the density landscape for any value
of our zoom parameter, 2) of the cluster allocation operated by our border
propagation rule. This results in a rigorous independence from the data
presentation ranking or any initialization parameter. We present here as a
first step the only assessment of a static view resulting from one year of the
CNRS/INIST Pascal database in the field of geotechnics.

9.   This paper has been withdrawn by the author due to extremely unscientific
errors.

10.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.

11.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

12.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

13.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

14.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

15.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

16.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

17.   The textured images' classification assumes to consider the images in terms
of area with the same texture. In uncertain environment, it could be better to
take an imprecise decision or to reject the area corresponding to an unlearning
class. Moreover, on the areas that are the classification units, we can have
more than one texture. These considerations allows us to develop a belief
decision model permitting to reject an area as unlearning and to decide on
unions and intersections of learning classes. The proposed approach finds all
its justification in an application of seabed characterization from sonar
images, which contributes to an illustration.

18.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

19.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.

20.   This paper has been withdrawn.

21.   We consider hexagonal cellular automata with immediate cell neighbourhood and
three cell-states. Every cell calculates its next state depending on the
integral representation of states in its neighbourhood, i.e. how many
neighbours are in each one state. We employ evolutionary algorithms to breed
local transition functions that support mobile localizations (gliders), and
characterize sets of the functions selected in terms of quasi-chemical systems.
Analysis of the set of functions evolved allows to speculate that mobile
localizations are likely to emerge in the quasi-chemical systems with limited
diffusion of one reagent, a small number of molecules is required for
amplification of travelling localizations, and reactions leading to stationary
localizations involve relatively equal amount of quasi-chemical species.
Techniques developed can be applied in cascading signals in nature-inspired
spatially extended computing devices, and phenomenological studies and
classification of non-linear discrete systems.

22.   This paper describes application of information granulation theory, on the
analysis of hydrocyclone perforamance. In this manner, using a combining of
Self Organizing Map (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and
fuzzy granules are obtained(briefly called SONFIS). Balancing of crisp granules
and sub fuzzy granules, within non fuzzy information (initial granulation), is
rendered in an open-close iteration. Using two criteria, "simplicity of rules
"and "adaptive threoshold error level", stability of algorithm is guaranteed.
Validation of the proposed method, on the data set of the hydrocyclone is
rendered.

23.   Cluster matching by permuting cluster labels is important in many clustering
contexts such as cluster validation and cluster ensemble techniques. The
classic approach is to minimize the euclidean distance between two cluster
solutions which induces inappropriate stability in certain settings. Therefore,
we present the truematch algorithm that introduces two improvements best
explained in the crisp case. First, instead of maximizing the trace of the
cluster crosstable, we propose to maximize a chi-square transformation of this
crosstable. Thus, the trace will not be dominated by the cells with the largest
counts but by the cells with the most non-random observations, taking into
account the marginals. Second, we suggest a probabilistic component in order to
break ties and to make the matching algorithm truly random on random data. The
truematch algorithm is designed as a building block of the truecluster
framework and scales in polynomial time. First simulation results confirm that
the truematch algorithm gives more consistent truecluster results for unequal
cluster sizes. Free R software is available.

24.   In this chapter, we propose a new practical codification of the elements of
the Venn diagram in order to easily manipulate the focal elements. In order to
reduce the complexity, the eventual constraints must be integrated in the
codification at the beginning. Hence, we only consider a reduced hyper power
set $D_r^\Theta$ that can be $2^\Theta$ or $D^\Theta$. We describe all the
steps of a general belief function framework. The step of decision is
particularly studied, indeed, when we can decide on intersections of the
singletons of the discernment space no actual decision functions are easily to
use. Hence, two approaches are proposed, an extension of previous one and an
approach based on the specificity of the elements on which to decide. The
principal goal of this chapter is to provide practical codes of a general
belief function framework for the researchers and users needing the belief
function theory.

25.   We extend Knuth's 16 Boolean binary logic operators to fuzzy logic and
neutrosophic logic binary operators. Then we generalize them to n-ary fuzzy
logic and neutrosophic logic operators using the smarandache codification of
the Venn diagram and a defined vector neutrosophic law. In such way, new
operators in neutrosophic logic/set/probability are built.

26.   The paper describes the proposition and application of a local search
metaheuristic for multi-objective optimization problems. It is based on two
main principles of heuristic search, intensification through variable
neighborhoods, and diversification through perturbations and successive
iterations in favorable regions of the search space. The concept is
successfully tested on permutation flow shop scheduling problems under multiple
objectives. While the obtained results are encouraging in terms of their
quality, another positive attribute of the approach is its' simplicity as it
does require the setting of only very few parameters. The implementation of the
Pareto Iterated Local Search metaheuristic is based on the MOOPPS computer
system of local search heuristics for multi-objective scheduling which has been
awarded the European Academic Software Award 2002 in Ronneby, Sweden
(http://www.easa-award.net/, http://www.bth.se/llab/easa_2002.nsf)


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"- "has a topic of fuzzy logic applications"
"has a topic of information fusion techniques"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of belief function theory"\n- "has a topic of neural network applications"\n- "has a topic of data clustering methods"\n- "has a topic of qualitative analysis in research"\n- "has a topic of optimization techniques"\n- "has a topic of narrative analysis in film scripts"\n- "has a topic of conflict prediction in international relations"\n- "has a topic of fuzzy set theory and applications"\n- "has a topic of hydrocyclone performance analysis"\n- "has a topic of statistical measures in linguistics"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

1.   In this chapter, we propose a new practical codification of the elements of
the Venn diagram in order to easily manipulate the focal elements. In order to
reduce the complexity, the eventual constraints must be integrated in the
codification at the beginning. Hence, we only consider a reduced hyper power
set $D_r^\Theta$ that can be $2^\Theta$ or $D^\Theta$. We describe all the
steps of a general belief function framework. The step of decision is
particularly studied, indeed, when we can decide on intersections of the
singletons of the discernment space no actual decision functions are easily to
use. Hence, two approaches are proposed, an extension of previous one and an
approach based on the specificity of the elements on which to decide. The
principal goal of this chapter is to provide practical codes of a general
belief function framework for the researchers and users needing the belief
function theory.

2.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

3.   We consider hexagonal cellular automata with immediate cell neighbourhood and
three cell-states. Every cell calculates its next state depending on the
integral representation of states in its neighbourhood, i.e. how many
neighbours are in each one state. We employ evolutionary algorithms to breed
local transition functions that support mobile localizations (gliders), and
characterize sets of the functions selected in terms of quasi-chemical systems.
Analysis of the set of functions evolved allows to speculate that mobile
localizations are likely to emerge in the quasi-chemical systems with limited
diffusion of one reagent, a small number of molecules is required for
amplification of travelling localizations, and reactions leading to stationary
localizations involve relatively equal amount of quasi-chemical species.
Techniques developed can be applied in cascading signals in nature-inspired
spatially extended computing devices, and phenomenological studies and
classification of non-linear discrete systems.

4.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

5.   Cluster matching by permuting cluster labels is important in many clustering
contexts such as cluster validation and cluster ensemble techniques. The
classic approach is to minimize the euclidean distance between two cluster
solutions which induces inappropriate stability in certain settings. Therefore,
we present the truematch algorithm that introduces two improvements best
explained in the crisp case. First, instead of maximizing the trace of the
cluster crosstable, we propose to maximize a chi-square transformation of this
crosstable. Thus, the trace will not be dominated by the cells with the largest
counts but by the cells with the most non-random observations, taking into
account the marginals. Second, we suggest a probabilistic component in order to
break ties and to make the matching algorithm truly random on random data. The
truematch algorithm is designed as a building block of the truecluster
framework and scales in polynomial time. First simulation results confirm that
the truematch algorithm gives more consistent truecluster results for unequal
cluster sizes. Free R software is available.

6.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

7.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

8.   This paper has been withdrawn by the author due to extremely unscientific
errors.

9.   Data-stream clustering is an ever-expanding subdomain of knowledge
extraction. Most of the past and present research effort aims at efficient
scaling up for the huge data repositories. Our approach focuses on qualitative
improvement, mainly for "weak signals" detection and precise tracking of
topical evolutions in the framework of information watch - though scalability
is intrinsically guaranteed in a possibly distributed implementation. Our
GERMEN algorithm exhaustively picks up the whole set of density peaks of the
data at time t, by identifying the local perturbations induced by the current
document vector, such as changing cluster borders, or new/vanishing clusters.
Optimality yields from the uniqueness 1) of the density landscape for any value
of our zoom parameter, 2) of the cluster allocation operated by our border
propagation rule. This results in a rigorous independence from the data
presentation ranking or any initialization parameter. We present here as a
first step the only assessment of a static view resulting from one year of the
CNRS/INIST Pascal database in the field of geotechnics.

10.   An ensemble based approach for dealing with missing data, without predicting
or imputing the missing values is proposed. This technique is suitable for
online operations of neural networks and as a result, is used for online
condition monitoring. The proposed technique is tested in both classification
and regression problems. An ensemble of Fuzzy-ARTMAPs is used for
classification whereas an ensemble of multi-layer perceptrons is used for the
regression problem. Results obtained using this ensemble-based technique are
compared to those obtained using a combination of auto-associative neural
networks and genetic algorithms and findings show that this method can perform
up to 9% better in regression problems. Another advantage of the proposed
technique is that it eliminates the need for finding the best estimate of the
data, and hence, saves time.

11.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

12.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

13.   This paper has been withdrawn.

14.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

15.   We extend Knuth's 16 Boolean binary logic operators to fuzzy logic and
neutrosophic logic binary operators. Then we generalize them to n-ary fuzzy
logic and neutrosophic logic operators using the smarandache codification of
the Venn diagram and a defined vector neutrosophic law. In such way, new
operators in neutrosophic logic/set/probability are built.

16.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

17.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

18.   This paper deals with enriched qualitative belief functions for reasoning
under uncertainty and for combining information expressed in natural language
through linguistic labels. In this work, two possible enrichments (quantitative
and/or qualitative) of linguistic labels are considered and operators
(addition, multiplication, division, etc) for dealing with them are proposed
and explained. We denote them $qe$-operators, $qe$ standing for
"qualitative-enriched" operators. These operators can be seen as a direct
extension of the classical qualitative operators ($q$-operators) proposed
recently in the Dezert-Smarandache Theory of plausible and paradoxist reasoning
(DSmT). $q$-operators are also justified in details in this paper. The
quantitative enrichment of linguistic label is a numerical supporting degree in
$[0,\infty)$, while the qualitative enrichment takes its values in a finite
ordered set of linguistic values. Quantitative enrichment is less precise than
qualitative enrichment, but it is expected more close with what human experts
can easily provide when expressing linguistic labels with supporting degrees.
Two simple examples are given to show how the fusion of qualitative-enriched
belief assignments can be done.

19.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

20.   Noise, corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust,
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise.

21.   The textured images' classification assumes to consider the images in terms
of area with the same texture. In uncertain environment, it could be better to
take an imprecise decision or to reject the area corresponding to an unlearning
class. Moreover, on the areas that are the classification units, we can have
more than one texture. These considerations allows us to develop a belief
decision model permitting to reject an area as unlearning and to decide on
unions and intersections of learning classes. The proposed approach finds all
its justification in an application of seabed characterization from sonar
images, which contributes to an illustration.

22.   The paper describes the proposition and application of a local search
metaheuristic for multi-objective optimization problems. It is based on two
main principles of heuristic search, intensification through variable
neighborhoods, and diversification through perturbations and successive
iterations in favorable regions of the search space. The concept is
successfully tested on permutation flow shop scheduling problems under multiple
objectives. While the obtained results are encouraging in terms of their
quality, another positive attribute of the approach is its' simplicity as it
does require the setting of only very few parameters. The implementation of the
Pareto Iterated Local Search metaheuristic is based on the MOOPPS computer
system of local search heuristics for multi-objective scheduling which has been
awarded the European Academic Software Award 2002 in Ronneby, Sweden
(http://www.easa-award.net/, http://www.bth.se/llab/easa_2002.nsf)

23.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

24.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.

25.   This paper describes application of information granulation theory, on the
analysis of hydrocyclone perforamance. In this manner, using a combining of
Self Organizing Map (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and
fuzzy granules are obtained(briefly called SONFIS). Balancing of crisp granules
and sub fuzzy granules, within non fuzzy information (initial granulation), is
rendered in an open-close iteration. Using two criteria, "simplicity of rules
"and "adaptive threoshold error level", stability of algorithm is guaranteed.
Validation of the proposed method, on the data set of the hydrocyclone is
rendered.

26.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of structural analysis using neural networks"
"has a topic of belief function theory"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of narrative analysis in film scripts"\n- "has a topic of belief function frameworks and decision making"\n- "has a topic of clustering algorithms and data analysis"\n- "has a topic of protein family detection using hidden Markov models"\n- "has a topic of unconscious preference awareness and verbalization"\n- "has a topic of data-stream clustering and weak signals detection"\n- "has a topic of multi-objective optimization in scheduling problems"\n- "has a topic of fuzzy logic applications in stability assessment"\n- "has a topic of conflict prediction in international relations"\n- "has a topic of information granulation in hydrocyclone performance analysis"']
147 descriptions proposed in total. Here are the descriptions:
- "has a topic of soft constraints and optimization
has a topic of conflict prediction using machine learning
- "has a topic of semantic web technologies
has a topic of qualitative temporal reasoning
has a topic of automated leaf recognition and classification
has a topic of Bayesian frameworks in modeling
has a topic of face recognition robustness
has a topic of Bayesian frameworks in machine learning
has a topic of multi-agent systems
- "has a topic of automated leaf recognition algorithms
has a topic of evolutionary robotics
has a topic of clustering techniques and algorithms
has a topic of quantum computation applications
has a topic of water demand forecasting
has a topic of quantum computation and artificial intelligence
has a topic of fault classification in cylindrical shells
has a topic of evolutionary learning and ensemble methods
has a topic of multi-cellular developmental design
has a topic of condition monitoring in engineering
- "has a topic of ensemble learning techniques
has a topic of water demand forecasting using computational intelligence
has a topic of HIV risk modeling
- "has a topic of computational intelligence techniques
has a topic of fault diagnosis and classification
has a topic of Bayesian methods in rough set models
has a topic of decentralized control in multi-agent systems
has a topic of protein family detection
has a topic of artificial intelligence in healthcare
has a topic of neural network classifiers
has a topic of neural network models
has a topic of semantic web technologies
- "has a topic of evolutionary robotics
has a topic of semantic web and natural language processing
has a topic of clustering algorithms and techniques
has a topic of ensemble learning approaches
has a topic of artificial intelligence history
has a topic of decentralized control systems
has a topic of qualitative temporal information representation
- "has a topic of evolutionary learning and ensemble methods
has a topic of automated leaf recognition algorithms
has a topic of handling missing data in neural networks
has a topic of conflict prediction and machine learning
- "has a topic of protein family detection
has a topic of condition monitoring frameworks
has a topic of semantic web and computational modeling
has a topic of neural network performance evaluation
has a topic of predicting militarized interstate disputes
has a topic of fault classification methods
has a topic of fault diagnosis and monitoring
has a topic of artificial intelligence applications
has a topic of fault classification and condition monitoring in engineering
has a topic of ontology and conceptualization in knowledge representation
- "has a topic of constraint satisfaction problems and algorithms
has a topic of social network analysis and covert nodes
has a topic of knowledge management and ontology development
- "has a topic of machine intelligence definitions and tests
has a topic of artificial intelligence and definitions of intelligence
has a topic of computational methods in social network analysis
has a topic of simulation and philosophical implications of intelligence
has a topic of knowledge management and ontology
has a topic of constraint satisfaction problems and algorithm design
has a topic of machine intelligence and its definitions
has a topic of terrorism analysis and intelligence visualization
has a topic of evolutionary algorithms and cellular automata
has a topic of evolutionary theory and simulations
- "has a topic of fitness landscapes and optimization algorithms
has a topic of constraint satisfaction problems and propagators
- "has a topic of causality and definitions in artificial intelligence
has a topic of scientific simulations and philosophical implications
has a topic of fuzzy logic in stability assessment of underground openings
has a topic of causality and default theories
has a topic of knowledge management systems in organizational clusters
- "has a topic of covert node discovery in social networks
- "has a topic of social network analysis and covert nodes
has a topic of definitions and measures of intelligence
has a topic of hydrocyclone operation analysis using rough set theory
has a topic of mining methods and operational efficiency
has a topic of terrorism and social network dynamics
has a topic of fuzzy logic and stability assessment in engineering
has a topic of computational methods in terrorism investigation
has a topic of simulation and philosophical implications
has a topic of social network analysis and covert node discovery
has a topic of fuzzy logic and stability assessment
has a topic of intelligence measurement and definitions in AI
has a topic of fuzzy logic and decision-making systems
has a topic of intelligence definitions and machine learning
has a topic of rough set theory applications in data analysis
has a topic of artificial intelligence and intelligence measurement
has a topic of fuzzy logic and fault classification
has a topic of constraint satisfaction problems and optimization
has a topic of fault classification using neuro-fuzzy models
has a topic of fitness landscapes and optimization
has a topic of constraint satisfaction problems and optimization techniques
has a topic of definitions and measurements of intelligence
has a topic of computational methods in terrorism analysis
has a topic of social network analysis and covert operations
has a topic of simulation and modeling of complex systems
has a topic of fuzzy block theory and stability assessment
has a topic of fuzzy logic and its applications
- "has a topic of fuzzy logic applications
has a topic of linguistic evidence and unithood measurement
has a topic of belief function theory
- "has a topic of narrative analysis in film scripts
has a topic of neural network applications
has a topic of optimization algorithms for multi-objective problems
has a topic of fuzzy decision-making models
- "has a topic of data mining challenges
has a topic of optimization techniques
has a topic of clustering algorithms
has a topic of narrative analysis in film scripts
has a topic of fuzzy logic applications in stability assessment
has a topic of clustering algorithms and data analysis
has a topic of optimization techniques in scheduling
has a topic of belief function theory and information fusion
- "has a topic of belief function theory and information fusion
has a topic of information fusion methods
- "has a topic of face recognition systems
has a topic of information fusion and conflict redistribution
has a topic of image processing and recognition systems
has a topic of structural protein analysis
has a topic of protein family detection using hidden Markov models
has a topic of data mining techniques
has a topic of data mining challenges
- "has a topic of belief function theory
has a topic of data-stream clustering and weak signals detection
- "has a topic of neural networks in structural engineering
has a topic of missing data handling in neural networks
has a topic of qualitative analysis in narratives
has a topic of information fusion techniques
has a topic of qualitative analysis in narrative structures
has a topic of narrative analysis in film
has a topic of fuzzy logic and decision-making
has a topic of belief function frameworks and decision making
has a topic of multi-objective optimization methods
has a topic of multi-objective optimization
- "has a topic of artificial neural networks in earthquake response prediction
has a topic of data mining and clustering algorithms
has a topic of multi-objective optimization in scheduling problems
has a topic of fuzzy set theory and applications
has a topic of fuzzy logic and optimization techniques
has a topic of unconscious preference awareness and verbalization
has a topic of fuzzy logic and neutrosophic operators
has a topic of data clustering methods
has a topic of conflict prediction in international relations
has a topic of qualitative belief functions and reasoning under uncertainty
has a topic of neural networks in earthquake response prediction
has a topic of hydrocyclone performance analysis using fuzzy theory
First prompt example:
You are tasked with classifying whether a TEXT satisfies a PROPERTY. You need to respond with Yes or No, and when uncertain, always respond with No.

<example>
<input>
Text: I like apples.
Property: is talking about fruits
</input>
<output>
Yes
</output>
</example>

Now, solve this new task:
Text:   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

Property: - "has a topic of fuzzy logic applications

Processing 48000 prompts with vLLM's automatic batching...
Stage Assign assign
Assigned all recall: 0.983
Assigned all precision: 0.03763987792472025
Assigned description: - "has a topic of soft constraints and optimization. recall: 0.213 precision: 0.0
Assigned description: has a topic of conflict prediction using machine learning. recall: 0.013 precision: 0.0
Assigned description: - "has a topic of semantic web technologies. recall: 0.042 precision: 0.0
Assigned description: has a topic of qualitative temporal reasoning. recall: 0.062 precision: 0.0
Assigned description: has a topic of automated leaf recognition and classification. recall: 0.001 precision: 0.0
Assigned description: has a topic of Bayesian frameworks in modeling. recall: 0.125 precision: 0.0
Assigned description: has a topic of face recognition robustness. recall: 0.001 precision: 0.0
Assigned description: has a topic of Bayesian frameworks in machine learning. recall: 0.104 precision: 0.0
Assigned description: has a topic of multi-agent systems. recall: 0.069 precision: 0.014492753623188406
Assigned description: - "has a topic of automated leaf recognition algorithms. recall: 0.001 precision: 0.0
Assigned description: has a topic of evolutionary robotics. recall: 0.013 precision: 0.0
Assigned description: has a topic of clustering techniques and algorithms. recall: 0.039 precision: 0.0
Assigned description: has a topic of quantum computation applications. recall: 0.002 precision: 0.0
Assigned description: has a topic of water demand forecasting. recall: 0.001 precision: 0.0
Assigned description: has a topic of quantum computation and artificial intelligence. recall: 0.005 precision: 0.0
Assigned description: has a topic of fault classification in cylindrical shells. recall: 0.002 precision: 0.0
Assigned description: has a topic of evolutionary learning and ensemble methods. recall: 0.032 precision: 0.0
Assigned description: has a topic of multi-cellular developmental design. recall: 0.001 precision: 0.0
Assigned description: has a topic of condition monitoring in engineering. recall: 0.014 precision: 0.0
Assigned description: - "has a topic of ensemble learning techniques. recall: 0.039 precision: 0.0
Assigned description: has a topic of water demand forecasting using computational intelligence. recall: 0.001 precision: 0.0
Assigned description: has a topic of HIV risk modeling. recall: 0.002 precision: 0.0
Assigned description: - "has a topic of computational intelligence techniques. recall: 0.624 precision: 0.003205128205128205
Assigned description: has a topic of fault diagnosis and classification. recall: 0.024 precision: 0.0
Assigned description: has a topic of Bayesian methods in rough set models. recall: 0.012 precision: 0.0
Assigned description: has a topic of decentralized control in multi-agent systems. recall: 0.034 precision: 0.0
Assigned description: has a topic of protein family detection. recall: 0.001 precision: 0.0
Assigned description: has a topic of artificial intelligence in healthcare. recall: 0.028 precision: 0.0
Assigned description: has a topic of neural network classifiers. recall: 0.031 precision: 0.0
Assigned description: has a topic of neural network models. recall: 0.046 precision: 0.0
Assigned description: has a topic of semantic web technologies. recall: 0.046 precision: 0.0
Assigned description: - "has a topic of evolutionary robotics. recall: 0.013 precision: 0.0
Assigned description: has a topic of semantic web and natural language processing. recall: 0.066 precision: 0.0
Assigned description: has a topic of clustering algorithms and techniques. recall: 0.034 precision: 0.0
Assigned description: has a topic of ensemble learning approaches. recall: 0.048 precision: 0.0
Assigned description: has a topic of artificial intelligence history. recall: 0.009 precision: 0.0
Assigned description: has a topic of decentralized control systems. recall: 0.023 precision: 0.0
Assigned description: has a topic of qualitative temporal information representation. recall: 0.101 precision: 0.0
Assigned description: - "has a topic of evolutionary learning and ensemble methods. recall: 0.028 precision: 0.0
Assigned description: has a topic of automated leaf recognition algorithms. recall: 0.001 precision: 0.0
Assigned description: has a topic of handling missing data in neural networks. recall: 0.002 precision: 0.0
Assigned description: has a topic of conflict prediction and machine learning. recall: 0.027 precision: 0.037037037037037035
Assigned description: - "has a topic of protein family detection. recall: 0.001 precision: 0.0
Assigned description: has a topic of condition monitoring frameworks. recall: 0.022 precision: 0.0
Assigned description: has a topic of semantic web and computational modeling. recall: 0.12 precision: 0.008333333333333333
Assigned description: has a topic of neural network performance evaluation. recall: 0.018 precision: 0.0
Assigned description: has a topic of predicting militarized interstate disputes. recall: 0.001 precision: 0.0
Assigned description: has a topic of fault classification methods. recall: 0.014 precision: 0.0
Assigned description: has a topic of fault diagnosis and monitoring. recall: 0.029 precision: 0.0
Assigned description: has a topic of artificial intelligence applications. recall: 0.406 precision: 0.0024630541871921183
Assigned description: has a topic of fault classification and condition monitoring in engineering. recall: 0.011 precision: 0.0
Assigned description: has a topic of ontology and conceptualization in knowledge representation. recall: 0.278 precision: 0.03597122302158273
Assigned description: - "has a topic of constraint satisfaction problems and algorithms. recall: 0.412 precision: 0.0
Assigned description: has a topic of social network analysis and covert nodes. recall: 0.015 precision: 0.0
Assigned description: has a topic of knowledge management and ontology development. recall: 0.098 precision: 0.0
Assigned description: - "has a topic of machine intelligence definitions and tests. recall: 0.044 precision: 0.0
Assigned description: has a topic of artificial intelligence and definitions of intelligence. recall: 0.141 precision: 0.0
Assigned description: has a topic of computational methods in social network analysis. recall: 0.026 precision: 0.0
Assigned description: has a topic of simulation and philosophical implications of intelligence. recall: 0.038 precision: 0.0
Assigned description: has a topic of knowledge management and ontology. recall: 0.11 precision: 0.00909090909090909
Assigned description: has a topic of constraint satisfaction problems and algorithm design. recall: 0.475 precision: 0.0
Assigned description: has a topic of machine intelligence and its definitions. recall: 0.186 precision: 0.0
Assigned description: has a topic of terrorism analysis and intelligence visualization. recall: 0.005 precision: 0.0
Assigned description: has a topic of evolutionary algorithms and cellular automata. recall: 0.005 precision: 0.0
Assigned description: has a topic of evolutionary theory and simulations. recall: 0.04 precision: 0.0
Assigned description: - "has a topic of fitness landscapes and optimization algorithms. recall: 0.043 precision: 0.0
Assigned description: has a topic of constraint satisfaction problems and propagators. recall: 0.172 precision: 0.0
Assigned description: - "has a topic of causality and definitions in artificial intelligence. recall: 0.122 precision: 0.0
Assigned description: has a topic of scientific simulations and philosophical implications. recall: 0.145 precision: 0.006896551724137931
Assigned description: has a topic of fuzzy logic in stability assessment of underground openings. recall: 0.001 precision: 0.0
Assigned description: has a topic of causality and default theories. recall: 0.048 precision: 0.0625
Assigned description: has a topic of knowledge management systems in organizational clusters. recall: 0.013 precision: 0.0
Assigned description: - "has a topic of covert node discovery in social networks. recall: 0.005 precision: 0.0
Assigned description: - "has a topic of social network analysis and covert nodes. recall: 0.014 precision: 0.0
Assigned description: has a topic of definitions and measures of intelligence. recall: 0.024 precision: 0.0
Assigned description: has a topic of hydrocyclone operation analysis using rough set theory. recall: 0.002 precision: 0.0
Assigned description: has a topic of mining methods and operational efficiency. recall: 0.107 precision: 0.0
Assigned description: has a topic of terrorism and social network dynamics. recall: 0.002 precision: 0.0
Assigned description: has a topic of fuzzy logic and stability assessment in engineering. recall: 0.016 precision: 0.0
Assigned description: has a topic of computational methods in terrorism investigation. recall: 0.005 precision: 0.0
Assigned description: has a topic of simulation and philosophical implications. recall: 0.08 precision: 0.0
Assigned description: has a topic of social network analysis and covert node discovery. recall: 0.013 precision: 0.0
Assigned description: has a topic of fuzzy logic and stability assessment. recall: 0.049 precision: 0.0
Assigned description: has a topic of intelligence measurement and definitions in AI. recall: 0.049 precision: 0.0
Assigned description: has a topic of fuzzy logic and decision-making systems. recall: 0.102 precision: 0.0
Assigned description: has a topic of intelligence definitions and machine learning. recall: 0.067 precision: 0.0
Assigned description: has a topic of rough set theory applications in data analysis. recall: 0.019 precision: 0.0
Assigned description: has a topic of artificial intelligence and intelligence measurement. recall: 0.384 precision: 0.0078125
Assigned description: has a topic of fuzzy logic and fault classification. recall: 0.005 precision: 0.0
Assigned description: has a topic of constraint satisfaction problems and optimization. recall: 0.359 precision: 0.0
Assigned description: has a topic of fault classification using neuro-fuzzy models. recall: 0.004 precision: 0.0
Assigned description: has a topic of fitness landscapes and optimization. recall: 0.048 precision: 0.0
Assigned description: has a topic of constraint satisfaction problems and optimization techniques. recall: 0.388 precision: 0.0
Assigned description: has a topic of definitions and measurements of intelligence. recall: 0.016 precision: 0.0
Assigned description: has a topic of computational methods in terrorism analysis. recall: 0.006 precision: 0.0
Assigned description: has a topic of social network analysis and covert operations. recall: 0.004 precision: 0.0
Assigned description: has a topic of simulation and modeling of complex systems. recall: 0.289 precision: 0.010380622837370242
Assigned description: has a topic of fuzzy block theory and stability assessment. recall: 0.002 precision: 0.0
Assigned description: has a topic of fuzzy logic and its applications. recall: 0.073 precision: 0.0
Assigned description: - "has a topic of fuzzy logic applications. recall: 0.058 precision: 0.0
Assigned description: has a topic of linguistic evidence and unithood measurement. recall: 0.035 precision: 0.05714285714285714
Assigned description: has a topic of belief function theory. recall: 0.119 precision: 0.0
Assigned description: - "has a topic of narrative analysis in film scripts. recall: 0.002 precision: 0.0
Assigned description: has a topic of neural network applications. recall: 0.048 precision: 0.0
Assigned description: has a topic of optimization algorithms for multi-objective problems. recall: 0.034 precision: 0.0
Assigned description: has a topic of fuzzy decision-making models. recall: 0.1 precision: 0.0
Assigned description: - "has a topic of data mining challenges. recall: 0.059 precision: 0.0
Assigned description: has a topic of optimization techniques. recall: 0.477 precision: 0.008385744234800839
Assigned description: has a topic of clustering algorithms. recall: 0.032 precision: 0.0
Assigned description: has a topic of narrative analysis in film scripts. recall: 0.002 precision: 0.0
Assigned description: has a topic of fuzzy logic applications in stability assessment. recall: 0.006 precision: 0.0
Assigned description: has a topic of clustering algorithms and data analysis. recall: 0.036 precision: 0.0
Assigned description: has a topic of optimization techniques in scheduling. recall: 0.041 precision: 0.0
Assigned description: has a topic of belief function theory and information fusion. recall: 0.162 precision: 0.0
Assigned description: - "has a topic of belief function theory and information fusion. recall: 0.227 precision: 0.004405286343612335
Assigned description: has a topic of information fusion methods. recall: 0.199 precision: 0.0
Assigned description: - "has a topic of face recognition systems. recall: 0.004 precision: 0.0
Assigned description: has a topic of information fusion and conflict redistribution. recall: 0.118 precision: 0.0
Assigned description: has a topic of image processing and recognition systems. recall: 0.023 precision: 0.043478260869565216
Assigned description: has a topic of structural protein analysis. recall: 0.002 precision: 0.0
Assigned description: has a topic of protein family detection using hidden Markov models. recall: 0.001 precision: 0.0
Assigned description: has a topic of data mining techniques. recall: 0.141 precision: 0.0
Assigned description: has a topic of data mining challenges. recall: 0.08 precision: 0.0
Assigned description: - "has a topic of belief function theory. recall: 0.155 precision: 0.0
Assigned description: has a topic of data-stream clustering and weak signals detection. recall: 0.002 precision: 0.0
Assigned description: - "has a topic of neural networks in structural engineering. recall: 0.003 precision: 0.0
Assigned description: has a topic of missing data handling in neural networks. recall: 0.005 precision: 0.0
Assigned description: has a topic of qualitative analysis in narratives. recall: 0.02 precision: 0.0
Assigned description: has a topic of information fusion techniques. recall: 0.174 precision: 0.0
Assigned description: has a topic of qualitative analysis in narrative structures. recall: 0.006 precision: 0.0
Assigned description: has a topic of narrative analysis in film. recall: 0.002 precision: 0.0
Assigned description: has a topic of fuzzy logic and decision-making. recall: 0.108 precision: 0.0
Assigned description: has a topic of belief function frameworks and decision making. recall: 0.17 precision: 0.0
Assigned description: has a topic of multi-objective optimization methods. recall: 0.04 precision: 0.0
Assigned description: has a topic of multi-objective optimization. recall: 0.041 precision: 0.0
Assigned description: - "has a topic of artificial neural networks in earthquake response prediction. recall: 0.001 precision: 0.0
Assigned description: has a topic of data mining and clustering algorithms. recall: 0.047 precision: 0.02127659574468085
Assigned description: has a topic of multi-objective optimization in scheduling problems. recall: 0.015 precision: 0.0
Assigned description: has a topic of fuzzy set theory and applications. recall: 0.084 precision: 0.011904761904761904
Assigned description: has a topic of fuzzy logic and optimization techniques. recall: 0.07 precision: 0.0
Assigned description: has a topic of unconscious preference awareness and verbalization. recall: 0.004 precision: 0.0
Assigned description: has a topic of fuzzy logic and neutrosophic operators. recall: 0.031 precision: 0.0
Assigned description: has a topic of data clustering methods. recall: 0.031 precision: 0.0
Assigned description: has a topic of conflict prediction in international relations. recall: 0.001 precision: 0.0
Assigned description: has a topic of qualitative belief functions and reasoning under uncertainty. recall: 0.141 precision: 0.0
Assigned description: has a topic of neural networks in earthquake response prediction. recall: 0.001 precision: 0.0
Assigned description: has a topic of hydrocyclone performance analysis using fuzzy theory. recall: 0.003 precision: 0.0
Drop 22 because it cover 624 from 1000
Dropping 1 descriptions because they are too popular or too unpopular:
Stage select select
Unmatched count: 219
Description: has a topic of neural network classifiers. Count: 31
Description: has a topic of ontology and conceptualization in knowledge representation. Count: 275
Description: has a topic of social network analysis and covert nodes. Count: 9
Description: has a topic of simulation and philosophical implications. Count: 49
Description: has a topic of rough set theory applications in data analysis. Count: 11
Description: has a topic of optimization techniques. Count: 406
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

1.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.

2.   In this paper we propose the CTS (Concious Tutoring System) technology, a
biologically plausible cognitive agent based on human brain functions.This
agent is capable of learning and remembering events and any related information
such as corresponding procedures, stimuli and their emotional valences. Our
proposed episodic memory and episodic learning mechanism are closer to the
current multiple-trace theory in neuroscience, because they are inspired by it
[5] contrary to other mechanisms that are incorporated in cognitive agents.
This is because in our model emotions play a role in the encoding and
remembering of events. This allows the agent to improve its behavior by
remembering previously selected behaviors which are influenced by its emotional
mechanism. Moreover, the architecture incorporates a realistic memory
consolidation process based on a data mining algorithm.

3.   The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.

4.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

5.   This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.

6.   This paper has been withdrawn by the author due to extremely unscientific
errors.

7.   A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.

8.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

9.   We present in this article a new evaluation method for classification and
segmentation of textured images in uncertain environments. In uncertain
environments, real classes and boundaries are known with only a partial
certainty given by the experts. Most of the time, in many presented papers,
only classification or only segmentation are considered and evaluated. Here, we
propose to take into account both the classification and segmentation results
according to the certainty given by the experts. We present the results of this
method on a fusion of classifiers of sonar images for a seabed
characterization.

10.   Stream computing is the use of multiple autonomic and parallel modules
together with integrative processors at a higher level of abstraction to embody
"intelligent" processing. The biological basis of this computing is sketched
and the matter of learning is examined.

11.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

12.   This paper has been withdrawn.

13.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

14.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

15.   Each cognitive science tries to understand a set of cognitive behaviors. The
structuring of knowledge of this nature's aspect is far from what it can be
expected about a science. Until now universal standard consistently describing
the set of cognitive behaviors has not been found, and there are many questions
about the cognitive behaviors for which only there are opinions of members of
the scientific community. This article has three proposals. The first proposal
is to raise to the scientific community the necessity of unified the cognitive
behaviors. The second proposal is claim the application of the Newton's
reasoning rules about nature of his book, Philosophiae Naturalis Principia
Mathematica, to the cognitive behaviors. The third is to propose a scientific
theory, currently developing, that follows the rules established by Newton to
make sense of nature, and could be the theory to explain all the cognitive
behaviors.

16.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

17.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

18.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

19.   Data mining algorithms are now able to efficiently deal with huge amount of
data. Various kinds of patterns may be discovered and may have some great
impact on the general development of knowledge. In many domains, end users may
want to have their data mined by data mining tools in order to extract patterns
that could impact their business. Nevertheless, those users are often
overwhelmed by the large quantity of patterns extracted in such a situation.
Moreover, some privacy issues, or some commercial one may lead the users not to
be able to mine the data by themselves. Thus, the users may not have the
possibility to perform many experiments integrating various constraints in
order to focus on specific patterns they would like to extract. Post processing
of patterns may be an answer to that drawback. Thus, in this paper we present a
framework that could allow end users to manage collections of patterns. We
propose to use an efficient data structure on which some algebraic operators
may be used in order to retrieve or access patterns in pattern bases.

20.   Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.

21.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

22.   We introduce novel results for approximate inference on planar graphical
models using the loop calculus framework. The loop calculus (Chertkov and
Chernyak, 2006) allows to express the exact partition function of a graphical
model as a finite sum of terms that can be evaluated once the belief
propagation (BP) solution is known. In general, full summation over all
correction terms is intractable. We develop an algorithm for the approach
presented in (Certkov et al., 2008) which represents an efficient truncation
scheme on planar graphs and a new representation of the series in terms of
Pfaffians of matrices. We analyze the performance of the algorithm for the
partition function approximation for models with binary variables and pairwise
interactions on grids and other planar graphs. We study in detail both the loop
series and the equivalent Pfaffian series and show that the first term of the
Pfaffian series for the general, intractable planar model, can provide very
accurate approximations. The algorithm outperforms previous truncation schemes
of the loop series and is competitive with other state-of-the-art methods for
approximate inference.

23.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

24.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

25.   Consumers of mass media must have a comprehensive, balanced and plural
selection of news to get an unbiased perspective; but achieving this goal can
be very challenging, laborious and time consuming. News stories development
over time, its (in)consistency, and different level of coverage across the
media outlets are challenges that a conscientious reader has to overcome in
order to alleviate bias.
  In this paper we present an intelligent agent framework currently
facilitating analysis of the main sources of on-line news in El Salvador. We
show how prior tools of text analysis and Web 2.0 technologies can be combined
with minimal manual intervention to help individuals on their rational decision
process, while holding media outlets accountable for their work.

26.   Domain experts should provide relevant domain knowledge to an Intelligent
Tutoring System (ITS) so that it can guide a learner during problemsolving
learning activities. However, for many ill-defined domains, the domain
knowledge is hard to define explicitly. In previous works, we showed how
sequential pattern mining can be used to extract a partial problem space from
logged user interactions, and how it can support tutoring services during
problem-solving exercises. This article describes an extension of this approach
to extract a problem space that is richer and more adapted for supporting
tutoring services. We combined sequential pattern mining with (1) dimensional
pattern mining (2) time intervals, (3) the automatic clustering of valued
actions and (4) closed sequences mining. Some tutoring services have been
implemented and an experiment has been conducted in a tutoring system.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of data mining and pattern extraction"
"has a topic of conflict redistribution in decision theory"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of cognitive agents and learning mechanisms"\n- "has a topic of program semantics and modularity"\n- "has a topic of quantum computation and artificial intelligence"\n- "has a topic of narrative analysis in film scripts"\n- "has a topic of intelligent tutoring systems and domain knowledge extraction"\n- "has a topic of fuzzy theory and stability assessment"\n- "has a topic of data mining algorithms and user interaction"\n- "has a topic of causality and default reasoning"\n- "has a topic of semantic web technologies and information retrieval"\n- "has a topic of evaluation methods in image classification"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

1.   The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.

2.   Consumers of mass media must have a comprehensive, balanced and plural
selection of news to get an unbiased perspective; but achieving this goal can
be very challenging, laborious and time consuming. News stories development
over time, its (in)consistency, and different level of coverage across the
media outlets are challenges that a conscientious reader has to overcome in
order to alleviate bias.
  In this paper we present an intelligent agent framework currently
facilitating analysis of the main sources of on-line news in El Salvador. We
show how prior tools of text analysis and Web 2.0 technologies can be combined
with minimal manual intervention to help individuals on their rational decision
process, while holding media outlets accountable for their work.

3.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

4.   Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.

5.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

6.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

7.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

8.   Stream computing is the use of multiple autonomic and parallel modules
together with integrative processors at a higher level of abstraction to embody
"intelligent" processing. The biological basis of this computing is sketched
and the matter of learning is examined.

9.   Data mining algorithms are now able to efficiently deal with huge amount of
data. Various kinds of patterns may be discovered and may have some great
impact on the general development of knowledge. In many domains, end users may
want to have their data mined by data mining tools in order to extract patterns
that could impact their business. Nevertheless, those users are often
overwhelmed by the large quantity of patterns extracted in such a situation.
Moreover, some privacy issues, or some commercial one may lead the users not to
be able to mine the data by themselves. Thus, the users may not have the
possibility to perform many experiments integrating various constraints in
order to focus on specific patterns they would like to extract. Post processing
of patterns may be an answer to that drawback. Thus, in this paper we present a
framework that could allow end users to manage collections of patterns. We
propose to use an efficient data structure on which some algebraic operators
may be used in order to retrieve or access patterns in pattern bases.

10.   We introduce novel results for approximate inference on planar graphical
models using the loop calculus framework. The loop calculus (Chertkov and
Chernyak, 2006) allows to express the exact partition function of a graphical
model as a finite sum of terms that can be evaluated once the belief
propagation (BP) solution is known. In general, full summation over all
correction terms is intractable. We develop an algorithm for the approach
presented in (Certkov et al., 2008) which represents an efficient truncation
scheme on planar graphs and a new representation of the series in terms of
Pfaffians of matrices. We analyze the performance of the algorithm for the
partition function approximation for models with binary variables and pairwise
interactions on grids and other planar graphs. We study in detail both the loop
series and the equivalent Pfaffian series and show that the first term of the
Pfaffian series for the general, intractable planar model, can provide very
accurate approximations. The algorithm outperforms previous truncation schemes
of the loop series and is competitive with other state-of-the-art methods for
approximate inference.

11.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

12.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

13.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.

14.   We present in this article a new evaluation method for classification and
segmentation of textured images in uncertain environments. In uncertain
environments, real classes and boundaries are known with only a partial
certainty given by the experts. Most of the time, in many presented papers,
only classification or only segmentation are considered and evaluated. Here, we
propose to take into account both the classification and segmentation results
according to the certainty given by the experts. We present the results of this
method on a fusion of classifiers of sonar images for a seabed
characterization.

15.   Domain experts should provide relevant domain knowledge to an Intelligent
Tutoring System (ITS) so that it can guide a learner during problemsolving
learning activities. However, for many ill-defined domains, the domain
knowledge is hard to define explicitly. In previous works, we showed how
sequential pattern mining can be used to extract a partial problem space from
logged user interactions, and how it can support tutoring services during
problem-solving exercises. This article describes an extension of this approach
to extract a problem space that is richer and more adapted for supporting
tutoring services. We combined sequential pattern mining with (1) dimensional
pattern mining (2) time intervals, (3) the automatic clustering of valued
actions and (4) closed sequences mining. Some tutoring services have been
implemented and an experiment has been conducted in a tutoring system.

16.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

17.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

18.   A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.

19.   This paper has been withdrawn.

20.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

21.   In this paper we propose the CTS (Concious Tutoring System) technology, a
biologically plausible cognitive agent based on human brain functions.This
agent is capable of learning and remembering events and any related information
such as corresponding procedures, stimuli and their emotional valences. Our
proposed episodic memory and episodic learning mechanism are closer to the
current multiple-trace theory in neuroscience, because they are inspired by it
[5] contrary to other mechanisms that are incorporated in cognitive agents.
This is because in our model emotions play a role in the encoding and
remembering of events. This allows the agent to improve its behavior by
remembering previously selected behaviors which are influenced by its emotional
mechanism. Moreover, the architecture incorporates a realistic memory
consolidation process based on a data mining algorithm.

22.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

23.   This paper has been withdrawn by the author due to extremely unscientific
errors.

24.   Each cognitive science tries to understand a set of cognitive behaviors. The
structuring of knowledge of this nature's aspect is far from what it can be
expected about a science. Until now universal standard consistently describing
the set of cognitive behaviors has not been found, and there are many questions
about the cognitive behaviors for which only there are opinions of members of
the scientific community. This article has three proposals. The first proposal
is to raise to the scientific community the necessity of unified the cognitive
behaviors. The second proposal is claim the application of the Newton's
reasoning rules about nature of his book, Philosophiae Naturalis Principia
Mathematica, to the cognitive behaviors. The third is to propose a scientific
theory, currently developing, that follows the rules established by Newton to
make sense of nature, and could be the theory to explain all the cognitive
behaviors.

25.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

26.   This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"- "has a topic of evaluation methods for classification and segmentation"
"has a topic of information fusion techniques"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of protein family detection using hidden Markov models"\n- "has a topic of symbolic controllers in evolutionary robotics"\n- "has a topic of intelligent agents for news analysis"\n- "has a topic of narrative analysis in film scripts"\n- "has a topic of semantic access and natural language processing"\n- "has a topic of dynamic data mining challenges"\n- "has a topic of conflict redistribution in belief function theory"\n- "has a topic of fuzzy block theory in underground stability assessment"\n- "has a topic of modular architecture in logic programming"\n- "has a topic of uncertainty in image classification and segmentation"\n- "has a topic of intelligent tutoring systems and pattern mining"\n- "has a topic of termhood and unithood in linguistic evidence"\n- "has a topic of quantum computation applications in artificial intelligence"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   This paper has been withdrawn.

1.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.

2.   Each cognitive science tries to understand a set of cognitive behaviors. The
structuring of knowledge of this nature's aspect is far from what it can be
expected about a science. Until now universal standard consistently describing
the set of cognitive behaviors has not been found, and there are many questions
about the cognitive behaviors for which only there are opinions of members of
the scientific community. This article has three proposals. The first proposal
is to raise to the scientific community the necessity of unified the cognitive
behaviors. The second proposal is claim the application of the Newton's
reasoning rules about nature of his book, Philosophiae Naturalis Principia
Mathematica, to the cognitive behaviors. The third is to propose a scientific
theory, currently developing, that follows the rules established by Newton to
make sense of nature, and could be the theory to explain all the cognitive
behaviors.

3.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

4.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

5.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

6.   This paper has been withdrawn by the author due to extremely unscientific
errors.

7.   Consumers of mass media must have a comprehensive, balanced and plural
selection of news to get an unbiased perspective; but achieving this goal can
be very challenging, laborious and time consuming. News stories development
over time, its (in)consistency, and different level of coverage across the
media outlets are challenges that a conscientious reader has to overcome in
order to alleviate bias.
  In this paper we present an intelligent agent framework currently
facilitating analysis of the main sources of on-line news in El Salvador. We
show how prior tools of text analysis and Web 2.0 technologies can be combined
with minimal manual intervention to help individuals on their rational decision
process, while holding media outlets accountable for their work.

8.   The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.

9.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

10.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

11.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

12.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

13.   This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.

14.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

15.   We introduce novel results for approximate inference on planar graphical
models using the loop calculus framework. The loop calculus (Chertkov and
Chernyak, 2006) allows to express the exact partition function of a graphical
model as a finite sum of terms that can be evaluated once the belief
propagation (BP) solution is known. In general, full summation over all
correction terms is intractable. We develop an algorithm for the approach
presented in (Certkov et al., 2008) which represents an efficient truncation
scheme on planar graphs and a new representation of the series in terms of
Pfaffians of matrices. We analyze the performance of the algorithm for the
partition function approximation for models with binary variables and pairwise
interactions on grids and other planar graphs. We study in detail both the loop
series and the equivalent Pfaffian series and show that the first term of the
Pfaffian series for the general, intractable planar model, can provide very
accurate approximations. The algorithm outperforms previous truncation schemes
of the loop series and is competitive with other state-of-the-art methods for
approximate inference.

16.   Data mining algorithms are now able to efficiently deal with huge amount of
data. Various kinds of patterns may be discovered and may have some great
impact on the general development of knowledge. In many domains, end users may
want to have their data mined by data mining tools in order to extract patterns
that could impact their business. Nevertheless, those users are often
overwhelmed by the large quantity of patterns extracted in such a situation.
Moreover, some privacy issues, or some commercial one may lead the users not to
be able to mine the data by themselves. Thus, the users may not have the
possibility to perform many experiments integrating various constraints in
order to focus on specific patterns they would like to extract. Post processing
of patterns may be an answer to that drawback. Thus, in this paper we present a
framework that could allow end users to manage collections of patterns. We
propose to use an efficient data structure on which some algebraic operators
may be used in order to retrieve or access patterns in pattern bases.

17.   A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.

18.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

19.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

20.   We present in this article a new evaluation method for classification and
segmentation of textured images in uncertain environments. In uncertain
environments, real classes and boundaries are known with only a partial
certainty given by the experts. Most of the time, in many presented papers,
only classification or only segmentation are considered and evaluated. Here, we
propose to take into account both the classification and segmentation results
according to the certainty given by the experts. We present the results of this
method on a fusion of classifiers of sonar images for a seabed
characterization.

21.   Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.

22.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

23.   Domain experts should provide relevant domain knowledge to an Intelligent
Tutoring System (ITS) so that it can guide a learner during problemsolving
learning activities. However, for many ill-defined domains, the domain
knowledge is hard to define explicitly. In previous works, we showed how
sequential pattern mining can be used to extract a partial problem space from
logged user interactions, and how it can support tutoring services during
problem-solving exercises. This article describes an extension of this approach
to extract a problem space that is richer and more adapted for supporting
tutoring services. We combined sequential pattern mining with (1) dimensional
pattern mining (2) time intervals, (3) the automatic clustering of valued
actions and (4) closed sequences mining. Some tutoring services have been
implemented and an experiment has been conducted in a tutoring system.

24.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

25.   In this paper we propose the CTS (Concious Tutoring System) technology, a
biologically plausible cognitive agent based on human brain functions.This
agent is capable of learning and remembering events and any related information
such as corresponding procedures, stimuli and their emotional valences. Our
proposed episodic memory and episodic learning mechanism are closer to the
current multiple-trace theory in neuroscience, because they are inspired by it
[5] contrary to other mechanisms that are incorporated in cognitive agents.
This is because in our model emotions play a role in the encoding and
remembering of events. This allows the agent to improve its behavior by
remembering previously selected behaviors which are influenced by its emotional
mechanism. Moreover, the architecture incorporates a realistic memory
consolidation process based on a data mining algorithm.

26.   Stream computing is the use of multiple autonomic and parallel modules
together with integrative processors at a higher level of abstraction to embody
"intelligent" processing. The biological basis of this computing is sketched
and the matter of learning is examined.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of data mining and pattern extraction"
"has a topic of improving protein family detection algorithms"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of cognitive behaviors and scientific proposals"\n- "has a topic of narrative analysis in film scripts"\n- "has a topic of information fusion and conflict redistribution"\n- "has a topic of quantum computation and artificial intelligence"\n- "has a topic of intelligent agents and media analysis"\n- "has a topic of tutoring systems and sequential pattern mining"\n- "has a topic of data mining algorithms and user interaction"\n- "has a topic of structural information in protein modeling"\n- "has a topic of stability in underground openings assessment"\n- "has a topic of intelligent processing in stream computing"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   This paper has been withdrawn.

1.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

2.   This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.

3.   Domain experts should provide relevant domain knowledge to an Intelligent
Tutoring System (ITS) so that it can guide a learner during problemsolving
learning activities. However, for many ill-defined domains, the domain
knowledge is hard to define explicitly. In previous works, we showed how
sequential pattern mining can be used to extract a partial problem space from
logged user interactions, and how it can support tutoring services during
problem-solving exercises. This article describes an extension of this approach
to extract a problem space that is richer and more adapted for supporting
tutoring services. We combined sequential pattern mining with (1) dimensional
pattern mining (2) time intervals, (3) the automatic clustering of valued
actions and (4) closed sequences mining. Some tutoring services have been
implemented and an experiment has been conducted in a tutoring system.

4.   We present in this article a new evaluation method for classification and
segmentation of textured images in uncertain environments. In uncertain
environments, real classes and boundaries are known with only a partial
certainty given by the experts. Most of the time, in many presented papers,
only classification or only segmentation are considered and evaluated. Here, we
propose to take into account both the classification and segmentation results
according to the certainty given by the experts. We present the results of this
method on a fusion of classifiers of sonar images for a seabed
characterization.

5.   This paper has been withdrawn by the author due to extremely unscientific
errors.

6.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

7.   The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.

8.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

9.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

10.   We introduce novel results for approximate inference on planar graphical
models using the loop calculus framework. The loop calculus (Chertkov and
Chernyak, 2006) allows to express the exact partition function of a graphical
model as a finite sum of terms that can be evaluated once the belief
propagation (BP) solution is known. In general, full summation over all
correction terms is intractable. We develop an algorithm for the approach
presented in (Certkov et al., 2008) which represents an efficient truncation
scheme on planar graphs and a new representation of the series in terms of
Pfaffians of matrices. We analyze the performance of the algorithm for the
partition function approximation for models with binary variables and pairwise
interactions on grids and other planar graphs. We study in detail both the loop
series and the equivalent Pfaffian series and show that the first term of the
Pfaffian series for the general, intractable planar model, can provide very
accurate approximations. The algorithm outperforms previous truncation schemes
of the loop series and is competitive with other state-of-the-art methods for
approximate inference.

11.   Consumers of mass media must have a comprehensive, balanced and plural
selection of news to get an unbiased perspective; but achieving this goal can
be very challenging, laborious and time consuming. News stories development
over time, its (in)consistency, and different level of coverage across the
media outlets are challenges that a conscientious reader has to overcome in
order to alleviate bias.
  In this paper we present an intelligent agent framework currently
facilitating analysis of the main sources of on-line news in El Salvador. We
show how prior tools of text analysis and Web 2.0 technologies can be combined
with minimal manual intervention to help individuals on their rational decision
process, while holding media outlets accountable for their work.

12.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

13.   A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.

14.   Each cognitive science tries to understand a set of cognitive behaviors. The
structuring of knowledge of this nature's aspect is far from what it can be
expected about a science. Until now universal standard consistently describing
the set of cognitive behaviors has not been found, and there are many questions
about the cognitive behaviors for which only there are opinions of members of
the scientific community. This article has three proposals. The first proposal
is to raise to the scientific community the necessity of unified the cognitive
behaviors. The second proposal is claim the application of the Newton's
reasoning rules about nature of his book, Philosophiae Naturalis Principia
Mathematica, to the cognitive behaviors. The third is to propose a scientific
theory, currently developing, that follows the rules established by Newton to
make sense of nature, and could be the theory to explain all the cognitive
behaviors.

15.   In this paper we propose the CTS (Concious Tutoring System) technology, a
biologically plausible cognitive agent based on human brain functions.This
agent is capable of learning and remembering events and any related information
such as corresponding procedures, stimuli and their emotional valences. Our
proposed episodic memory and episodic learning mechanism are closer to the
current multiple-trace theory in neuroscience, because they are inspired by it
[5] contrary to other mechanisms that are incorporated in cognitive agents.
This is because in our model emotions play a role in the encoding and
remembering of events. This allows the agent to improve its behavior by
remembering previously selected behaviors which are influenced by its emotional
mechanism. Moreover, the architecture incorporates a realistic memory
consolidation process based on a data mining algorithm.

16.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

17.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

18.   Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.

19.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

20.   Stream computing is the use of multiple autonomic and parallel modules
together with integrative processors at a higher level of abstraction to embody
"intelligent" processing. The biological basis of this computing is sketched
and the matter of learning is examined.

21.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

22.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

23.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.

24.   Data mining algorithms are now able to efficiently deal with huge amount of
data. Various kinds of patterns may be discovered and may have some great
impact on the general development of knowledge. In many domains, end users may
want to have their data mined by data mining tools in order to extract patterns
that could impact their business. Nevertheless, those users are often
overwhelmed by the large quantity of patterns extracted in such a situation.
Moreover, some privacy issues, or some commercial one may lead the users not to
be able to mine the data by themselves. Thus, the users may not have the
possibility to perform many experiments integrating various constraints in
order to focus on specific patterns they would like to extract. Post processing
of patterns may be an answer to that drawback. Thus, in this paper we present a
framework that could allow end users to manage collections of patterns. We
propose to use an efficient data structure on which some algebraic operators
may be used in order to retrieve or access patterns in pattern bases.

25.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

26.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"- "has a topic of evaluation methods for classification and segmentation"
"has a topic of information fusion techniques"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of quantum computation and artificial intelligence"\n- "has a topic of intelligent tutoring systems and domain knowledge"\n- "has a topic of image classification and segmentation in uncertain environments"\n- "has a topic of definitions and theories of intelligence"\n- "has a topic of symbolic controllers in evolutionary robotics"\n- "has a topic of linguistic evidence and term extraction"\n- "has a topic of cognitive behaviors and scientific theories"\n- "has a topic of conscious tutoring systems and cognitive agents"\n- "has a topic of unconscious preferences and verbalization methods"\n- "has a topic of fuzzy block theory and stability assessment"\n- "has a topic of semantic access and natural language processing"\n- "has a topic of data mining and pattern extraction"\n- "has a topic of narrative analysis in film scripts"\n- "has a topic of dynamic data mining and clustering algorithms"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

1.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

2.   Domain experts should provide relevant domain knowledge to an Intelligent
Tutoring System (ITS) so that it can guide a learner during problemsolving
learning activities. However, for many ill-defined domains, the domain
knowledge is hard to define explicitly. In previous works, we showed how
sequential pattern mining can be used to extract a partial problem space from
logged user interactions, and how it can support tutoring services during
problem-solving exercises. This article describes an extension of this approach
to extract a problem space that is richer and more adapted for supporting
tutoring services. We combined sequential pattern mining with (1) dimensional
pattern mining (2) time intervals, (3) the automatic clustering of valued
actions and (4) closed sequences mining. Some tutoring services have been
implemented and an experiment has been conducted in a tutoring system.

3.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

4.   We introduce novel results for approximate inference on planar graphical
models using the loop calculus framework. The loop calculus (Chertkov and
Chernyak, 2006) allows to express the exact partition function of a graphical
model as a finite sum of terms that can be evaluated once the belief
propagation (BP) solution is known. In general, full summation over all
correction terms is intractable. We develop an algorithm for the approach
presented in (Certkov et al., 2008) which represents an efficient truncation
scheme on planar graphs and a new representation of the series in terms of
Pfaffians of matrices. We analyze the performance of the algorithm for the
partition function approximation for models with binary variables and pairwise
interactions on grids and other planar graphs. We study in detail both the loop
series and the equivalent Pfaffian series and show that the first term of the
Pfaffian series for the general, intractable planar model, can provide very
accurate approximations. The algorithm outperforms previous truncation schemes
of the loop series and is competitive with other state-of-the-art methods for
approximate inference.

5.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

6.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

7.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

8.   A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.

9.   This paper has been withdrawn.

10.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

11.   Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.

12.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

13.   This paper has been withdrawn by the author due to extremely unscientific
errors.

14.   Each cognitive science tries to understand a set of cognitive behaviors. The
structuring of knowledge of this nature's aspect is far from what it can be
expected about a science. Until now universal standard consistently describing
the set of cognitive behaviors has not been found, and there are many questions
about the cognitive behaviors for which only there are opinions of members of
the scientific community. This article has three proposals. The first proposal
is to raise to the scientific community the necessity of unified the cognitive
behaviors. The second proposal is claim the application of the Newton's
reasoning rules about nature of his book, Philosophiae Naturalis Principia
Mathematica, to the cognitive behaviors. The third is to propose a scientific
theory, currently developing, that follows the rules established by Newton to
make sense of nature, and could be the theory to explain all the cognitive
behaviors.

15.   This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.

16.   Stream computing is the use of multiple autonomic and parallel modules
together with integrative processors at a higher level of abstraction to embody
"intelligent" processing. The biological basis of this computing is sketched
and the matter of learning is examined.

17.   Consumers of mass media must have a comprehensive, balanced and plural
selection of news to get an unbiased perspective; but achieving this goal can
be very challenging, laborious and time consuming. News stories development
over time, its (in)consistency, and different level of coverage across the
media outlets are challenges that a conscientious reader has to overcome in
order to alleviate bias.
  In this paper we present an intelligent agent framework currently
facilitating analysis of the main sources of on-line news in El Salvador. We
show how prior tools of text analysis and Web 2.0 technologies can be combined
with minimal manual intervention to help individuals on their rational decision
process, while holding media outlets accountable for their work.

18.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.

19.   In this paper we propose the CTS (Concious Tutoring System) technology, a
biologically plausible cognitive agent based on human brain functions.This
agent is capable of learning and remembering events and any related information
such as corresponding procedures, stimuli and their emotional valences. Our
proposed episodic memory and episodic learning mechanism are closer to the
current multiple-trace theory in neuroscience, because they are inspired by it
[5] contrary to other mechanisms that are incorporated in cognitive agents.
This is because in our model emotions play a role in the encoding and
remembering of events. This allows the agent to improve its behavior by
remembering previously selected behaviors which are influenced by its emotional
mechanism. Moreover, the architecture incorporates a realistic memory
consolidation process based on a data mining algorithm.

20.   Data mining algorithms are now able to efficiently deal with huge amount of
data. Various kinds of patterns may be discovered and may have some great
impact on the general development of knowledge. In many domains, end users may
want to have their data mined by data mining tools in order to extract patterns
that could impact their business. Nevertheless, those users are often
overwhelmed by the large quantity of patterns extracted in such a situation.
Moreover, some privacy issues, or some commercial one may lead the users not to
be able to mine the data by themselves. Thus, the users may not have the
possibility to perform many experiments integrating various constraints in
order to focus on specific patterns they would like to extract. Post processing
of patterns may be an answer to that drawback. Thus, in this paper we present a
framework that could allow end users to manage collections of patterns. We
propose to use an efficient data structure on which some algebraic operators
may be used in order to retrieve or access patterns in pattern bases.

21.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

22.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

23.   We present in this article a new evaluation method for classification and
segmentation of textured images in uncertain environments. In uncertain
environments, real classes and boundaries are known with only a partial
certainty given by the experts. Most of the time, in many presented papers,
only classification or only segmentation are considered and evaluated. Here, we
propose to take into account both the classification and segmentation results
according to the certainty given by the experts. We present the results of this
method on a fusion of classifiers of sonar images for a seabed
characterization.

24.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

25.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

26.   The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of unconscious preferences and verbalization methods"
"has a topic of cognitive agents and learning mechanisms"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of term extraction and unithood measurement"\n- "has a topic of intelligent tutoring systems and domain knowledge"\n- "has a topic of narrative analysis in film scripts"\n- "has a topic of graphical models and approximate inference"\n- "has a topic of cognitive behaviors and scientific theories"\n- "has a topic of quantum computation and artificial intelligence"\n- "has a topic of data mining and pattern extraction"\n- "has a topic of conflict redistribution in belief function theory"\n- "has a topic of fuzzy block theory and underground stability assessment"\n- "has a topic of evolutionary robotics and controller design"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   Consumers of mass media must have a comprehensive, balanced and plural
selection of news to get an unbiased perspective; but achieving this goal can
be very challenging, laborious and time consuming. News stories development
over time, its (in)consistency, and different level of coverage across the
media outlets are challenges that a conscientious reader has to overcome in
order to alleviate bias.
  In this paper we present an intelligent agent framework currently
facilitating analysis of the main sources of on-line news in El Salvador. We
show how prior tools of text analysis and Web 2.0 technologies can be combined
with minimal manual intervention to help individuals on their rational decision
process, while holding media outlets accountable for their work.

1.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.

2.   The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.

3.   This paper has been withdrawn by the author due to extremely unscientific
errors.

4.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

5.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

6.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

7.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

8.   Data mining algorithms are now able to efficiently deal with huge amount of
data. Various kinds of patterns may be discovered and may have some great
impact on the general development of knowledge. In many domains, end users may
want to have their data mined by data mining tools in order to extract patterns
that could impact their business. Nevertheless, those users are often
overwhelmed by the large quantity of patterns extracted in such a situation.
Moreover, some privacy issues, or some commercial one may lead the users not to
be able to mine the data by themselves. Thus, the users may not have the
possibility to perform many experiments integrating various constraints in
order to focus on specific patterns they would like to extract. Post processing
of patterns may be an answer to that drawback. Thus, in this paper we present a
framework that could allow end users to manage collections of patterns. We
propose to use an efficient data structure on which some algebraic operators
may be used in order to retrieve or access patterns in pattern bases.

9.   Domain experts should provide relevant domain knowledge to an Intelligent
Tutoring System (ITS) so that it can guide a learner during problemsolving
learning activities. However, for many ill-defined domains, the domain
knowledge is hard to define explicitly. In previous works, we showed how
sequential pattern mining can be used to extract a partial problem space from
logged user interactions, and how it can support tutoring services during
problem-solving exercises. This article describes an extension of this approach
to extract a problem space that is richer and more adapted for supporting
tutoring services. We combined sequential pattern mining with (1) dimensional
pattern mining (2) time intervals, (3) the automatic clustering of valued
actions and (4) closed sequences mining. Some tutoring services have been
implemented and an experiment has been conducted in a tutoring system.

10.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

11.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

12.   We present in this article a new evaluation method for classification and
segmentation of textured images in uncertain environments. In uncertain
environments, real classes and boundaries are known with only a partial
certainty given by the experts. Most of the time, in many presented papers,
only classification or only segmentation are considered and evaluated. Here, we
propose to take into account both the classification and segmentation results
according to the certainty given by the experts. We present the results of this
method on a fusion of classifiers of sonar images for a seabed
characterization.

13.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

14.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

15.   This paper has been withdrawn.

16.   This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.

17.   Stream computing is the use of multiple autonomic and parallel modules
together with integrative processors at a higher level of abstraction to embody
"intelligent" processing. The biological basis of this computing is sketched
and the matter of learning is examined.

18.   We introduce novel results for approximate inference on planar graphical
models using the loop calculus framework. The loop calculus (Chertkov and
Chernyak, 2006) allows to express the exact partition function of a graphical
model as a finite sum of terms that can be evaluated once the belief
propagation (BP) solution is known. In general, full summation over all
correction terms is intractable. We develop an algorithm for the approach
presented in (Certkov et al., 2008) which represents an efficient truncation
scheme on planar graphs and a new representation of the series in terms of
Pfaffians of matrices. We analyze the performance of the algorithm for the
partition function approximation for models with binary variables and pairwise
interactions on grids and other planar graphs. We study in detail both the loop
series and the equivalent Pfaffian series and show that the first term of the
Pfaffian series for the general, intractable planar model, can provide very
accurate approximations. The algorithm outperforms previous truncation schemes
of the loop series and is competitive with other state-of-the-art methods for
approximate inference.

19.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

20.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

21.   A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.

22.   Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.

23.   In this paper we propose the CTS (Concious Tutoring System) technology, a
biologically plausible cognitive agent based on human brain functions.This
agent is capable of learning and remembering events and any related information
such as corresponding procedures, stimuli and their emotional valences. Our
proposed episodic memory and episodic learning mechanism are closer to the
current multiple-trace theory in neuroscience, because they are inspired by it
[5] contrary to other mechanisms that are incorporated in cognitive agents.
This is because in our model emotions play a role in the encoding and
remembering of events. This allows the agent to improve its behavior by
remembering previously selected behaviors which are influenced by its emotional
mechanism. Moreover, the architecture incorporates a realistic memory
consolidation process based on a data mining algorithm.

24.   Each cognitive science tries to understand a set of cognitive behaviors. The
structuring of knowledge of this nature's aspect is far from what it can be
expected about a science. Until now universal standard consistently describing
the set of cognitive behaviors has not been found, and there are many questions
about the cognitive behaviors for which only there are opinions of members of
the scientific community. This article has three proposals. The first proposal
is to raise to the scientific community the necessity of unified the cognitive
behaviors. The second proposal is claim the application of the Newton's
reasoning rules about nature of his book, Philosophiae Naturalis Principia
Mathematica, to the cognitive behaviors. The third is to propose a scientific
theory, currently developing, that follows the rules established by Newton to
make sense of nature, and could be the theory to explain all the cognitive
behaviors.

25.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

26.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of conflict redistribution in decision theory"
"has a topic of data mining and pattern extraction"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of intelligent agent frameworks in media analysis"\n- "has a topic of modular architecture in logic programming"\n- "has a topic of symbolic controllers in evolutionary robotics"\n- "has a topic of unconscious preference awareness methods"\n- "has a topic of protein family detection using hidden Markov models"\n- "has a topic of configuration frameworks in computing"\n- "has a topic of definitions of intelligence in cognitive science"\n- "has a topic of data mining challenges and user interaction"\n- "has a topic of intelligent tutoring systems and pattern mining"\n- "has a topic of narrative analysis in film scripts"\n- "has a topic of fuzzy block theory in stability assessment"\n- "has a topic of classification and segmentation in uncertain environments"\n- "has a topic of unithood measurement in linguistics"\n- "has a topic of quantum computation applications in artificial intelligence"\n- "has a topic of stream computing and learning processes"\n- "has a topic of approximate inference in graphical models"\n- "has a topic of dynamic data mining and clustering algorithms"\n- "has a topic of information fusion and redistribution rules"\n- "has a topic of causality definitions in philosophical logic"\n- "has a topic of semantic web access and natural language processing"\n- "has a topic of cognitive agents and memory mechanisms"\n- "has a topic of cognitive behavior theories in science"\n- "has a topic of conflict management in belief function theory"\n- "has a topic of geometric data analysis in statistical methods"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

1.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

2.   We present in this article a new evaluation method for classification and
segmentation of textured images in uncertain environments. In uncertain
environments, real classes and boundaries are known with only a partial
certainty given by the experts. Most of the time, in many presented papers,
only classification or only segmentation are considered and evaluated. Here, we
propose to take into account both the classification and segmentation results
according to the certainty given by the experts. We present the results of this
method on a fusion of classifiers of sonar images for a seabed
characterization.

3.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

4.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

5.   The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.

6.   Each cognitive science tries to understand a set of cognitive behaviors. The
structuring of knowledge of this nature's aspect is far from what it can be
expected about a science. Until now universal standard consistently describing
the set of cognitive behaviors has not been found, and there are many questions
about the cognitive behaviors for which only there are opinions of members of
the scientific community. This article has three proposals. The first proposal
is to raise to the scientific community the necessity of unified the cognitive
behaviors. The second proposal is claim the application of the Newton's
reasoning rules about nature of his book, Philosophiae Naturalis Principia
Mathematica, to the cognitive behaviors. The third is to propose a scientific
theory, currently developing, that follows the rules established by Newton to
make sense of nature, and could be the theory to explain all the cognitive
behaviors.

7.   This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.

8.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

9.   We introduce novel results for approximate inference on planar graphical
models using the loop calculus framework. The loop calculus (Chertkov and
Chernyak, 2006) allows to express the exact partition function of a graphical
model as a finite sum of terms that can be evaluated once the belief
propagation (BP) solution is known. In general, full summation over all
correction terms is intractable. We develop an algorithm for the approach
presented in (Certkov et al., 2008) which represents an efficient truncation
scheme on planar graphs and a new representation of the series in terms of
Pfaffians of matrices. We analyze the performance of the algorithm for the
partition function approximation for models with binary variables and pairwise
interactions on grids and other planar graphs. We study in detail both the loop
series and the equivalent Pfaffian series and show that the first term of the
Pfaffian series for the general, intractable planar model, can provide very
accurate approximations. The algorithm outperforms previous truncation schemes
of the loop series and is competitive with other state-of-the-art methods for
approximate inference.

10.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

11.   Stream computing is the use of multiple autonomic and parallel modules
together with integrative processors at a higher level of abstraction to embody
"intelligent" processing. The biological basis of this computing is sketched
and the matter of learning is examined.

12.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

13.   Domain experts should provide relevant domain knowledge to an Intelligent
Tutoring System (ITS) so that it can guide a learner during problemsolving
learning activities. However, for many ill-defined domains, the domain
knowledge is hard to define explicitly. In previous works, we showed how
sequential pattern mining can be used to extract a partial problem space from
logged user interactions, and how it can support tutoring services during
problem-solving exercises. This article describes an extension of this approach
to extract a problem space that is richer and more adapted for supporting
tutoring services. We combined sequential pattern mining with (1) dimensional
pattern mining (2) time intervals, (3) the automatic clustering of valued
actions and (4) closed sequences mining. Some tutoring services have been
implemented and an experiment has been conducted in a tutoring system.

14.   This paper has been withdrawn by the author due to extremely unscientific
errors.

15.   In this paper we propose the CTS (Concious Tutoring System) technology, a
biologically plausible cognitive agent based on human brain functions.This
agent is capable of learning and remembering events and any related information
such as corresponding procedures, stimuli and their emotional valences. Our
proposed episodic memory and episodic learning mechanism are closer to the
current multiple-trace theory in neuroscience, because they are inspired by it
[5] contrary to other mechanisms that are incorporated in cognitive agents.
This is because in our model emotions play a role in the encoding and
remembering of events. This allows the agent to improve its behavior by
remembering previously selected behaviors which are influenced by its emotional
mechanism. Moreover, the architecture incorporates a realistic memory
consolidation process based on a data mining algorithm.

16.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

17.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

18.   Data mining algorithms are now able to efficiently deal with huge amount of
data. Various kinds of patterns may be discovered and may have some great
impact on the general development of knowledge. In many domains, end users may
want to have their data mined by data mining tools in order to extract patterns
that could impact their business. Nevertheless, those users are often
overwhelmed by the large quantity of patterns extracted in such a situation.
Moreover, some privacy issues, or some commercial one may lead the users not to
be able to mine the data by themselves. Thus, the users may not have the
possibility to perform many experiments integrating various constraints in
order to focus on specific patterns they would like to extract. Post processing
of patterns may be an answer to that drawback. Thus, in this paper we present a
framework that could allow end users to manage collections of patterns. We
propose to use an efficient data structure on which some algebraic operators
may be used in order to retrieve or access patterns in pattern bases.

19.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.

20.   A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.

21.   Consumers of mass media must have a comprehensive, balanced and plural
selection of news to get an unbiased perspective; but achieving this goal can
be very challenging, laborious and time consuming. News stories development
over time, its (in)consistency, and different level of coverage across the
media outlets are challenges that a conscientious reader has to overcome in
order to alleviate bias.
  In this paper we present an intelligent agent framework currently
facilitating analysis of the main sources of on-line news in El Salvador. We
show how prior tools of text analysis and Web 2.0 technologies can be combined
with minimal manual intervention to help individuals on their rational decision
process, while holding media outlets accountable for their work.

22.   This paper has been withdrawn.

23.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

24.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

25.   Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.

26.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of information fusion techniques"
"- "has a topic of evaluation methods for classification and segmentation"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of fuzzy block theory applications"\n- "has a topic of definitions of intelligence"\n- "has a topic of evaluation methods in uncertain environments"\n- "has a topic of unithood measurement techniques"\n- "has a topic of protein family detection using pHMMs"\n- "has a topic of cognitive behavior theories"\n- "has a topic of quantum computation in artificial intelligence"\n- "has a topic of narrative analysis in film scripts"\n- "has a topic of approximate inference in graphical models"\n- "has a topic of conflict redistribution in belief function theory"\n- "has a topic of intelligent tutoring systems"\n- "has a topic of unconscious preference awareness methods"\n- "has a topic of dynamic data mining challenges"\n- "has a topic of semantic access in information retrieval"']
192 descriptions proposed in total. Here are the descriptions:
- "has a topic of soft constraints and optimization
has a topic of conflict prediction using machine learning
- "has a topic of semantic web technologies
has a topic of qualitative temporal reasoning
has a topic of automated leaf recognition and classification
has a topic of Bayesian frameworks in modeling
has a topic of face recognition robustness
has a topic of Bayesian frameworks in machine learning
has a topic of multi-agent systems
- "has a topic of automated leaf recognition algorithms
has a topic of evolutionary robotics
has a topic of clustering techniques and algorithms
has a topic of quantum computation applications
has a topic of water demand forecasting
has a topic of quantum computation and artificial intelligence
has a topic of fault classification in cylindrical shells
has a topic of evolutionary learning and ensemble methods
has a topic of multi-cellular developmental design
has a topic of condition monitoring in engineering
- "has a topic of ensemble learning techniques
has a topic of water demand forecasting using computational intelligence
has a topic of HIV risk modeling
- "has a topic of computational intelligence techniques
has a topic of fault diagnosis and classification
has a topic of Bayesian methods in rough set models
has a topic of decentralized control in multi-agent systems
has a topic of protein family detection
has a topic of artificial intelligence in healthcare
has a topic of neural network classifiers
has a topic of neural network models
has a topic of semantic web technologies
- "has a topic of evolutionary robotics
has a topic of semantic web and natural language processing
has a topic of clustering algorithms and techniques
has a topic of ensemble learning approaches
has a topic of artificial intelligence history
has a topic of decentralized control systems
has a topic of qualitative temporal information representation
- "has a topic of evolutionary learning and ensemble methods
has a topic of automated leaf recognition algorithms
has a topic of handling missing data in neural networks
has a topic of conflict prediction and machine learning
- "has a topic of protein family detection
has a topic of condition monitoring frameworks
has a topic of semantic web and computational modeling
has a topic of neural network performance evaluation
has a topic of predicting militarized interstate disputes
has a topic of fault classification methods
has a topic of fault diagnosis and monitoring
has a topic of artificial intelligence applications
has a topic of fault classification and condition monitoring in engineering
has a topic of ontology and conceptualization in knowledge representation
- "has a topic of constraint satisfaction problems and algorithms
has a topic of social network analysis and covert nodes
has a topic of knowledge management and ontology development
- "has a topic of machine intelligence definitions and tests
has a topic of artificial intelligence and definitions of intelligence
has a topic of computational methods in social network analysis
has a topic of simulation and philosophical implications of intelligence
has a topic of knowledge management and ontology
has a topic of constraint satisfaction problems and algorithm design
has a topic of machine intelligence and its definitions
has a topic of terrorism analysis and intelligence visualization
has a topic of evolutionary algorithms and cellular automata
has a topic of evolutionary theory and simulations
- "has a topic of fitness landscapes and optimization algorithms
has a topic of constraint satisfaction problems and propagators
- "has a topic of causality and definitions in artificial intelligence
has a topic of scientific simulations and philosophical implications
has a topic of fuzzy logic in stability assessment of underground openings
has a topic of causality and default theories
has a topic of knowledge management systems in organizational clusters
- "has a topic of covert node discovery in social networks
- "has a topic of social network analysis and covert nodes
has a topic of definitions and measures of intelligence
has a topic of hydrocyclone operation analysis using rough set theory
has a topic of mining methods and operational efficiency
has a topic of terrorism and social network dynamics
has a topic of fuzzy logic and stability assessment in engineering
has a topic of computational methods in terrorism investigation
has a topic of simulation and philosophical implications
has a topic of social network analysis and covert node discovery
has a topic of fuzzy logic and stability assessment
has a topic of intelligence measurement and definitions in AI
has a topic of fuzzy logic and decision-making systems
has a topic of intelligence definitions and machine learning
has a topic of rough set theory applications in data analysis
has a topic of artificial intelligence and intelligence measurement
has a topic of fuzzy logic and fault classification
has a topic of constraint satisfaction problems and optimization
has a topic of fault classification using neuro-fuzzy models
has a topic of fitness landscapes and optimization
has a topic of constraint satisfaction problems and optimization techniques
has a topic of definitions and measurements of intelligence
has a topic of computational methods in terrorism analysis
has a topic of social network analysis and covert operations
has a topic of simulation and modeling of complex systems
has a topic of fuzzy block theory and stability assessment
has a topic of fuzzy logic and its applications
- "has a topic of fuzzy logic applications
has a topic of linguistic evidence and unithood measurement
has a topic of belief function theory
- "has a topic of narrative analysis in film scripts
has a topic of neural network applications
has a topic of optimization algorithms for multi-objective problems
has a topic of fuzzy decision-making models
- "has a topic of data mining challenges
has a topic of optimization techniques
has a topic of clustering algorithms
has a topic of narrative analysis in film scripts
has a topic of fuzzy logic applications in stability assessment
has a topic of clustering algorithms and data analysis
has a topic of optimization techniques in scheduling
has a topic of belief function theory and information fusion
- "has a topic of belief function theory and information fusion
has a topic of information fusion methods
- "has a topic of face recognition systems
has a topic of information fusion and conflict redistribution
has a topic of image processing and recognition systems
has a topic of structural protein analysis
has a topic of protein family detection using hidden Markov models
has a topic of data mining techniques
has a topic of data mining challenges
- "has a topic of belief function theory
has a topic of data-stream clustering and weak signals detection
- "has a topic of neural networks in structural engineering
has a topic of missing data handling in neural networks
has a topic of qualitative analysis in narratives
has a topic of information fusion techniques
has a topic of qualitative analysis in narrative structures
has a topic of narrative analysis in film
has a topic of fuzzy logic and decision-making
has a topic of belief function frameworks and decision making
has a topic of multi-objective optimization methods
has a topic of multi-objective optimization
- "has a topic of artificial neural networks in earthquake response prediction
has a topic of data mining and clustering algorithms
has a topic of multi-objective optimization in scheduling problems
has a topic of fuzzy set theory and applications
has a topic of fuzzy logic and optimization techniques
has a topic of unconscious preference awareness and verbalization
has a topic of fuzzy logic and neutrosophic operators
has a topic of data clustering methods
has a topic of conflict prediction in international relations
has a topic of qualitative belief functions and reasoning under uncertainty
has a topic of neural networks in earthquake response prediction
has a topic of hydrocyclone performance analysis using fuzzy theory
has a topic of conscious tutoring systems and cognitive agents
has a topic of program semantics and modularity
has a topic of improving protein family detection algorithms
has a topic of data mining and pattern extraction
has a topic of dynamic data mining challenges
- "has a topic of cognitive behaviors and scientific proposals
- "has a topic of evaluation methods for classification and segmentation
has a topic of unconscious preferences and verbalization methods
has a topic of conflict redistribution in decision theory
has a topic of measuring unithood in linguistics
has a topic of cognitive behaviors and scientific theories
has a topic of fuzzy theory and stability assessment
has a topic of unconscious preference awareness methods
has a topic of data mining algorithms and user interaction
has a topic of conflict redistribution in belief function theory
has a topic of symbolic controllers in evolutionary robotics
has a topic of graphical models and approximate inference
- "has a topic of protein family detection using hidden Markov models
- "has a topic of term extraction and unithood measurement
has a topic of fuzzy block theory in underground stability assessment
has a topic of definitions and theories of intelligence
has a topic of unithood measurement techniques
has a topic of data mining challenges and user interaction
has a topic of intelligent tutoring systems and domain knowledge
- "has a topic of quantum computation and artificial intelligence
has a topic of protein family detection using pHMMs
has a topic of semantic access and natural language processing
- "has a topic of fuzzy block theory applications
has a topic of configuration frameworks in computing
has a topic of intelligent agents for news analysis
has a topic of image classification and segmentation in uncertain environments
has a topic of definitions of intelligence
has a topic of cognitive agents and learning mechanisms
has a topic of intelligent agents and media analysis
has a topic of definitions of intelligence in cognitive science
has a topic of tutoring systems and sequential pattern mining
has a topic of structural information in protein modeling
has a topic of linguistic evidence and term extraction
has a topic of cognitive behavior theories
has a topic of causality and default reasoning
has a topic of intelligent tutoring systems and domain knowledge extraction
- "has a topic of intelligent agent frameworks in media analysis
has a topic of modular architecture in logic programming
has a topic of evaluation methods in uncertain environments
- "has a topic of cognitive agents and learning mechanisms
First prompt example:
You are tasked with classifying whether a TEXT satisfies a PROPERTY. You need to respond with Yes or No, and when uncertain, always respond with No.

<example>
<input>
Text: I like apples.
Property: is talking about fruits
</input>
<output>
Yes
</output>
</example>

Now, solve this new task:
Text:   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

Property: has a topic of conscious tutoring systems and cognitive agents

Processing 45000 prompts with vLLM's automatic batching...
Stage Assign assign
Assigned all recall: 0.993
Assigned all precision: 0.02416918429003021
Assigned description: - "has a topic of soft constraints and optimization. recall: 0.213 precision: 0.0
Assigned description: has a topic of conflict prediction using machine learning. recall: 0.013 precision: 0.0
Assigned description: - "has a topic of semantic web technologies. recall: 0.042 precision: 0.0
Assigned description: has a topic of qualitative temporal reasoning. recall: 0.062 precision: 0.0
Assigned description: has a topic of automated leaf recognition and classification. recall: 0.001 precision: 0.0
Assigned description: has a topic of Bayesian frameworks in modeling. recall: 0.125 precision: 0.0
Assigned description: has a topic of face recognition robustness. recall: 0.001 precision: 0.0
Assigned description: has a topic of Bayesian frameworks in machine learning. recall: 0.104 precision: 0.0
Assigned description: has a topic of multi-agent systems. recall: 0.069 precision: 0.0
Assigned description: - "has a topic of automated leaf recognition algorithms. recall: 0.001 precision: 0.0
Assigned description: has a topic of evolutionary robotics. recall: 0.013 precision: 0.0
Assigned description: has a topic of clustering techniques and algorithms. recall: 0.039 precision: 0.0
Assigned description: has a topic of quantum computation applications. recall: 0.002 precision: 0.0
Assigned description: has a topic of water demand forecasting. recall: 0.001 precision: 0.0
Assigned description: has a topic of quantum computation and artificial intelligence. recall: 0.005 precision: 0.0
Assigned description: has a topic of fault classification in cylindrical shells. recall: 0.002 precision: 0.0
Assigned description: has a topic of evolutionary learning and ensemble methods. recall: 0.032 precision: 0.0
Assigned description: has a topic of multi-cellular developmental design. recall: 0.001 precision: 0.0
Assigned description: has a topic of condition monitoring in engineering. recall: 0.014 precision: 0.0
Assigned description: - "has a topic of ensemble learning techniques. recall: 0.039 precision: 0.0
Assigned description: has a topic of water demand forecasting using computational intelligence. recall: 0.001 precision: 0.0
Assigned description: has a topic of HIV risk modeling. recall: 0.002 precision: 0.0
Assigned description: - "has a topic of computational intelligence techniques. recall: 0.624 precision: 0.0
Assigned description: has a topic of fault diagnosis and classification. recall: 0.024 precision: 0.0
Assigned description: has a topic of Bayesian methods in rough set models. recall: 0.012 precision: 0.0
Assigned description: has a topic of decentralized control in multi-agent systems. recall: 0.034 precision: 0.0
Assigned description: has a topic of protein family detection. recall: 0.001 precision: 0.0
Assigned description: has a topic of artificial intelligence in healthcare. recall: 0.028 precision: 0.0
Assigned description: has a topic of neural network classifiers. recall: 0.031 precision: 0.0
Assigned description: has a topic of neural network models. recall: 0.046 precision: 0.0
Assigned description: has a topic of semantic web technologies. recall: 0.046 precision: 0.0
Assigned description: - "has a topic of evolutionary robotics. recall: 0.013 precision: 0.0
Assigned description: has a topic of semantic web and natural language processing. recall: 0.066 precision: 0.0
Assigned description: has a topic of clustering algorithms and techniques. recall: 0.034 precision: 0.0
Assigned description: has a topic of ensemble learning approaches. recall: 0.048 precision: 0.0
Assigned description: has a topic of artificial intelligence history. recall: 0.009 precision: 0.0
Assigned description: has a topic of decentralized control systems. recall: 0.023 precision: 0.0
Assigned description: has a topic of qualitative temporal information representation. recall: 0.101 precision: 0.0
Assigned description: - "has a topic of evolutionary learning and ensemble methods. recall: 0.028 precision: 0.0
Assigned description: has a topic of automated leaf recognition algorithms. recall: 0.001 precision: 0.0
Assigned description: has a topic of handling missing data in neural networks. recall: 0.002 precision: 0.0
Assigned description: has a topic of conflict prediction and machine learning. recall: 0.027 precision: 0.0
Assigned description: - "has a topic of protein family detection. recall: 0.001 precision: 0.0
Assigned description: has a topic of condition monitoring frameworks. recall: 0.022 precision: 0.0
Assigned description: has a topic of semantic web and computational modeling. recall: 0.12 precision: 0.008333333333333333
Assigned description: has a topic of neural network performance evaluation. recall: 0.018 precision: 0.0
Assigned description: has a topic of predicting militarized interstate disputes. recall: 0.001 precision: 0.0
Assigned description: has a topic of fault classification methods. recall: 0.014 precision: 0.0
Assigned description: has a topic of fault diagnosis and monitoring. recall: 0.029 precision: 0.0
Assigned description: has a topic of artificial intelligence applications. recall: 0.406 precision: 0.0
Assigned description: has a topic of fault classification and condition monitoring in engineering. recall: 0.011 precision: 0.0
Assigned description: has a topic of ontology and conceptualization in knowledge representation. recall: 0.278 precision: 0.025179856115107913
Assigned description: - "has a topic of constraint satisfaction problems and algorithms. recall: 0.412 precision: 0.0
Assigned description: has a topic of social network analysis and covert nodes. recall: 0.015 precision: 0.0
Assigned description: has a topic of knowledge management and ontology development. recall: 0.098 precision: 0.0
Assigned description: - "has a topic of machine intelligence definitions and tests. recall: 0.044 precision: 0.0
Assigned description: has a topic of artificial intelligence and definitions of intelligence. recall: 0.141 precision: 0.0
Assigned description: has a topic of computational methods in social network analysis. recall: 0.026 precision: 0.0
Assigned description: has a topic of simulation and philosophical implications of intelligence. recall: 0.038 precision: 0.0
Assigned description: has a topic of knowledge management and ontology. recall: 0.11 precision: 0.0
Assigned description: has a topic of constraint satisfaction problems and algorithm design. recall: 0.475 precision: 0.0
Assigned description: has a topic of machine intelligence and its definitions. recall: 0.186 precision: 0.0
Assigned description: has a topic of terrorism analysis and intelligence visualization. recall: 0.005 precision: 0.0
Assigned description: has a topic of evolutionary algorithms and cellular automata. recall: 0.005 precision: 0.0
Assigned description: has a topic of evolutionary theory and simulations. recall: 0.04 precision: 0.0
Assigned description: - "has a topic of fitness landscapes and optimization algorithms. recall: 0.043 precision: 0.0
Assigned description: has a topic of constraint satisfaction problems and propagators. recall: 0.172 precision: 0.0
Assigned description: - "has a topic of causality and definitions in artificial intelligence. recall: 0.122 precision: 0.0
Assigned description: has a topic of scientific simulations and philosophical implications. recall: 0.145 precision: 0.0
Assigned description: has a topic of fuzzy logic in stability assessment of underground openings. recall: 0.001 precision: 0.0
Assigned description: has a topic of causality and default theories. recall: 0.048 precision: 0.0
Assigned description: has a topic of knowledge management systems in organizational clusters. recall: 0.013 precision: 0.0
Assigned description: - "has a topic of covert node discovery in social networks. recall: 0.005 precision: 0.0
Assigned description: - "has a topic of social network analysis and covert nodes. recall: 0.014 precision: 0.0
Assigned description: has a topic of definitions and measures of intelligence. recall: 0.024 precision: 0.0
Assigned description: has a topic of hydrocyclone operation analysis using rough set theory. recall: 0.002 precision: 0.0
Assigned description: has a topic of mining methods and operational efficiency. recall: 0.107 precision: 0.0
Assigned description: has a topic of terrorism and social network dynamics. recall: 0.002 precision: 0.0
Assigned description: has a topic of fuzzy logic and stability assessment in engineering. recall: 0.016 precision: 0.0
Assigned description: has a topic of computational methods in terrorism investigation. recall: 0.005 precision: 0.0
Assigned description: has a topic of simulation and philosophical implications. recall: 0.08 precision: 0.0
Assigned description: has a topic of social network analysis and covert node discovery. recall: 0.013 precision: 0.0
Assigned description: has a topic of fuzzy logic and stability assessment. recall: 0.049 precision: 0.0
Assigned description: has a topic of intelligence measurement and definitions in AI. recall: 0.049 precision: 0.0
Assigned description: has a topic of fuzzy logic and decision-making systems. recall: 0.102 precision: 0.0
Assigned description: has a topic of intelligence definitions and machine learning. recall: 0.067 precision: 0.0
Assigned description: has a topic of rough set theory applications in data analysis. recall: 0.019 precision: 0.0
Assigned description: has a topic of artificial intelligence and intelligence measurement. recall: 0.384 precision: 0.0078125
Assigned description: has a topic of fuzzy logic and fault classification. recall: 0.005 precision: 0.0
Assigned description: has a topic of constraint satisfaction problems and optimization. recall: 0.359 precision: 0.0
Assigned description: has a topic of fault classification using neuro-fuzzy models. recall: 0.004 precision: 0.0
Assigned description: has a topic of fitness landscapes and optimization. recall: 0.048 precision: 0.0
Assigned description: has a topic of constraint satisfaction problems and optimization techniques. recall: 0.388 precision: 0.0
Assigned description: has a topic of definitions and measurements of intelligence. recall: 0.016 precision: 0.0
Assigned description: has a topic of computational methods in terrorism analysis. recall: 0.006 precision: 0.0
Assigned description: has a topic of social network analysis and covert operations. recall: 0.004 precision: 0.0
Assigned description: has a topic of simulation and modeling of complex systems. recall: 0.289 precision: 0.006920415224913495
Assigned description: has a topic of fuzzy block theory and stability assessment. recall: 0.002 precision: 0.0
Assigned description: has a topic of fuzzy logic and its applications. recall: 0.073 precision: 0.0
Assigned description: - "has a topic of fuzzy logic applications. recall: 0.058 precision: 0.0
Assigned description: has a topic of linguistic evidence and unithood measurement. recall: 0.035 precision: 0.0
Assigned description: has a topic of belief function theory. recall: 0.119 precision: 0.0
Assigned description: - "has a topic of narrative analysis in film scripts. recall: 0.002 precision: 0.0
Assigned description: has a topic of neural network applications. recall: 0.048 precision: 0.0
Assigned description: has a topic of optimization algorithms for multi-objective problems. recall: 0.034 precision: 0.0
Assigned description: has a topic of fuzzy decision-making models. recall: 0.1 precision: 0.0
Assigned description: - "has a topic of data mining challenges. recall: 0.059 precision: 0.0
Assigned description: has a topic of optimization techniques. recall: 0.477 precision: 0.0041928721174004195
Assigned description: has a topic of clustering algorithms. recall: 0.032 precision: 0.0
Assigned description: has a topic of narrative analysis in film scripts. recall: 0.002 precision: 0.0
Assigned description: has a topic of fuzzy logic applications in stability assessment. recall: 0.006 precision: 0.0
Assigned description: has a topic of clustering algorithms and data analysis. recall: 0.036 precision: 0.0
Assigned description: has a topic of optimization techniques in scheduling. recall: 0.041 precision: 0.0
Assigned description: has a topic of belief function theory and information fusion. recall: 0.162 precision: 0.0
Assigned description: - "has a topic of belief function theory and information fusion. recall: 0.227 precision: 0.004405286343612335
Assigned description: has a topic of information fusion methods. recall: 0.199 precision: 0.0
Assigned description: - "has a topic of face recognition systems. recall: 0.004 precision: 0.0
Assigned description: has a topic of information fusion and conflict redistribution. recall: 0.118 precision: 0.0
Assigned description: has a topic of image processing and recognition systems. recall: 0.023 precision: 0.043478260869565216
Assigned description: has a topic of structural protein analysis. recall: 0.002 precision: 0.0
Assigned description: has a topic of protein family detection using hidden Markov models. recall: 0.001 precision: 0.0
Assigned description: has a topic of data mining techniques. recall: 0.141 precision: 0.0
Assigned description: has a topic of data mining challenges. recall: 0.08 precision: 0.0
Assigned description: - "has a topic of belief function theory. recall: 0.155 precision: 0.0
Assigned description: has a topic of data-stream clustering and weak signals detection. recall: 0.002 precision: 0.0
Assigned description: - "has a topic of neural networks in structural engineering. recall: 0.003 precision: 0.0
Assigned description: has a topic of missing data handling in neural networks. recall: 0.005 precision: 0.0
Assigned description: has a topic of qualitative analysis in narratives. recall: 0.02 precision: 0.0
Assigned description: has a topic of information fusion techniques. recall: 0.174 precision: 0.0
Assigned description: has a topic of qualitative analysis in narrative structures. recall: 0.006 precision: 0.0
Assigned description: has a topic of narrative analysis in film. recall: 0.002 precision: 0.0
Assigned description: has a topic of fuzzy logic and decision-making. recall: 0.108 precision: 0.0
Assigned description: has a topic of belief function frameworks and decision making. recall: 0.17 precision: 0.0
Assigned description: has a topic of multi-objective optimization methods. recall: 0.04 precision: 0.0
Assigned description: has a topic of multi-objective optimization. recall: 0.041 precision: 0.0
Assigned description: - "has a topic of artificial neural networks in earthquake response prediction. recall: 0.001 precision: 0.0
Assigned description: has a topic of data mining and clustering algorithms. recall: 0.047 precision: 0.0
Assigned description: has a topic of multi-objective optimization in scheduling problems. recall: 0.015 precision: 0.0
Assigned description: has a topic of fuzzy set theory and applications. recall: 0.084 precision: 0.0
Assigned description: has a topic of fuzzy logic and optimization techniques. recall: 0.07 precision: 0.0
Assigned description: has a topic of unconscious preference awareness and verbalization. recall: 0.004 precision: 0.0
Assigned description: has a topic of fuzzy logic and neutrosophic operators. recall: 0.031 precision: 0.0
Assigned description: has a topic of data clustering methods. recall: 0.031 precision: 0.0
Assigned description: has a topic of conflict prediction in international relations. recall: 0.001 precision: 0.0
Assigned description: has a topic of qualitative belief functions and reasoning under uncertainty. recall: 0.141 precision: 0.0
Assigned description: has a topic of neural networks in earthquake response prediction. recall: 0.001 precision: 0.0
Assigned description: has a topic of hydrocyclone performance analysis using fuzzy theory. recall: 0.003 precision: 0.0
Assigned description: has a topic of conscious tutoring systems and cognitive agents. recall: 0.026 precision: 0.0
Assigned description: has a topic of program semantics and modularity. recall: 0.093 precision: 0.010752688172043012
Assigned description: has a topic of improving protein family detection algorithms. recall: 0.001 precision: 0.0
Assigned description: has a topic of data mining and pattern extraction. recall: 0.209 precision: 0.0
Assigned description: has a topic of dynamic data mining challenges. recall: 0.069 precision: 0.0
Assigned description: - "has a topic of cognitive behaviors and scientific proposals. recall: 0.305 precision: 0.0
Assigned description: - "has a topic of evaluation methods for classification and segmentation. recall: 0.074 precision: 0.013513513513513514
Assigned description: has a topic of unconscious preferences and verbalization methods. recall: 0.003 precision: 0.0
Assigned description: has a topic of conflict redistribution in decision theory. recall: 0.007 precision: 0.0
Assigned description: has a topic of measuring unithood in linguistics. recall: 0.002 precision: 0.0
Assigned description: has a topic of cognitive behaviors and scientific theories. recall: 0.172 precision: 0.0
Assigned description: has a topic of fuzzy theory and stability assessment. recall: 0.056 precision: 0.0
Assigned description: has a topic of unconscious preference awareness methods. recall: 0.007 precision: 0.0
Assigned description: has a topic of data mining algorithms and user interaction. recall: 0.024 precision: 0.0
Assigned description: has a topic of conflict redistribution in belief function theory. recall: 0.006 precision: 0.0
Assigned description: has a topic of symbolic controllers in evolutionary robotics. recall: 0.003 precision: 0.0
Assigned description: has a topic of graphical models and approximate inference. recall: 0.152 precision: 0.0
Assigned description: - "has a topic of protein family detection using hidden Markov models. recall: 0.001 precision: 0.0
Assigned description: - "has a topic of term extraction and unithood measurement. recall: 0.037 precision: 0.0
Assigned description: has a topic of fuzzy block theory in underground stability assessment. recall: 0.001 precision: 0.0
Assigned description: has a topic of definitions and theories of intelligence. recall: 0.039 precision: 0.0
Assigned description: has a topic of unithood measurement techniques. recall: 0.009 precision: 0.0
Assigned description: has a topic of data mining challenges and user interaction. recall: 0.036 precision: 0.0
Assigned description: has a topic of intelligent tutoring systems and domain knowledge. recall: 0.019 precision: 0.0
Assigned description: - "has a topic of quantum computation and artificial intelligence. recall: 0.005 precision: 0.0
Assigned description: has a topic of protein family detection using pHMMs. recall: 0.001 precision: 0.0
Assigned description: has a topic of semantic access and natural language processing. recall: 0.109 precision: 0.0
Assigned description: - "has a topic of fuzzy block theory applications. recall: 0.003 precision: 0.0
Assigned description: has a topic of configuration frameworks in computing. recall: 0.082 precision: 0.024390243902439025
Assigned description: has a topic of intelligent agents for news analysis. recall: 0.001 precision: 0.0
Assigned description: has a topic of image classification and segmentation in uncertain environments. recall: 0.006 precision: 0.0
Assigned description: has a topic of definitions of intelligence. recall: 0.018 precision: 0.0
Assigned description: has a topic of cognitive agents and learning mechanisms. recall: 0.186 precision: 0.0
Assigned description: has a topic of intelligent agents and media analysis. recall: 0.039 precision: 0.0
Assigned description: has a topic of definitions of intelligence in cognitive science. recall: 0.01 precision: 0.0
Assigned description: has a topic of tutoring systems and sequential pattern mining. recall: 0.005 precision: 0.0
Assigned description: has a topic of structural information in protein modeling. recall: 0.004 precision: 0.0
Assigned description: has a topic of linguistic evidence and term extraction. recall: 0.085 precision: 0.011764705882352941
Assigned description: has a topic of cognitive behavior theories. recall: 0.048 precision: 0.0
Assigned description: has a topic of causality and default reasoning. recall: 0.107 precision: 0.009345794392523364
Assigned description: has a topic of intelligent tutoring systems and domain knowledge extraction. recall: 0.012 precision: 0.0
Assigned description: - "has a topic of intelligent agent frameworks in media analysis. recall: 0.009 precision: 0.0
Assigned description: has a topic of modular architecture in logic programming. recall: 0.012 precision: 0.0
Assigned description: has a topic of evaluation methods in uncertain environments. recall: 0.305 precision: 0.003278688524590164
Assigned description: - "has a topic of cognitive agents and learning mechanisms. recall: 0.18 precision: 0.0
Drop 22 because it cover 624 from 1000
Dropping 1 descriptions because they are too popular or too unpopular:
Stage select select
Unmatched count: 222
Description: has a topic of condition monitoring in engineering. Count: 14
Description: has a topic of ontology and conceptualization in knowledge representation. Count: 274
Description: has a topic of simulation and philosophical implications. Count: 51
Description: has a topic of rough set theory applications in data analysis. Count: 12
Description: has a topic of optimization techniques. Count: 414
Description: has a topic of image processing and recognition systems. Count: 13
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

1.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

2.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

3.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.

4.   Methods to solve a node discovery problem for a social network are presented.
Covert nodes refer to the nodes which are not observable directly. They
transmit the influence and affect the resulting collaborative activities among
the persons in a social network, but do not appear in the surveillance logs
which record the participants of the collaborative activities. Discovering the
covert nodes is identifying the suspicious logs where the covert nodes would
appear if the covert nodes became overt. The performance of the methods is
demonstrated with a test dataset generated from computationally synthesized
networks and a real organization.

5.   The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.

6.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

7.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

8.   This paper has been withdrawn by the author due to extremely unscientific
errors.

9.   In this paper, I present a method to solve a node discovery problem in a
networked organization. Covert nodes refer to the nodes which are not
observable directly. They affect social interactions, but do not appear in the
surveillance logs which record the participants of the social interactions.
Discovering the covert nodes is defined as identifying the suspicious logs
where the covert nodes would appear if the covert nodes became overt. A
mathematical model is developed for the maximal likelihood estimation of the
network behind the social interactions and for the identification of the
suspicious logs. Precision, recall, and F measure characteristics are
demonstrated with the dataset generated from a real organization and the
computationally synthesized datasets. The performance is close to the
theoretical limit for any covert nodes in the networks of any topologies and
sizes if the ratio of the number of observation to the number of possible
communication patterns is large.

10.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

11.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

12.   Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.

13.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

14.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

15.   The investigation of the terrorist attack is a time-critical task. The
investigators have a limited time window to diagnose the organizational
background of the terrorists, to run down and arrest the wire-pullers, and to
take an action to prevent or eradicate the terrorist attack. The intuitive
interface to visualize the intelligence data set stimulates the investigators'
experience and knowledge, and aids them in decision-making for an immediately
effective action. This paper presents a computational method to analyze the
intelligence data set on the collective actions of the perpetrators of the
attack, and to visualize it into the form of a social network diagram which
predicts the positions where the wire-pullers conceals themselves.

16.   An empty spot refers to an empty hard-to-fill space which can be found in the
records of the social interaction, and is the clue to the persons in the
underlying social network who do not appear in the records. This contribution
addresses a problem to predict relevant empty spots in social interaction.
Homogeneous and inhomogeneous networks are studied as a model underlying the
social interaction. A heuristic predictor function approach is presented as a
new method to address the problem. Simulation experiment is demonstrated over a
homogeneous network. A test data in the form of baskets is generated from the
simulated communication. Precision to predict the empty spots is calculated to
demonstrate the performance of the presented approach.

17.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

18.   A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.

19.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

20.   This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.

21.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

22.   Each cognitive science tries to understand a set of cognitive behaviors. The
structuring of knowledge of this nature's aspect is far from what it can be
expected about a science. Until now universal standard consistently describing
the set of cognitive behaviors has not been found, and there are many questions
about the cognitive behaviors for which only there are opinions of members of
the scientific community. This article has three proposals. The first proposal
is to raise to the scientific community the necessity of unified the cognitive
behaviors. The second proposal is claim the application of the Newton's
reasoning rules about nature of his book, Philosophiae Naturalis Principia
Mathematica, to the cognitive behaviors. The third is to propose a scientific
theory, currently developing, that follows the rules established by Newton to
make sense of nature, and could be the theory to explain all the cognitive
behaviors.

23.   Stream computing is the use of multiple autonomic and parallel modules
together with integrative processors at a higher level of abstraction to embody
"intelligent" processing. The biological basis of this computing is sketched
and the matter of learning is examined.

24.   This paper has been withdrawn.

25.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

26.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of social network prediction and analysis"
"has a topic of protein family detection using hidden markov models"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of earthquake response prediction using artificial neural networks"\n- "has a topic of conflict management in decision theory"\n- "has a topic of term extraction and unithood measurement"\n- "has a topic of predicting militarized interstate disputes"\n- "has a topic of covert node discovery in social networks"\n- "has a topic of symbolic controllers in evolutionary robotics"\n- "has a topic of fuzzy block theory in stability assessment"\n- "has a topic of intelligence definitions and analysis"\n- "has a topic of information fusion and redistribution rules"\n- "has a topic of semantic processing in web search engines"\n- "has a topic of narrative analysis in film scripts"\n- "has a topic of terrorist attack investigation and intelligence visualization"\n- "has a topic of predicting empty spots in social interactions"\n- "has a topic of unconscious preference awareness and visualization"\n- "has a topic of causality definitions and theories"\n- "has a topic of dynamic data mining challenges"\n- "has a topic of quantum computation applications in artificial intelligence"\n- "has a topic of cognitive behavior unification proposals"\n- "has a topic of stream computing and intelligent processing"\n- "has a topic of program module architecture in logic programming"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

1.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.

2.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

3.   This paper has been withdrawn.

4.   This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.

5.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

6.   An empty spot refers to an empty hard-to-fill space which can be found in the
records of the social interaction, and is the clue to the persons in the
underlying social network who do not appear in the records. This contribution
addresses a problem to predict relevant empty spots in social interaction.
Homogeneous and inhomogeneous networks are studied as a model underlying the
social interaction. A heuristic predictor function approach is presented as a
new method to address the problem. Simulation experiment is demonstrated over a
homogeneous network. A test data in the form of baskets is generated from the
simulated communication. Precision to predict the empty spots is calculated to
demonstrate the performance of the presented approach.

7.   Methods to solve a node discovery problem for a social network are presented.
Covert nodes refer to the nodes which are not observable directly. They
transmit the influence and affect the resulting collaborative activities among
the persons in a social network, but do not appear in the surveillance logs
which record the participants of the collaborative activities. Discovering the
covert nodes is identifying the suspicious logs where the covert nodes would
appear if the covert nodes became overt. The performance of the methods is
demonstrated with a test dataset generated from computationally synthesized
networks and a real organization.

8.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

9.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

10.   Each cognitive science tries to understand a set of cognitive behaviors. The
structuring of knowledge of this nature's aspect is far from what it can be
expected about a science. Until now universal standard consistently describing
the set of cognitive behaviors has not been found, and there are many questions
about the cognitive behaviors for which only there are opinions of members of
the scientific community. This article has three proposals. The first proposal
is to raise to the scientific community the necessity of unified the cognitive
behaviors. The second proposal is claim the application of the Newton's
reasoning rules about nature of his book, Philosophiae Naturalis Principia
Mathematica, to the cognitive behaviors. The third is to propose a scientific
theory, currently developing, that follows the rules established by Newton to
make sense of nature, and could be the theory to explain all the cognitive
behaviors.

11.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

12.   Stream computing is the use of multiple autonomic and parallel modules
together with integrative processors at a higher level of abstraction to embody
"intelligent" processing. The biological basis of this computing is sketched
and the matter of learning is examined.

13.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

14.   In this paper, I present a method to solve a node discovery problem in a
networked organization. Covert nodes refer to the nodes which are not
observable directly. They affect social interactions, but do not appear in the
surveillance logs which record the participants of the social interactions.
Discovering the covert nodes is defined as identifying the suspicious logs
where the covert nodes would appear if the covert nodes became overt. A
mathematical model is developed for the maximal likelihood estimation of the
network behind the social interactions and for the identification of the
suspicious logs. Precision, recall, and F measure characteristics are
demonstrated with the dataset generated from a real organization and the
computationally synthesized datasets. The performance is close to the
theoretical limit for any covert nodes in the networks of any topologies and
sizes if the ratio of the number of observation to the number of possible
communication patterns is large.

15.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.

16.   A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.

17.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

18.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

19.   Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.

20.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

21.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

22.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

23.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

24.   The investigation of the terrorist attack is a time-critical task. The
investigators have a limited time window to diagnose the organizational
background of the terrorists, to run down and arrest the wire-pullers, and to
take an action to prevent or eradicate the terrorist attack. The intuitive
interface to visualize the intelligence data set stimulates the investigators'
experience and knowledge, and aids them in decision-making for an immediately
effective action. This paper presents a computational method to analyze the
intelligence data set on the collective actions of the perpetrators of the
attack, and to visualize it into the form of a social network diagram which
predicts the positions where the wire-pullers conceals themselves.

25.   This paper has been withdrawn by the author due to extremely unscientific
errors.

26.   The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"- "has a topic of network analysis and covert nodes"
"has a topic of dynamic data mining techniques"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of protein family detection using hidden Markov models"\n- "has a topic of predicting militarised interstate disputes"\n- "has a topic of measuring unithood in linguistics"\n- "has a topic of quantum computation and artificial intelligence"\n- "has a topic of information fusion and conflict redistribution"\n- "has a topic of social network analysis and empty spots"\n- "has a topic of narrative analysis in film scripts"\n- "has a topic of semantic web and natural language processing"\n- "has a topic of earthquake response prediction using neural networks"\n- "has a topic of fuzzy block theory in underground stability assessment"\n- "has a topic of intelligence data visualization for terrorism investigation"\n- "has a topic of evolutionary robotics and symbolic controllers"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.

1.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

2.   The investigation of the terrorist attack is a time-critical task. The
investigators have a limited time window to diagnose the organizational
background of the terrorists, to run down and arrest the wire-pullers, and to
take an action to prevent or eradicate the terrorist attack. The intuitive
interface to visualize the intelligence data set stimulates the investigators'
experience and knowledge, and aids them in decision-making for an immediately
effective action. This paper presents a computational method to analyze the
intelligence data set on the collective actions of the perpetrators of the
attack, and to visualize it into the form of a social network diagram which
predicts the positions where the wire-pullers conceals themselves.

3.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

4.   This paper has been withdrawn.

5.   A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.

6.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

7.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

8.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.

9.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

10.   This paper has been withdrawn by the author due to extremely unscientific
errors.

11.   Methods to solve a node discovery problem for a social network are presented.
Covert nodes refer to the nodes which are not observable directly. They
transmit the influence and affect the resulting collaborative activities among
the persons in a social network, but do not appear in the surveillance logs
which record the participants of the collaborative activities. Discovering the
covert nodes is identifying the suspicious logs where the covert nodes would
appear if the covert nodes became overt. The performance of the methods is
demonstrated with a test dataset generated from computationally synthesized
networks and a real organization.

12.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

13.   The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.

14.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

15.   Each cognitive science tries to understand a set of cognitive behaviors. The
structuring of knowledge of this nature's aspect is far from what it can be
expected about a science. Until now universal standard consistently describing
the set of cognitive behaviors has not been found, and there are many questions
about the cognitive behaviors for which only there are opinions of members of
the scientific community. This article has three proposals. The first proposal
is to raise to the scientific community the necessity of unified the cognitive
behaviors. The second proposal is claim the application of the Newton's
reasoning rules about nature of his book, Philosophiae Naturalis Principia
Mathematica, to the cognitive behaviors. The third is to propose a scientific
theory, currently developing, that follows the rules established by Newton to
make sense of nature, and could be the theory to explain all the cognitive
behaviors.

16.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

17.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

18.   An empty spot refers to an empty hard-to-fill space which can be found in the
records of the social interaction, and is the clue to the persons in the
underlying social network who do not appear in the records. This contribution
addresses a problem to predict relevant empty spots in social interaction.
Homogeneous and inhomogeneous networks are studied as a model underlying the
social interaction. A heuristic predictor function approach is presented as a
new method to address the problem. Simulation experiment is demonstrated over a
homogeneous network. A test data in the form of baskets is generated from the
simulated communication. Precision to predict the empty spots is calculated to
demonstrate the performance of the presented approach.

19.   Stream computing is the use of multiple autonomic and parallel modules
together with integrative processors at a higher level of abstraction to embody
"intelligent" processing. The biological basis of this computing is sketched
and the matter of learning is examined.

20.   In this paper, I present a method to solve a node discovery problem in a
networked organization. Covert nodes refer to the nodes which are not
observable directly. They affect social interactions, but do not appear in the
surveillance logs which record the participants of the social interactions.
Discovering the covert nodes is defined as identifying the suspicious logs
where the covert nodes would appear if the covert nodes became overt. A
mathematical model is developed for the maximal likelihood estimation of the
network behind the social interactions and for the identification of the
suspicious logs. Precision, recall, and F measure characteristics are
demonstrated with the dataset generated from a real organization and the
computationally synthesized datasets. The performance is close to the
theoretical limit for any covert nodes in the networks of any topologies and
sizes if the ratio of the number of observation to the number of possible
communication patterns is large.

21.   This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.

22.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

23.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.

24.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

25.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

26.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of social network prediction and analysis"
"has a topic of narrative structure analysis in film scripts"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of semantic querying and indexing in information retrieval"\n- "has a topic of conflict redistribution in decision-making processes"\n- "has a topic of intelligence data analysis for terrorist investigations"\n- "has a topic of earthquake response prediction using neural networks"\n- "has a topic of causality definitions and theories"\n- "has a topic of fuzzy block theory in structural stability assessment"\n- "has a topic of unithood measurement in linguistic evidence"\n- "has a topic of militarized conflict prediction using machine learning"\n- "has a topic of node discovery in social networks"\n- "has a topic of quantum computation applications in artificial intelligence"\n- "has a topic of protein family detection using hidden Markov models"\n- "has a topic of program module architecture in logic programming"\n- "has a topic of information fusion and mass redistribution techniques"\n- "has a topic of informal definitions of intelligence"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.

1.   An empty spot refers to an empty hard-to-fill space which can be found in the
records of the social interaction, and is the clue to the persons in the
underlying social network who do not appear in the records. This contribution
addresses a problem to predict relevant empty spots in social interaction.
Homogeneous and inhomogeneous networks are studied as a model underlying the
social interaction. A heuristic predictor function approach is presented as a
new method to address the problem. Simulation experiment is demonstrated over a
homogeneous network. A test data in the form of baskets is generated from the
simulated communication. Precision to predict the empty spots is calculated to
demonstrate the performance of the presented approach.

2.   The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.

3.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.

4.   In this paper, I present a method to solve a node discovery problem in a
networked organization. Covert nodes refer to the nodes which are not
observable directly. They affect social interactions, but do not appear in the
surveillance logs which record the participants of the social interactions.
Discovering the covert nodes is defined as identifying the suspicious logs
where the covert nodes would appear if the covert nodes became overt. A
mathematical model is developed for the maximal likelihood estimation of the
network behind the social interactions and for the identification of the
suspicious logs. Precision, recall, and F measure characteristics are
demonstrated with the dataset generated from a real organization and the
computationally synthesized datasets. The performance is close to the
theoretical limit for any covert nodes in the networks of any topologies and
sizes if the ratio of the number of observation to the number of possible
communication patterns is large.

5.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

6.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

7.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.

8.   A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.

9.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

10.   Each cognitive science tries to understand a set of cognitive behaviors. The
structuring of knowledge of this nature's aspect is far from what it can be
expected about a science. Until now universal standard consistently describing
the set of cognitive behaviors has not been found, and there are many questions
about the cognitive behaviors for which only there are opinions of members of
the scientific community. This article has three proposals. The first proposal
is to raise to the scientific community the necessity of unified the cognitive
behaviors. The second proposal is claim the application of the Newton's
reasoning rules about nature of his book, Philosophiae Naturalis Principia
Mathematica, to the cognitive behaviors. The third is to propose a scientific
theory, currently developing, that follows the rules established by Newton to
make sense of nature, and could be the theory to explain all the cognitive
behaviors.

11.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

12.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

13.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

14.   The investigation of the terrorist attack is a time-critical task. The
investigators have a limited time window to diagnose the organizational
background of the terrorists, to run down and arrest the wire-pullers, and to
take an action to prevent or eradicate the terrorist attack. The intuitive
interface to visualize the intelligence data set stimulates the investigators'
experience and knowledge, and aids them in decision-making for an immediately
effective action. This paper presents a computational method to analyze the
intelligence data set on the collective actions of the perpetrators of the
attack, and to visualize it into the form of a social network diagram which
predicts the positions where the wire-pullers conceals themselves.

15.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

16.   Methods to solve a node discovery problem for a social network are presented.
Covert nodes refer to the nodes which are not observable directly. They
transmit the influence and affect the resulting collaborative activities among
the persons in a social network, but do not appear in the surveillance logs
which record the participants of the collaborative activities. Discovering the
covert nodes is identifying the suspicious logs where the covert nodes would
appear if the covert nodes became overt. The performance of the methods is
demonstrated with a test dataset generated from computationally synthesized
networks and a real organization.

17.   Stream computing is the use of multiple autonomic and parallel modules
together with integrative processors at a higher level of abstraction to embody
"intelligent" processing. The biological basis of this computing is sketched
and the matter of learning is examined.

18.   This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.

19.   This paper has been withdrawn by the author due to extremely unscientific
errors.

20.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

21.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

22.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

23.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

24.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

25.   This paper has been withdrawn.

26.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"- "has a topic of network analysis and covert nodes"
"has a topic of dynamic data mining techniques"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of semantic web and natural language processing"\n- "has a topic of social network analysis and prediction"\n- "has a topic of evolutionary robotics and controller design"\n- "has a topic of program semantics and modularity"\n- "has a topic of fuzzy logic and stability assessment"\n- "has a topic of earthquake response prediction using neural networks"\n- "has a topic of conflict prediction in international relations"\n- "has a topic of causality and default reasoning"\n- "has a topic of cognitive behavior and scientific theory"\n- "has a topic of linguistic evidence and term extraction"\n- "has a topic of narrative analysis in film scripts"\n- "has a topic of intelligence definitions and theoretical exploration"\n- "has a topic of protein family detection using hidden Markov models"\n- "has a topic of unconscious preferences and visualization methods"\n- "has a topic of conflict redistribution in belief function theory"\n- "has a topic of incremental clustering algorithms in data mining"\n- "has a topic of information fusion and mass redistribution techniques"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.

1.   Methods to solve a node discovery problem for a social network are presented.
Covert nodes refer to the nodes which are not observable directly. They
transmit the influence and affect the resulting collaborative activities among
the persons in a social network, but do not appear in the surveillance logs
which record the participants of the collaborative activities. Discovering the
covert nodes is identifying the suspicious logs where the covert nodes would
appear if the covert nodes became overt. The performance of the methods is
demonstrated with a test dataset generated from computationally synthesized
networks and a real organization.

2.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.

3.   This paper has been withdrawn.

4.   Each cognitive science tries to understand a set of cognitive behaviors. The
structuring of knowledge of this nature's aspect is far from what it can be
expected about a science. Until now universal standard consistently describing
the set of cognitive behaviors has not been found, and there are many questions
about the cognitive behaviors for which only there are opinions of members of
the scientific community. This article has three proposals. The first proposal
is to raise to the scientific community the necessity of unified the cognitive
behaviors. The second proposal is claim the application of the Newton's
reasoning rules about nature of his book, Philosophiae Naturalis Principia
Mathematica, to the cognitive behaviors. The third is to propose a scientific
theory, currently developing, that follows the rules established by Newton to
make sense of nature, and could be the theory to explain all the cognitive
behaviors.

5.   This paper has been withdrawn by the author due to extremely unscientific
errors.

6.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

7.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

8.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

9.   Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.

10.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

11.   This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.

12.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

13.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

14.   The investigation of the terrorist attack is a time-critical task. The
investigators have a limited time window to diagnose the organizational
background of the terrorists, to run down and arrest the wire-pullers, and to
take an action to prevent or eradicate the terrorist attack. The intuitive
interface to visualize the intelligence data set stimulates the investigators'
experience and knowledge, and aids them in decision-making for an immediately
effective action. This paper presents a computational method to analyze the
intelligence data set on the collective actions of the perpetrators of the
attack, and to visualize it into the form of a social network diagram which
predicts the positions where the wire-pullers conceals themselves.

15.   The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.

16.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

17.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

18.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

19.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

20.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

21.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

22.   An empty spot refers to an empty hard-to-fill space which can be found in the
records of the social interaction, and is the clue to the persons in the
underlying social network who do not appear in the records. This contribution
addresses a problem to predict relevant empty spots in social interaction.
Homogeneous and inhomogeneous networks are studied as a model underlying the
social interaction. A heuristic predictor function approach is presented as a
new method to address the problem. Simulation experiment is demonstrated over a
homogeneous network. A test data in the form of baskets is generated from the
simulated communication. Precision to predict the empty spots is calculated to
demonstrate the performance of the presented approach.

23.   In this paper, I present a method to solve a node discovery problem in a
networked organization. Covert nodes refer to the nodes which are not
observable directly. They affect social interactions, but do not appear in the
surveillance logs which record the participants of the social interactions.
Discovering the covert nodes is defined as identifying the suspicious logs
where the covert nodes would appear if the covert nodes became overt. A
mathematical model is developed for the maximal likelihood estimation of the
network behind the social interactions and for the identification of the
suspicious logs. Precision, recall, and F measure characteristics are
demonstrated with the dataset generated from a real organization and the
computationally synthesized datasets. The performance is close to the
theoretical limit for any covert nodes in the networks of any topologies and
sizes if the ratio of the number of observation to the number of possible
communication patterns is large.

24.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

25.   Stream computing is the use of multiple autonomic and parallel modules
together with integrative processors at a higher level of abstraction to embody
"intelligent" processing. The biological basis of this computing is sketched
and the matter of learning is examined.

26.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of fuzzy logic applications in stability assessment"
"has a topic of linguistic evidence and term extraction"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of causality and its definitions"\n- "has a topic of node discovery in social networks"\n- "has a topic of narrative analysis in film scripts"\n- "has a topic of quantum computation and artificial intelligence"\n- "has a topic of information fusion and conflict redistribution"\n- "has a topic of structural response prediction using neural networks"\n- "has a topic of social network analysis and covert nodes"\n- "has a topic of dynamic data mining challenges"\n- "has a topic of cognitive behaviors and scientific proposals"\n- "has a topic of terrorist attack investigation and data visualization"']
Running the proposer model...
In this task you will need to come up with categories to group the text together based on a goal, where each category can be described with a natural language predicate. For example

0. How are you doing?
1. How do I apply for visa?
2. Isn't it ridiculous?
3. How to get there by bus?
4. How is it going?
5. Aren't technological advancement good for the humanity?

Here is our goal: I want to cluster the questions based on their functionalities. Your responses are:
- "is a rhetorical quesiton"
- "is meant for greetings"
- "is asking for practically useful information"

(note that all of them need to be natural language predicates that can be validated on the given samples)

Here are some texts:

0.   Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.

1.   In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.

2.   Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.

3.   This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.

4.   Methods to solve a node discovery problem for a social network are presented.
Covert nodes refer to the nodes which are not observable directly. They
transmit the influence and affect the resulting collaborative activities among
the persons in a social network, but do not appear in the surveillance logs
which record the participants of the collaborative activities. Discovering the
covert nodes is identifying the suspicious logs where the covert nodes would
appear if the covert nodes became overt. The performance of the methods is
demonstrated with a test dataset generated from computationally synthesized
networks and a real organization.

5.   Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.

6.   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

7.   This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.

8.   We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.

9.   In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.

10.   This paper has been withdrawn.

11.   Stream computing is the use of multiple autonomic and parallel modules
together with integrative processors at a higher level of abstraction to embody
"intelligent" processing. The biological basis of this computing is sketched
and the matter of learning is examined.

12.   In this paper, I present a method to solve a node discovery problem in a
networked organization. Covert nodes refer to the nodes which are not
observable directly. They affect social interactions, but do not appear in the
surveillance logs which record the participants of the social interactions.
Discovering the covert nodes is defined as identifying the suspicious logs
where the covert nodes would appear if the covert nodes became overt. A
mathematical model is developed for the maximal likelihood estimation of the
network behind the social interactions and for the identification of the
suspicious logs. Precision, recall, and F measure characteristics are
demonstrated with the dataset generated from a real organization and the
computationally synthesized datasets. The performance is close to the
theoretical limit for any covert nodes in the networks of any topologies and
sizes if the ratio of the number of observation to the number of possible
communication patterns is large.

13.   This paper has been withdrawn by the author due to extremely unscientific
errors.

14.   A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.

15.   Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.

16.   The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.

17.   This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.

18.   Each cognitive science tries to understand a set of cognitive behaviors. The
structuring of knowledge of this nature's aspect is far from what it can be
expected about a science. Until now universal standard consistently describing
the set of cognitive behaviors has not been found, and there are many questions
about the cognitive behaviors for which only there are opinions of members of
the scientific community. This article has three proposals. The first proposal
is to raise to the scientific community the necessity of unified the cognitive
behaviors. The second proposal is claim the application of the Newton's
reasoning rules about nature of his book, Philosophiae Naturalis Principia
Mathematica, to the cognitive behaviors. The third is to propose a scientific
theory, currently developing, that follows the rules established by Newton to
make sense of nature, and could be the theory to explain all the cognitive
behaviors.

19.   In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.

20.   We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, "Story: Substance, Structure, Style, and the Principles of
Screenwriting". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.

21.   Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.

22.   This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.

23.   Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.

24.   The investigation of the terrorist attack is a time-critical task. The
investigators have a limited time window to diagnose the organizational
background of the terrorists, to run down and arrest the wire-pullers, and to
take an action to prevent or eradicate the terrorist attack. The intuitive
interface to visualize the intelligence data set stimulates the investigators'
experience and knowledge, and aids them in decision-making for an immediately
effective action. This paper presents a computational method to analyze the
intelligence data set on the collective actions of the perpetrators of the
attack, and to visualize it into the form of a social network diagram which
predicts the positions where the wire-pullers conceals themselves.

25.   A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.

26.   An empty spot refers to an empty hard-to-fill space which can be found in the
records of the social interaction, and is the clue to the persons in the
underlying social network who do not appear in the records. This contribution
addresses a problem to predict relevant empty spots in social interaction.
Homogeneous and inhomogeneous networks are studied as a model underlying the
social interaction. A heuristic predictor function approach is presented as a
new method to address the problem. Simulation experiment is demonstrated over a
homogeneous network. A test data in the form of baskets is generated from the
simulated communication. Precision to predict the empty spots is calculated to
demonstrate the performance of the presented approach.


Here is our goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>' 
Please suggest me a few descriptions, one in a line, starting with "-" and surrounded by quotes"". In terms of formatting, each of them needs to be a predicate about a text, for example:
- "uses double negation"
- "has a conservative stance"

Do not output anything else. (Note that the examples might not be goal related, and your response should be both formatted correct as above and related to the goal.)
Here are some example predicates; the generated predicates should have a similar granularity and perspective:

"has a topic of protein family detection using hidden markov models"
"has a topic of social network prediction and analysis"


Again, here's the goal: I would like to cluster them based on topic; each cluster should have a description of 'has a topic of <something>'. Your responses are:
- "
Proposer model response:
['- "has a topic of linguistic evidence and statistical measures in term extraction"\n- "has a topic of conflict prediction using machine learning techniques"\n- "has a topic of fuzzy logic applications in stability assessment"\n- "has a topic of covert node discovery in social networks"\n- "has a topic of structural response prediction in earthquake engineering"\n- "has a topic of dynamic data mining and clustering algorithms"\n- "has a topic of causal reasoning and definitions of causality"\n- "has a topic of semantic processing and information retrieval"\n- "has a topic of symbolic controllers in evolutionary robotics"\n- "has a topic of quantum computation applications in artificial intelligence"\n- "has a topic of terrorist attack investigation and intelligence analysis"\n- "has a topic of unconscious preference awareness and visualization methods"\n- "has a topic of predicting empty spots in social interactions"']
225 descriptions proposed in total. Here are the descriptions:
- "has a topic of soft constraints and optimization
has a topic of conflict prediction using machine learning
- "has a topic of semantic web technologies
has a topic of qualitative temporal reasoning
has a topic of automated leaf recognition and classification
has a topic of Bayesian frameworks in modeling
has a topic of face recognition robustness
has a topic of Bayesian frameworks in machine learning
has a topic of multi-agent systems
- "has a topic of automated leaf recognition algorithms
has a topic of evolutionary robotics
has a topic of clustering techniques and algorithms
has a topic of quantum computation applications
has a topic of water demand forecasting
has a topic of quantum computation and artificial intelligence
has a topic of fault classification in cylindrical shells
has a topic of evolutionary learning and ensemble methods
has a topic of multi-cellular developmental design
has a topic of condition monitoring in engineering
- "has a topic of ensemble learning techniques
has a topic of water demand forecasting using computational intelligence
has a topic of HIV risk modeling
- "has a topic of computational intelligence techniques
has a topic of fault diagnosis and classification
has a topic of Bayesian methods in rough set models
has a topic of decentralized control in multi-agent systems
has a topic of protein family detection
has a topic of artificial intelligence in healthcare
has a topic of neural network classifiers
has a topic of neural network models
has a topic of semantic web technologies
- "has a topic of evolutionary robotics
has a topic of semantic web and natural language processing
has a topic of clustering algorithms and techniques
has a topic of ensemble learning approaches
has a topic of artificial intelligence history
has a topic of decentralized control systems
has a topic of qualitative temporal information representation
- "has a topic of evolutionary learning and ensemble methods
has a topic of automated leaf recognition algorithms
has a topic of handling missing data in neural networks
has a topic of conflict prediction and machine learning
- "has a topic of protein family detection
has a topic of condition monitoring frameworks
has a topic of semantic web and computational modeling
has a topic of neural network performance evaluation
has a topic of predicting militarized interstate disputes
has a topic of fault classification methods
has a topic of fault diagnosis and monitoring
has a topic of artificial intelligence applications
has a topic of fault classification and condition monitoring in engineering
has a topic of ontology and conceptualization in knowledge representation
- "has a topic of constraint satisfaction problems and algorithms
has a topic of social network analysis and covert nodes
has a topic of knowledge management and ontology development
- "has a topic of machine intelligence definitions and tests
has a topic of artificial intelligence and definitions of intelligence
has a topic of computational methods in social network analysis
has a topic of simulation and philosophical implications of intelligence
has a topic of knowledge management and ontology
has a topic of constraint satisfaction problems and algorithm design
has a topic of machine intelligence and its definitions
has a topic of terrorism analysis and intelligence visualization
has a topic of evolutionary algorithms and cellular automata
has a topic of evolutionary theory and simulations
- "has a topic of fitness landscapes and optimization algorithms
has a topic of constraint satisfaction problems and propagators
- "has a topic of causality and definitions in artificial intelligence
has a topic of scientific simulations and philosophical implications
has a topic of fuzzy logic in stability assessment of underground openings
has a topic of causality and default theories
has a topic of knowledge management systems in organizational clusters
- "has a topic of covert node discovery in social networks
- "has a topic of social network analysis and covert nodes
has a topic of definitions and measures of intelligence
has a topic of hydrocyclone operation analysis using rough set theory
has a topic of mining methods and operational efficiency
has a topic of terrorism and social network dynamics
has a topic of fuzzy logic and stability assessment in engineering
has a topic of computational methods in terrorism investigation
has a topic of simulation and philosophical implications
has a topic of social network analysis and covert node discovery
has a topic of fuzzy logic and stability assessment
has a topic of intelligence measurement and definitions in AI
has a topic of fuzzy logic and decision-making systems
has a topic of intelligence definitions and machine learning
has a topic of rough set theory applications in data analysis
has a topic of artificial intelligence and intelligence measurement
has a topic of fuzzy logic and fault classification
has a topic of constraint satisfaction problems and optimization
has a topic of fault classification using neuro-fuzzy models
has a topic of fitness landscapes and optimization
has a topic of constraint satisfaction problems and optimization techniques
has a topic of definitions and measurements of intelligence
has a topic of computational methods in terrorism analysis
has a topic of social network analysis and covert operations
has a topic of simulation and modeling of complex systems
has a topic of fuzzy block theory and stability assessment
has a topic of fuzzy logic and its applications
- "has a topic of fuzzy logic applications
has a topic of linguistic evidence and unithood measurement
has a topic of belief function theory
- "has a topic of narrative analysis in film scripts
has a topic of neural network applications
has a topic of optimization algorithms for multi-objective problems
has a topic of fuzzy decision-making models
- "has a topic of data mining challenges
has a topic of optimization techniques
has a topic of clustering algorithms
has a topic of narrative analysis in film scripts
has a topic of fuzzy logic applications in stability assessment
has a topic of clustering algorithms and data analysis
has a topic of optimization techniques in scheduling
has a topic of belief function theory and information fusion
- "has a topic of belief function theory and information fusion
has a topic of information fusion methods
- "has a topic of face recognition systems
has a topic of information fusion and conflict redistribution
has a topic of image processing and recognition systems
has a topic of structural protein analysis
has a topic of protein family detection using hidden Markov models
has a topic of data mining techniques
has a topic of data mining challenges
- "has a topic of belief function theory
has a topic of data-stream clustering and weak signals detection
- "has a topic of neural networks in structural engineering
has a topic of missing data handling in neural networks
has a topic of qualitative analysis in narratives
has a topic of information fusion techniques
has a topic of qualitative analysis in narrative structures
has a topic of narrative analysis in film
has a topic of fuzzy logic and decision-making
has a topic of belief function frameworks and decision making
has a topic of multi-objective optimization methods
has a topic of multi-objective optimization
- "has a topic of artificial neural networks in earthquake response prediction
has a topic of data mining and clustering algorithms
has a topic of multi-objective optimization in scheduling problems
has a topic of fuzzy set theory and applications
has a topic of fuzzy logic and optimization techniques
has a topic of unconscious preference awareness and verbalization
has a topic of fuzzy logic and neutrosophic operators
has a topic of data clustering methods
has a topic of conflict prediction in international relations
has a topic of qualitative belief functions and reasoning under uncertainty
has a topic of neural networks in earthquake response prediction
has a topic of hydrocyclone performance analysis using fuzzy theory
has a topic of conscious tutoring systems and cognitive agents
has a topic of program semantics and modularity
has a topic of improving protein family detection algorithms
has a topic of data mining and pattern extraction
has a topic of dynamic data mining challenges
- "has a topic of cognitive behaviors and scientific proposals
- "has a topic of evaluation methods for classification and segmentation
has a topic of unconscious preferences and verbalization methods
has a topic of conflict redistribution in decision theory
has a topic of measuring unithood in linguistics
has a topic of cognitive behaviors and scientific theories
has a topic of fuzzy theory and stability assessment
has a topic of unconscious preference awareness methods
has a topic of data mining algorithms and user interaction
has a topic of conflict redistribution in belief function theory
has a topic of symbolic controllers in evolutionary robotics
has a topic of graphical models and approximate inference
- "has a topic of protein family detection using hidden Markov models
- "has a topic of term extraction and unithood measurement
has a topic of fuzzy block theory in underground stability assessment
has a topic of definitions and theories of intelligence
has a topic of unithood measurement techniques
has a topic of data mining challenges and user interaction
has a topic of intelligent tutoring systems and domain knowledge
- "has a topic of quantum computation and artificial intelligence
has a topic of protein family detection using pHMMs
has a topic of semantic access and natural language processing
- "has a topic of fuzzy block theory applications
has a topic of configuration frameworks in computing
has a topic of intelligent agents for news analysis
has a topic of image classification and segmentation in uncertain environments
has a topic of definitions of intelligence
has a topic of cognitive agents and learning mechanisms
has a topic of intelligent agents and media analysis
has a topic of definitions of intelligence in cognitive science
has a topic of tutoring systems and sequential pattern mining
has a topic of structural information in protein modeling
has a topic of linguistic evidence and term extraction
has a topic of cognitive behavior theories
has a topic of causality and default reasoning
has a topic of intelligent tutoring systems and domain knowledge extraction
- "has a topic of intelligent agent frameworks in media analysis
has a topic of modular architecture in logic programming
has a topic of evaluation methods in uncertain environments
- "has a topic of cognitive agents and learning mechanisms
- "has a topic of earthquake response prediction using artificial neural networks
has a topic of fuzzy block theory in structural stability assessment
has a topic of conflict prediction using machine learning techniques
has a topic of social network prediction and analysis
has a topic of semantic processing and information retrieval
has a topic of militarized conflict prediction using machine learning
has a topic of covert node discovery in social networks
has a topic of causality definitions and theories
- "has a topic of causality and its definitions
has a topic of intelligence data analysis for terrorist investigations
has a topic of conflict management in decision theory
- "has a topic of semantic web and natural language processing
has a topic of predicting militarised interstate disputes
has a topic of social network analysis and prediction
has a topic of dynamic data mining techniques
has a topic of conflict redistribution in decision-making processes
- "has a topic of linguistic evidence and statistical measures in term extraction
has a topic of term extraction and unithood measurement
has a topic of social network analysis and empty spots
has a topic of intelligence definitions and analysis
has a topic of node discovery in social networks
has a topic of causal reasoning and definitions of causality
has a topic of dynamic data mining and clustering algorithms
- "has a topic of semantic querying and indexing in information retrieval
has a topic of narrative structure analysis in film scripts
has a topic of earthquake response prediction using neural networks
has a topic of structural response prediction using neural networks
- "has a topic of network analysis and covert nodes
has a topic of evolutionary robotics and controller design
has a topic of fuzzy block theory in stability assessment
has a topic of cognitive behaviors and scientific proposals
has a topic of unithood measurement in linguistic evidence
has a topic of structural response prediction in earthquake engineering
First prompt example:
You are tasked with classifying whether a TEXT satisfies a PROPERTY. You need to respond with Yes or No, and when uncertain, always respond with No.

<example>
<input>
Text: I like apples.
Property: is talking about fruits
</input>
<output>
Yes
</output>
</example>

Now, solve this new task:
Text:   In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.

Property: - "has a topic of earthquake response prediction using artificial neural networks

Processing 33000 prompts with vLLM's automatic batching...
Stage Assign assign
Assigned all recall: 0.994
Assigned all precision: 0.01609657947686117
Assigned description: - "has a topic of soft constraints and optimization. recall: 0.213 precision: 0.0
Assigned description: has a topic of conflict prediction using machine learning. recall: 0.013 precision: 0.0
Assigned description: - "has a topic of semantic web technologies. recall: 0.042 precision: 0.0
Assigned description: has a topic of qualitative temporal reasoning. recall: 0.062 precision: 0.0
Assigned description: has a topic of automated leaf recognition and classification. recall: 0.001 precision: 0.0
Assigned description: has a topic of Bayesian frameworks in modeling. recall: 0.125 precision: 0.0
Assigned description: has a topic of face recognition robustness. recall: 0.001 precision: 0.0
Assigned description: has a topic of Bayesian frameworks in machine learning. recall: 0.104 precision: 0.0
Assigned description: has a topic of multi-agent systems. recall: 0.069 precision: 0.0
Assigned description: - "has a topic of automated leaf recognition algorithms. recall: 0.001 precision: 0.0
Assigned description: has a topic of evolutionary robotics. recall: 0.013 precision: 0.0
Assigned description: has a topic of clustering techniques and algorithms. recall: 0.039 precision: 0.0
Assigned description: has a topic of quantum computation applications. recall: 0.002 precision: 0.0
Assigned description: has a topic of water demand forecasting. recall: 0.001 precision: 0.0
Assigned description: has a topic of quantum computation and artificial intelligence. recall: 0.005 precision: 0.0
Assigned description: has a topic of fault classification in cylindrical shells. recall: 0.002 precision: 0.0
Assigned description: has a topic of evolutionary learning and ensemble methods. recall: 0.032 precision: 0.0
Assigned description: has a topic of multi-cellular developmental design. recall: 0.001 precision: 0.0
Assigned description: has a topic of condition monitoring in engineering. recall: 0.014 precision: 0.0
Assigned description: - "has a topic of ensemble learning techniques. recall: 0.039 precision: 0.0
Assigned description: has a topic of water demand forecasting using computational intelligence. recall: 0.001 precision: 0.0
Assigned description: has a topic of HIV risk modeling. recall: 0.002 precision: 0.0
Assigned description: - "has a topic of computational intelligence techniques. recall: 0.624 precision: 0.0
Assigned description: has a topic of fault diagnosis and classification. recall: 0.024 precision: 0.0
Assigned description: has a topic of Bayesian methods in rough set models. recall: 0.012 precision: 0.0
Assigned description: has a topic of decentralized control in multi-agent systems. recall: 0.034 precision: 0.0
Assigned description: has a topic of protein family detection. recall: 0.001 precision: 0.0
Assigned description: has a topic of artificial intelligence in healthcare. recall: 0.028 precision: 0.0
Assigned description: has a topic of neural network classifiers. recall: 0.031 precision: 0.0
Assigned description: has a topic of neural network models. recall: 0.046 precision: 0.0
Assigned description: has a topic of semantic web technologies. recall: 0.046 precision: 0.0
Assigned description: - "has a topic of evolutionary robotics. recall: 0.013 precision: 0.0
Assigned description: has a topic of semantic web and natural language processing. recall: 0.066 precision: 0.0
Assigned description: has a topic of clustering algorithms and techniques. recall: 0.034 precision: 0.0
Assigned description: has a topic of ensemble learning approaches. recall: 0.048 precision: 0.0
Assigned description: has a topic of artificial intelligence history. recall: 0.009 precision: 0.0
Assigned description: has a topic of decentralized control systems. recall: 0.023 precision: 0.0
Assigned description: has a topic of qualitative temporal information representation. recall: 0.101 precision: 0.0
Assigned description: - "has a topic of evolutionary learning and ensemble methods. recall: 0.028 precision: 0.0
Assigned description: has a topic of automated leaf recognition algorithms. recall: 0.001 precision: 0.0
Assigned description: has a topic of handling missing data in neural networks. recall: 0.002 precision: 0.0
Assigned description: has a topic of conflict prediction and machine learning. recall: 0.027 precision: 0.0
Assigned description: - "has a topic of protein family detection. recall: 0.001 precision: 0.0
Assigned description: has a topic of condition monitoring frameworks. recall: 0.022 precision: 0.0
Assigned description: has a topic of semantic web and computational modeling. recall: 0.12 precision: 0.0
Assigned description: has a topic of neural network performance evaluation. recall: 0.018 precision: 0.0
Assigned description: has a topic of predicting militarized interstate disputes. recall: 0.001 precision: 0.0
Assigned description: has a topic of fault classification methods. recall: 0.014 precision: 0.0
Assigned description: has a topic of fault diagnosis and monitoring. recall: 0.029 precision: 0.0
Assigned description: has a topic of artificial intelligence applications. recall: 0.406 precision: 0.0
Assigned description: has a topic of fault classification and condition monitoring in engineering. recall: 0.011 precision: 0.0
Assigned description: has a topic of ontology and conceptualization in knowledge representation. recall: 0.278 precision: 0.017985611510791366
Assigned description: - "has a topic of constraint satisfaction problems and algorithms. recall: 0.412 precision: 0.0
Assigned description: has a topic of social network analysis and covert nodes. recall: 0.015 precision: 0.0
Assigned description: has a topic of knowledge management and ontology development. recall: 0.098 precision: 0.0
Assigned description: - "has a topic of machine intelligence definitions and tests. recall: 0.044 precision: 0.0
Assigned description: has a topic of artificial intelligence and definitions of intelligence. recall: 0.141 precision: 0.0
Assigned description: has a topic of computational methods in social network analysis. recall: 0.026 precision: 0.0
Assigned description: has a topic of simulation and philosophical implications of intelligence. recall: 0.038 precision: 0.0
Assigned description: has a topic of knowledge management and ontology. recall: 0.11 precision: 0.0
Assigned description: has a topic of constraint satisfaction problems and algorithm design. recall: 0.475 precision: 0.0
Assigned description: has a topic of machine intelligence and its definitions. recall: 0.186 precision: 0.0
Assigned description: has a topic of terrorism analysis and intelligence visualization. recall: 0.005 precision: 0.0
Assigned description: has a topic of evolutionary algorithms and cellular automata. recall: 0.005 precision: 0.0
Assigned description: has a topic of evolutionary theory and simulations. recall: 0.04 precision: 0.0
Assigned description: - "has a topic of fitness landscapes and optimization algorithms. recall: 0.043 precision: 0.0
Assigned description: has a topic of constraint satisfaction problems and propagators. recall: 0.172 precision: 0.0
Assigned description: - "has a topic of causality and definitions in artificial intelligence. recall: 0.122 precision: 0.0
Assigned description: has a topic of scientific simulations and philosophical implications. recall: 0.145 precision: 0.0
Assigned description: has a topic of fuzzy logic in stability assessment of underground openings. recall: 0.001 precision: 0.0
Assigned description: has a topic of causality and default theories. recall: 0.048 precision: 0.0
Assigned description: has a topic of knowledge management systems in organizational clusters. recall: 0.013 precision: 0.0
Assigned description: - "has a topic of covert node discovery in social networks. recall: 0.005 precision: 0.0
Assigned description: - "has a topic of social network analysis and covert nodes. recall: 0.014 precision: 0.0
Assigned description: has a topic of definitions and measures of intelligence. recall: 0.024 precision: 0.0
Assigned description: has a topic of hydrocyclone operation analysis using rough set theory. recall: 0.002 precision: 0.0
Assigned description: has a topic of mining methods and operational efficiency. recall: 0.107 precision: 0.0
Assigned description: has a topic of terrorism and social network dynamics. recall: 0.002 precision: 0.0
Assigned description: has a topic of fuzzy logic and stability assessment in engineering. recall: 0.016 precision: 0.0
Assigned description: has a topic of computational methods in terrorism investigation. recall: 0.005 precision: 0.0
Assigned description: has a topic of simulation and philosophical implications. recall: 0.08 precision: 0.0
Assigned description: has a topic of social network analysis and covert node discovery. recall: 0.013 precision: 0.0
Assigned description: has a topic of fuzzy logic and stability assessment. recall: 0.049 precision: 0.0
Assigned description: has a topic of intelligence measurement and definitions in AI. recall: 0.049 precision: 0.0
Assigned description: has a topic of fuzzy logic and decision-making systems. recall: 0.102 precision: 0.0
Assigned description: has a topic of intelligence definitions and machine learning. recall: 0.067 precision: 0.0
Assigned description: has a topic of rough set theory applications in data analysis. recall: 0.019 precision: 0.0
Assigned description: has a topic of artificial intelligence and intelligence measurement. recall: 0.384 precision: 0.0078125
Assigned description: has a topic of fuzzy logic and fault classification. recall: 0.005 precision: 0.0
Assigned description: has a topic of constraint satisfaction problems and optimization. recall: 0.359 precision: 0.0
Assigned description: has a topic of fault classification using neuro-fuzzy models. recall: 0.004 precision: 0.0
Assigned description: has a topic of fitness landscapes and optimization. recall: 0.048 precision: 0.0
Assigned description: has a topic of constraint satisfaction problems and optimization techniques. recall: 0.388 precision: 0.0
Assigned description: has a topic of definitions and measurements of intelligence. recall: 0.016 precision: 0.0
Assigned description: has a topic of computational methods in terrorism analysis. recall: 0.006 precision: 0.0
Assigned description: has a topic of social network analysis and covert operations. recall: 0.004 precision: 0.0
Assigned description: has a topic of simulation and modeling of complex systems. recall: 0.289 precision: 0.0
Assigned description: has a topic of fuzzy block theory and stability assessment. recall: 0.002 precision: 0.0
Assigned description: has a topic of fuzzy logic and its applications. recall: 0.073 precision: 0.0
Assigned description: - "has a topic of fuzzy logic applications. recall: 0.058 precision: 0.0
Assigned description: has a topic of linguistic evidence and unithood measurement. recall: 0.035 precision: 0.0
Assigned description: has a topic of belief function theory. recall: 0.119 precision: 0.0
Assigned description: - "has a topic of narrative analysis in film scripts. recall: 0.002 precision: 0.0
Assigned description: has a topic of neural network applications. recall: 0.048 precision: 0.0
Assigned description: has a topic of optimization algorithms for multi-objective problems. recall: 0.034 precision: 0.0
Assigned description: has a topic of fuzzy decision-making models. recall: 0.1 precision: 0.0
Assigned description: - "has a topic of data mining challenges. recall: 0.059 precision: 0.0
Assigned description: has a topic of optimization techniques. recall: 0.477 precision: 0.0041928721174004195
Assigned description: has a topic of clustering algorithms. recall: 0.032 precision: 0.0
Assigned description: has a topic of narrative analysis in film scripts. recall: 0.002 precision: 0.0
Assigned description: has a topic of fuzzy logic applications in stability assessment. recall: 0.006 precision: 0.0
Assigned description: has a topic of clustering algorithms and data analysis. recall: 0.036 precision: 0.0
Assigned description: has a topic of optimization techniques in scheduling. recall: 0.041 precision: 0.0
Assigned description: has a topic of belief function theory and information fusion. recall: 0.162 precision: 0.0
Assigned description: - "has a topic of belief function theory and information fusion. recall: 0.227 precision: 0.004405286343612335
Assigned description: has a topic of information fusion methods. recall: 0.199 precision: 0.0
Assigned description: - "has a topic of face recognition systems. recall: 0.004 precision: 0.0
Assigned description: has a topic of information fusion and conflict redistribution. recall: 0.118 precision: 0.0
Assigned description: has a topic of image processing and recognition systems. recall: 0.023 precision: 0.043478260869565216
Assigned description: has a topic of structural protein analysis. recall: 0.002 precision: 0.0
Assigned description: has a topic of protein family detection using hidden Markov models. recall: 0.001 precision: 0.0
Assigned description: has a topic of data mining techniques. recall: 0.141 precision: 0.0
Assigned description: has a topic of data mining challenges. recall: 0.08 precision: 0.0
Assigned description: - "has a topic of belief function theory. recall: 0.155 precision: 0.0
Assigned description: has a topic of data-stream clustering and weak signals detection. recall: 0.002 precision: 0.0
Assigned description: - "has a topic of neural networks in structural engineering. recall: 0.003 precision: 0.0
Assigned description: has a topic of missing data handling in neural networks. recall: 0.005 precision: 0.0
Assigned description: has a topic of qualitative analysis in narratives. recall: 0.02 precision: 0.0
Assigned description: has a topic of information fusion techniques. recall: 0.174 precision: 0.0
Assigned description: has a topic of qualitative analysis in narrative structures. recall: 0.006 precision: 0.0
Assigned description: has a topic of narrative analysis in film. recall: 0.002 precision: 0.0
Assigned description: has a topic of fuzzy logic and decision-making. recall: 0.108 precision: 0.0
Assigned description: has a topic of belief function frameworks and decision making. recall: 0.17 precision: 0.0
Assigned description: has a topic of multi-objective optimization methods. recall: 0.04 precision: 0.0
Assigned description: has a topic of multi-objective optimization. recall: 0.041 precision: 0.0
Assigned description: - "has a topic of artificial neural networks in earthquake response prediction. recall: 0.001 precision: 0.0
Assigned description: has a topic of data mining and clustering algorithms. recall: 0.047 precision: 0.0
Assigned description: has a topic of multi-objective optimization in scheduling problems. recall: 0.015 precision: 0.0
Assigned description: has a topic of fuzzy set theory and applications. recall: 0.084 precision: 0.0
Assigned description: has a topic of fuzzy logic and optimization techniques. recall: 0.07 precision: 0.0
Assigned description: has a topic of unconscious preference awareness and verbalization. recall: 0.004 precision: 0.0
Assigned description: has a topic of fuzzy logic and neutrosophic operators. recall: 0.031 precision: 0.0
Assigned description: has a topic of data clustering methods. recall: 0.031 precision: 0.0
Assigned description: has a topic of conflict prediction in international relations. recall: 0.001 precision: 0.0
Assigned description: has a topic of qualitative belief functions and reasoning under uncertainty. recall: 0.141 precision: 0.0
Assigned description: has a topic of neural networks in earthquake response prediction. recall: 0.001 precision: 0.0
Assigned description: has a topic of hydrocyclone performance analysis using fuzzy theory. recall: 0.003 precision: 0.0
Assigned description: has a topic of conscious tutoring systems and cognitive agents. recall: 0.026 precision: 0.0
Assigned description: has a topic of program semantics and modularity. recall: 0.093 precision: 0.010752688172043012
Assigned description: has a topic of improving protein family detection algorithms. recall: 0.001 precision: 0.0
Assigned description: has a topic of data mining and pattern extraction. recall: 0.209 precision: 0.0
Assigned description: has a topic of dynamic data mining challenges. recall: 0.069 precision: 0.0
Assigned description: - "has a topic of cognitive behaviors and scientific proposals. recall: 0.305 precision: 0.0
Assigned description: - "has a topic of evaluation methods for classification and segmentation. recall: 0.074 precision: 0.0
Assigned description: has a topic of unconscious preferences and verbalization methods. recall: 0.003 precision: 0.0
Assigned description: has a topic of conflict redistribution in decision theory. recall: 0.007 precision: 0.0
Assigned description: has a topic of measuring unithood in linguistics. recall: 0.002 precision: 0.0
Assigned description: has a topic of cognitive behaviors and scientific theories. recall: 0.172 precision: 0.0
Assigned description: has a topic of fuzzy theory and stability assessment. recall: 0.056 precision: 0.0
Assigned description: has a topic of unconscious preference awareness methods. recall: 0.007 precision: 0.0
Assigned description: has a topic of data mining algorithms and user interaction. recall: 0.024 precision: 0.0
Assigned description: has a topic of conflict redistribution in belief function theory. recall: 0.006 precision: 0.0
Assigned description: has a topic of symbolic controllers in evolutionary robotics. recall: 0.003 precision: 0.0
Assigned description: has a topic of graphical models and approximate inference. recall: 0.152 precision: 0.0
Assigned description: - "has a topic of protein family detection using hidden Markov models. recall: 0.001 precision: 0.0
Assigned description: - "has a topic of term extraction and unithood measurement. recall: 0.037 precision: 0.0
Assigned description: has a topic of fuzzy block theory in underground stability assessment. recall: 0.001 precision: 0.0
Assigned description: has a topic of definitions and theories of intelligence. recall: 0.039 precision: 0.0
Assigned description: has a topic of unithood measurement techniques. recall: 0.009 precision: 0.0
Assigned description: has a topic of data mining challenges and user interaction. recall: 0.036 precision: 0.0
Assigned description: has a topic of intelligent tutoring systems and domain knowledge. recall: 0.019 precision: 0.0
Assigned description: - "has a topic of quantum computation and artificial intelligence. recall: 0.005 precision: 0.0
Assigned description: has a topic of protein family detection using pHMMs. recall: 0.001 precision: 0.0
Assigned description: has a topic of semantic access and natural language processing. recall: 0.109 precision: 0.0
Assigned description: - "has a topic of fuzzy block theory applications. recall: 0.003 precision: 0.0
Assigned description: has a topic of configuration frameworks in computing. recall: 0.082 precision: 0.024390243902439025
Assigned description: has a topic of intelligent agents for news analysis. recall: 0.001 precision: 0.0
Assigned description: has a topic of image classification and segmentation in uncertain environments. recall: 0.006 precision: 0.0
Assigned description: has a topic of definitions of intelligence. recall: 0.018 precision: 0.0
Assigned description: has a topic of cognitive agents and learning mechanisms. recall: 0.186 precision: 0.0
Assigned description: has a topic of intelligent agents and media analysis. recall: 0.039 precision: 0.0
Assigned description: has a topic of definitions of intelligence in cognitive science. recall: 0.01 precision: 0.0
Assigned description: has a topic of tutoring systems and sequential pattern mining. recall: 0.005 precision: 0.0
Assigned description: has a topic of structural information in protein modeling. recall: 0.004 precision: 0.0
Assigned description: has a topic of linguistic evidence and term extraction. recall: 0.085 precision: 0.0
Assigned description: has a topic of cognitive behavior theories. recall: 0.048 precision: 0.0
Assigned description: has a topic of causality and default reasoning. recall: 0.107 precision: 0.0
Assigned description: has a topic of intelligent tutoring systems and domain knowledge extraction. recall: 0.012 precision: 0.0
Assigned description: - "has a topic of intelligent agent frameworks in media analysis. recall: 0.009 precision: 0.0
Assigned description: has a topic of modular architecture in logic programming. recall: 0.012 precision: 0.0
Assigned description: has a topic of evaluation methods in uncertain environments. recall: 0.305 precision: 0.0
Assigned description: - "has a topic of cognitive agents and learning mechanisms. recall: 0.18 precision: 0.0
Assigned description: - "has a topic of earthquake response prediction using artificial neural networks. recall: 0.001 precision: 0.0
Assigned description: has a topic of fuzzy block theory in structural stability assessment. recall: 0.001 precision: 0.0
Assigned description: has a topic of conflict prediction using machine learning techniques. recall: 0.014 precision: 0.0
Assigned description: has a topic of social network prediction and analysis. recall: 0.01 precision: 0.0
Assigned description: has a topic of semantic processing and information retrieval. recall: 0.35 precision: 0.0
Assigned description: has a topic of militarized conflict prediction using machine learning. recall: 0.004 precision: 0.0
Assigned description: has a topic of covert node discovery in social networks. recall: 0.005 precision: 0.0
Assigned description: has a topic of causality definitions and theories. recall: 0.029 precision: 0.0
Assigned description: - "has a topic of causality and its definitions. recall: 0.071 precision: 0.0
Assigned description: has a topic of intelligence data analysis for terrorist investigations. recall: 0.005 precision: 0.0
Assigned description: has a topic of conflict management in decision theory. recall: 0.012 precision: 0.0
Assigned description: - "has a topic of semantic web and natural language processing. recall: 0.077 precision: 0.0
Assigned description: has a topic of predicting militarised interstate disputes. recall: 0.001 precision: 0.0
Assigned description: has a topic of social network analysis and prediction. recall: 0.012 precision: 0.0
Assigned description: has a topic of dynamic data mining techniques. recall: 0.113 precision: 0.0
Assigned description: has a topic of conflict redistribution in decision-making processes. recall: 0.01 precision: 0.0
Assigned description: - "has a topic of linguistic evidence and statistical measures in term extraction. recall: 0.034 precision: 0.0
Assigned description: has a topic of term extraction and unithood measurement. recall: 0.026 precision: 0.0
Assigned description: has a topic of social network analysis and empty spots. recall: 0.008 precision: 0.0
Assigned description: has a topic of intelligence definitions and analysis. recall: 0.074 precision: 0.0
Assigned description: has a topic of node discovery in social networks. recall: 0.006 precision: 0.0
Assigned description: has a topic of causal reasoning and definitions of causality. recall: 0.092 precision: 0.0
Assigned description: has a topic of dynamic data mining and clustering algorithms. recall: 0.038 precision: 0.0
Assigned description: - "has a topic of semantic querying and indexing in information retrieval. recall: 0.057 precision: 0.0
Assigned description: has a topic of narrative structure analysis in film scripts. recall: 0.002 precision: 0.0
Assigned description: has a topic of earthquake response prediction using neural networks. recall: 0.001 precision: 0.0
Assigned description: has a topic of structural response prediction using neural networks. recall: 0.013 precision: 0.0
Assigned description: - "has a topic of network analysis and covert nodes. recall: 0.093 precision: 0.0
Assigned description: has a topic of evolutionary robotics and controller design. recall: 0.023 precision: 0.0
Assigned description: has a topic of fuzzy block theory in stability assessment. recall: 0.001 precision: 0.0
Assigned description: has a topic of cognitive behaviors and scientific proposals. recall: 0.336 precision: 0.002976190476190476
Assigned description: has a topic of unithood measurement in linguistic evidence. recall: 0.006 precision: 0.0
Assigned description: has a topic of structural response prediction in earthquake engineering. recall: 0.002 precision: 0.0
Drop 22 because it cover 624 from 1000
Dropping 1 descriptions because they are too popular or too unpopular:
Stage select select
Unmatched count: 215
Description: has a topic of neural network classifiers. Count: 31
Description: has a topic of ontology and conceptualization in knowledge representation. Count: 275
Description: has a topic of simulation and philosophical implications. Count: 50
Description: has a topic of optimization techniques. Count: 409
Description: has a topic of causality definitions and theories. Count: 12
Description: has a topic of social network analysis and prediction. Count: 8
Clustering results saved at:  experiments/arxiv_ai_abstract/2024-10-30-23-35-26
