{
    "goal": "I would like to cluster the papers based on their primary field of study; each cluster should have a description of 'has a field of <something>'",
    "example_descriptions": [
        "has a field of astrophysics",
        "has a field of database systems"
    ],
    "texts": [
        "  We present SPARSI, a theoretical framework for partitioning sensitive data\nacross multiple non-colluding adversaries. Most work in privacy-aware data\nsharing has considered disclosing summaries where the aggregate information\nabout the data is preserved, but sensitive user information is protected.\nNonetheless, there are applications, including online advertising, cloud\ncomputing and crowdsourcing markets, where detailed and fine-grained user-data\nmust be disclosed. We consider a new data sharing paradigm and introduce the\nproblem of privacy-aware data partitioning, where a sensitive dataset must be\npartitioned among k untrusted parties (adversaries). The goal is to maximize\nthe utility derived by partitioning and distributing the dataset, while\nminimizing the amount of sensitive information disclosed. The data should be\ndistributed so that an adversary, without colluding with other adversaries,\ncannot draw additional inferences about the private information, by linking\ntogether multiple pieces of information released to her. The assumption of no\ncollusion is both reasonable and necessary in the above application domains\nthat require release of private user information. SPARSI enables us to formally\ndefine privacy-aware data partitioning using the notion of sensitive properties\nfor modeling private information and a hypergraph representation for describing\nthe interdependencies between data entries and private information. We show\nthat solving privacy-aware partitioning is, in general, NP-hard, but for\nspecific information disclosure functions, good approximate solutions can be\nfound using relaxation techniques. Finally, we present a local search algorithm\napplicable to generic information disclosure functions. We apply SPARSI\ntogether with the proposed algorithms on data from a real advertising scenario\nand show that we can partition data with no disclosure to any single\nadvertiser.\n",
        "  We determine the abelianization of the symmetric mapping class group of a\ndouble unbranched cover using the Riemann theta constant, Schottky theta\nconstant, and the theta multiplier. We also give lower bounds of the\nabelianizations of some finite index subgroups of the mapping class group.\n",
        "  Long and Moody gave a method of constructing representations of the braid\ngroup B_n. We discuss some ways to generalize their construction. One of these\ngives representations of subgroups of B_n, including the Gassner representation\nof the pure braid group as a special case. Another gives representations of the\nHecke algebra.\n",
        "  Identifying a preferable route is an important problem that finds\napplications in map services. When a user plans a trip within a city, the user\nmay want to find \"a most popular route such that it passes by shopping mall,\nrestaurant, and pub, and the travel time to and from his hotel is within 4\nhours.\" However, none of the algorithms in the existing work on route planning\ncan be used to answer such queries. Motivated by this, we define the problem of\nkeyword-aware optimal route query, denoted by KOR, which is to find an optimal\nroute such that it covers a set of user-specified keywords, a specified budget\nconstraint is satisfied, and an objective score of the route is optimal. The\nproblem of answering KOR queries is NP-hard. We devise an approximation\nalgorithm OSScaling with provable approximation bounds. Based on this\nalgorithm, another more efficient approximation algorithm BucketBound is\nproposed. We also design a greedy approximation algorithm. Results of empirical\nstudies show that all the proposed algorithms are capable of answering KOR\nqueries efficiently, while the BucketBound and Greedy algorithms run faster.\nThe empirical studies also offer insight into the accuracy of the proposed\nalgorithms.\n",
        "  Stellar migration is an important dynamical process in Galactic disk. Here we\nmodel the radial stellar migration in the Galactic disk with an analytical\nmethod, then add it to detailed Galactic chemical evolution model to study the\ninfluence of radial stellar migration on the chemical evolution of the Milky\nWay, especially for the abundance gradients. We found that the radial stellar\nmigration in the Galactic disk can make the profile of the G-dwarf metallicity\ndistribution of the solar neighborhood taller and narrower, thus it becomes\nanother solution to the \"G-dwarf problem\". It can also scatter the\nage-metallicity relation. However, after the migration, the abundance\ndistributions along the Galactic radius don't change much, namely the abundance\ngradients would not be flattened by the radial stellar migration, which is\ndifferent from the predictions of many theoretical works. But it can flatten\nthe radial gradients of the mean chemical abundance of stars, and older stars\npossess flatter abundance gradients than younger stars. The most significant\neffect of the radial stellar migration on the chemical abundance is that at a\nposition it scatters the abundance of stars there from a relatively\nconcentrated value to a range.\n",
        "  We use Heegaard Floer homology to define an invariant of homology cobordism.\nThis invariant is isomorphic to a summand of the reduced Heegaard Floer\nhomology of a rational homology sphere equipped with a spin structure and is\nanalogous to Stoffregen's connected Seiberg-Witten Floer homology. We use this\ninvariant to study the structure of the homology cobordism group and, along the\nway, compute the involutive correction terms for certain families of\nthree-manifolds.\n",
        "  The database community has long recognized the importance of graphical query\ninterface to the usability of data management systems. Yet, relatively less has\nbeen done. We present Orion, a visual interface for querying\nultra-heterogeneous graphs. It iteratively assists users in query graph\nconstruction by making suggestions via machine learning methods. In its active\nmode, Orion automatically suggests top-k edges to be added to a query graph. In\nits passive mode, the user adds a new edge manually, and Orion suggests a\nranked list of labels for the edge. Orion's edge ranking algorithm, Random\nDecision Paths (RDP), makes use of a query log to rank candidate edges by how\nlikely they will match the user's query intent. Extensive user studies using\nFreebase demonstrated that Orion users have a 70% success rate in constructing\ncomplex query graphs, a significant improvement over the 58% success rate by\nthe users of a baseline system that resembles existing visual query builders.\nFurthermore, using active mode only, the RDP algorithm was compared with\nseveral methods adapting other machine learning algorithms such as random\nforests and naive Bayes classifier, as well as class association rules and\nrecommendation systems based on singular value decomposition. On average, RDP\nrequired 40 suggestions to correctly reach a target query graph (using only its\nactive mode of suggestion) while other methods required 1.5--4 times as many\nsuggestions.\n",
        "  Handling skew is one of the major challenges in query processing. In\ndistributed computational environments such as MapReduce, uneven distribution\nof the data to the servers is not desired. One of the dominant measures that we\nwant to optimize in distributed environments is communication cost. In a\nMapReduce job this is the amount of data that is transferred from the mappers\nto the reducers. In this paper we will introduce a novel technique for handling\nskew when we want to compute a multiway join in one MapReduce round with\nminimum communication cost. This technique is actually an adaptation of the\nShares algorithm [Afrati et. al, TKDE 2011].\n",
        "  This paper presents an approach to the task of predicting an event\ndescription from a preceding sentence in a text. Our approach explores\nsequence-to-sequence learning using a bidirectional multi-layer recurrent\nneural network. Our approach substantially outperforms previous work in terms\nof the BLEU score on two datasets derived from WikiHow and DeScript\nrespectively. Since the BLEU score is not easy to interpret as a measure of\nevent prediction, we complement our study with a second evaluation that\nexploits the rich linguistic annotation of gold paraphrase sets of events.\n",
        "  We employ abundances from the Sloan Digital Sky Survey (SDSS) and the Sloan\nExtension for Galactic Understanding and Exploration (SEGUE) to study the\nalpha-element distribution of the stellar members of the Sagittarius stream. To\ntest the reliability of SDSS/SEGUE abundances for the study of Sagittarius, we\nselect high-likelihood samples tracing the different components of the Milky\nWay, and recover known literature alpha-element distributions. Using selection\ncriteria based on the spatial position, radial velocity, distance and colours\nof individual stars, we obtain a robust sample of Sagittarius-stream stars. The\nalpha-element distribution of the Sagittarius stream forms a narrow sequence at\nintermediate metallicities with a clear turn-down, consistent with the presence\nof an alpha-element \"knee\". This is the first time that the alpha-element knee\nof the Sagittarius dwarf galaxy has been detected. Fitting a toy model to our\ndata, we determine that the alpha-knee in Sagittarius takes place at\n[Fe/H]=-1.27pm0.05, only slightly less metal-poor than the knee in the Milky\nWay. This indicates that a small number of Sagittarius-like galaxies could have\ncontributed significantly to the build-up of the Milky Way's stellar halo\nsystem at ancient times.\n",
        "  A boolean expression is in read-once form if each of its variables appears\nexactly once. When the variables denote independent events in a probability\nspace, the probability of the event denoted by the whole expression in\nread-once form can be computed in polynomial time (whereas the general problem\nfor arbitrary expressions is #P-complete). Known approaches to checking\nread-once property seem to require putting these expressions in disjunctive\nnormal form. In this paper, we tell a better story for a large subclass of\nboolean event expressions: those that are generated by conjunctive queries\nwithout self-joins and on tuple-independent probabilistic databases. We first\nshow that given a tuple-independent representation and the provenance graph of\nan SPJ query plan without self-joins, we can, without using the DNF of a result\nevent expression, efficiently compute its co-occurrence graph. From this, the\nread-once form can already, if it exists, be computed efficiently using\nexisting techniques. Our second and key contribution is a complete, efficient,\nand simple to implement algorithm for computing the read-once forms (whenever\nthey exist) directly, using a new concept, that of co-table graph, which can be\nsignificantly smaller than the co-occurrence graph.\n",
        "  Ventricular arrhythmias comprise a group of disorders which manifest\nclinically in a variety of ways from ventricular premature beats (VPB) and no\nsustained ventricular tachycardia (in healthy subjects) to sudden cardiac death\ndue to ventricular tachyarrhythmia in patients with and/or without structural\nheart disease. Ventricular fibrillation (VF) and ventricular tachycardia (VT)\nare the most common electrical mechanisms for cardiac arrest. Accurate and\nautomatic recognition of these arrhythmias from electrocardiography (ECG) is a\ncrucial task for medical professionals. The purpose of this research is to\ndevelop a new index for the differential diagnosis of normal sinus rhythm (SR)\nand ventricular arrhythmias, based on phase space reconstruction (PSR).\n",
        "  We present a Bayesian method for characterizing the mating system of\npopulations reproducing through a mixture of self-fertilization and random\noutcrossing. Our method uses patterns of genetic variation across the genome as\na basis for inference about pure hermaphroditism, androdioecy, and gynodioecy.\nWe extend the standard coalescence model to accommodate these mating systems,\naccounting explicitly for multilocus identity disequilibrium, inbreeding\ndepression, and variation in fertility among mating types. We incorporate the\nEwens Sampling Formula (ESF) under the infinite-alleles model of mutation to\nobtain a novel expression for the likelihood of mating system parameters. Our\nMarkov chain Monte Carlo (MCMC) algorithm assigns locus-specific mutation\nrates, drawn from a common mutation rate distribution that is itself estimated\nfrom the data using a Dirichlet Process Prior (DPP) model. Among the parameters\njointly inferred are the population-wide rate of self-fertilization,\nlocus-specific mutation rates, and the number of generations since the most\nrecent outcrossing event for each sampled individual.\n",
        "  Superconductivity with transition temperature $T_c=1.7$ K has been reported\nin bilayer graphene [1,2]. The main factors, which may shed light on the\nmechanism of the formation of this superconductivity, are the following.\nSuperconductivity is observed in bilayer graphene, when the two layers are\ntwisted, and the maximum of $T_c$ takes place at the \"magic angle\" of twist, at\nwhich the electronic band structure becomes nearly flat. The same factors have\nbeen suggested [3] to explain the experiments in graphite [4-8], which reported\nhigh-T superconductivity in highly oriented pyrolytic graphite (HOPG). The\nhints of room-T superconductivity are present, only when the sample contains\nquasi two-dimensional interfaces between the domains of HOPG. These domains\nshould be twisted with respect to each other in order to form the flat band in\nelectronic spectrum. This dispersionless energy spectrum has a singular density\nof states, which provides the transition temperature being proportional to the\ncoupling constant instead of the exponential suppression. The graphite and its\nsuperconductivity is now becoming the mainstream. One may say that we are\ncoming to graphite era of superconductivity. It is time to combine the\ntheoretical and experimental efforts to reach the bulk room-T superconductivity\nin graphite and in similar real or artificial materials.\n",
        "  Composite structures such as High-Tc multi-filamentary tapes display a\ncomplex anisotropy arising from the combination of the intrinsic anisotropy of\nthe Bi-2223 grains, and that associated to the superconducting phase\ndistribution in the superconductor-metal composite, as well as cracks and other\ndefects. In this paper we characterize the in-plane anisotropy of BSCCO-Ag\ntapes, i.e., the difference between the transport properties along the\nlongitudinal axis and those along the transverse direction also lying on the\nwide face of the tape. In particular, we demonstrate that the dissipation\nassociated to transport along the transverse direction approaches that of the\nlongitudinal direction as the temperature or the current increase, which may be\nrelevant to transport applications in situations where the superconducting\nproperties have significantly degraded.\n",
        "  In contrast to its bulk crystals, the FeSe film or layer exhibits better\nsuperconductivity performance, which attract much interest in its fundamental\nresearch as well as potential application. In present work, transition from\ninsulator to superconductor and significant enhancement of superconductivity\nwere achieved in the high quality (00l) oriented FeSe films via controlling the\nthickness as well as the Fe/Se ratio. The highest Tc up to 15.2 K (almost 2\ntimes higher than those of bulk crystals) and Hc2 up to 35.5 T are obtained in\nour atmosphere-stable FeSe thin film with practical thickness (240 nm),\nimplying their great potential application in the electronic devices at high\nmagnetic fields. More importantly, it was found that the Fe-vacancy disorder in\nFeSe films is the intrinsic factor determining the evolution of the\nsuperconductivity, rather than thickness effect. In our non-superconducting\nFeSe film with Fe/Se ratio of 1.00:1.09, insulating \\b{eta}-Fe1-xSe phase with\niron-vacancy disorders is the main phase and more likely to be the parent phase\nof FeSe superconducting system. Tuning the Fe-vacancy disorders via changing\nthe Fe/Se ratio can dramatically vary the concentration of charge carrier and\nintroduce proper electron doping, which finally leads to the transition from\ninsulator to superconductor and further enhancement in the superconductivity.\nIntriguingly, our results also indicate that when the Fe/Se ratio of film is\nbeyond a critical value, superconducting FeSe films will become instable, and\nphase separation occurs with new non-superconducting phase precipitating in the\nsuperconducting matrix, causing the degradation in the superconductivity. The\nresults in present work help us to well understand the intrinsic mechanism of\nsuperconductivity among Fe-Se superconducting system and provide a new strategy\nto further pursue higher Tc in these materials.\n",
        "  We performed magneto-optical (MO) measurements on FeTe$_{0.5}$Se$_{0.5}$ thin\nfilms grown on LaAlO$_3$ (LAO) and Yttria-stabilized zirconia (YSZ)\nsingle-crystalline substrates. These thin films show superconducting transition\ntemperature, ${T_c}$ $\\sim$ 19 K, 4 K higher than the bulk sample. Typical\nroof-top patterns can be observed in the MO images of thin films grown on LAO\nand YSZ, from which a large and homogeneous critical current density, ${J_c}$,\nover 1 $\\times$ 10$^6$ A/cm$^2$ at 5 K was obtained. Magnetic flux penetration\nmeasurement reveals that the current is almost isotropically distributed in the\ntwo thin films. Compared with bulk crystals, FeTe$_{0.5}$Se$_{0.5}$ thin film\ndemonstrates not only higher ${T_c}$, but also much larger ${J_c}$, which is\nattractive for applications.\n",
        "  Accurately identifying distant recurrences in breast cancer from the\nElectronic Health Records (EHR) is important for both clinical care and\nsecondary analysis. Although multiple applications have been developed for\ncomputational phenotyping in breast cancer, distant recurrence identification\nstill relies heavily on manual chart review. In this study, we aim to develop a\nmodel that identifies distant recurrences in breast cancer using clinical\nnarratives and structured data from EHR. We apply MetaMap to extract features\nfrom clinical narratives and also retrieve structured clinical data from EHR.\nUsing these features, we train a support vector machine model to identify\ndistant recurrences in breast cancer patients. We train the model using 1,396\ndouble-annotated subjects and validate the model using 599 double-annotated\nsubjects. In addition, we validate the model on a set of 4,904 single-annotated\nsubjects as a generalization test. We obtained a high area under curve (AUC)\nscore of 0.92 (SD=0.01) in the cross-validation using the training dataset,\nthen obtained AUC scores of 0.95 and 0.93 in the held-out test and\ngeneralization test using 599 and 4,904 samples respectively. Our model can\naccurately and efficiently identify distant recurrences in breast cancer by\ncombining features extracted from unstructured clinical narratives and\nstructured clinical data.\n",
        "  In evolutionary biology, genetic sequences carry with them a trace of the\nunderlying tree that describes their evolution from a common ancestral\nsequence. The question of how many sequence sites are required to recover this\nevolutionary relationship accurately depends on the model of sequence\nevolution, the substitution rate, divergence times and the method used to infer\nphylogenetic history. A particularly challenging problem for phylogenetic\nmethods arises when a rapid divergence event occurred in the distant past. We\nanalyse an idealised form of this problem in which the terminal edges of a\nsymmetric four--taxon tree are some factor ($p$) times the length of the\ninterior edge. We determine an order $p^2$ lower bound on the growth rate for\nthe sequence length required to resolve the tree (independent of any particular\nbranch length). We also show that this rate of sequence length growth can be\nachieved by existing methods (including the simple `maximum parsimony' method),\nand compare these order $p^2$ bounds with an order $p$ growth rate for a model\nthat describes low-homoplasy evolution. In the final section, we provide a\ngeneric bound on the sequence length requirement for a more general class of\nMarkov processes.\n",
        "  A simplified 2-D model which is an example of regional RF hyperthermia is\npresented. Human body is inside the wire with exciting current and the\nelectromagnetic energy is concentrated within the tumor. The analyzed model is\ntherefore a coupling of the electromagnetic field and the temperature field.\nExciting current density in human body has been calculated using the finite\nelement method, and then bioheat equation in timedepended nonstationary case\nhas been resolved. At the and obtained results are presented.\n",
        "  According to the current cosmological cold dark matter paradigm, the Galactic\nhalo could have been the result of the assemblage of smaller structures. Here\nwe explore the hypothesis that the classical and ultra-faint dwarf spheroidal\nsatellites of the Milky Way have been the building blocks of the Galactic halo\nby comparing their [$\\alpha$/Fe] and [Ba/Fe] versus [Fe/H] patterns with the\nones observed in Galactic halo stars. The $\\alpha$ elements deviate\nsubstantially from the observed abundances in the Galactic halo stars for\n[Fe/H] values larger than -2 dex, while they overlap for lower metallicities.\nOn the other hand, for the [Ba/Fe] ratio the discrepancy is extended at all\n[Fe/H] values, suggesting that the majority of stars in the halo are likely to\nhave been formed in situ. Therefore, we suggest that [Ba/Fe] ratios are a\nbetter diagnostic than [$\\alpha$/Fe] ratios. Moreover, for the first time we\nconsider the effects of an enriched infall of gas with the same chemical\nabundances as the matter ejected and/or stripped from dwarf satellites of the\nMilky Way on the chemical evolution of the Galactic halo. We find that the\nresulting chemical abundances of the halo stars depend on the assumed infall\ntime scale, and the presence of a threshold in the gas for star formation. In\nparticular, in models with an infall timescale for the halo around 0.8 Gyr\ncoupled with a threshold in the surface gas density for the star formation (4\n$\\mathrm{M}_{\\odot}\\,\\mathrm{pc}^{-2}$), and the enriched infall from dwarf\nspheroidal satellites, the first halo stars formed show [Fe/H]$>$-2.4 dex. In\nthis case, to explain [$\\alpha$/Fe] data for stars with [Fe/H]$<$-2.4 dex we\nneed stars formed in dSph systems.\n",
        "  In this paper, we propose an algorithm of searching for both positive and\nnegative itemsets of interest which should be given at the first stage for\npositive and negative association rules mining. Traditional association rule\nmining algorithms extract positive association rules based on frequent\nitemsets, for which the frequent itemsets, i.e. only positive itemsets of\ninterest are searched. Further, there are useful itemsets among the frequent\nitemsets pruned from the traditional algorithms to reduce the search space, for\nmining of negative association rules. Therefore, the traditional algorithms\nhave not come true to find negative itemsets needed in mining of negative\nassociation rules. Our new algorithm to search for both positive and negative\nitemsets of interest prepares preconditions for mining of all positive and\nnegative association rules.\n",
        "  We report on a large-scale study of the distribution of globular clusters\n(GCs) throughout the Virgo cluster, based on photometry from the Next\nGeneration Virgo Cluster Survey, a large imaging survey covering Virgo's\nprimary subclusters to their virial radii. Using the g', (g'-i')\ncolor-magnitude diagram of unresolved and marginally-resolved sources, we\nconstructed 2-D maps of the GC distribution. We present the clearest evidence\nto date showing the difference in concentration between red and blue GCs over\nthe extent of the cluster, where the red (metal-rich) GCs are largely located\naround the massive early-type galaxies, whilst the blue (metal-poor) GCs have a\nmore extended spatial distribution, with significant populations present beyond\n83' (215 kpc) along the major axes of M49 and M87. The GC distribution around\nM87 and M49 shows remarkable agreement with the shape, ellipticity and boxiness\nof the diffuse light surrounding both galaxies. We find evidence for spatial\nenhancements of GCs surrounding M87 that may be indicative of recent\ninteractions or an ongoing merger history. We compare the GC map to the\nlocations of Virgo galaxies and the intracluster X-ray gas, and find good\nagreement between these baryonic structures. The Virgo cluster contains a total\npopulation of 67300$\\pm$14400 GCs, of which 35% are located in M87 and M49\nalone. We compute a cluster-wide specific frequency S_N,CL=$2.8\\pm0.7$,\nincluding Virgo's diffuse light. The GC-to-baryonic mass fraction is\ne_b=$5.7\\pm1.1\\times10^{-4} $and the GC-to-total cluster mass formation\nefficiency is e_t=$2.9\\pm0.5\\times10^{-5}$, values slightly lower than, but\nconsistent with, those derived for individual galactic halos. Our results show\nthat the production of the complex structures in the unrelaxed Virgo cluster\ncore (including the diffuse intracluster light) is an ongoing\nprocess.(abridged)\n",
        "  End users of recent biomedical information systems are often unaware of the\nstorage structure and access mechanisms of the underlying data sources and can\nrequire simplified mechanisms for writing domain specific complex queries. This\nresearch aims to assist users and their applications in formulating queries\nwithout requiring complete knowledge of the information structure of underlying\ndata sources. To achieve this, query reformulation techniques and algorithms\nhave been developed that can interpret ontology-based search criteria and\nassociated domain knowledge in order to reformulate a relational query. These\nquery reformulation algorithms exploit the semantic relationships and assertion\ncapabilities of OWL-DL based domain ontologies for query reformulation. In this\npaper, this approach is applied to the integrated database schema of the EU\nfunded Health-e-Child (HeC) project with the aim of providing ontology assisted\nquery reformulation techniques to simplify the global access that is needed to\nmillions of medical records across the UK and Europe.\n",
        "  This paper presents an approach to classify documents in any language into an\nEnglish topical label space, without any text categorization training data. The\napproach, Cross-Lingual Dataless Document Classification (CLDDC) relies on\nmapping the English labels or short category description into a Wikipedia-based\nsemantic representation, and on the use of the target language Wikipedia.\nConsequently, performance could suffer when Wikipedia in the target language is\nsmall. In this paper, we focus on languages with small Wikipedias,\n(Small-Wikipedia languages, SWLs). We use a word-level dictionary to convert\ndocuments in a SWL to a large-Wikipedia language (LWLs), and then perform CLDDC\nbased on the LWL's Wikipedia. This approach can be applied to thousands of\nlanguages, which can be contrasted with machine translation, which is a\nsupervision heavy approach and can be done for about 100 languages. We also\ndevelop a ranking algorithm that makes use of language similarity metrics to\nautomatically select a good LWL, and show that this significantly improves\nclassification of SWLs' documents, performing comparably to the best bridge\npossible.\n",
        "  Calculating the semantic similarity between sentences is a long dealt problem\nin the area of natural language processing. The semantic analysis field has a\ncrucial role to play in the research related to the text analytics. The\nsemantic similarity differs as the domain of operation differs. In this paper,\nwe present a methodology which deals with this issue by incorporating semantic\nsimilarity and corpus statistics. To calculate the semantic similarity between\nwords and sentences, the proposed method follows an edge-based approach using a\nlexical database. The methodology can be applied in a variety of domains. The\nmethodology has been tested on both benchmark standards and mean human\nsimilarity dataset. When tested on these two datasets, it gives highest\ncorrelation value for both word and sentence similarity outperforming other\nsimilar models. For word similarity, we obtained Pearson correlation\ncoefficient of 0.8753 and for sentence similarity, the correlation obtained is\n0.8794.\n",
        "  We have studied the structural and superconductivity properties of the\ncompound LaFeAsO0.9F0.1 under pressures up to 32GPa using synchrotron radiation\nand diamond anvil cells. We obtain an ambient pressure bulk modulus K_0 =\n78(2)GPa, compressibility comparable to some cuprates. At high pressures, the\nsample is in the overdoped region, with a linear decrease with pressure\nvariation of the superconducting transition temperature.\n",
        "  Activity at the centers of galaxies, during which the central supermassive\nblack hole is accreting material, is nowadays accepted to be rather ubiquitous\nand most probably a phase of every galaxy's evolution. It has been suggested\nthat galactic mergers and interactions may be the culprits behind the\ntriggering of nuclear activity. We use near-infrared data from the new Infrared\nMedium-Deep Survey (IMS) and the Deep eXtragalactic Survey (DXS) of the\nVIMOS-SA22 field and radio data at 1.4 GHz from the FIRST survey and a deep VLA\nsurvey to study the environments of radio-AGN over an area of ~25 sq. degrees\nand down to a radio flux limit of 0.1 mJy and a J-band magnitude of 23 mag AB.\nRadio-AGN are predominantly found in environments similar to those of control\ngalaxies at similar redshift, J-band magnitude, and U-R rest-frame absolute\ncolor. However, a sub-population of radio-AGN is found in environments up to\n100 times denser than their control sources. We thus preclude merging as the\ndominant triggering mechanism of radio-AGN. Through the fitting of the\nbroadband spectral energy distribution of radio-AGN in the least and most dense\nenvironments, we find that those in the least dense environments show higher\nradio-loudness, higher star formation efficiencies, and higher accretion rates,\ntypical of the so-called high-excitation radio-AGN. These differences tend to\ndisappear at z>1. We interpret our results in terms of a different triggering\nmechanism for these sources that is driven by mass-loss through winds of young\nstars created during the observed ongoing star formation.\n",
        "  SPARQL is the standard query language for RDF graphs. In its strict\ninstantiation, it only offers querying according to the RDF semantics and would\nthus ignore the semantics of data expressed with respect to (RDF) schemas or\n(OWL) ontologies. Several extensions to SPARQL have been proposed to query RDF\ndata modulo RDFS, i.e., interpreting the query with RDFS semantics and/or\nconsidering external ontologies. We introduce a general framework which allows\nfor expressing query answering modulo a particular semantics in an homogeneous\nway. In this paper, we discuss extensions of SPARQL that use regular\nexpressions to navigate RDF graphs and may be used to answer queries\nconsidering RDFS semantics. We also consider their embedding as extensions of\nSPARQL. These SPARQL extensions are interpreted within the proposed framework\nand their drawbacks are presented. In particular, we show that the PSPARQL\nquery language, a strict extension of SPARQL offering transitive closure,\nallows for answering SPARQL queries modulo RDFS graphs with the same complexity\nas SPARQL through a simple transformation of the queries. We also consider\nlanguages which, in addition to paths, provide constraints. In particular, we\npresent and compare nSPARQL and our proposal CPSPARQL. We show that CPSPARQL is\nexpressive enough to answer full SPARQL queries modulo RDFS. Finally, we\ncompare the expressiveness and complexity of both nSPARQL and the corresponding\nfragment of CPSPARQL, that we call cpSPARQL. We show that both languages have\nthe same complexity through cpSPARQL, being a proper extension of SPARQL graph\npatterns, is more expressive than nSPARQL.\n",
        "  Through the lens of game theory, cooperation is frequently considered an\nunsustainable strategy: if an entire population is cooperating, each indi-\nvidual can increase its overall fitness by choosing not to cooperate, thereby\nstill receiving all the benefit of its cooperating neighbors while no longer\nexpending its own energy. Observable cooperation in naturally-occurring public\ngoods games is consequently of great interest, as such systems offer insight\ninto both the emergence and sustainability of cooperation. Here we consider a\npopulation that obeys a public goods game on a network of discrete regions\n(that we call nests), between any two of which individuals are free to migrate.\nWe construct a system of piecewise-smooth ordinary differential equations that\ncouple the within-nest population dynamics and the between-nest migratory\ndynamics. Through a combination of analytical and numerical methods, we show\nthat if the workers within the population migrate sufficiently fast relative to\nthe cheaters, the network loses stability first through a Hopf bifurcation,\nthen a torus bifurcation, after which one or more nests collapse. Our results\nindicate that fast moving cheaters can act to stabilize worker-cheater\ncoexistence within network that would otherwise collapse. We end with a\ncomparison of our results with the dynamics observed in colonies of the ant\nspecies Pristomyrmex punctatus and in those of the Cape honeybee Apis mellifera\ncapensis, and argue that they qualitatively agree.\n",
        "  If in the sexual Penna ageing model conditions are applied leading to\ncomplementary bit-strings, then marriages between brothers and sisters, or\nbetween close cousins, may lead to more offspring than for unrelated couples.\n",
        "  Knowledge exploration from the large set of data,generated as a result of the\nvarious data processing activities due to data mining only. Frequent Pattern\nMining is a very important undertaking in data mining. Apriori approach applied\nto generate frequent item set generally espouse candidate generation and\npruning techniques for the satisfaction of the desired objective. This paper\nshows how the different approaches achieve the objective of frequent mining\nalong with the complexities required to perform the job. This paper\ndemonstrates the use of WEKA tool for association rule mining using Apriori\nalgorithm.\n",
        "  We prove that any Bonahon-Siebenmann family of Conway spheres for a\nhyperbolic link is associated to an ideal point of the character variety of the\nlink.\n",
        "  High temperature superconductivity in K-doped 1,2:8,9-dibenzenopentacene\n(C30H18) has been recently reported [1] with Tc = 33 K, the highest among\norganic superconductors at ambient pressure. Here we report on our search for\nsuperconductivity in K, Ba, and Ca-doped hydrocarbon organic materials. We find\nthat Ba-anthracene (C14H10) and K-Picene (C22H14) show features characteristics\nof superconducting state, although very weak. The data suggests that\nBa-anthracene might be a new organic superconductor with Tc ~ 35 K.\n",
        "  The emerging threat of a human pandemic caused by the H5N1 avian influenza\nvirus strain magnifies the need for controlling the incidence of H5N1 infection\nin domestic bird populations. Culling is one of the most widely used control\nmeasures and has proved effective for isolated outbreaks. However, the\nsocio-economic impacts of mass culling, in the face of a disease which has\nbecome endemic in many regions of the world, can affect the implementation and\nsuccess of culling as a control measure. We use mathematical modeling to\nunderstand the dynamics of avian influenza under different culling approaches.\nWe incorporate culling into an SI model by considering the per capita culling\nrates to be general functions of the number of infected birds. Complex dynamics\nof the system, such as backward bifurcation and forward hysteresis, along with\nbi-stability, are detected and analyzed for two distinct culling scenarios. In\nthese cases, employing other control measures temporarily can drastically\nchange the dynamics of the solutions to a more favorable outcome for disease\ncontrol.\n",
        "  Admixed populations are formed by the merging of two or more ancestral\npopulations, and the ancestry of each locus in an admixed genome derives from\neither source. Consider a simple \"pulse\" admixture model, where populations A\nand B merged t generations ago without subsequent gene flow. We derive the\ndistribution of the proportion of an admixed chromosome that has A (or B)\nancestry, as a function of the chromosome length L, t, and the initial\ncontribution of the A source, m. We demonstrate that these results can be used\nfor inference of the admixture parameters. For more complex admixture models,\nwe derive an expression in Laplace space for the distribution of ancestry\nproportions that depends on having the distribution of the lengths of segments\nof each ancestry. We obtain explicit results for the special case of a\n\"two-wave\" admixture model, where population A contributed additional migrants\nin one of the generations between the present and the initial admixture event.\nSpecifically, we derive formulas for the distribution of A and B segment\nlengths and numerical results for the distribution of ancestry proportions. We\nshow that for recent admixture, data generated under a two-wave model can\nhardly be distinguished from that generated under a pulse model.\n",
        "  We study the unresolved X-ray emission in three Local Group dwarf elliptical\ngalaxies (NGC 147, NGC 185 and NGC 205) using XMM-Newton observations, which\nmost likely originates from a collection of weak X-ray sources, mainly\ncataclysmic variables and coronally active binaries. Precise knowledge of this\nstellar X-ray emission is crucial not only for understanding the relevant\nstellar astrophysics but also for disentangling and quantifying the thermal\nemission from diffuse hot gas in nearby galaxies.We find that the integrated\nX-ray emissivities of the individual dwarf ellipticals agree well with that of\nthe Solar vicinity, supporting an often assumed but untested view that the\nX-ray emissivity of old stellar populations is quasi-universal in normal\ngalactic environments, in which dynamical effects on the formation and\ndestruction of binary systems are not important. The average X-ray emissivity\nof the dwarf ellipticals, including M32 studied in the literature, is measured\nto be $L_{0.5-2\\ \\rm {keV}}/M_{\\ast} = (6.0 \\pm 0.5 \\pm 1.8) \\times 10^{27} \\\n\\rm{erg \\ s^{-1} \\ M_\\odot^{-1}}$. We also compare this value to the integrated\nX-ray emissivities of Galactic globular clusters and old open clusters and\ndiscuss the role of dynamical effects in these dense stellar systems.\n",
        "  Using a sample of 98 galaxy clusters recently imaged in the near infra-red\nwith the ESO NTT, WIYN and WHT telescopes, supplemented with 33 clusters from\nthe ESO archive, we measure how the stellar mass of the most massive galaxies\nin the universe, namely Brightest Cluster Galaxies (BCG), increases with time.\nMost of the BCGs in this new sample lie in the redshift range $0.2<z<0.6$,\nwhich has been noted in recent works to mark an epoch over which the growth in\nthe stellar mass of BCGs stalls. From this sample of 132 clusters, we create a\nsubsample of 102 systems that includes only those clusters that have estimates\nof the cluster mass. We combine the BCGs in this subsample with BCGs from the\nliterature, and find that the growth in stellar mass of BCGs from 10 billion\nyears ago to the present epoch is broadly consistent with recent semi-analytic\nand semi-empirical models. As in other recent studies, tentative evidence\nindicates that the stellar mass growth rate of BCGs may be slowing in the past\n3.5 billion years. Further work in collecting larger samples, and in better\ncomparing observations with theory using mock images is required if a more\ndetailed comparison between the models and the data is to be made.\n",
        "  We attain the previously unaccessed full superconducting dome in a pristine\nhigh temperature cuprate superconductor by applying pressures up to 280 kbar to\nsamples of near stoichiometric YBa2Cu3O7. The obtained superconducting phase\nboundary as a function of hole doping closely follows the superconducting dome\nin La2-xSrxCuO4. Measurements are now enabled to trace the evolution of various\nentangled phases and the Fermi surface from the underdoped to overdoped regime\nin a single high purity cuprate superconducting family of materials.\n",
        "  Mutant knots, in the sense of Conway, are known to share the same Homfly\npolynomial. Their 2-string satellites also share the same Homfly polynomial,\nbut in general their m-string satellites can have different Homfly polynomials\nfor m>2. We show that, under conditions of extra symmetry on the constituent\n2-tangles, the directed m-string satellites of mutants share the same Homfly\npolynomial for m<6 in general, and for all choices of m when the satellite is\nbased on a cable knot pattern.\n  We give examples of mutants with extra symmetry whose Homfly polynomials of\nsome 6-string satellites are different, by comparing their quantum sl(3)\ninvariants.\n",
        "  So far, most of association rule minings have considered about positive\nassociation rules based on frequent itemsets in databases[2,5-7], but they have\nnot considered the problem of mining negative association rules correlated with\nfrequent and infrequent itemsets. Negative association rule mining is much more\ndifficult than positive association rule mining because it needs infrequent\nitemsets, and only the rare association rule mining which is a kind of negative\nassociation rule minings has been studied. This paper presents a mathematical\nmodel to mine positive and negative association rules precisely, for which in a\npoint of view that negation of a frequent itemset is an infrequent itemset, we\nmake clear the importance of the problem of mining negative association rules\nbased on certain infrequent itemsets and study on what conditions infrequent\nitemsets of interest should satisfy for negative association rules.\n",
        "  Stochastic reaction-diffusion equations are a popular modelling approach for\nstudying interacting populations in a heterogeneous environment under the\ninfluence of environmental fluctuations. Although the theoretical basis of\nalternative models such as Fokker-Planck diffusion is not less convincing,\nmovement of populations is commonly modelled using the diffusion law due to\nFick. It is an interesting feature of Fokker-Planck diffusion that for\nspatially varying diffusion coefficients the stationary solution is not a\nhomogeneous distribution; in contrast to Fickian diffusion. Instead,\nconcentration accumulates in regions of low diffusivity and tends to lower\nlevels for areas of high diffusivity. Thus, we may interpret the stationary\ndistribution of the Fokker-Planck diffusion as a reflection of different levels\nof habitat quality. Moreover, the most common model for environmental\nfluctuations, linear multiplicative noise, is based on the assumption that\nindividuals respond independently to stochastic environmental fluctuations. For\nlarge population densities the assumption of independence is debatable and the\nmodel further implies that noise intensities can increase to arbitrarily high\nlevels. Therefore, instead of the commonly used linear multiplicative noise\nmodel, we implement environmental variability by an alternative nonlinear noise\nterm which never exceeds a certain maximum noise intensity. With Fokker-Planck\ndiffusion and the nonlinear noise model replacing the classical approaches we\ninvestigate a simple invasive system based on the Lotka-Volterra competition\nmodel. We observe that the heterogeneous stationary distribution generated by\nFokker-Planck diffusion generally facilitates the formation of segregated\nhabitats of resident and invader. However, this segregation can be broken by\nnonlinear noise leading to coexistence of resident and invader across the whole\nspatial domain.\n",
        "  The low-temperature magnetic structure of NdFeAsO has been revisited using\nneutron powder diffraction and symmetry analysis using the Sarah\nrepresentational analysis program. Four magnetic models with one magnetic\nvariable for each of the Nd and Fe sublattices were tested. The best fit was\nobtained using a model with Fe moments pointing along the c-direction, and Nd\nmoments along the a-direction. This signals a significant interplay between\nrare-earth and transition metal magnetism, which results in a\nspin-reorientation of the Fe sublattice upon ordering of the Nd moments. All\nmodels that fit the data well, including collinear models with more than one\nmagnetic variable per sublattice, were found to have an Fe moment of 0.5 BM and\na Nd moment of 0.9 BM, demonstrating that the low-temperature Fe moment is not\nsubstantially enhanced compared to the spin-density wave (SDW) state.\n",
        "  We give sufficient conditions for a satellite knot to admit an L-space\nsurgery, and use this result to give new infinite families of patterns which\nproduce satellite L-space knots.\n",
        "  We compute both natural and smooth models for the $SL_2(\\mathbb C)$ character\nvarieties of the two component double twist links, an infinite family of\ntwo-bridge links indexed as $J(k,l)$. For each $J(k,l)$, the component(s) of\nthe character variety containing characters of irreducible representations are\nbirational to a surface of the form $C\\times \\mathbb C$ where $C$ is a curve.\nThe same is true of the canonical component. We compute the genus of this\ncurve, and the degree of irrationality of the canonical component. We realize\nthe natural model of the canonical component of the $SL_2(\\mathbb C)$ character\nvariety of the $J(3,2m+1)$ links as the surface obtained from\n$\\mathbb{P}^1\\times \\mathbb{P}^1$ as a series of blow-ups.\n",
        "  The gas-phase chemistry of water in protoplanetary disks is analyzed with a\nmodel based on X-ray heating and ionization of the disk atmosphere. Several\nuncertain processes appear to play critical roles in generating the column\ndensities of warm water that are detected from disks at infrared wavelengths.\nThe dominant factors are the reactions that form molecular hydrogen, including\nformation on warm grains, and the ionization and heating of the atmosphere. All\nof these can work together to produce a region of high water abundances in the\nmolecular transition layer of the inner disk atmosphere, where atoms are\ntransformed into molecules, the temperature drops from thousands to hundreds of\nKelvins, and the ionization begins to be dominated by the heavy elements. Grain\nformation of molecular hydrogen and mechanical heating of the atmosphere can\nplay important roles in this region and directly affect the amount of warm\nwater in protoplanetary disk atmospheres. Thus it may be possible to account\nfor the existing measurements of water emission from Tauri disks without\ninvoking transport of water from cooler to warmer regions. The hydroxyl radical\nOH is under-abundant in this model of disk atmospheres and requires\nconsideration of additional production and excitation processes.\n",
        "  With the randomization approach, sensitive data items of records are\nrandomized to protect privacy of individuals while allowing the distribution\ninformation to be reconstructed for data analysis. In this paper, we\ndistinguish between reconstruction that has potential privacy risk, called\nmicro reconstruction, and reconstruction that does not, called aggregate\nreconstruction. We show that the former could disclose sensitive information\nabout a target individual, whereas the latter is more useful for data analysis\nthan for privacy breaches. To limit the privacy risk of micro reconstruction,\nwe propose a privacy definition, called (epsilon,delta)-reconstruction-privacy.\nIntuitively, this privacy notion requires that micro reconstruction has a large\nerror with a large probability. The promise of this approach is that micro\nreconstruction is more sensitive to the number of independent trials in the\nrandomization process than aggregate reconstruction is; therefore, reducing the\nnumber of independent trials helps achieve\n(epsilon,delta)-reconstruction-privacy while preserving the accuracy of\naggregate reconstruction. We present an algorithm based on this idea and\nevaluate the effectiveness of this approach using real life data sets.\n",
        "  Facility location queries identify the best locations to set up new\nfacilities for providing service to its users. Majority of the existing works\nin this space assume that the user locations are static. Such limitations are\ntoo restrictive for planning many modern real-life services such as fuel\nstations, ATMs, convenience stores, cellphone base-stations, etc. that are\nwidely accessed by mobile users. The placement of such services should,\ntherefore, factor in the mobility patterns or trajectories of the users rather\nthan simply their static locations. In this work, we introduce the TOPS\n(Trajectory-Aware Optimal Placement of Services) query that locates the best k\nsites on a road network. The aim is to optimize a wide class of objective\nfunctions defined over the user trajectories. We show that the problem is\nNP-hard and even the greedy heuristic with an approximation bound of (1-1/e)\nfails to scale on urban-scale datasets. To overcome this challenge, we develop\na multi-resolution clustering based indexing framework called NetClus.\nEmpirical studies on real road network trajectory datasets show that NetClus\noffers solutions that are comparable in terms of quality with those of the\ngreedy heuristic, while having practical response times and low memory\nfootprints. Additionally, the NetClus framework can absorb dynamic updates in\nmobility patterns, handle constraints such as site-costs and capacity, and\nexisting services, thereby providing an effective solution for modern\nurban-scale scenarios.\n",
        "  We investigate the spin-triplet superconducting state of Sr2RuO4 in the\nmagnetic field along the c-axis on the basis of the four-component\nGinzburg-Landau (GL) model with a weak spin-orbit coupling. We consider\nsuperconducting states described by the d-vector parallel to the ab-plane, and\nfind that three spin-triplet pairing states are stabilized in the magnetic\nfield-temperature phase diagram. Although a helical state is stable at low\nmagnetic fields, a chiral II state is stabilized at high magnetic fields. A\nnon-unitary spin-triplet pairing state appears near the transition temperature\nowing to the coupling of magnetic field and chirality. We elucidate synergistic\nand/or competing roles of the magnetic field, chirality, and spin-orbit\ncoupling. It is shown that a fractional vortex lattice is stabilized in the\nchiral II phase owing to the spin-orbit coupling.\n",
        "  We present the results of a densely sampled spectroscopic survey of the SSA22\nprotocluster at $z\\approx 3.09$. Our sample with Keck/LRIS spectroscopy\nincludes 106 Ly$\\alpha$ Emitters (LAEs) and 40 Lyman Break Galaxies (LBGs) at\n$z=3.05-3.12$. These galaxies are contained within the $9'\\times9'$ region in\nwhich the protocluster was discovered, which also hosts the maximum galaxy\noverdensity in the SSA22 region. The redshift histogram of our spectroscopic\nsample reveals two distinct peaks, at $z=3.069$ (blue, 43 galaxies) and\n$z=3.095$ (red, 103 galaxies). Furthermore, objects in the blue and red peaks\nare segregated on the sky, with galaxies in the blue peak concentrating towards\nthe western half of the field. These results suggest that the blue and red\nredshift peaks represent two distinct structures in physical space. Although\nthe double-peaked redshift histogram is traced in the same manner by LBGs and\nLAEs, and brighter and fainter galaxies, we find that nine out of 10 X-ray AGNs\nin SSA22, and all seven spectroscopically-confirmed giant Ly$\\alpha$ \"blobs,\"\nreside in the red peak. We combine our dataset with sparsely sampled\nspectroscopy from the literature over a significantly wider area, finding\npreliminary evidence that the double-peaked structure in redshift space extends\nbeyond the region of our dense spectroscopic sampling. In order to fully\ncharacterize the three-dimensional structure, dynamics, and evolution of\nlarge-scale structure in the SSA22 overdensity, we require the measurement of\nlarge samples of LAE and LBG redshifts over a significantly wider area, as well\nas detailed comparisons with cosmological simulations of massive cluster\nformation.\n",
        "  We present C and O abundances in the Magellanic Clouds derived from deep\nspectra of HII regions. The data have been taken with the Ultraviolet-Visual\nEchelle Spectrograph at the 8.2-m VLT. The sample comprises 5 HII regions in\nthe Large Magellanic Cloud (LMC) and 4 in the Small Magellanic Cloud (SMC). We\nmeasure pure recombination lines (RLs) of CII and OII in all the objects,\npermitting to derive the abundance discrepancy factors (ADFs) for O^2+, as well\nas their O/H, C/H and C/O ratios. We compare the ADFs with those of other HII\nregions in different galaxies. The results suggest a possible metallicity\ndependence of the ADF for the low-metallicity objects, but more uncertain for\nhigh-metallicity objects. We compare nebular and B-type stellar abundances and\nwe find that the stellar abundances agree better with the nebular ones derived\nfrom collisionally excited lines (CELs). Comparing these results with other\ngalaxies we observe that stellar abundances seem to agree better with the\nnebular ones derived from CELs in low-metallicity environments and from RLs in\nhigh-metallicity environments. The C/H, O/H and C/O ratios show almost flat\nradial gradients, in contrast with the spiral galaxies where such gradients are\nnegative. We explore the chemical evolution analysing C/O vs. O/H and comparing\nwith the results of HII regions in other galaxies. The LMC seems to show a\nsimilar chemical evolution to the external zones of small spiral galaxies and\nthe SMC behaves as a typical star-forming dwarf galaxy.\n",
        "  We have estimated a metallicity map of the Large Magellanic Cloud (LMC) using\nthe Magellanic Cloud Photometric Survey (MCPS) and Optical Gravitational\nLensing Experiment (OGLE III) photometric data. This is a first of its kind map\nof metallicity up to a radius of 4 - 5 degrees, derived using photometric data\nand calibrated using spectroscopic data of Red Giant Branch (RGB) stars. We\nidentify the RGB in the V, (V$-$I) colour magnitude diagrams of small\nsubregions of varying sizes in both data sets. We use the slope of the RGB as\nan indicator of the average metallicity of a subregion, and calibrate the RGB\nslope to metallicity using spectroscopic data for field and cluster red giants\nin selected subregions. The average metallicity of the LMC is found to be\n[Fe/H] = $-$0.37 dex ($\\sigma$[Fe/H] = 0.12) from MCPS data, and [Fe/H] =\n$-$0.39 dex ($\\sigma$[Fe/H] = 0.10) from OGLE III data. The bar is found be the\nmost metal-rich region of the LMC. Both the data sets suggest a shallow radial\nmetallicity gradient up to a radius of 4 kpc ($-$0.049$\\pm$0.002 dex kpc$^{-1}$\nto $-$0.066$\\pm$0.006 dex kpc$^{-1}$). Subregions in which the mean metallicity\ndiffers from the surrounding areas do not appear to correlate with previously\nknown features; spectroscopic studies are required in order to assess their\nphysical significance.\n",
        "  Finite Element mesh generation remains an important issue for patient\nspecific biomechanical modeling. While some techniques make automatic mesh\ngeneration possible, in most cases, manual mesh generation is preferred for\nbetter control over the sub-domain representation, element type, layout and\nrefinement that it provides. Yet, this option is time consuming and not suited\nfor intraoperative situations where model generation and computation time is\ncritical. To overcome this problem we propose a fast and automatic mesh\ngeneration technique based on the elastic registration of a generic mesh to the\nspecific target organ in conjunction with element regularity and quality\ncorrection. This Mesh-Match-and-Repair (MMRep) approach combines control over\nthe mesh structure along with fast and robust meshing capabilities, even in\nsituations where only partial organ geometry is available. The technique was\nsuccessfully tested on a database of 5 pre-operatively acquired complete femora\nCT scans, 5 femoral heads partially digitized at intraoperative stage, and 50\nCT volumes of patients' heads. The MMRep algorithm succeeded in all 60 cases,\nyielding for each patient a hex-dominant, Atlas based, Finite Element mesh with\nsubmillimetric surface representation accuracy, directly exploitable within a\ncommercial FE software.\n",
        "  Results from molecular systematics and comparative developmental genetics\nchanged the picture of metazoan and especially bilaterian radiation. According\nto this new animal phylogeny (introduced by Adoutte et al. 1999/2000), Grobbens\n(1908) widely favoured protostome-deuterostome division of the Bilateria can be\nupheld, but only with major rearrangements within these superphyla. On the\ncladogenetic level, the Protostomia are split into two unexpected subgroups,\nthe Lophotrochozoa and Ecdysozoa. The deuterostomes are split into the\nsubgroups Chordata and Ambulacraria, which is not novel since Grobben (1908)\nintroduced the Deuterostomia in this way (together with the Chaetognatha as a\nthird line). However, many details of the new deuterostome phylogeny do not fit\ntraditional, morphology-based reconstructions. As a consequence, three\nrelatively unexpected proposals for early deuterostome evolution are favoured\ntoday: An ambulacraria-scenario, a xenoturbellid-scenario, and a\nchordate-scenario. The first two proposals are often discussed in the\nliterature, while the chordate-scenario is almost completely neglected.\nTherefore, the paper presented focuses on the chordate scenario, i.e. the\nhypothesis of an acrania-like ur-deuterostomian. It is argued that the\nacrania-hypothesis is clearly preferable when biomechanic options of a\npolysegmented, hydroskeletal body plan are taken into account. The so called\nhydroskeleton hypothesis, rooted in the work of W. F. Gutmann, is the most\ndetailed anagenetic scenario which depicts an acrania-like ur-deuterostome.\n",
        "  In this paper, we introduce \\textsc{Yedda}, a lightweight but efficient and\ncomprehensive open-source tool for text span annotation. \\textsc{Yedda}\nprovides a systematic solution for text span annotation, ranging from\ncollaborative user annotation to administrator evaluation and analysis. It\novercomes the low efficiency of traditional text annotation tools by annotating\nentities through both command line and shortcut keys, which are configurable\nwith custom labels. \\textsc{Yedda} also gives intelligent recommendations by\nlearning the up-to-date annotated text. An administrator client is developed to\nevaluate annotation quality of multiple annotators and generate detailed\ncomparison report for each annotator pair. Experiments show that the proposed\nsystem can reduce the annotation time by half compared with existing annotation\ntools. And the annotation time can be further compressed by 16.47\\% through\nintelligent recommendation.\n",
        "  We want to characterize the properties of the cold dust clumps in the Carina\nNebula Complex (CNC), which shows a very high level of massive star feedback.\nWe derive the Clump Mass Function (ClMF), explore the reliability of different\nclump extraction algorithms, and investigate the influence of the temperatures\nwithin the clouds on the resulting shape of the ClMF.\n  We analyze a 1.25x1.25 deg^2 wide-field sub-mm map obtained with LABOCA\n(APEX), which provides the first spatially complete survey of the clouds in the\nCNC. We use the three clump-finding algorithms CLUMPFIND (CF), GAUSSCLUMPS (GC)\nand SExtractor (SE) to identify individual clumps and determine their total\nfluxes. In addition to assuming a common `typical' temperature for all clouds,\nwe also employ an empirical relation between cloud column densities and\ntemperature to determine an estimate of the individual clump temperatures, and\nuse this to determine individual clump masses.\n  While the ClMF based on the CF extraction is very well described by a\npower-law, the ClMFs based on GC and SE are better represented by a log-normal\ndistribution. We also find that the use of individual clump temperatures leads\nto a shallower ClMF slope than the assumption of a common temperature (e.g. 20\nK) of all clumps.\n  The power-law of dN/dM \\propto M^-1.95 we find for the CF sample is in good\nagreement with ClMF slopes found in previous studies of other regions. The\ndependence of the ClMF shape (power-law vs. log-normal distribution) on the\nemployed extraction method suggests that observational determinations of the\nClMF shape yields only very limited information about the true structure of the\ncloud. Interpretations of log-normal ClMF shape as a signature of turbulent\npre-stellar clouds vs. power-law ClMFs as a signature of star-forming clouds\nmay be taken with caution for a single extraction algorithm without additional\ninformation.\n",
        "  Two extensions to the AMR smatch scoring script are presented. The first\nextension com-bines the smatch scoring script with the C6.0 rule-based\nclassifier to produce a human-readable report on the error patterns frequency\nobserved in the scored AMR graphs. This first extension results in 4% gain over\nthe state-of-art CAMR baseline parser by adding to it a manually crafted\nwrapper fixing the identified CAMR parser errors. The second extension combines\na per-sentence smatch with an en-semble method for selecting the best AMR graph\namong the set of AMR graphs for the same sentence. This second modification\nau-tomatically yields further 0.4% gain when ap-plied to outputs of two\nnondeterministic AMR parsers: a CAMR+wrapper parser and a novel character-level\nneural translation AMR parser. For AMR parsing task the character-level neural\ntranslation attains surprising 7% gain over the carefully optimized word-level\nneural translation. Overall, we achieve smatch F1=62% on the SemEval-2016\nofficial scor-ing set and F1=67% on the LDC2015E86 test set.\n",
        "  We present the first scientific results from the Sydney-AAO Multi-Object IFS\n(SAMI) at the Anglo-Australian Telescope. This unique instrument deploys 13\nfused fibre bundles (hexabundles) across a one-degree field of view allowing\nsimultaneous spatially-resolved spectroscopy of 13 galaxies. During the first\nSAMI commissioning run, targeting a single galaxy field, one object (ESO\n185-G031) was found to have extended minor axis emission with ionisation and\nkinematic properties consistent with a large-scale galactic wind. The\nimportance of this result is two-fold: (i) fibre bundle spectrographs are able\nto identify low-surface brightness emission arising from extranuclear activity;\n(ii) such activity may be more common than presently assumed because\nconventional multi-object spectrographs use single-aperture fibres and spectra\nfrom these are nearly always dominated by nuclear emission. These early results\ndemonstrate the extraordinary potential of multi-object hexabundle spectroscopy\nin future galaxy surveys.\n",
        "  It has been known since 1981 that if one fixes an orientable surface $S$ of\ngenus $g$, then there is a real number $\\lambda_{min,g} > 1$ that is the\ndilatation of a pA diffeomorphism of $S$, and every other pA diffeomorphism of\n$S$ has dilatation $\\geq \\lambda_{min,g}$. We will show how a little-known\ntheorem about digraphs gives some insight into $\\lambda_{min,g}$.\n",
        "  In this paper we take closer look at recent developments for the chase\nprocedure, and provide additional results. Our analysis allows us create a\ntaxonomy of the chase variations and the properties they satisfy. Two of the\nmost central problems regarding the chase is termination, and discovery of\nrestricted classes of sets of dependencies that guarantee termination of the\nchase. The search for the restricted classes has been motivated by a fairly\nrecent result that shows that it is undecidable to determine whether the chase\nwith a given dependency set will terminate on a given instance. There is a\nsmall dissonance here, since the quest has been for classes of sets of\ndependencies guaranteeing termination of the chase on all instances, even\nthough the latter problem was not known to be undecidable. We resolve the\ndissonance in this paper by showing that determining whether the chase with a\ngiven set of dependencies terminates on all instances is coRE-complete. For the\nhardness proof we use a reduction from word rewriting systems, thereby also\nshowing the close connection between the chase and word rewriting. The same\nreduction also gives us the aforementioned instance-dependent RE-completeness\nresult as a byproduct. For one of the restricted classes guaranteeing\ntermination on all instances, the stratified sets dependencies, we provide new\ncomplexity results for the problem of testing whether a given set of\ndependencies belongs to it. These results rectify some previous claims that\nhave occurred in the literature.\n",
        "  The interplay between parasites and their hosts is found in all kinds of\nspecies and plays an important role in understanding the principles of\nevolution and coevolution. Usually, the different genotypes of hosts and\nparasites oscillate in their abundances. The well-established theory of\noscillatory Red Queen dynamics proposes an ongoing change in frequencies of the\ndifferent types within each species. So far, it is unclear in which way Red\nQueen dynamics persists with more than two types of hosts and parasites. In our\nanalysis, an arbitrary number of types within two species are examined in a\ndeterministic framework with constant or changing population size. This general\nframework allows for analytical solutions for internal fixed points and their\nstability. For more than two species, apparently chaotic dynamics has been\nreported. Here we show that even for two species, once more than two types are\nconsidered per species, irregular dynamics in their frequencies can be observed\nin the long run. The nature of the dynamics depends strongly on the initial\nconfiguration of the system; the usual regular Red Queen oscillations are only\nobserved in some parts of the parameter region.\n",
        "  Over the past century, personality theory and research has successfully\nidentified core sets of characteristics that consistently describe and explain\nfundamental differences in the way people think, feel and behave. Such\ncharacteristics were derived through theory, dictionary analyses, and survey\nresearch using explicit self-reports. The availability of social media data\nspanning millions of users now makes it possible to automatically derive\ncharacteristics from language use -- at large scale. Taking advantage of\nlinguistic information available through Facebook, we study the process of\ninferring a new set of potential human traits based on unprompted language use.\nWe subject these new traits to a comprehensive set of evaluations and compare\nthem with a popular five factor model of personality. We find that our\nlanguage-based trait construct is often more generalizable in that it often\npredicts non-questionnaire-based outcomes better than questionnaire-based\ntraits (e.g. entities someone likes, income and intelligence quotient), while\nthe factors remain nearly as stable as traditional factors. Our approach\nsuggests a value in new constructs of personality derived from everyday human\nlanguage use.\n",
        "  We describe measurements on microwave coplanar resonators designed for\nquantum bit experiments. Resonators have been patterned onto sapphire and\nsilicon substrates, and quality factors in excess of a million have been\nobserved. The resonant frequency shows a high sensitivity to magnetic field\napplied perpendicular to the plane of the film, with a quadratic dependence for\nthe fundamental, second and third harmonics. Frequency shift of hundreds of\nlinewidths can be obtained.\n",
        "  New laboratory data of ethyl mercaptan, CH$_{3}$CH$_{2}$SH, in the millimeter\nand submillimeter-wave domains (up to 880 GHz) provided very precise values of\nthe spectroscopic constants that allowed the detection of\n$gauche$-CH$_3$CH$_2$SH towards Orion KL. 77 unblended or slightly blended\nlines plus no missing transitions in the range 80-280 GHz support this\nidentification. A detection of methyl mercaptan, CH$_{3}$SH, in the spectral\nsurvey of Orion KL is reported as well. Our column density results indicate\nthat methyl mercaptan is $\\simeq$ 5 times more abundant than ethyl mercaptan in\nthe hot core of Orion KL.\n",
        "  Adaptive indexing initializes and optimizes indexes incrementally, as a side\neffect of query processing. The goal is to achieve the benefits of indexes\nwhile hiding or minimizing the costs of index creation. However,\nindex-optimizing side effects seem to turn read-only queries into update\ntransactions that might, for example, create lock contention. This paper\nstudies concurrency control in the context of adaptive indexing. We show that\nthe design and implementation of adaptive indexing rigorously separates index\nstructures from index contents; this relaxes the constraints and requirements\nduring adaptive indexing compared to those of traditional index updates. Our\ndesign adapts to the fact that an adaptive index is refined continuously, and\nexploits any concurrency opportunities in a dynamic way. A detailed\nexperimental analysis demonstrates that (a) adaptive indexing maintains its\nadaptive properties even when running concurrent queries, (b) adaptive indexing\ncan exploit the opportunity for parallelism due to concurrent queries, (c) the\nnumber of concurrency conflicts and any concurrency administration overheads\nfollow an adaptive behavior, decreasing as the workload evolves and adapting to\nthe workload needs.\n",
        "  Southern and eastern African populations that speak non-Bantu languages with\nclick consonants are known to harbour some of the most ancient genetic lineages\nin humans, but their relationships are poorly understood. Here, we report data\nfrom 23 populations analyzed at over half a million single nucleotide\npolymorphisms, using a genome-wide array designed for studying human history.\nThe southern African Khoisan fall into two genetic groups, loosely\ncorresponding to the northwestern and southeastern Kalahari, which we show\nseparated within the last 30,000 years. We find that all individuals derive at\nleast a few percent of their genomes from admixture with non-Khoisan\npopulations that began approximately 1,200 years ago. In addition, the east\nAfrican Hadza and Sandawe derive a fraction of their ancestry from admixture\nwith a population related to the Khoisan, supporting the hypothesis of an\nancient link between southern and eastern Africa\n",
        "  With the help of Gaia DR2, we are able to obtain the full 6-D phase space\ninformation for stars from LAMOST DR5. With high precision of position,\nvelocity, and metallicity, the rotation of the local stellar halo is presented\nusing the K giant stars with [Fe/H]$<-1$ dex within 4 kpc from the Sun. By\nfitting the rotational velocity distribution with three Gaussian components,\nstellar halo, disk, and a counter-rotating hot population, we find that the\nlocal halo progradely rotates with $V_T=+27^{+4}_{-5}$ km s$^{-1}$ providing\nthe local standard of rest velocity of $V_{LSR}=232$ km s$^{-1}$. Meanwhile, we\nobtain the dispersion of rotational velocity is $\\sigma_{T}=72^{+4}_{-4}$ km\ns$^{-1}$. Although the rotational velocity strongly depends on the choice of\n$V_{LSR}$, the trend of prograde rotation is substantial even when $V_{LSR}$ is\nset at as low as 220 km s$^{-1}$. Moreover, we derive the rotation for\nsubsamples with different metallicities and find that the rotational velocity\nis essentially not correlated with [Fe/H]. This may hint a secular evolution\norigin of the prograde rotation. It shows that the metallicity of the\nprogradely rotating halo is peaked within -1.9$<$[Fe/H]$<$-1.6 without\nconsidering the selection effect. We also find a small fraction of\ncounter-rotating stars with larger dispersion and lower metallicity. Finally,\nthe disk component rotates with $V_T=+182^{+6}_{-6}$ km s$^{-1}$ and\n$\\sigma_T=45^{+3}_{-3}$ km s$^{-1}$, which is quite consistent with the\nmetal-weak thick disk population.\n",
        "  In various approaches, data cubes are pre-computed in order to answer\nefficiently OLAP queries. The notion of data cube has been declined in various\nways: iceberg cubes, range cubes or differential cubes. In this paper, we\nintroduce the concept of convex cube which captures all the tuples of a\ndatacube satisfying a constraint combination. It can be represented in a very\ncompact way in order to optimize both computation time and required storage\nspace. The convex cube is not an additional structure appended to the list of\ncube variants but we propose it as a unifying structure that we use to\ncharacterize, in a simple, sound and homogeneous way, the other quoted types of\ncubes. Finally, we introduce the concept of emerging cube which captures the\nsignificant trend inversions. characterizations.\n",
        "  $^{75}$As NMR investigation of a single crystal of superconducting LiFeAs is\npresented. The Knight shift and the \\textit{in situ} ac susceptibility\nmeasurements as a function of temperature and external field are indicative of\ntwo superconducting (SC) transition temperatures, each of which is associated\nwith its own upper critical field. Strikingly, the Knight shift maintains its\nnormal state value over a temperature range in the SC state before it drops\nabruptly being consistent with spin-singlet pairing. Together with our previous\nNMR study, the anomalous SC state featured by the constant Knight shift is\nattributed to the extremely sensitive SC properties of LiFeAs, probably\nstemming from its proximity to a critical instability.\n",
        "  Magnetic fields interact with biological cells affecting them in variety of\nways which are usually hard to predict. Among them, it was observed that strong\nfields can align dividing cells in a preferred direction. It was also\ndemonstrated that dividing cancer cells are effectively destroyed by applying\nelectric fields in vivo with a success rate dependent on the cell-to-field\norientation. Based on these facts, the present note aims to suggest the use of\nmagnetic and electric fields for improved cancer treatment. Several\npossibilities of generating the electric fields inside the magnetic field\nvolume are reviewed, main tentative approaches are described and discussed.\nMost if not all of them require special magnet configuration research which can\nbe based on existing magnet systems in operation or in development.\n",
        "  High radiation dose in CT scans increases a lifetime risk of cancer and has\nbecome a major clinical concern. Recently, iterative reconstruction algorithms\nwith Total Variation (TV) regularization have been developed to reconstruct CT\nimages from highly undersampled data acquired at low mAs levels in order to\nreduce the imaging dose. Nonetheless, TV regularization may lead to\nover-smoothed images and lost edge information. To solve this problem, in this\nwork we develop an iterative CT reconstruction algorithm with edge-preserving\nTV regularization to reconstruct CT images from highly undersampled data\nobtained at low mAs levels. The CT image is reconstructed by minimizing an\nenergy consisting of an edge-preserving TV norm and a data fidelity term posed\nby the x-ray projections. The edge-preserving TV term is proposed to\npreferentially perform smoothing only on non-edge part of the image in order to\navoid over-smoothing, which is realized by introducing a penalty weight to the\noriginal total variation norm. Our iterative algorithm is implemented on GPU to\nimprove its speed. We test our reconstruction algorithm on a digital NCAT\nphantom, a physical chest phantom, and a Catphan phantom. Reconstruction\nresults from a conventional FBP algorithm and a TV regularization method\nwithout edge preserving penalty are also presented for comparison purpose. The\nexperimental results illustrate that both TV-based algorithm and our\nedge-preserving TV algorithm outperform the conventional FBP algorithm in\nsuppressing the streaking artifacts and image noise under the low dose context.\nOur edge-preserving algorithm is superior to the TV-based algorithm in that it\ncan preserve more information of fine structures and therefore maintain\nacceptable spatial resolution.\n",
        "  We investigate the topological aspect of the spin-triplet $f$-wave\nsuperconductor UPt$_3$ through microscopic calculations of edge- and\nvortex-bound states based on the quasiclassical Eilenberger and Bogoliubov-de\nGennes theories. It is shown that a gapless and linear dispersion exists at the\nedge of the $ab$-plane. This forms a Majorana valley, protected by the mirror\nchiral symmetry. We also demonstrate that, with increasing magnetic field,\nvortex-bound quasiparticles undergo a topological phase transition from\ntopologically trivial states in the double-core vortex to zero-energy states in\nthe normal-core vortex. As long as the $d$-vector is locked into the\n$ab$-plane, the mirror symmetry holds the Majorana property of the zero-energy\nstates, and thus UPt$_3$ preserves topological crystalline superconductivity\nthat is robust against the crystal field and spin-orbit interaction.\n",
        "  We compare the predictions of Horizon-AGN, a hydro-dynamical cosmological\nsimulation that uses an adaptive mesh refinement code, to observational data in\nthe redshift range 0<z<6. We study the reproduction, by the simulation, of\nquantities that trace the aggregate stellar-mass growth of galaxies over cosmic\ntime: luminosity and stellar-mass functions, the star formation main sequence,\nrest-frame UV-optical-near infrared colours and the cosmic star-formation\nhistory. We show that Horizon-AGN, which is not tuned to reproduce the local\nUniverse, produces good overall agreement with these quantities, from the\npresent day to the epoch when the Universe was 5% of its current age. By\ncomparison to Horizon-noAGN, a twin simulation without AGN feedback, we\nquantify how feedback from black holes is likely to help shape galaxy\nstellar-mass growth in the redshift range 0<z<6, particularly in the most\nmassive galaxies. Our results demonstrate that Horizon-AGN successfully\ncaptures the evolutionary trends of observed galaxies over the lifetime of the\nUniverse, making it an excellent tool for studying the processes that drive\ngalaxy evolution and making predictions for the next generation of galaxy\nsurveys.\n",
        "  For evaluating generation systems, automatic metrics such as BLEU cost\nnothing to run but have been shown to correlate poorly with human judgment,\nleading to systematic bias against certain model improvements. On the other\nhand, averaging human judgments, the unbiased gold standard, is often too\nexpensive. In this paper, we use control variates to combine automatic metrics\nwith human evaluation to obtain an unbiased estimator with lower cost than\nhuman evaluation alone. In practice, however, we obtain only a 7-13% cost\nreduction on evaluating summarization and open-response question answering\nsystems. We then prove that our estimator is optimal: there is no unbiased\nestimator with lower cost. Our theory further highlights the two fundamental\nbottlenecks---the automatic metric and the prompt shown to human\nevaluators---both of which need to be improved to obtain greater cost savings.\n",
        "  The oxygen deficiency $\\delta$ in YBa$_2$Cu$_3$O$_{7-\\delta}$ (YBCO) plays a\ncrucial role for affecting high-temperature superconductivity. We applied\n(coincident) Doppler broadening spectroscopy of the electron-positron\nannihilation line to study in situ the temperature dependence of the oxygen\nconcentration and its depth profile in single crystalline YBCO film grown on\nSrTiO$_3$ (STO) substrates. The oxygen diffusion during tempering was found to\nlead to a distinct depth dependence of $\\delta$, which is not accessible using\nX-ray diffraction. A steady-state reached within a few minutes is defined by\nboth, the oxygen exchange at the surface and at the interface to the STO\nsubstrate. Moreover, we revealed the depth dependent critical temperature\n$T_{\\mathrm{c}}$ in the as prepared and tempered YBCO film.\n",
        "  Suppose that a hyperbolic knot in $S^3$ admits a finite surgery, Boyer and\nZhang proved that the surgery slope must be either integral or half-integral,\nand they conjectured that the latter case does not happen. Using the correction\nterms in Heegaard Floer homology, we prove that if a hyperbolic knot in $S^3$\nadmits a half-integral finite surgery, then the knot must have the same knot\nFloer homology as one of eight non-hyperbolic knots which are known to admit\nsuch surgeries, and the resulting manifold must be one of ten spherical space\nforms. As knot Floer homology carries a lot of information about the knot, this\ngives a strong evidence to Boyer--Zhang's conjecture.\n",
        "  We point out that a simple and generic strategy to lower the risk for\nextinction consists in the developing a dormant stage in which the organism is\nunable to multiply but may die. The dormant organism is protected against the\npoisonous environment. The result is to increase the survival probability of\nthe entire population by introducing a type of zero reproductive fitness. This\nis possible, because the reservoir of dormant individuals act as a buffer that\ncan cushion fatal fluctuations in the number of births and deaths which without\nthe dormant population would have driven the entire population to extinction.\n",
        "  We have developed a model for proton depth dose and lateral distributions\nbased on Monte Carlo calculations (GEANT4) and an integration procedure of the\nBethe-Bloch equation (BBE). The model accounts for the transport of primary and\nsecondary protons, the creation of recoil protons and heavy recoil nuclei as\nwell as lateral scattering of these contributions. The buildup, which is\nexperimentally observed in higher energy depth dose curves, is modeled by\ninclusion of two different origins: 1. Secondary reaction protons with a\ncontribution of ca. 65 % of the buildup (for monoenergetic protons). 2. Landau\ntails as well as Gaussian type of fluctuations for range straggling effects.\nAll parameters of the model for initially monoenergetic proton beams have been\nobtained from Monte Carlo calculations or checked by them. Furthermore, there\nare a few parameters, which can be obtained by fitting the model to measured\ndepth dose curves in order to describe individual characteristics of the\nbeamline - the most important being the initial energy spread. We find that the\nfree parameters of the depth dose model can be predicted for any intermediate\nenergy from a couple of measured curves.\n",
        "  Predicting adaptive evolutionary trajectories is a primary goal of\nevolutionary biology. One can differentiate between forward and backward\npredictability, where forward predictability measures the likelihood of the\nsame adaptive trajectory occurring in independent evolutions and backward\npredictability measures the likelihood of a particular adaptive path given the\nknowledge of starting and final states. Recent studies have attempted to\nmeasure both forward and backward predictability using experimental evolution\nin asexual haploid microorganisms. Similar experiments in diploid organisms\nhave not been conducted. Here we simulate adaptive walks using Fisher's\nGeometric Model in haploids and diploids and find that adaptive walks in\ndiploids are less forward- and more backward-predictable than adaptive walks in\nhaploids. We argue that the difference is due to the ability of diploids in our\nsimulations to generate transiently stable polymorphisms and to allow adaptive\nmutations of larger phenotypic effect. As stable polymorphisms can be generated\nin both haploid and diploid natural populations through a number of mechanisms,\nwe argue that inferences based on experiments in which adaptive walks proceed\nthrough succession of monomorphic states might miss many of the key features of\nadaptation.\n",
        "  We study the problem of consistent query answering under primary key\nviolations. In this setting, the relations in a database violate the key\nconstraints and we are interested in maximal subsets of the database that\nsatisfy the constraints, which we call repairs. For a boolean query Q, the\nproblem CERTAINTY(Q) asks whether every such repair satisfies the query or not;\nthe problem is known to be always in coNP for conjunctive queries. However,\nthere are queries for which it can be solved in polynomial time. It has been\nconjectured that there exists a dichotomy on the complexity of CERTAINTY(Q) for\nconjunctive queries: it is either in PTIME or coNP-complete. In this paper, we\nprove that the conjecture is indeed true for the case of conjunctive queries\nwithout self-joins, where each atom has as a key either a single attribute\n(simple key) or all attributes of the atom.\n",
        "  We study the relative importance of \"top-speed\" (long-term growth rate) and\n\"acceleration\" (how quickly the long-term growth rate can be reached) in the\nevolutionary race to increase population size. We observe that fitness alone\ndoes not capture growth rate: robustness, a property of neutral network shape,\ncombines with fitness to include the effect of deleterious mutations, giving\ngrowth rate. Similarly, we show that growth rate alone does not capture\npopulation size: regularity, a different property of neutral network shape,\ncombines with growth rate to include the effect of higher depletion rates early\non, giving size. Whereas robustness is a function of the principal eigenvalue\nof the neutral network adjacency matrix, regularity is a function of the\nprincipal eigenvector. We show that robustness is not correlated with\nregularity, and observe in silico the selection for regularity by evolving RNA\nribozymes. Despite having smaller growth rates, the more regular ribozymes have\nthe biggest populations.\n",
        "  To understand biological diversification, it is important to account for\nlarge-scale processes that affect the evolutionary history of groups of\nco-distributed populations of organisms. Such events predict temporally\nclustered divergences times, a pattern that can be estimated using genetic data\nfrom co-distributed species. I introduce a new approximate-Bayesian method for\ncomparative phylogeographical model-choice that estimates the temporal\ndistribution of divergences across taxa from multi-locus DNA sequence data. The\nmodel is an extension of that implemented in msBayes. By reparameterizing the\nmodel, introducing more flexible priors on demographic and divergence-time\nparameters, and implementing a non-parametric Dirichlet-process prior over\ndivergence models, I improved the robustness, accuracy, and power of the method\nfor estimating shared evolutionary history across taxa. The results demonstrate\nthe improved performance of the new method is due to (1) more appropriate\npriors on divergence-time and demographic parameters that avoid prohibitively\nsmall marginal likelihoods for models with more divergence events, and (2) the\nDirichlet-process providing a flexible prior on divergence histories that does\nnot strongly disfavor models with intermediate numbers of divergence events.\nThe new method yields more robust estimates of posterior uncertainty, and thus\ngreatly reduces the tendency to incorrectly estimate models of shared\nevolutionary history with strong support.\n",
        "  A selective sweep describes the reduction of diversity due to strong positive\nselection. If the mutation rate to a selectively beneficial allele is\nsufficiently high, Pennings and Hermisson (2006a) have shown, that it becomes\nlikely, that a selective sweep is caused by several individuals. Such an event\nis called a soft sweep and the complementary event of a single origin of the\nbeneficial allele, the classical case, a hard sweep. We give analytical\nexpressions for the linkage disequilibrium (LD) between two neutral loci linked\nto the selected locus, depending on the recurrent mutation to the beneficial\nallele, measured by $D$ and $\\hat{\\sigma_D^2}$, a quantity introduced by Ohta\nand Kimura (1969), and conclude that the LD-pattern of a soft sweep differs\nsubstantially from that of a hard sweep due to haplotype structure. We compare\nour results with simulations.\n",
        "  The rates of escape and reversion in response to selection pressure arising\nfrom the host immune system, notably the cytotoxic T-lymphocyte (CTL) response,\nare key factors determining the evolution of HIV. Existing methods for\nestimating these parameters from cross-sectional population data using ordinary\ndifferential equations (ODE) ignore information about the genealogy of sampled\nHIV sequences, which has the potential to cause systematic bias and\nover-estimate certainty. Here, we describe an integrated approach, validated\nthrough extensive simulations, which combines genealogical inference and\nepidemiological modelling, to estimate rates of CTL escape and reversion in HIV\nepitopes. We show that there is substantial uncertainty about rates of viral\nescape and reversion from cross-sectional data, which arises from the inherent\nstochasticity in the evolutionary process. By application to empirical data, we\nfind that point estimates of rates from a previously published ODE model and\nthe integrated approach presented here are often similar, but can also differ\nseveral-fold depending on the structure of the genealogy. The model-based\napproach we apply provides a framework for the statistical analysis of escape\nand reversion in population data and highlights the need for longitudinal and\ndenser cross-sectional sampling to enable accurate estimate of these key\nparameters.\n",
        "  To achieve magnetic resonance (MR)-only radiotherapy, a method needs to be\nemployed to estimate a synthetic CT (sCT) for generating electron density maps\nand patient positioning reference images. We investigated 2D and 3D\nconvolutional neural network (CNN) methods to generate a male pelvic sCT using\na T1-weighted MR image. A retrospective study was performed using CTs and\nT1-weighted MR images of 20 prostate cancer patients. The proposed 2D CNN\nmodel, which contained 27 convolutional layers, was modified from the SegNet\nfor better performance. 3D version of the CNN model was also developed. Both\nCNN models were trained from scratch to map intensities of T1-weighted MR\nimages to CT Hounsfield Unit (HU) values. Each sCT was generated in a\nfive-fold-cross-validation framework and compared with the corresponding CT\nusing voxel-wise mean absolute error (MAE), and dice similarity coefficient\n(DSC), recall, and precision for bony structures. Wilcoxon signed-rank tests\nwere performed to evaluate the differences between the both models. The MAE\naveraged across all patients were 40.5 $\\pm$ 5.4 HU and 37.6 $\\pm$ 5.1 HU for\nthe 2D and 3D CNN models, respectively. The DSC, recall, and precision of the\nbony structures were 0.81 $\\pm$ 0.04, 0.85 $\\pm$ 0.04, and 0.77 $\\pm$ 0.09 for\nthe 2D CNN model, and 0.82 $\\pm$ 0.04, 0.84 $\\pm$ 0.04, and 0.80 $\\pm$ 0.08 for\nthe 3D CNN model, respectively. P values of the Wilcoxon signed-rank tests were\nless than 0.05 except for recall, which was 0.6. The 2D and 3D CNN models\ngenerated accurate pelvic sCTs for the 20 patients using T1-weighted MR images.\nThe evaluation metrics and statistical tests indicated that the 3D model was\nable to generate sCTs with better MAE, bone DSC, and bone precision. The\naccuracy of the dose calculation and patient positioning using generated sCTs\nwill be tested and compared for the two models in the future.\n",
        "  In radiation therapy, mathematical methods have been used for optimizing\ntreatment planning for delivery of sufficient dose to the cancerous cells while\nkeeping the dose to critical surrounding structures minimal. This optimization\nproblem can be modeled using mixed integer programming (MIP) whose solution\ngives the optimal beam orientation as well as optimal beam intensity. The\nchallenge, however, is the computation time for this large scale MIP. We\npropose and investigate two novel heuristic approaches to reduce the\ncomputation time considerably while attaining high-quality solutions. We\nintroduce a family of heuristic cuts based on the concept of 'adjacent beams'\nand a beam elimination scheme based on the contribution of each beam to deliver\nthe dose to the tumor in the ideal plan in which all potential beams can be\nused simultaneously. We show the effectiveness of these heuristics for\nintensity modulated radiation therapy (IMRT) and stereotactic body radiation\ntherapy (SBRT) on a clinical liver case.\n",
        "  This paper makes a simple increment to state-of-the-art in sarcasm detection\nresearch. Existing approaches are unable to capture subtle forms of context\nincongruity which lies at the heart of sarcasm. We explore if prior work can be\nenhanced using semantic similarity/discordance between word embeddings. We\naugment word embedding-based features to four feature sets reported in the\npast. We also experiment with four types of word embeddings. We observe an\nimprovement in sarcasm detection, irrespective of the word embedding used or\nthe original feature set to which our features are augmented. For example, this\naugmentation results in an improvement in F-score of around 4\\% for three out\nof these four feature sets, and a minor degradation in case of the fourth, when\nWord2Vec embeddings are used. Finally, a comparison of the four embeddings\nshows that Word2Vec and dependency weight-based features outperform LSA and\nGloVe, in terms of their benefit to sarcasm detection.\n",
        "  An Extended Phase Graph framework for modelling systems with exchange or\nmagnetization transfer (MT) is proposed. The framework, referred to as EPG-X,\nmodels coupled two-compartment systems by describing each compartment with\nseparate phase graphs that exchange during evolution periods. There are two\nvariants: EPG-X(BM) for systems governed by the Bloch-McConnell equations; and\nEPG-X(MT) for the pulsed MT formalism. For the MT case the \"bound\" protons have\nno transverse components so their phase graph consists only longitudinal\nstates. EPG-X was used to model steady-state gradient echo imaging, MT effects\nin multislice Turbo Spin Echo imaging, multiecho CPMG for multicomponent T2\nrelaxometry and transient variable flip angle gradient echo imaging of the type\nused for MR Fingerprinting. Experimental data were also collected for the final\ncase. Steady-state predictions from EPG-X closely match directly derived\nsteady-state solutions which differ substantially from classic \"single pool\"\nEPG predictions. EPG-X(MT) predicts similar MT related levels of signal\nattenuation in white matter as have been reported elsewhere in the literature.\nModelling of CPMG echo trains with EPG-X(BM) suggests that exchange processes\ncan lead to an underestimate of the fraction of short T2 species. Modelling of\ntransient gradient echo sequences with EPG-X(MT) suggests that measurable MT\neffects result from variable saturation of bound protons, particularly after\ninversion pulses. In conclusion, EPG-X can be used for modelling of the\ntransient signal response of systems exhibiting chemical exchange or MT. This\nmay be particularly beneficial for relaxometry approaches that rely on\ncharacterising transient rather than steady-state sequences.\n",
        "  Broad emission lines in quasars enable us to \"resolve\" structure and\nkinematics of the broad line emitting region (BLR) thought to in- volve an\naccretion disk feeding a supermassive black hole. Interpretation of broad line\nmeasures within the 4DE1 formalism simplifies the apparent confusion among such\ndata by contrasting and unifying properties of so-called high and low accreting\nPopulation A and B sources. H{\\beta} serves as an estimator of black hole mass,\nEddington ratio and source rest frame, the latter a valuable input for\nCiv{\\lambda}1549 studies which allow us to isolate the blueshifted wind\ncomponent. Optical and HST-UV spectra yield H{\\beta} and Civ{\\lambda}1549\nspectra for low-luminosity sources while VLT-ISAAC and FORS and TNG-LRS provide\nspectra for high Luminosity sources. New high S/N data for Civ in\nhigh-luminosity quasars are presented here for comparison with the other\npreviously published data. Comparison of H{\\beta} and Civ{\\lambda}1549 profile\nwidths/shifts indicates that much of the emission from the two lines arise in\nregions with different structure and kinematics. Covering a wide range of\nluminosity and redshift shows evidence for a correlation between\nCiv{\\lambda}1549 blueshift and source Eddington ratio, with a weaker trend with\nsource luminosity (similar amplitude outflows are seen over 4 of the 5 dex\nluminosity range in our combined samples). At low luminosity (z < 0.7) only\nPopulation A sources show evidence for a significant outflow while at high\nluminosity the outflow signature begins to appear in Population B quasars as\nwell.\n",
        "  Broad Absorption Line Regions - BALR are composed of a number of successive\nindependent absorbing density layers. Using the GR model, we analyze the UV Si\nIV ({\\lambda}{\\lambda}1393.755 - 1402.770), O IV ({\\lambda}1401.156) and C IV\n({\\lambda}{\\lambda}1548.187 - 1550.772) resonance lines in the spectra of a\ncertain QSO and discuss the results concerning its kinematic properties\n(rotational, radial and random velocities).\n",
        "  A Seifert surgery is a pair (K, m) of a knot K in the 3-sphere and an integer\nm such that m-Dehn surgery on K results in a Seifert fiber space allowed to\ncontain fibers of index zero. Twisting K along a trivial knot called a\nseiferter for (K, m) yields Seifert surgeries. We study Seifert surgeries\nobtained from those on a trefoil knot by twisting along their seiferters.\nAlthough Seifert surgeries on a trefoil knot are the most basic ones, this\nfamily is rich in variety. For any m which is not -2 it contains a successive\ntriple of Seifert surgeries (K, m), (K, m +1), (K, m +2) on a hyperbolic knot\nK, e.g. 17-, 18-, 19-surgeries on the (-2, 3, 7) pretzel knot. It contains\ninfinitely many Seifert surgeries on strongly invertible hyperbolic knots none\nof which arises from the primitive/Seifert-fibered construction, e.g.\n(-1)-surgery on the (3, -3, -3) pretzel knot.\n",
        "  In this article, we propose an approach to breeding which focuses on mating\ninstead of truncation selection, our method uses genome-wide marker information\nin a similar fashion to genomic selection so we refer it to as genomic mating.\nUsing concepts of estimated breeding values, risk (usefulness) and inbreeding,\nan efficient mating approach is formulated for improvement of breeding values\nin the long run. We have used a genetic algorithm to find solutions to this\noptimization problem. Results from our simulations point to the efficiency of\ngenomic mating for breeding complex traits compared to truncation selection.\n",
        "  Inelastic neutron scattering experiments were performed to investigate the\ncrystalline electric field (CEF) excitations of Nd3+ (J = 9/2) in the iron\npnictide NdFeAsO. The crystal field level structures for both the\nhigh-temperature paramagnetic phase and the low-temperature antiferromagnetic\nphase of NdFeAsO are constructed. The variation of CEF excitations of Nd3+\nreflects not only the change of local symmetry but also the change of magnetic\nordered state of the Fe sublattice. By analyzing the crystal field interaction\nwith a crystal field Hamiltonian, the crystal field parameters are obtained. It\nwas found that the sign of the fourth and sixth-order crystal field parameters\nchange upon the magnetic phase transition at 140 K, which may be due to the\nvariation of exchange interactions between the 4f and conduction electrons.\n",
        "  Purpose: To evaluate the latent variance (LV) of TrueBeam photon phase-space\nfiles (PSF) for open 10x10$cm^2$ and small fields.\n  Methods: BEAMnrc/DOSXYZnrc was used to transport particles from Varian\nphase-space files (PSFA) through the secondary collimators. Transported\nparticles were scored into another phase-space located under the jaws (PSFB),\nor transported further through the cone collimators and scored straight below,\nforming PSFC. PFSB were scored for 6MV-FFF, 6MV, 10MV-FFF, 10MV and 15MV beams\nwith 10x10$cm^2$ field size, and PSFC were scored for 6MV beam under cones of\n0.13, 0.25, 0.35, 1.0, 1.2, 1.5 and 4cm diameter. PSFB and PSFC were\ntransported into a water phantom with particle recycling number ranging from 10\nto 1000. For 10x10$cm^2$ fields 0.5x0.5x0.5cm^3 voxels were used to score the\ndose, whereas the dose was scored in 0.1x0.1x0.5cm^3 voxels for beams\ncollimated with small cones. For small 0.25cm diameter cone-collimated 6MV\nbeam, phantom voxel size varied as 0.02x0.02x0.5cm^3, 0.05x0.05x0.5cm^3 and\n0.1x0.1x0.5cm^3. Dose variances were scored in all cases and LV evaluated as\nper Sempau et al.\n  Results: For the 10x10cm^2 fields calculated LVs were greatest at the phantom\nsurface and decreased with depth until reached a plateau at 5cm depth. LVs were\n0.54%, 0.96%, 0.35%, 0.69% and 0.57% for the 6MV-FFF, 6MV, 10MV-FFF, 10MV and\n15MV energies, respectively at 10cm depth. For the 6 MV phase-space collimated\nwith cones of 0.13, 0.25, 0.35, 1.0cm diameter, the LVs calculated at 1.5cm\ndepth were 75.6%, 25.4%, 17.6% and 8.0% respectively. Calculated LV for the\n0.25cm cone-collimated 6MV beam were 61.2%, 40.7%, 22.5% in 0.02x0.02x0.5cm^3,\n0.05x0.05x0.5cm^3 and 0.1x0.1x0.5cm^3 voxels respectively.\n  Conclusion: Single PSF can be used to achieve sub-percent latent variance in\nopen 10x10cm^2 field MC simulations, whereas more PSFs would have to be summed\nfor small SRS fields.\n",
        "  The transmission dynamics of Tuberculosis (TB) involve complex\nepidemiological and socio-economical interactions between individuals living in\nhighly distinct regional conditions. The level of exogenous reinfection and\nfirst time infection rates within high-incidence settings may influence the\nimpact of control programs on TB prevalence. This study aims at enhancing the\nunderstanding of TB dynamics via the study of scenarios, within {\\it\nsimplified}, two patch, risk-defined environments, in the presence of short\nterm mobility and variations in reinfection and infection rates. The modeling\nframework captures the role of individuals' `daily' dynamics within and between\nplaces of residency, work or business via the proportion of time spent in\nresidence and as visitors to TB-risk environments (patches). As a result, the\n{\\it effective population size} of Patch $i$ (home of $i$-residents) at time\n$t$ must account for visitors and residents of Patch $i$, at time $t$. The\nimpact that {\\it effective population size} and the distribution of {\\it\nindividuals' residence times} in different patches have on TB transmission and\ncontrol are studied using selected scenarios where risk is defined by the\nestimated or perceive first time infection and/or exogenous re-infection rates.\nOur results suggest that, under certain conditions, allowing infected\nindividuals to move from high to low TB prevalence areas (for example via the\nsharing of treatment and isolation facilities) may lead to a reduction in the\ntotal TB prevalence in the overall, here two-patch, population.\n",
        "  Relative biological effectiveness (RBE) plays an important role in designing\na uniform dose response for ion beam therapy. In this study the biological\neffectiveness of a carbon ion beam delivery system was investigated using Monte\nCarlo simulation. A carbon ion beam delivery line was designed for the Korea\nHeavy Ion Medical Accelerator (KHIMA) project. The GEANT4 simulation tool kit\nwas used to simulate carbon beam transporting into media. An incident energy\ncarbon ion beam in the range between 220 MeV/u and 290 MeV/u was chosen to\ngenerate secondary particles. The microdosimetric-kinetic (MK) model is applied\nto describe the RBE of 10% survival in human salivary gland (HSG) cells. The\nRBE weighted dose was estimated as a function of the penetrating depth of the\nwater phantom along the incident beam direction. A biologically\nphoton-equivalent Spread Out Bragg Peak (SOBP) was designed using the RBE\nweighted absorbed dose. Finally, the RBE of mixed beams was predicted as a\nfunction of the water phantom depth.\n",
        "  There is a large observational scatter toward low velocities in the stellar\nmass Tully-Fisher relation if disturbed and compact objects are included.\nHowever, this scatter can be eliminated if one replaces rotation velocity with\n$\\rm S_{\\rm 0.5}$, a quantity that includes a velocity dispersion term added in\nquadrature with the rotation velocity. In this work we use a large suite of\nhydrodynamic N-body galaxy merger simulations to explore a possible mechanism\nfor creating the observed relations. Using mock observations of the\nsimulations, we test for the presence of observational effects and explore the\nrelationship between $\\rm S_{\\rm 0.5}$ and intrinsic properties of the\ngalaxies. We find that galaxy mergers can explain the scatter in the TF as well\nas the tight $\\rm S_{\\rm 0.5}$-stellar mass relation. Furthermore, $\\rm S_{\\rm\n0.5}$ is correlated with the total central mass of a galaxy, including\ncontributions due to dark matter.\n",
        "  In this paper we present RTMML, a markup language for the tenses of verbs and\ntemporal relations between verbs. There is a richness to tense in language that\nis not fully captured by existing temporal annotation schemata. Following\nReichenbach we present an analysis of tense in terms of abstract time points,\nwith the aim of supporting automated processing of tense and temporal relations\nin language. This allows for precise reasoning about tense in documents, and\nthe deduction of temporal relations between the times and verbal events in a\ndiscourse. We define the syntax of RTMML, and demonstrate the markup in a range\nof situations.\n",
        "  The emergence of new higher education institutions has created the\ncompetition in higher education market, and data warehouse can be used as an\neffective technology tools for increasing competitiveness in the higher\neducation market. Data warehouse produce reliable reports for the institution's\nhigh-level management in short time for faster and better decision making, not\nonly on increasing the admission number of students, but also on the\npossibility to find extraordinary, unconventional funds for the institution.\nEfficiency comparison was based on length and amount of processed records,\ntotal processed byte, amount of processed tables, time to run query and\nproduced record on OLTP database and data warehouse. Efficiency percentages was\nmeasured by the formula for percentage increasing and the average efficiency\npercentage of 461.801,04% shows that using data warehouse is more powerful and\nefficient rather than using OLTP database. Data warehouse was modeled based on\nhypercube which is created by limited high demand reports which usually used by\nhigh level management. In every table of fact and dimension fields will be\ninserted which represent the loading constructive merge where the ETL\n(Extraction, Transformation and Loading) process is run based on the old and\nnew files.\n",
        "  The study of the iron-based superconductor FeSe has blossomed with the\navailability of high quality single crystals, obtained through\nflux/vapor-transport growth techniques below the structural transformation\ntemperature of its tetragonal phase, T~450 C. Here, we report on the variation\nof sample morphology and properties due to small modifications in the growth\nconditions. A considerable variation of the superconducting transition\ntemperature Tc, from 8.8 K to 3 K, which cannot be correlated with the sample\ncomposition, is observed. Instead, we point out a clear correlation between Tc\nand disorder, as measured by the residual resistivity ratio. Notably, the\ntetragonal-to-orthorhombic structural transition is also found to be quite\nstrongly disorder dependent (Ts =72 - 90 K), and linearly correlated with Tc.\n",
        "  We present medium-resolution, near-ultraviolet VLT/FLAMES observations of the\nstar USNO-A0600-15865535. We adapt a standard method of stellar typing to our\nmeasurement of the shape of the Balmer epsilon absorption line to demonstrates\nthat USNO-A0600-15865535 is a blue horizontal branch star, residing in the\nlower stellar halo at a distance of 4.4 kpc from the Sun. We measure the H & K\nlines of singly-ionized calcium and find two isolated velocity components, one\noriginating in the disk, and one associated with high-velocity cloud complex\nWD. This detection demonstrated that complex WD is closer than ~4.4 kpc and is\nthe first distance constraint on the +100 km/s Galactic complex of clouds. We\nfind that Complex WD is not in corotation with the Galactic disk as has been\nassumed for decades. We examine a number of scenarios, and find that the most\nlikely is that Complex WD was ejected from the solar neighborhood and is only a\nfew kpc from the Sun.\n",
        "  We prove the existence of lattice isomorphic line arrangements having\n$\\pi_1$-equivalent or homotopy-equivalent complements and non homeomorphic\nembeddings in the complex projective plane. We also provide two explicit\nexamples, one is formed by real-complexified arrangements while the second is\nnot.\n",
        "  Entity linking has recently been the subject of a significant body of\nresearch. Currently, the best performing approaches rely on trained\nmono-lingual models. Porting these approaches to other languages is\nconsequently a difficult endeavor as it requires corresponding training data\nand retraining of the models. We address this drawback by presenting a novel\nmultilingual, knowledge-based agnostic and deterministic approach to entity\nlinking, dubbed MAG. MAG is based on a combination of context-based retrieval\non structured knowledge bases and graph algorithms. We evaluate MAG on 23 data\nsets and in 7 languages. Our results show that the best approach trained on\nEnglish datasets (PBOH) achieves a micro F-measure that is up to 4 times worse\non datasets in other languages. MAG, on the other hand, achieves\nstate-of-the-art performance on English datasets and reaches a micro F-measure\nthat is up to 0.6 higher than that of PBOH on non-English languages.\n",
        "  Several messages express opinions about events, products, and services,\npolitical views or even their author's emotional state and mood. Sentiment\nanalysis has been used in several applications including analysis of the\nrepercussions of events in social networks, analysis of opinions about products\nand services, and simply to better understand aspects of social communication\nin Online Social Networks (OSNs). There are multiple methods for measuring\nsentiments, including lexical-based approaches and supervised machine learning\nmethods. Despite the wide use and popularity of some methods, it is unclear\nwhich method is better for identifying the polarity (i.e., positive or\nnegative) of a message as the current literature does not provide a method of\ncomparison among existing methods. Such a comparison is crucial for\nunderstanding the potential limitations, advantages, and disadvantages of\npopular methods in analyzing the content of OSNs messages. Our study aims at\nfilling this gap by presenting comparisons of eight popular sentiment analysis\nmethods in terms of coverage (i.e., the fraction of messages whose sentiment is\nidentified) and agreement (i.e., the fraction of identified sentiments that are\nin tune with ground truth). We develop a new method that combines existing\napproaches, providing the best coverage results and competitive agreement. We\nalso present a free Web service called iFeel, which provides an open API for\naccessing and comparing results across different sentiment methods for a given\ntext.\n",
        "  By stacking publicly available deep Spitzer/MIPS 24 $\\mu$m and Herschel/PACS\nimages for 213 $z \\simeq 2.18$ Ly$\\alpha$ Emitters (LAEs) in GOODS-South, we\nobtain a strong upper limit to the IR luminosity of typical LAEs and discuss\ntheir attenuation curve for the first time. The $3\\sigma$ upper limit $L_{\\rm\nTIR}^{3\\sigma}= 1.1 \\times 10^{10} L_\\odot$, determined from the MIPS data\nproviding the lowest limit, gives $IRX \\equiv L_{\\rm TIR}/L_{\\rm UV} \\leq 2.2$.\nHere we assume that the local calibration between the 8 $\\mu$m emission and the\ndust SED shape and metallicity applies at high redshifts and that our LAEs have\nlow metallicities as suggested by previous studies. The inferred escape\nfractions of Ly$\\alpha$, $16$--$37%$, and UV continuum, $\\ge 44%$, are higher\nthan the cosmic averages at the same epoch. The SMC attenuation curve is\nconsistent with the IRX and the UV slope $\\beta = -1.4^{+0.2}_{-0.2}$ of our\nstacked LAE, while the Meurer's relation (Calzetti curve) predicts a 3.8 times\nhigher IRX; we also discuss the validity of PACS-based $L_{\\rm TIR}^{3\\sigma}$\nallowing the Meurer's relation. SED fitting using the Calzetti curve also gives\na $\\sim 10$ times higher SFR than from the $L_{\\rm TIR}^{3\\sigma}$ and $L_{\\rm\nUV}$. With $M_{\\star}=6.3^{+0.8}_{-2.0} \\times10^8 \\mathrm{M_{\\odot}}$, our\nLAEs lie on a lower-mass extrapolation of the star formation main sequence at\n$z \\sim 2$, suggesting that the majority of $z \\sim 2$ LAEs are mildly star\nforming with relatively old ages of $\\sim 200$ Myr. The faint $L_{\\rm\nTIR}^{3\\sigma}$ implies that LAEs contribute little to the faint ($\\gtrsim 100\n\\mu$Jy) submm number counts by ALMA.\n",
        "  The BVRcIc CCD photometry in the fields of six open clusters toward the\nPerseus spiral arm is presented. These data along with JHKs magnitudes taken\nfrom 2MASS catalog, have been used to determine cluster's ages, distances and\ncolor excesses. In addition, the gaps in mass function of Be 97, King 12 and\nNGC 7788 clusters have been revealed in mass intervals of 1.3-1.5, 1.4-1.6 and\n1.5-1.7 solar masses, respectively.\n",
        "  Sequencing of pools of individuals (Pool-Seq) represents a reliable and cost-\neffective approach for estimating genome-wide SNP and transposable element\ninsertion frequencies. However, Pool-Seq does not provide direct information on\nhaplotypes so that for example obtaining inversion frequencies has not been\npossible until now. Here, we have developed a new set of diagnostic marker SNPs\nfor 7 cosmopolitan inversions in Drosophila melanogaster that can be used to\ninfer inversion frequencies from Pool-Seq data. We applied our novel marker set\nto Pool-Seq data from an experimental evolution study and from North American\nand Australian latitudinal clines. In the experimental evolution data, we find\nevidence that positive selection has driven the frequencies of In(3R)C and\nIn(3R)Mo to increase over time. In the clinal data, we confirm the existence of\nfrequency clines for In(2L)t, In(3L)P and In(3R)Payne in both North America and\nAustralia and detect a previously unknown latitudinal cline for In(3R)Mo in\nNorth America. The inversion markers developed here provide a versatile and\nrobust tool for characterizing inversion frequencies and their dynamics in\nPool- Seq data from diverse D. melanogaster populations.\n",
        "  In this paper, we provide a polynomial time algorithm to calculate the\nprobability of a {\\it ranked} gene tree topology for a given species tree,\nwhere a ranked tree topology is a tree topology with the internal vertices\nbeing ordered. The probability of a gene tree topology can thus be calculated\nin polynomial time if the number of orderings of the internal vertices is a\npolynomial number. However, the complexity of calculating the probability of a\ngene tree topology with an exponential number of rankings for a given species\ntree remains unknown.\n",
        "  Transition metal dichalcogenides (TMDs) are quasi-two-dimensional layered\ncompounds exhibiting strongly competing charge-density wave (CDW) and\nsuperconducting (SC) order parameters (OPs). The weak van der Waals interlayer\nbonding between hexagonal layers of octahedral or trigonal prismatic TMD\nbuilding blocks allows for many polytypes. The non-superconducting $1T$\npolytypes can have one or more CDWs. The $2H$ polytypes have two or more Fermi\nsurfaces and saddle bands, allowing for dual orderings, which can be coexisting\nCDW and SC orderings, two SC gaps as in MgB$_2$, or two CDW gaps. The CDW\ntransitions $T_{\\rm CDW}$s usually greatly exceed the low superconducting\n$T_{\\rm c}$s, their orbital OPs are generally highly anisotropic and can even\ncontain nodes, are remarkably similar to the the high-$T_{\\rm c}$ cuprate\npseudogaps, and the SC OPs can be greatly affected by their presence. In\n2$H$-NbSe$_2$, the CDW renders its general $s$-wave SC OP orbital symmetry to\nbe highly anisotropic and strongly reduces its Josephson coupling strength\n($I_{\\rm c}R_{\\rm n}$) with Pb. Pressure and intercalation generally suppress\nthe CDWs, enhancing $T_{\\rm c}$. The misfit intercalation compound\n(LaSe)$_{1.14}$(NbSe$_2$) and many intercalated $2H$-TMDs, such as\nTaS$_2$(pyridine)$_{1/2}$, have completely incoherent $c$-axis transport,\ndimensional-crossover effects, and behave as stacks of intrinsic Josephson\njunctions. Except for the anomalously large violation of the Pauli limit of the\nupper critical field of (LaSe)$_{1.14}$(NbSe$_2$), these properties are very\nsimilar to those of the cuprate Bi$_2$Sr$_2$CaCu$_2$O$_{8+\\delta}$ and of the\norganic layered superconductor, $\\kappa$-(ET)$_2$Cu[N(CN)$_2$]Br. Intercalates\nof TMDs with water and metallic ions are very similar to Na$_x$CoO$_2\\cdot\ny$H$_2$O.\n",
        "  We use a sample of 4178 Lyman break galaxies (LBGs) at z = 3, 4 and 5 in the\nUKIRT Infrared Deep Sky Survey (UKIDSS) Ultra Deep Survey (UDS) field to\ninvestigate the relationship between the observed slope of the stellar\ncontinuum emission in the ultraviolet, {\\beta}, and the thermal dust emission,\nas quantified via the so-called 'infrared excess' (IRX = LIR/LUV). Through a\nstacking analysis we directly measure the 850-{\\mu}m flux density of LBGs in\nour deep (0.9mJy) James Clerk Maxwell Telescope (JCMT) SCUBA-2 850-{\\mu}m map,\nas well as deep public Herschel/SPIRE 250-, 350- and 500-{\\mu}m imaging. We\nestablish functional forms for the IRX-{\\beta} relation to z ~ 5, confirming\nthat there is no significant redshift evolution of the relation and that the\nresulting average IRX-{\\beta} curve is consistent with a Calzetti-like\nattenuation law. We compare our results with recent work in the literature,\nfinding that discrepancies in the slope of the IRX-{\\beta} relation are driven\nby biases in the methodology used to determine the ultraviolet slopes.\nConsistent results are found when IRX-{\\beta} is evaluated by stacking in bins\nof stellar mass, M, and we argue that the near-linear IRX-M relationship is a\nbetter proxy for correcting observed UV luminosities to total star formation\nrates, provided an accurate handle on M can be had, and also gives clues as to\nthe physical driver of the role of dust-obscured star formation in\nhigh-redshift galaxies.\n",
        "  We investigate the spectral properties of the UV ($\\lambda\\lambda$2650-3050\n\\AA) and optical ($\\lambda\\lambda$4000-5500 \\AA) Fe II emission features in a\nsample of 293 type 1 active galactic nuclei (AGNs) from Sloan Digital Sky\nSurvey (SDSS) database. We explore different correlations between their\nemission line properties, as well as the correlations with the other emission\nlines from the spectral range. We find several interesting correlations and we\ncan outline the most interesting results as follows. (i) There is a kinematical\nconnection between the UV and optical Fe II lines, indicating that the UV and\noptical Fe II lines originate from the outer part of the broad line region,\nso-called intermediate line region; (ii) The unexplained anticorrelations of\nthe optical Fe II (EW Fe II$_{opt}$) versus EW [O III] 5007 \\AA\\ and EW Fe\nII$_{opt}$ versus FWHM Hbeta have not been detected for the UV Fe II lines;\n(iii) The significant averaged redshift in the UV Fe II lines, which is not\npresent in optical Fe II, indicates an inflow in the UV Fe II emitting clouds,\nand probably their asymmetric distribution. (iv) Also, we confirm the\nanticorrelation between the intensity ratio of the optical and UV Fe II lines\nand FWHM of Hbeta, and we find the anticorrelations of this ratio with the\nwidths of Mg II 2800 \\AA, optical Fe II and UV Fe II. This indicates a very\nimportant role for the column density and microturbulence in the emitting gas.\nWe discuss the starburst activity in high-density regions of young AGNs as a\npossible explanation of the detected optical Fe II correlations and intensity\nline ratios of the UV and optical Fe II lines.\n",
        "  We present a muon spin rotation (\\muSR) study of the magnetic and\nsuperconducting properties of single crystals of electron-doped BaFe2-xCoxAs2\nwith x=0.08, 0.20, and 0.25 (Tc=9, 25 and 20K) and of polycrystalline\nhole-doped Pr1-xSrxFeAsO with x=0 and 0.2 (Tc=15 K). In the former series we\nobserve some interesting parallels with the electron doped SmFeAsO1-xFx\n1111-type system [A.J. Drew et al., to appear in Nature Materials 2009 and\narXiv:0807.4876]. In particular, we obtain evidence that strongly disordered\nstatic magnetism coexists with superconductivity on a microscopic scale in\nunderdoped samples and even at optimum doping there is a slowing down (or\nenhancement) of dynamic magnetic correlations below Tc\\approx25K. To the\ncontrary, for the hole-doped Pr1-xSrxFeAsO samples we obtain evidence for a\nmesoscopic phase segregation into regions with nearly unperturbed AF order and\nothers that are non magnetic and most likely superconducting. The observed\ntrend resembles the one that was previously reported for hole-doped\nBa1-xKxFe2As2 [A.A. Aczel et al., Phys. Rev. B 78, 214503 (2008); J.T. Park et\nal., arXiv:0811.2224] and thus seems to be fairly common in these hole doped\nsystems.\n",
        "  We report an evaluation of the effectiveness of the existing knowledge base\nembedding models for relation prediction and for relation extraction on a wide\nrange of benchmarks. We also describe a new benchmark, which is much larger and\ncomplex than previous ones, which we introduce to help validate the\neffectiveness of both tasks. The results demonstrate that knowledge base\nembedding models are generally effective for relation prediction but unable to\ngive improvements for the state-of-art neural relation extraction model with\nthe existing strategies, while pointing limitations of existing methods.\n",
        "  The standard interpretation of the phase diagram of type-II superconductors\nwas developed in 1960s and has since been considered a well-established part of\nclassical superconductivity. However, upon closer examination a number of\nfundamental issues arise that leads one to question this standard picture. To\naddress these issues we studied equilibrium properties of niobium samples near\nand above the upper critical field Hc2 in parallel and perpendicular magnetic\nfields. The samples investigated were very high quality films and single\ncrystal discs with the Ginzburg-Landau parameters 0.8 and 1.3, respectively. A\nrange of complementary measurements have been performed, which include dc\nmagnetometry, electrical transport, muSR spectroscopy and scanning Hall-probe\nmicroscopy. Contrarily to the standard scenario, we observed that a\nsuperconducting phase is present in the sample bulk above Hc2 and the field Hc3\nis the same in both parallel and perpendicular fields. Our findings suggest\nthat above Hc2 the superconducting phase forms filaments parallel to the field\nregardless on the field orientation. Near Hc2 the filaments preserve the\nhexagonal structure of the preceding vortex lattice of the mixed state and the\nfilament density continuously falls to zero at Hc3. Our work has important\nimplications for the correct interpretation of properties of type-II\nsuperconductors and can also be essential for practical applications of these\nmaterials.\n",
        "  A knot is an an embedding of a circle into three-dimensional space. We say\nthat a knot is unknotted if there is an ambient isotopy of the embedding to a\nstandard circle. By representing knots via planar diagrams, we discuss the\nproblem of unknotting a knot diagram when we know that it is unknotted. This\nproblem is surprisingly difficult, since it has been shown that knot diagrams\nmay need to be made more complicated before they may be simplified. We do not\nyet know, however, how much more complicated they must get. We give an\nintroduction to the work of Dynnikov who discovered the key use of\narc--presentations to solve the problem of finding a way to detect the unknot\ndirectly from a diagram of the knot. Using Dynnikov's work, we show how to\nobtain a quadratic upper bound for the number of crossings that must be\nintroduced into a sequence of unknotting moves. We also apply Dynnikov's\nresults to find an upper bound for the number of moves required in an\nunknotting sequence.\n",
        "  We report neutron scattering data and DFT calculations of the stoichiometric\niron-arsenide superconductor Sr2VO3FeAs. Rietveld refinements of neutron powder\npatterns confirm the ideal composition without oxygen deficiencies. Experiments\nwith polarized neutrons prove weak magnetic ordering in the V-sublattice of\nSr2VO3FeAs at ~ 45 K with a probable propagation vector q = (1/8,1/8,0). The\nordered moment of ~ 0.1 muB is too small to remove the V-3d bands from the\nFermi level by magnetic exchange splitting, and much smaller than predicted\nfrom a recent LDA+U study. By using DFT calculations with a GGA+EECE functional\nwe recover the typical quasi-nested Fermi-surface even without magnetic moment.\nFrom this we suggest that the V-atoms are in a Mott-state where the electronic\ncorrelations are dominated by on-site Coulomb-repulsion which shifts the V-3d\nstates away from the Fermi energy. Our results are consistent with\nphotoemission data and clearly reveal that Sr2VO3FeAs is a typical\niron-arsenide superconductor with quasi-nested hole- and electron-like Fermi\nsurface sheets, and constitutes no new paradigm. We suggest that intrinsic\nelectron-doping through V3+/V4+ mixed valence is responsible for the absence of\nSDW ordering.\n",
        "  An ab-initio numerical study of the density-dependent, evolutionary stable\ndispersal strategy is presented. The simulations are based on a simple\ndiscretei generation island model with four processes: reproduction, dispersal,\ncompetition and local catastrophe. We do not impose any a priori constraints on\nthe dispersal schedule, allowing the entire schedule to evolve. We find that\nthe system converges at long times to a unique nontrivial dispersal schedule\nsuch that the dispersal probability is a monotonically increasing function of\nthe density. We have explored the dependence of the selected dispersal strategy\non the various system parameters: mean number of offspring, site carrying\ncapacity, dispersal cost and system size. A few general scaling laws are seen\nto emerge from the data.\n",
        "  We study the tunneling conductance of a ballistic normal metal / ferromagnet\n/ spin-triplet superconductor junction using the extended\nBlonder-Tinkham-Klapwijk formalism as a model for a $c$-axis oriented Au /\nSrRuO$_{3}$ / Sr$_{2}$RuO$_{4}$ junction. We compare chiral $p$-wave (CPW) and\nhelical $p$-wave (HPW) pair potentials, combined with ferromagnet magnetization\ndirections parallel and perpendicular to the interface. For fixed $\\theta_{M}$,\nwhere $\\theta_{M}$ is a direction of magnetization in the ferromagnet measured\nfrom the $c$-axis, the tunneling conductance of CPW and HPW clearly show\ndifferent voltage dependencies. It is found that the cases where the $d$-vector\nis perpendicular to the magnetization direction (CPW with $\\theta_{M} = \\pi/2$\nand HPW with $\\theta_{M} = 0$) are identical. The obtained results serve as a\nguide to determine the pairing symmetry of the spin-triplet superconductor\nSr$_{2}$RuO$_{4}$.\n",
        "  Purpose: To develop a rapid imaging framework for balanced steady-state free\nprecession (bSSFP) that jointly reconstructs undersampled data (by a factor of\nR) across multiple coils (D) and multiple acquisitions (N). To devise a\nmulti-acquisition coil compression technique for improved computational\nefficiency.\n  Methods: The bSSFP image for a given coil and acquisition is modeled to be\nmodulated by a coil sensitivity and a bSSFP profile. The proposed\nreconstruction by calibration over tensors (ReCat) recovers missing data by\ntensor interpolation over the coil and acquisition dimensions. Coil compression\nis achieved using a new method based on multilinear singular value\ndecomposition (MLCC). ReCat is compared with iterative self-consistent parallel\nimaging (SPIRiT) and profile encoding (PE-SSFP) reconstructions.\n  Results: Compared to parallel imaging or profile-encoding methods, ReCat\nattains sensitive depiction of high-spatial-frequency information even at\nhigher R. In the brain, ReCat improves peak SNR (PSNR) by 1.1$\\pm$1.0 dB over\nSPIRiT and by 0.9$\\pm$0.3 dB over PE-SSFP (mean$\\pm$std across subjects;\naverage for N=2-8, R=8-16). Furthermore, reconstructions based on MLCC achieve\n0.8$\\pm$0.6 dB higher PSNR compared to those based on geometric coil\ncompression (GCC) (average for N=2-8, R=4-16).\n  Conclusion: ReCat is a promising acceleration framework for\nbanding-artifact-free bSSFP imaging with high image quality; and MLCC offers\nimproved computational efficiency for tensor-based reconstructions.\n",
        "  In modern computing, RDBMS are great to store different types of data. To a\ndeveloper, one of the major objectives is to provide a very low cost and easy\nto use solution to an existing problem. While commercial databases are more\neasy to use along with their new as well as documented features come with\ncomplicated licensing cost, free open source databases are not that\nstraightforward under many situations. This paper shows how a completely free\nadvanced open source RDBMS like PostgreSQL could be designed and modified to\nstore and retrieve high quality images in order to use them along with a\nfrontend application.\n",
        "  We have performed magnetic susceptibility measurements in Mo$_{(1-x)}$Ge$_x$\namorphous thin films biased with an electrical current using anisotropic coils.\nWe tested the symmetry of the vortex response changing the relative orientation\nbetween the bias current and the susceptibility coils. We found a region in the\nDC current - temperature phase diagram where the dynamical vortex structures\nbehave anisotropically. In this region the shielding capability of the\nsuperconducting currents measured by the susceptibility coils is less effective\nalong the direction of vortex motion compared to the transverse direction. This\nanisotropic response is found in the same region where the peak effect in the\ncritical current is developed. On rising temperature the isotropic behavior is\nrecovered.\n",
        "  A model for beam customization with collimators and a range-compensating\nfilter based on the phase-space theory for beam transport is presented for dose\ndistribution calculation in treatment planning of radiotherapy with protons and\nheavier ions. Independent handling of pencil beams in conventional pencil-beam\nalgorithms causes unphysical collimator-height dependence in the middle of\nlarge fields, which is resolved by the framework comprised of generation,\ntransport, collimation, regeneration, range-compensation, and edge-sharpening\nprocesses with a matrix of pencil beams. The model was verified to be\nconsistent with measurement and analytic estimation at a submillimeter level in\npenumbra of individual collimators with a combinational-collimated carbon-ion\nbeam. The model computation is fast, accurate, and readily applicable to\npencil-beam algorithms in treatment planning with capability of combinational\ncollimation to make best use of the beam-customization devices.\n",
        "  This article reviews quantitative methods to estimate the basic reproduction\nnumber of pandemic influenza, a key threshold quantity to help determine the\nintensity of interventions required to control the disease. Although it is\ndifficult to assess the transmission potential of a probable future pandemic,\nhistorical epidemiologic data is readily available from previous pandemics, and\nas a reference quantity for future pandemic planning, mathematical and\nstatistical analyses of historical data are crucial. In particular, because\nmany historical records tend to document only the temporal distribution of\ncases or deaths (i.e. epidemic curve), our review focuses on methods to\nmaximize the utility of time-evolution data and to clarify the detailed\nmechanisms of the spread of influenza. First, we highlight structured epidemic\nmodels and their parameter estimation method which can quantify the detailed\ndisease dynamics including those we cannot observe directly.\nDuration-structured epidemic systems are subsequently presented, offering firm\nunderstanding of the definition of the basic and effective reproduction\nnumbers. When the initial growth phase of an epidemic is investigated, the\ndistribution of the generation time is key statistical information to\nappropriately estimate the transmission potential using the intrinsic growth\nrate. Applications of stochastic processes are also highlighted to estimate the\ntransmission potential using the similar data. Critically important\ncharacteristics of influenza data are subsequently summarized, followed by our\nconclusions to suggest potential future methodological improvements.\n",
        "  Recently proposed data collection frameworks for endangered language\ndocumentation aim not only to collect speech in the language of interest, but\nalso to collect translations into a high-resource language that will render the\ncollected resource interpretable. We focus on this scenario and explore whether\nwe can improve transcription quality under these extremely low-resource\nsettings with the assistance of text translations. We present a neural\nmulti-source model and evaluate several variations of it on three low-resource\ndatasets. We find that our multi-source model with shared attention outperforms\nthe baselines, reducing transcription character error rate by up to 12.3%.\n",
        "  We consider a model for superconductivity in a two-band superconductor,\nhaving an anisotropic electronic structure made of two partially overlapping\nbands with a first hole-like and a second electron-like fermi surface. In this\npairing scenario, driven by the interplay between interband $V_{i,j}$ and\nintraband $V_{i,i}$ pairing terms, we have solved the two gap equations at the\ncritical temperature $T = T_c$ and calculate $T_c$ and the chemical potential\n$\\mu$ as a function of the number of carriers $n$ for various values of pairing\ninteractions, $V_{1,1}$, $V_{2,2}$, and $V_{1,2}$. The results show the\ncomplexity of the physics of condensates with multiple order parameters with\nthe chemical potential near band edges.\n",
        "  The maximum current (critical current) a type-II superconductor can transmit\nwithout energy loss is limited by the motion of the quantized magnetic flux\npenetrating into a superconductor. Introducing nanoscale holes into a\nsuperconducting film has been long pursued as a promising way to increase the\ncritical current. So far the critical current enhancement was found to be\nmostly limited to low magnetic fields. Here we experimentally investigate the\ncritical currents of superconducting films with a conformal array of nanoscale\nholes that have non-uniform density while preserving the local ordering. We\nfind that the conformal array of nanoscle holes provides a more significant\ncritical current enhancement at high magnetic fields. The better performance\ncan be attributed to its arching effect that not only gives rise to the\ngradient in hole-density for pinning vortices with a wide range of densities\nbut also prevent vortex channeling occurring in samples with a regular lattice\nof holes.\n",
        "  A filling Dehn sphere $\\Sigma$ in a closed 3-manifold $M$ is a sphere\ntransversely immersed in $M$ that defines a cell decomposition of $M$. Every\nclosed 3-manifold has a filling Dehn sphere. The Montesinos complexity of a\n$3$-manifold $M$ is defined as the minimal number of triple points among all\nthe filling Dehn spheres of $M$. A sharp upper bound for the Montesinos\ncomplexity of the connected sum of two 3-manifolds is given.\n",
        "  The paper determines the algebraic and logic structure of the multiset\nsemantics of the core patterns of SPARQL. We prove that the fragment formed by\nAND, UNION, OPTIONAL, FILTER, MINUS and SELECT corresponds precisely to both,\nthe intuitive multiset relational algebra (projection, selection, natural join,\narithmetic union and except), and the multiset non-recursive Datalog with safe\nnegation.\n",
        "  We have investigated vortex states in two-dimensional superconductors under a\noscillating magnetic field from a chiral helimagnet. We have solved the\ntwo-dimensional Ginzburg-Landau equations with finite element method. We have\nfound that when the magnetic field from the chiral helimagnet increases,\nvortices appear all at once in all periodic regions. This transition is\ndifferent from that under the uniform magnetic field. Under the composite\nmagnetic field with the oscillating and uniform fields (down-vortices),\nvortices antiparallel to the uniform magnetic field disappear. Then, the small\nuniform magnetic field easily remove down-vortices.\n",
        "  We obtained follow-up HST observations of the seven low surface brightness\ngalaxies discovered with the Dragonfly Telephoto Array in the field of the\nmassive spiral galaxy M101. Out of the seven galaxies, only three were resolved\ninto stars and are potentially associated with the M101 group at $D=7\\text{\nMpc}$. Based on HST ACS photometry in the broad F606W and F814W filters, we use\na maximum likelihood algorithm to locate the Tip of the Red Giant Branch (TRGB)\nin galaxy color-magnitude diagrams. Distances are $6.38^{+0.35}_{-0.35},\n6.87^{+0.21}_{-0.30}$ and $6.52^{+0.25}_{-0.27} \\text{ Mpc}$ and we confirm\nthat they are members of the M101 group. Combining the three confirmed low\nluminosity satellites with previous results for brighter group members, we find\nthe M101 galaxy group to be a sparsely populated galaxy group consisting of\nseven group members, down to $M_V = -9.2 \\text{ mag}$. We compare the M101\ncumulative luminosity function to that of the Milky Way and M31. We find that\nthey are remarkably similar; In fact, the cumulative luminosity function of the\nM101 group gets even flatter for fainter magnitudes, and we show that the M101\ngroup might exhibit the two known small-scale flaws in the\n$\\Lambda\\textrm{CDM}$ model, namely `the missing satellite' problem and the\n`too big to fail' problem. Kinematic measurements of M101$'$s satellite\ngalaxies are required to determine whether the `too big to fail' problem does\nin fact exist in the M101 group.\n",
        "  In this work we establish and investigate the connections between causality\nfor query answers in databases, database repairs wrt. denial constraints, and\nconsistency-based diagnosis. The first two are relatively new problems in\ndatabases, and the third one is an established subject in knowledge\nrepresentation. We show how to obtain database repairs from causes and the\nother way around. The vast body of research on database repairs can be applied\nto the newer problem of determining actual causes for query answers. By\nformulating a causality problem as a diagnosis problem, we manage to\ncharacterize causes in terms of a system's diagnoses.\n",
        "  We conduct a deep narrow-band imaging survey of 13 Ly$\\alpha$ blobs (LABs)\nlocated in the SSA22 proto-cluster at z~3.1 in the CIV and HeII emission lines\nin an effort to constrain the physical process powering the Ly$\\alpha$ emission\nin LABs. Our observations probe down to unprecedented surface brightness limits\nof 2.1 $-$ 3.4 $\\times$ 10$^{-18}$ erg s$^{-1}$ cm$^{-2}$ arcsec$^{-2}$ per 1\narcsec$^2$ aperture (5$\\sigma$) for the HeII$\\lambda$1640 and CIV$\\lambda$1549\nlines, respectively. We do not detect extended HeII and CIV emission in any of\nthe LABs, placing strong upper limits on the HeII/Ly$\\alpha$ and CIV/Ly$\\alpha$\nline ratios, of 0.11 and 0.16, for the brightest two LABs in the field. We\nconduct detailed photoionization modeling of the expected line ratios and find\nthat, although our data constitute the deepest ever observations of these\nlines, they are still not deep enough to rule out a scenario where the\nLy$\\alpha$ emission is powered by the ionizing luminosity of an obscured AGN.\nOur models can accommodate HeII/Ly$\\alpha$ and CIV/Ly$\\alpha$ ratios as low as\n$\\simeq$0.05 and $\\simeq$0.07 respectively, implying that one needs to reach\nsurface brightness as low as 1 $-$ 1.5 $\\times$ 10$^{-18}$ erg s$^{-1}$\ncm$^{-2}$ arcsec$^{-2}$ (at 5$\\sigma$) in order to rule out a photoionization\nscenario. These depths will be achievable with the new generation of\nimage-slicing integral field units such as VLT/MUSE or Keck/KCWI. We also model\nthe expected HeII/Ly$\\alpha$ and CIV/Ly$\\alpha$ in a different scenario, where\nLy$\\alpha$ emission is powered by shocks generated in a large-scale superwind,\nbut find that our observational constraints can only be met for shock\nvelocities $v_{\\rm s} \\gtrsim$ 250 km s$^{-1}$, which appear to be in conflict\nwith recent observations of quiescent kinematics in LABs.\n",
        "  We present the results of a survey for intervening 21cm HI absorption in a\nsample of 10 nearby, gas-rich galaxies selected from the HI Parkes All-Sky\nSurvey (HIPASS). This follows the six HIPASS galaxies searched in previous work\nand completes our full sample. In this paper we searched for absorption along\n17 sightlines with impact parameters between 6 and 46 kpc, making one new\ndetection. We also obtained simultaneous HI emission-line data, allowing us to\ndirectly relate the absorption-line detection rate to the HI distribution. From\nthis we find the majority of the non-detections in the current sample are\nbecause sightline does not intersect the HI disc of the galaxy at sufficiently\nhigh column density, but that source structure is also an important factor.\n  The detected absorption-line arises in the galaxy NGC 5156 ($z = 0.01$) at an\nimpact parameter of 19 kpc. The line is deep and narrow with an integrated\noptical depth of 0.82 km s$^{-1}$. High resolution Australia Telescope Compact\nArray (ATCA) images at 5 and 8 GHz reveal that the background source is\nresolved into two components with a separation of 2.6 arcsec (500 pc at the\nredshift of the galaxy), with the absorption likely occurring against a single\ncomponent. We estimate that the ratio of the spin temperature and covering\nfactor, $T_{\\mathrm{S}}/f$, is approximately 950 K in the outer disc of NGC\n5156, but further observations using VLBI would allow us to accurately measure\nthe covering factor and spin temperature of the gas.\n",
        "  Fisheries management agencies around the world collect age data for the\npurpose of assessing the status of natural resources in their jurisdiction.\nEstimates of mortality rates represent a key information to assess the\nsustainability of fish stocks exploitation. Contrary to medical research or\nmanufacturing where survival analysis is routinely applied to estimate failure\nrates, survival analysis has seldom been applied in fisheries stock assessment\ndespite similar purposes between these fields of applied statistics. In this\npaper, we developed hazard functions to model the dynamic of an exploited fish\npopulation. These functions were used to estimate all parameters necessary for\nstock assessment (including natural and fishing mortality rates as well as gear\nselectivity) by maximum likelihood using age data from a sample of catch. This\nnovel application of survival analysis to fisheries stock assessment was tested\nby Monte Carlo simulations to assert that it provided un-biased estimations of\nrelevant quantities. The method was applied to data from the Queensland\n(Australia) sea mullet (Mugil cephalus) commercial fishery collected between\n2007 and 2014. It provided, for the first time, an estimate of natural\nmortality affecting this stock: 0.22 $\\pm$ 0.08 year$^{-1}$.\n",
        "  We first construct a genus zero positive allowable Lefschetz fibration over\nthe disk (a genus zero PALF for short) on the Akbulut cork and describe the\nmonodromy as a positive factorization in the mapping class group of a surface\nof genus zero with five boundary components. We then construct genus zero PALFs\non infinitely many exotic pairs of compact Stein surfaces such that one is a\ncork twist of the other along an Akbulut cork. The difference of smooth\nstructures on each of exotic pairs of compact Stein surface is interpreted as\nthe difference of the corresponding positive factorizations in the mapping\nclass group of a common surface of genus zero.\n",
        "  A positive Dehn twist product for a $\\mathbb{Z}_3$ action with $g+2$ fixed\npoints on the 2-dimensional closed, compact, oriented surface $\\Sigma_g$ is\npresented. The homeomorphism invariants of the resulting symplectic 4-manifolds\nare computed.\n",
        "  General laws in ecological parasitology are scarce. Here we evaluate data\npublished by over 100 authors to determine whether the number of hosts in a\nlife cycle is associated with the degree of aggregation of fish parasites at\ndifferent stages. Parasite species were grouped taxonomically to produce 20 or\nmore data points per group as far as possible. Most parasites that remained at\none trophic level were less aggregated than those that had passed up a food\nchain. We use a stochastic model to show that high parasite overdispersion in\npredators can be solely the result of the accumulation of parasites in their\nprey. The model is further developed to show that a change in the predators\nfeeding behaviour with age may further increase parasite aggregation.\n",
        "  Background: Guillain-Barr\\'e Syndrome (GBS) is a common type of severe acute\nparalytic neuropathy and associated with other virus infections such as dengue\nfever and Zika. This study investigate the relationship between GBS, dengue,\nlocal meteorological factors in Hong Kong and global climatic factors from\nJanuary 2000 to June 2016.\n  Methods: The correlations between GBS, dengue, Multivariate El Nino Southern\nOscillation Index (MEI) and local meteorological data were explored by the\nSpearman Rank correlations and cross-correlations between these time series.\nPoisson regression models were fitted to identify nonlinear associations\nbetween MEI and dengue. Cross wavelet analysis was applied to infer potential\nnon-stationary oscillating associations among MEI, dengue and GBS.\n  Findings : An increasing trend was found for both GBS cases and imported\ndengue cases in Hong Kong. We found a weak but statistically significant\nnegative correlation between GBS and local meteorological factors. MEI\nexplained over 12\\% of dengue's variations from Poisson regression models.\nWavelet analyses showed that there is possible non-stationary oscillating\nassociation between dengue and GBS from 2005 to 2015 in Hong Kong. Our study\nhas led to an improved understanding of the timing and relationship between\nGBS, dengue and MEI.\n",
        "  We develop a low-energy model of a unidirectional Larkin-Ovchinnikov (LO)\nstate. Because the underlying rotational and translational symmetries are\nbroken spontaneously, this gapless superfluid is a smectic liquid crystal, that\nexhibits fluctuations that are qualitatively stronger than in a conventional\nsuperfluid, thus requiring a fully nonlinear description of its Goldstone\nmodes. Consequently, at nonzero temperature the LO superfluid is an algebraic\nphase even in 3d. It exhibits half-integer vortex-dislocation defects, whose\nunbinding leads to transitions to a superfluid nematic and other phases. In 2d\nat nonzero temperature, the LO state is always unstable to a charge-4 nematic\nsuperfluid. We expect this superfluid liquid-crystal phenomenology to be\nrealizable in imbalanced resonant Fermi gases trapped isotropically.\n",
        "  Studies of cluster galaxies are increasingly finding galaxies with\nspectacular one-sided tails of gas and young stars, suggestive of intense\nram-pressure stripping. These so-called \"jellyfish\" galaxies typically have\nlate-type morphology. In this paper, we present MUSE observations of an\nelliptical galaxy in Abell 2670 with long tails of material visible in the\noptical spectra, as well as blobs with tadpole-like morphology. The spectra in\nthe central part of the galaxy reveals a stellar component as well as ionized\ngas. The stellar component does not have significant rotation, while the\nionized gas defines a clear star-forming gas disk. We argue, based on deep\noptical images of the galaxy, that the gas was most likely acquired during a\npast wet merger. It is possible that the star-forming blobs are also remnants\nof the merger. In addition, the direction and kinematics of the one-sided\nionized tails, combined with the tadpole morphology of the star-forming blobs,\nstrongly suggests that the system is undergoing ram pressure from the\nintracluster medium. In summary, this paper presents the discovery of a\npost-merger elliptical galaxy undergoing ram pressure stripping.\n",
        "  The production of color language is essential for grounded language\ngeneration. Color descriptions have many challenging properties: they can be\nvague, compositionally complex, and denotationally rich. We present an\neffective approach to generating color descriptions using recurrent neural\nnetworks and a Fourier-transformed color representation. Our model outperforms\nprevious work on a conditional language modeling task over a large corpus of\nnaturalistic color descriptions. In addition, probing the model's output\nreveals that it can accurately produce not only basic color terms but also\ndescriptors with non-convex denotations (\"greenish\"), bare modifiers (\"bright\",\n\"dull\"), and compositional phrases (\"faded teal\") not seen in training.\n",
        "  We propose a series of methods and models in order to explore the Global\nBurden of Disease Study and the provided healthy life expectancy HALE estimates\nfrom the World Health Organization WHO based on the mortality mx of a\npopulation provided in a classical life table and a mortality diagram. Our\nestimates are compared with the HALE estimates for the World territories and\nthe WHO regions along with providing comparative results with to findings of\nChang, Molla, Truman et al. (2015) on the Differences in healthy life\nexpectancy for the US population by sex, race or ethnicity and geographic\nregion in 2008 and from Yong and Saito (2009) regarding Trends in healthy life\nexpectancy in Japan. From the mortality point of view we have developed a\nsimple model for the estimation of a characteristic parameter b related to the\nhealthy life years lost to disability and providing full application details\nalong with characteristic parameter selection and stability of the\ncoefficients. We also provide a direct estimation method of the parameter b\nfrom the life tables. We straighten the importance of our methodology by\nproposing and applying estimates of the parameter b by using the Gompertz and\nthe Weibull models. From the Health State point of view we summarize the main\npoints of the first exit time theory to life table data and present the basic\nmodels starting from the first related model published by Janssen and Skiadas\n(1995). Even more we develop the simpler 2-parameter health state model and an\nextension of a model expressing the infant mortality to a 4-parameter model\nwhich is the simpler model providing very good fitting on the logarithm of the\nforce of mortality. More important is the use of the Health State Function and\nthe relative impact on mortality to find an estimate for the healthy life years\nlost to disability.\n",
        "  We prove that the ending lamination space of the five-punctured sphere is\nhomeomorphic to the Noebeling curve.\n",
        "  Modern RDBMSs support the ability to compress data using methods such as null\nsuppression and dictionary encoding. Data compression offers the promise of\nsignificantly reducing storage requirements and improving I/O performance for\ndecision support queries. However, compression can also slow down update and\nquery performance due to the CPU costs of compression and decompression. In\nthis paper, we study how data compression affects choice of appropriate\nphysical database design, such as indexes, for a given workload. We observe\nthat approaches that decouple the decision of whether or not to choose an index\nfrom whether or not to compress the index can result in poor solutions. Thus,\nwe focus on the novel problem of integrating compression into physical database\ndesign in a scalable manner. We have implemented our techniques by modifying\nMicrosoft SQL Server and the Database Engine Tuning Advisor (DTA) physical\ndesign tool. Our techniques are general and are potentially applicable to DBMSs\nthat support other compression methods. Our experimental results on real world\nas well as TPC-H benchmark workloads demonstrate the effectiveness of our\ntechniques.\n",
        "  The field of particle therapy is quickly growing and yet it's more widespread\nadoption is limited by size, cost and adaptation to the more conformal\ntreatment techniques. In order to realize the benefits of this modality the\nequipment used to generate and deliver the beam is evolving. The accelerator is\none of the key components and its future is dictated by the ability to\naccommodate the clinical requirements. This lecture is intended to provide an\nintroduction to these requirements and identify how synchrotrons are designed\nto deliver the desired beams as well as what limitations exist and expectations\nfor the future of synchrotrons.\n",
        "  Monodisperse microbubble ultrasound contrast agents have been proposed to\nfurther increase the signal-to-noise-ratio of contrast enhanced ultrasound\nimaging. Here, the sensitivity of a polydisperse preclinical agent was compared\nexperimentally to that of its size- and acoustically-sorted derivatives by\nusing narrowband pressure- and frequency-dependent scattering and attenuation\nmeasurements. The sorted monodisperse agents showed up to a two orders of\nmagnitude increase in sensitivity, i.e. in the average scattering cross-section\nper bubble. Moreover, we demonstrate here, for the first time, that the highly\nnonlinear response of acoustically sorted microbubbles can be exploited to\nconfine scattering and attenuation to the focal region of ultrasound fields\nused in clinical imaging. This property is a result of minimal prefocal\nscattering and attenuation and can be used to minimize shadowing effects in\ndeep tissue imaging. Moreover, it potentially allows for more localized therapy\nusing microbubbles through the spatial control of resonant microbubble\noscillations.\n",
        "  Two criteria for a closed connected definite 4-manifold with infinite cyclic\nfundamental group to be TOP-split are given. One criterion extends a sufficient\ncondition made in a previous paper. The result is equivalent to a purely\nalgebraic result on the question asking when a positive definite Hermitian form\nover the ring of integral one-variable Laurent polynomials is represented by an\ninteger matrix. As an application, an infinite family of orthogonally\nindecomposable unimodular odd definite symmetric $Z$-forms is produced.\n",
        "  We show that an in-plane magnetic field can drive two-dimensional\nspin-orbit-coupled systems under superconducting proximity effect into a\ngapless phase where parts of the normal state Fermi surface are gapped, and the\nungapped parts are reconstructed into a small Fermi surface of Bogoliubov\nquasiparticles at zero energy. Charge distribution, spin texture, and density\nof states of such \"partial Fermi surface\" are discussed. Material platforms for\nits physical realization are proposed.\n",
        "  Short Message Service (SMS) messages are largely sent directly from one\nperson to another from their mobile phones. They represent a means of personal\ncommunication that is an important communicative artifact in our current\ndigital era. As most existing studies have used private access to SMS corpora,\ncomparative studies using the same raw SMS data has not been possible up to\nnow. We describe our efforts to collect a public SMS corpus to address this\nproblem. We use a battery of methodologies to collect the corpus, paying\nparticular attention to privacy issues to address contributors' concerns. Our\nlive project collects new SMS message submissions, checks their quality and\nadds the valid messages, releasing the resultant corpus as XML and as SQL\ndumps, along with corpus statistics, every month. We opportunistically collect\nas much metadata about the messages and their sender as possible, so as to\nenable different types of analyses. To date, we have collected about 60,000\nmessages, focusing on English and Mandarin Chinese.\n",
        "  This paper presents a survey of X-ray selected active galactic nuclei (AGN)\nwith optical spectroscopic follow-up in a $\\sim 18\\, \\rm{deg^2}$ area of the\nequatorial XMM-XXL north field. A sample of 8445 point-like X-ray sources\ndetected by XMM-Newton above a limiting flux of $F_{\\rm 0.5-10\\, keV} >\n10^{-15} \\rm\\,erg\\, cm^{-2}\\, s^{-1}$ was matched to optical (SDSS) and\ninfrared (WISE) counterparts. We followed up 3042 sources brighter than\n$r=22.5$ mag with the SDSS BOSS spectrograph. The spectra yielded a reliable\nredshift measurement for 2578 AGN in the redshift range $z=0.02-5.0$, with\n$0.5-2\\rm\\, keV$ luminosities ranging from $10^{39}-10^{46}\\rm\\,erg\\,s^{-1}$.\nThis is currently the largest published spectroscopic sample of X-ray selected\nAGN in a contiguous area. The BOSS spectra of AGN candidates show a bimodal\ndistribution of optical line widths allowing a separation between broad- and\nnarrow-emission line AGN. The former dominate our sample (70 per cent) due to\nthe relatively bright X-ray flux limit and the optical BOSS magnitude limit. We\nclassify the narrow emission line objects (22 per cent of full sample) using\nstandard BPT diagnostics: the majority have line ratios indicating the dominant\nsource of ionization is the AGN. A small number (8 per cent of full sample)\nexhibit the typical narrow line ratios of star-forming galaxies, or only have\nabsorption lines in their spectra. We term the latter two classes \"elusive''\nAGN. We also compare X-ray, optical and infrared color AGN selections in this\nfield. X-ray observations reveal, the largest number of AGN. The overlap\nbetween the selections, which is a strong function of the imaging depth in a\ngiven band, is also remarkably small. We show using spectral stacking that a\nlarge fraction of the X-ray AGN would not be selectable via optical or IR\ncolours due to host galaxy contamination.\n",
        "  The Planck Early Release Compact Source Catalogue (ERCSC) includes nine lists\nof highly reliable sources, individually extracted at each of the nine Planck\nfrequency channels. To facilitate the study of the Planck sources, especially\ntheir spectral behaviour across the radio/infrared frequencies, we provide a\n\"bandmerged\" catalogue of the ERCSC sources. This catalogue consists of 15191\nentries, with 79 sources detected in all nine frequency channels of Planck and\n6818 sources detected in only one channel. We describe the bandmerging\nalgorithm, including the various steps used to disentangle sources in confused\nregions. The multi-frequency matching allows us to develop spectral energy\ndistributions of sources between 30 and 857 GHz, in particular across the 100\nGHz band, where the energetically important CO J=1->0 line enters the Planck\nbandpass. We find ~3-5sigma evidence for contribution to the 100 GHz intensity\nfrom foreground CO along the line of sight to 147 sources with |b|>30 deg. The\nmedian excess contribution is 4.5+/-0.9 percent of their measured 100 GHz flux\ndensity which cannot be explained by calibration or beam uncertainties. This\ntranslates to 0.5+/-0.1 K km s^{-1} of CO which must be clumped on the scale of\nthe Planck 100 GHz beam, i.e., ~10 arcmin. If this is due to a population of\nlow mass (~15 Msun) molecular gas clumps, the total mass in these clumps may be\nmore than 2000 Msun. Further, high-spatial-resolution, ground-based\nobservations of the high-latitude sky will help shed light on the origin of\nthis diffuse, clumpy CO emission.\n",
        "  This article proves a uniform exponential decay estimate for Seiberg-Witten\nequations on non-compact 4-manifolds with exact symplectic ends of bounded\ngeometry. This is an extension of the analysis for asymptotically flat almost\nK\\\"ahler (AFAK) structures by Kronheimer and Mrowka. As an application, we\nconstruct an invariant for smooth foliations without holonomy-invariant\ntransverse measure, which takes value in the boundary-stable version of the\nmonopole Floer homology group, without invoking the Eliashberg-Thurston\nperturbation.\n",
        "  Cocycles are constructed by polynomial expressions for Alexander quandles. As\napplications, non-triviality of some quandle homology groups are proved, and\nquandle cocycle invariants of knots are studied. In particular, for an infinite\nfamily of quandles, the non-triviality of quandle homology groups is proved for\nall odd dimensions.\n",
        "  We present results of magnetic measurements relating to vortex phase diagram\nin a single crystal of FeSe$_{0.5}$Te$_{0.5}$ which displays second\nmagnetization peak anomaly for $H \\parallel c$. The possible role of the\ncrystalline anisotropy on vortex pinning is explored via magnetic torque\nmagnetometry. We present evidence in favor of pinning related to spatial\nvariations of the charge carrier mean free path leading to small bundle vortex\npinning by randomly distributed (weak) pinning centers for both $H \\parallel c$\nand $H \\perp c$. This is further corroborated using magnetization data for $H\n\\parallel c$ in a single crystal of FeSe$_{0.35}$Te$_{0.65}$. Dynamical\nresponse across second magnetization peak (SMP) anomaly in\nFeSe$_{0.5}$Te$_{0.5}$ has been compared with that across the well researched\nphenomenon of peak effect (PE) in a single crystal of CeRu$_2$.\n",
        "  We recently built an analytical source model for GPU-based MC dose engine. In\nthis paper, we present a sampling strategy to efficiently utilize this source\nmodel in GPU-based dose calculation. Our source model was based on a concept of\nphase-space-ring (PSR). This ring structure makes it effective to account for\nbeam rotational symmetry, but not suitable for dose calculations due to\nrectangular jaw settings. Hence, we first convert PSR source model to its\nphase-space let (PSL) representation. Then in dose calculation, different types\nof sub-sources were separately sampled. Source sampling and particle transport\nwere iterated. So that the particles being sampled and transported\nsimultaneously are of same type and close in energy to alleviate GPU thread\ndivergence. We also present an automatic commissioning approach to adjust the\nmodel for a good representation of a clinical linear accelerator . Weighting\nfactors were introduced to adjust relative weights of PSRs, determined by\nsolving a quadratic minimization problem with a non-negativity constraint. We\ntested the efficiency gain of our model over a previous source model using PSL\nfiles. The efficiency was improved by 1.70 ~ 4.41, due to the avoidance of long\ndata reading and transferring. The commissioning problem can be solved in ~20\nsec. Its efficacy was tested by comparing the doses computed using the\ncommissioned model and the uncommissioned one, with measurements in different\nopen fields in a water phantom under a clinical Varian Truebeam 6MV beam. For\nthe depth dose curves, the average distance-to-agreement was improved from\n0.04~0.28 cm to 0.04~0.12 cm for build-up region and the root-mean-square (RMS)\ndose difference after build-up region was reduced from 0.32%~0.67% to\n0.21%~0.48%. For lateral dose profiles, RMS difference was reduced from\n0.31%~2.0% to 0.06%~0.78% at inner beam and from 0.20%~1.25% to 0.10%~0.51% at\nouter beam.\n",
        "  Selecting optimal parameters for a neural network architecture can often make\nthe difference between mediocre and state-of-the-art performance. However,\nlittle is published which parameters and design choices should be evaluated or\nselected making the correct hyperparameter optimization often a \"black art that\nrequires expert experiences\" (Snoek et al., 2012). In this paper, we evaluate\nthe importance of different network design choices and hyperparameters for five\ncommon linguistic sequence tagging tasks (POS, Chunking, NER, Entity\nRecognition, and Event Detection). We evaluated over 50.000 different setups\nand found, that some parameters, like the pre-trained word embeddings or the\nlast layer of the network, have a large impact on the performance, while other\nparameters, for example the number of LSTM layers or the number of recurrent\nunits, are of minor importance. We give a recommendation on a configuration\nthat performs well among different tasks.\n",
        "  We report the first extragalactic detection of CF+, the fluoromethylidynium\nion, in the z=0.89 absorber toward PKS1830-211. We estimate an abundance of\n~3E-10 relative to H2 and that ~1% of fluorine is captured in CF+. The\nabsorption line profile of CF+ is found to be markedly different from that of\nother species observed within the same tuning, and is notably anti-correlated\nwith CH3OH. On the other hand, the CF+ profile resembles that of [C I]. Our\nresults are consistent with expected fluorine chemistry and point to chemical\ndifferentiation in the column of absorbing gas.\n",
        "  We consider the problem of privately releasing a class of queries that we\ncall hierarchical count-of-counts histograms. Count-of-counts histograms\npartition the rows of an input table into groups (e.g., group of people in the\nsame household), and for every integer j report the number of groups of size j.\nHierarchical count-of-counts queries report count-of-counts histograms at\ndifferent granularities as per hierarchy defined on an attribute in the input\ndata (e.g., geographical location of a household at the national, state and\ncounty levels). In this paper, we introduce this problem, along with\nappropriate error metrics and propose a differentially private solution that\ngenerates count-of-counts histograms that are consistent across all levels of\nthe hierarchy.\n",
        "  Two aspects of filamentary molecular cloud evolution are addressed: (1)\nExploring analytically the role of the environment for the evolution of\nfilaments demonstrates that considering them in isolation (i.e. just addressing\nthe fragmentation stability) will result in unphysical conclusions about the\nfilament's properties. Accretion can also explain the observed decorrelation\nbetween FWHM and peak column density. (2) Free-fall accretion onto finite\nfilaments can lead to the characteristic \"fans\" of infrared-dark clouds around\nstar-forming regions. The fans may form due to tidal forces mostly arising at\nthe ends of the filaments, consistent with numerical models and earlier\nanalytical studies.\n",
        "  Data Warehouse (DW) is an essential part of Business Intelligence. DW emerged\nas a fast growing reporting and analysis technique in early 1980s. Today, it\nhas almost replaced relational databases. However, with passage of time, static\nand historic data of DWs could not produce Real Time reporting and analysis,\nthus giving a way to emerge the Idea of Real Time Data Warehouse (RTDW).\nAlthough, there are problems with RTDWs, but with advancement in technology and\nresearchers focus, RTDWs will be able to generate real time reports, analysis\nand forecasting.\n",
        "  Medical differential phase contrast x-ray imaging (DPCI) promises improved\nsoft-tissue contrast at lower x-ray dose. The dose strongly depends on both the\nangular sensitivity and on the visibility of a grating-based Talbot-Lau\ninterferometer. Using a conventional x-ray tube, a high sensitivity and a high\nvisibility are somewhat contradicting goals: To increase sensitivity, the\ngrating period has to be reduced and/or the grating distance increased.\nTechnically, this means using a higher Talbot order (3rd or 5th one instead of\nfirst one). This however reduces the visibility somewhat, because only a\nsmaller part of the tube spectrum will get used. This work proposes to relax\nthis problem by changing the phase grating geometry. This allows to double\nsensitivity (i.e., double the Talbot order) without reducing the visibility.\nOne proposed grating geometry is an older binary one (75% of a period\n$\\pi$-shifting), but applied in a novel way. The second proposed geometry is a\nnovel one, requiring three height levels for polychromatic correction. The\nadvantage is quantified by a simulation of the resulting interference patterns.\nVisibilities for the common $\\pi$-shifting gratings are compared with the\nproposed alternative geometries. This is done depending on photon energy and\nopening ratio of the coherence grating G0. It shows that despite of doubled\nsensitivity of the proposed gratings, the overall visibility might even improve\na little.\n",
        "  Dose volume histograms are a useful tool in state-of-the-art radiotherapy\nplanning, and it is essential to be aware of their limitations. Dose\ndistributions computed by treatment planning systems are affected by several\nsources of uncertainty such as algorithm limitations, measurement uncertainty\nin the data used to model the beam and residual differences between measured\nand computed dose, once the model is optimized. In order to take into account\nthe effect of uncertainty, a probabilistic approach is proposed and a new kind\nof histogram, a dose-expected volume histogram, is introduced. The expected\nvalue of the volume in the region of interest receiving an absorbed dose equal\nor greater than a certain value is found using the probability distribution of\nthe dose at each point. A rectangular probability distribution is assumed for\nthis point dose, and a relationship is given for practical computations. This\nmethod is applied to a set of dose volume histograms for different regions of\ninterest for 6 brain patients, 8 lung patients, 8 pelvis patients and 6\nprostate patients planned for IMRT. These results show how dose computation\nuncertainty has effects on PTV coverage and, to a lesser extent, in dose to\norgans at risk. This method allows to quantify these effects.\n",
        "  We use integral field spectroscopic (IFS) observations from the Gemini North\nMulti-Object Spectrograph (GMOS-N) to study the central H II region in a nearby\nblue compact dwarf (BCD) galaxy NGC 4449. The IFS data enable us to explore the\nvariation of physical and chemical conditions of the star-forming region and\nthe surrounding gas on spatial scales as small as 5.5 pc. Our kinematical\nanalysis shows possible signatures of shock ionisation and shell structures in\nthe surroundings of the star-forming region. The metallicity maps of the\nregion, created using direct T$_e$ and indirect strong line methods (R$_{23}$,\nO3N2 and N2), do not show any chemical variation. From the integrated spectrum\nof the central H II region, we find a metallicity of 12 + log(O/H) = 7.88 $\\pm$\n0.14 ($\\sim$ 0.15$^{+0.06}_{-0.04}$ Z$_{\\odot}$) using the direct method.\nComparing the central H II region metallicity derived here with those of H II\nregions throughout this galaxy from previous studies, we find evidence of\nincreasing metallicity with distance from the central nucleus. Such chemical\ninhomogeneities can be due to several mechanisms, including gas-loss via\nsupernova blowout, galactic winds, or metal-poor gas accretion. However, we\nfind that the localised area of decreased metallicity aligns spatially with the\npeak of star-forming activity in the galaxy, suggesting that gas-accretion may\nbe at play here. Spatially-resolved IFS data for the entire galaxy is required\nto confirm the metallicity inhomogeneity found in this study, and determine its\npossible cause.\n",
        "  Prostate cancer is the most common disease in men and the second leading\ncause of death from cancer. Generic large imaging instruments used in cancer\ndiagnosis have sensitivity, spatial resolution, and contrast inadequate for the\ntask of imaging details of a small organ such as the prostate. In addition,\nmultimodality imaging can play a significant role merging anatomical and\nfunctional details coming from simultaneous PET and MRI. Indeed,\nmulti-parametric PET/MRI was demonstrated to improve diagnosis, but it suffers\nfrom too many false positives. In order to address the above limits of the\ncurrent techniques, we have proposed, built and tested, thanks to the TOPEM\nproject funded by Italian National Institute of Nuclear Phisics a prototype of\nan endorectal PET-TOF/MRI probe. In the applied magnification PET geometry,\nperformance is dominated by a high-resolution detector placed closer to the\nsource. The expected spatial resolution in the selected geometry is about 1.5\nmm FWHM and efficiency a factor of 2 with respect to what obtained with the\nconventional PET scanner. In our experimental studies, we have obtained timing\nresolution of ~ 320 ps FWHM and at the same time Depth of Interaction (DOI)\nresolution of under 1 mm. Tests also showed that mutual adverse PET-MR effects\nare minimal. In addition, the matching endorectal RF coil was designed, built\nand tested. In the next planned studies, we expect that benefiting from the\nfurther progress in scintillator crystal surface treatment, in SiPM technology\nand associated electronics would allow us to significantly improve TOF\nresolution\n",
        "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
        "  We consider the effect of non-magnetic impurities on the onset temperature\n$T^*$ for the $d-$wave pairing in spin-fluctuation scenario for the cuprates.\nWe analyze intermediate coupling regime when the magnetic correlation length\n$\\xi/a >1$ and the dimensionless coupling $u$ is O(1). In the clean limit, $T^*\n\\approx 0.02 v_f/a$ in this parameter range, and weakly depends on $\\xi$ and\n$u$. We found numerically that this universal pairing scale is also quite\nrobust with respect to impurities: the scattering rate $\\Gamma_{cr}$ needed to\nbring $T^*$ down to zero is about 4 times larger than in weak coupling, in good\nquantitative agreement with experiments. We provide analytical reasoning for\nthis result.\n",
        "  We present a sequential model for temporal relation classification between\nintra-sentence events. The key observation is that the overall syntactic\nstructure and compositional meanings of the multi-word context between events\nare important for distinguishing among fine-grained temporal relations.\nSpecifically, our approach first extracts a sequence of context words that\nindicates the temporal relation between two events, which well align with the\ndependency path between two event mentions. The context word sequence, together\nwith a parts-of-speech tag sequence and a dependency relation sequence that are\ngenerated corresponding to the word sequence, are then provided as input to\nbidirectional recurrent neural network (LSTM) models. The neural nets learn\ncompositional syntactic and semantic representations of contexts surrounding\nthe two events and predict the temporal relation between them. Evaluation of\nthe proposed approach on TimeBank corpus shows that sequential modeling is\ncapable of accurately recognizing temporal relations between events, which\noutperforms a neural net model using various discrete features as input that\nimitates previous feature based models.\n",
        "  We show that totally real elliptic Lefschetz fibrations that admit a real\nsection are classified by their \"real loci\" which is nothing but an\n$S^1$-valued Morse function on the real part of the total space. We assign to\neach such real locus a certain combinatorial object that we call a\n\\emph{necklace diagram}. On the one hand, each necklace diagram corresponds to\nan isomorphism class of a totally real elliptic Lefschetz fibration that admits\na real section, and on the other hand, it refers to a decomposition of the\nidentity into a product of certain matrices in $PSL(2,\\Z)$. Using an algorithm\nto find such decompositions, we obtain an explicit list of necklace diagrams\nassociated with certain classes of totally real elliptic Lefschetz fibrations.\nMoreover, we introduce refinements of necklace diagrams and show that refined\nnecklace diagrams determine uniquely the isomorphism classes of the totally\nreal elliptic Lefschetz fibrations which may not have a real section. By means\nof necklace diagrams we observe some interesting phenomena underlying special\nfeature of real fibrations.\n",
        "  We refine Matveev's result asserting that any two closed oriented 3-manifolds\ncan be related by a sequence of borromean surgeries if and only if they have\nisomorphic first homology groups and linking pairings. Indeed, a borromean\nsurgery induces a canonical isomorphism between the first homology groups of\nthe involved 3-manifolds, which preserves the linking pairing. We prove that\nany such isomorphism is induced by a sequence of borromean surgeries. As an\nintermediate result, we prove that a given algebraic square finite presentation\nof the first homology group of a 3-manifold, which encodes the linking pairing,\ncan always be obtained from a surgery presentation of the manifold.\n",
        "  The World Wide Web (WWW) is the repository of large number of web pages which\ncan be accessed via Internet by multiple users at the same time and therefore\nit is Ubiquitous in nature. The search engine is a key application used to\nsearch the web pages from this huge repository, which uses the link analysis\nfor ranking the web pages without considering the facts provided by them. A new\nalgorithm called Probability of Correctness of Facts(PCF)-Engine is proposed to\nfind the accuracy of the facts provided by the web pages. It uses the\nProbability based similarity function (SIM) which performs the string matching\nbetween the true facts and the facts of web pages to find their probability of\ncorrectness. The existing semantic search engines, may give the relevant result\nto the user query but may not be 100% accurate. Our algorithm computes\ntrustworthiness of websites to rank the web pages. Simulation results show that\nour approach is efficient when compared with existing Voting and Truthfinder[1]\nalgorithms with respect to the trustworthiness of the websites.\n",
        "  In this paper, we present the guidelines for an XML-based approach for the\nsociological study of Web data such as the analysis of mailing lists or\ndatabases available online. The use of an XML warehouse is a flexible solution\nfor storing and processing this kind of data. We propose an implemented\nsolution and show possible applications with our case study of profiles of\nexperts involved in W3C standard-setting activity. We illustrate the\nsociological use of semi-structured databases by presenting our XML Schema for\nmailing-list warehousing. An XML Schema allows many adjunctions or crossings of\ndata sources, without modifying existing data sets, while allowing possible\nstructural evolution. We also show that the existence of hidden data implies\nincreased complexity for traditional SQL users. XML content warehousing allows\naltogether exhaustive warehousing and recursive queries through contents, with\nfar less dependence on the initial storage. We finally present the possibility\nof exporting the data stored in the warehouse to commonly-used advanced\nsoftware devoted to sociological analysis.\n",
        "  Recent progress of Reciprocal Quantum Logic (RQL) has renewed interest in AC\npowering of superconductor digital circuits, which had been abandoned since the\nfamous IBM project of 1970s. In this work we propose and demonstrate new\nAC-biased Single Flux Quantum (SFQ) circuits, and search for synergy of AC and\ncurrently dominating DC biasing schemes. As the first step, we suggest an\non-chip AC/DC converter capable of feeding a few DC-biased gates surrounded by\ntheir AC-biased counterparts. As the second step, we introduce and present the\nfirst successful demonstration of a new AC-powered circuit - an 8192-bit shift\nregister with over 32,800 Josephson junctions (JJs) and JJ density of about\n6x$10^5$ JJ per $cm^2$. We suggest a few niche applications for this type of\nAC-biased circuits, not requiring high clock rates. E.g., these, scalable to\nmillions of JJs per chip, circuits can serve as a convenient benchmark for new\nSFQ fabrication technology nodes, allowing the operating margins of individual\ncells to be extracted and, thus, 'visualize' individual fabrication defects and\nflux trapping events. The circuit can also be developed into a mega-pixel\nimaging array for a magnetic field microscope.\n",
        "  Based on the K\\\"onigl's inhomogeneous jet model, we estimate the jet\nparameters, such as bulk Lorentz factor $\\Gamma$, viewing angle $\\theta$ and\nelectron number density $n_{\\rm e}$ from radio VLBI and X-ray data for a sample\nof active galactic nuclei (AGNs) assuming that the X-rays are from the jet\nrather than the intracluster gas. The bulk kinetic power of jets is then\ncalculated using the derived jet parameters. We find a strong correlation\nbetween the total luminosity of broad emission lines and the bulk kinetic power\nof the jets. This result supports the scenario that the accretion process are\ntightly linked with the radio jets, though how the disk and jet are coupled is\nnot revealed by present correlation analysis. Moreover, we find a significant\ncorrelation between the bulk kinetic power and radio extended luminosity. This\nimplies that the emission from the radio lobes are closely related with the\nenergy flux transported through jets from the central part of AGNs.\n",
        "  Word discovery is the task of extracting words from unsegmented text. In this\npaper we examine to what extent neural networks can be applied to this task in\na realistic unwritten language scenario, where only small corpora and limited\nannotations are available. We investigate two scenarios: one with no\nsupervision and another with limited supervision with access to the most\nfrequent words. Obtained results show that it is possible to retrieve at least\n27% of the gold standard vocabulary by training an encoder-decoder neural\nmachine translation system with only 5,157 sentences. This result is close to\nthose obtained with a task-specific Bayesian nonparametric model. Moreover, our\napproach has the advantage of generating translation alignments, which could be\nused to create a bilingual lexicon. As a future perspective, this approach is\nalso well suited to work directly from speech.\n",
        "  The web is changing the way in which data warehouses are designed, used, and\nqueried. With the advent of initiatives such as Open Data and Open Government,\norganizations want to share their multidimensional data cubes and make them\navailable to be queried online. The RDF data cube vocabulary (QB), the W3C\nstandard to publish statistical data in RDF, presents several limitations to\nfully support the multidimensional model. The QB4OLAP vocabulary extends QB to\novercome these limitations, allowing to im- plement the typical OLAP\noperations, such as rollup, slice, dice, and drill-across using standard SPARQL\nqueries. In this paper we introduce a formal data model where the main object\nis the data cube, and define OLAP operations using this model, independent of\nthe underlying representation of the cube. We show then that a cube expressed\nusing our model can be represented using the QB4OLAP vocabulary, and finally we\nprovide a SPARQL implementation of OLAP operations over data cubes in QB4OLAP.\n",
        "  We present a minimal model to describe the onset of collective motion seen\nwhen a population of locusts are placed in an annular arena. At low densities\nmotion is disordered, while at high densities locusts march in a common\ndirection, which may reverse during the experiment. The data is well-captured\nby an individual-based model, in which demographic noise leads to the observed\ndensity-dependent effects. By fitting the model parameters to equation-free\ncoefficients, we give a quantitative comparison, showing time series,\nstationary distributions and the mean switching times between states.\n",
        "  We use elastic and inelastic neutron scattering to systematically investigate\nthe evolution of the low-energy spin excitations of the iron arsenide\nsuperconductor BaFe2-xNixAs2 as a function of nickel doping x. In the undoped\nstate, BaFe2As2 exhibits a tetragonal-to-orthorhombic structural phase\ntransition and simultaneously develops a collinear antiferromagnetic (AF) order\nbelow TN = 143 K. Upon electron-doping of x = 0.075 to induce bulk\nsuperconductivity with Tc = 12.3 K, the AF ordering temperature reduces to TN =\n58 K.We show that the appearance of bulk superconductivity in\nBaFe1.925Ni0.075As2 coincides with a dispersive neutron spin resonance in the\nspin excitation spectra, and a reduction in the static ordered moment. For\noptimally doped BaFe1.9Ni0.1As2 (Tc = 20 K) and overdoped BaFe1.85Ni0.15As2 (Tc\n= 15 K) superconductors, the static AF long-range order is completely\nsuppressed and the spin excitation spectra are dominated by a resonance and\nspin-gap at lower energies. We determine the electron-doping dependence of the\nneutron spin resonance and spin gap energies, and demonstrate that the\nthree-dimensional nature of the resonance survives into the overdoped regime.\nIf spin excitations are important for superconductivity, these results would\nsuggest that the three-dimensional character of the electronic superconducting\ngaps are prevalent throughout the phase diagram, and may be critical for\nsuperconductivity in these materials.\n",
        "  There has been a controversy as to whether or not the non-pathological flat\nfoot and high-arched foot have an effect on human walking activities. The 3D\nfoot scanning system was employed to obtain static footprints from subjects\nadopting a half-weight-bearing stance. Based upon their footprints, the\nsubjects were divided into two groups: the flat-footed and the high-arched. The\nplantar pressure measurement system was used to measure and record the\nsubjects' successive natural gaits. Two indices were proposed: distribution of\nvertical ground reaction force (VGRF) of plantar and the rate of the footprint\nareas. Using these two indices to compare the natural gaits of the two subject\ngroups, we found that (1) in stance phase, there is a significant difference\n(p<0.01) in the distributions of VGRF of plantar; (2) in a stride cycle, there\nis also a significant difference (p<0.01) in the rates of the footprint areas.\nOur analysis suggests that when walking, the VGRF of the plantar brings greater\nmuscle tension to the flat-footed while a smaller rate of the footprint areas\nbrings greater stability to the high-arched.\n",
        "  The magnetic-field-induced variations of the microwave surface resistance,\nR_s, have been investigated in ceramic Mg_{1-x}(LiAl)_xB_2, with x in the range\n0.1 - 0.4. The measurements have been performed on increasing and decreasing\nthe DC magnetic field, H_0, at fixed temperatures. At low temperatures, we have\nobserved a magnetic hysteresis in the R_s(H_0) curves in all the investigated\nsamples. On increasing the temperature, the range of H_0 in which the\nhysteretic behavior is visible shrinks; however, in the sample with x = 0.1 it\nis present up to temperatures close to T_c. We show that the field dependence\nof R_s can be quantitatively justified taking into account the critical-state\neffects on the fluxon lattice only in the sample with x = 0.4. On the contrary,\nin the samples with x < 0.4 the hysteresis exhibits an unusual shape, similar\nto that observed in others two-gap MgB_2 samples, which cannot be justified in\nthe framework of the critical-state models.\n",
        "  Recently much attention has been devoted to the construction of phylogenetic\nnetworks which generalize phylogenetic trees in order to accommodate complex\nevolutionary processes. Here we present an efficient, practical algorithm for\nreconstructing level-1 phylogenetic networks - a type of network slightly more\ngeneral than a phylogenetic tree - from triplets. Our algorithm has been made\npublicly available as the program LEV1ATHAN. It combines ideas from several\nknown theoretical algorithms for phylogenetic tree and network reconstruction\nwith two novel subroutines. Namely, an exponential-time exact and a greedy\nalgorithm both of which are of independent theoretical interest. Most\nimportantly, LEV1ATHAN runs in polynomial time and always constructs a level-1\nnetwork. If the data is consistent with a phylogenetic tree, then the algorithm\nconstructs such a tree. Moreover, if the input triplet set is dense and, in\naddition, is fully consistent with some level-1 network, it will find such a\nnetwork. The potential of LEV1ATHAN is explored by means of an extensive\nsimulation study and a biological data set. One of our conclusions is that\nLEV1ATHAN is able to construct networks consistent with a high percentage of\ninput triplets, even when these input triplets are affected by a low to\nmoderate level of noise.\n",
        "  Minimizing time delays in manufacturing vaccines appropriate to rapidly\nmutating viruses is the key step for improving vaccine effectiveness. The\nvaccine for the H3N2 flu type has failed for the last two years (~ 15%\neffective). Here we summarize the state of the predictive art and report the\nmost current results for H3N2 flu vaccine design. Using a 2006 model of\ndimensional reduction of viral mutational complexity, we show that this model\ncan reduce vaccine time delays by a year or more in some cases.\n",
        "  Natural language inference (NLI) is the task of determining if a natural\nlanguage hypothesis can be inferred from a given premise in a justifiable\nmanner. NLI was proposed as a benchmark task for natural language\nunderstanding. Existing models perform well at standard datasets for NLI,\nachieving impressive results across different genres of text. However, the\nextent to which these models understand the semantic content of sentences is\nunclear. In this work, we propose an evaluation methodology consisting of\nautomatically constructed \"stress tests\" that allow us to examine whether\nsystems have the ability to make real inferential decisions. Our evaluation of\nsix sentence-encoder models on these stress tests reveals strengths and\nweaknesses of these models with respect to challenging linguistic phenomena,\nand suggests important directions for future work in this area.\n",
        "  We present a proper-motion study on models of the dwarf spheroidal galaxy\nSculptor, based on the predicted proper-motion accuracy of Gaia measurements.\nGaia will measure proper motions of several hundreds of stars for a\nSculptor-like system. Even with an uncertainty on the proper motion of order\n1.5 times the size of an individual proper-motion value of ~10 mas/century, we\nfind that it is possible to recover Sculptor's systemic proper motion at its\ndistance of 79 kpc.\n",
        "  We present a brief history of Galactic astrophysics, and explain the origin\nof halo substructure in the Galaxy. We motivate our study of tidal streams by\nhighlighting the tight constraints that analysis of the trajectories of tidal\nstreams can place on the form of the Galactic potential.\n  We address the reconstruction of orbits from observations of tidal streams.\nWe upgrade the scheme reported by Binney (2008) and Jin & Lynden-Bell (2007),\nwhich reconstructs orbits from streams using radial-velocity measurements, to\nallow it to work with erroneous input data. The upgraded algorithm can correct\nfor both statistical error on observations, and systematic error due to streams\nnot delineating individual orbits, and given high-quality but realistic input\ndata, it can diagnose the potential with considerable accuracy.\n  We complement the work of Binney (2008) by deriving a new algorithm, which\nreconstructs orbits from streams using proper-motion data rather than radial\nvelocities. We show that the new algorithm has a similar potency for diagnosing\nthe Galactic potential.\n  We explore the concept of Galactic parallax, which arises in connection with\nour proper-motion study. Galactic parallax allows trigonometric distance\ncalculation to stars at 40 times the range of conventional parallax, although\nits applicability is limited to only those stars in tidal streams.\n  We examine from first principles the mechanics of tidal stream formation and\npropagation. We find that the mechanics of tidal streams has a natural\nexpression in terms of action-angle variables. We find that tidal streams in\nrealistic galaxy potentials will generally not delineate orbits, and that\nattempting to constrain the Galactic potential by assuming that they do can\nlead to large systematic error. We show that we can accurately predict the\nreal-space trajectories of streams, even when they differ significantly from\norbits.\n",
        "  This paper describes a context free grammar (CFG) based grammatical relations\nfor Myanmar sentences which combine corpus-based function tagging system. Part\nof the challenge of statistical function tagging for Myanmar sentences comes\nfrom the fact that Myanmar has free-phrase-order and a complex morphological\nsystem. Function tagging is a pre-processing step to show grammatical relations\nof Myanmar sentences. In the task of function tagging, which tags the function\nof Myanmar sentences with correct segmentation, POS (part-of-speech) tagging\nand chunking information, we use Naive Bayesian theory to disambiguate the\npossible function tags of a word. We apply context free grammar (CFG) to find\nout the grammatical relations of the function tags. We also create a functional\nannotated tagged corpus for Myanmar and propose the grammar rules for Myanmar\nsentences. Experiments show that our analysis achieves a good result with\nsimple sentences and complex sentences.\n",
        "  Junctions and interfaces consisting of unconventional superconductors provide\nan excellent experimental playground to study exotic phenomena related to the\nphase of the order parameter. Not only the complex structure of unconventional\norder parameters have an impact on the Josephson effects, but also may\nprofoundly alter the quasi-particle excitation spectrum near a junction. Here,\nby using spectroscopic-imaging scanning tunneling microscopy, we visualize the\nspatial evolution of the local density of states (LDOS) near twin boundaries\n(TBs) of the nodal superconductor FeSe. The $\\pi/2$ rotation of the\ncrystallographic orientation across the TB twists the structure of the\nunconventional order parameter, which may, in principle, bring about a\nzero-energy LDOS peak at the TB. The LDOS at the TB observed in our study, in\ncontrast, does not exhibit any signature of a zero-energy peak and an apparent\ngap amplitude remains finite all the way across the TB. The low-energy\nquasiparticle excitations associated with the gap nodes are affected by the TB\nover a distance more than an order of magnitude larger than the coherence\nlength $\\xi_{ab}$. The modification of the low-energy states is even more\nprominent in the region between two neighboring TBs separated by a distance\n$\\approx7\\xi_{ab}$. In this region the spectral weight near the Fermi level\n($\\approx\\pm$0.2~meV) due to the nodal quasiparticle spectrum is almost\ncompletely removed. These behaviors suggest that the TB induces a fully-gapped\nstate, invoking a possible twist of the order parameter structure which breaks\ntime-reversal symmetry.\n",
        "  We address recent criticisms (Liu et al., 2015; Ferrer-i-Cancho and\nG\\'omez-Rodr\\'iguez, 2015) of our work on empirical evidence of dependency\nlength minimization across languages (Futrell et al., 2015). First, we\nacknowledge error in failing to acknowledge Liu (2008)'s previous work on\ncorpora of 20 languages with similar aims. A correction will appear in PNAS.\nNevertheless, we argue that our work provides novel, strong evidence for\ndependency length minimization as a universal quantitative property of\nlanguages, beyond this previous work, because it provides baselines which focus\non word order preferences. Second, we argue that our choices of baselines were\nappropriate because they control for alternative theories.\n",
        "  Over the past few years, we have built a system that has exposed large\nvolumes of Deep-Web content to Google.com users. The content that our system\nexposes contributes to more than 1000 search queries per-second and spans over\n50 languages and hundreds of domains. The Deep Web has long been acknowledged\nto be a major source of structured data on the web, and hence accessing\nDeep-Web content has long been a problem of interest in the data management\ncommunity. In this paper, we report on where we believe the Deep Web provides\nvalue and where it does not. We contrast two very different approaches to\nexposing Deep-Web content -- the surfacing approach that we used, and the\nvirtual integration approach that has often been pursued in the data management\nliterature. We emphasize where the values of each of the two approaches lie and\ncaution against potential pitfalls. We outline important areas of future\nresearch and, in particular, emphasize the value that can be derived from\nanalyzing large collections of potentially disparate structured data on the\nweb.\n",
        "  Provenance for transactional updates is critical for many applications such\nas auditing and debugging of transactions. Recently, we have introduced\nMV-semirings, an extension of the semiring provenance model that supports\nupdates and transactions. Furthermore, we have proposed reenactment, a\ndeclarative form of replay with provenance capture, as an efficient and\nnon-invasive method for computing this type of provenance. However, this\napproach is limited to the snapshot isolation (SI) concurrency control protocol\nwhile many real world applications apply the read committed version of snapshot\nisolation (RC-SI) to improve performance at the cost of consistency. We present\nnon-trivial extensions of the model and reenactment approach to be able to\ncompute provenance of RC-SI transactions efficiently. In addition, we develop\ntechniques for applying reenactment across multiple RC-SI transactions. Our\nexperiments demonstrate that our implementation in the GProM system supports\nefficient re-construction and querying of provenance.\n",
        "  Kidney cancer is a severe disease which can be treated non-invasively using\nhigh-intensity focused ultrasound (HIFU) therapy. However, tissue in front of\nthe transducer and the deep location of kidney can cause significant losses to\nthe efficiency of the treatment. The effect of attenuation, refraction and\nreflection due to different tissue types on HIFU therapy of the kidney was\nstudied using a nonlinear ultrasound simulation model. The geometry of the\ntissue was derived from a computed tomography (CT) dataset of a patient which\nhad been segmented for water, bone, soft tissue, fat and kidney. The combined\neffect of inhomogeneous attenuation and sound-speed was found to result in an\n11.0 dB drop in spatial peak-temporal average (SPTA) intensity in the kidney\ncompared to pure water. The simulation without refraction effects showed a 6.3\ndB decrease indicating that both attenuation and refraction contribute to the\nloss in focal intensity. The losses due to reflections at soft tissue\ninterfaces were less than 0.1 dB. Focal point shifting due to refraction\neffects resulted in -1.3, 2.6 and 1.3 mm displacements in x-, y- and\nz-directions respectively. Furthermore, focal point splitting into several\nsmaller subvolumes was observed. The total volume of the secondary focal points\nwas approximately 46% of the largest primary focal point. This could\npotentially lead to undesired heating outside the target location and longer\ntherapy times.\n",
        "  In this paper we extend previous results concerning the behaviour of JSJ\ndecompositions of closed 3-manifolds with respect to the profinite completion\nto the case of compact 3-manifolds with boundary.\n  We also illustrate an alternative and perhaps more natural approach to part\nof the original theorem, using relative cohomology to analyse the actions of\nan-annular atoroidal groups on profinite trees.\n",
        "  The effects of gold nanoparticles in 125I brachytherapy dose enhancement on\nchoroidal Melanoma are examined using the Monte Carlo simulation technique.\nUsually, Monte Carlo ophthalmic brachytherapy dosimetry is performed in a water\nphantom. However, here, the compositions of human eye have been considered\ninstead of water. Both human eye and water phantoms have been simulated with\nMCNP5 code. These simulations were performed for a fully-loaded 16 mm COMS eye\nplaque containing 13 125I seeds. The dose delivered to the tumor and healthy\ntissues have been calculated in both phantoms, with and without GNPs. The\nresults indicates that the dose to the tumor in an eye-ball implanted with COMS\nplaque increases with increasing GNPs concentration inside the target.\nTherefore, the required irradiation time for the tumors in the eye is decreased\nby adding the GNPs prior to treatment. As a result, the dose to healthy tissues\ndecreases when the irradiation time is reduced. Furthermore, a comparison\nbetween the simulated data in an eye phantom made of water and eye phantom made\nof human-eye composition, in the presence of GNPs shows the significance of\nutilizing the composition of eye in ophthalmic brachytherapy dosimetry.\nNormally, the radiation therapy of cancer patients is designed to deliver a\nrequired dose to the tumor while sparing the surrounding healthy tissues. The\nresults demonstrated that the use of GNPs enable us to overcome this challenge.\nAlso, defining the eye composition instead of water will leads to more accurate\ncalculations of GNPs radiation effects in ophthalmic brachytherapy dosimetry.\n",
        "  One strategy for winning a coevolutionary struggle is to evolve rapidly. Most\nof the literature on host-pathogen coevolution focuses on this phenomenon, and\nlooks for consequent evidence of coevolutionary arms races. An alternative\nstrategy, less often considered in the literature, is to deter rapid\nevolutionary change by the opponent. To study how this can be done, we\nconstruct an evolutionary game between a controller that must process\ninformation, and an adversary that can tamper with this information processing.\nIn this game, a species can foil its antagonist by processing information in a\nway that is hard for the antagonist to manipulate. We show that the structure\nof the information processing system induces a fitness landscape on which the\nadversary population evolves, and that complex processing logic is required to\nmake that landscape rugged. Drawing on the rich literature concerning rates of\nevolution on rugged landscapes, we show how a species can slow adaptive\nevolution in the adversary population. We suggest that this type of defensive\ncomplexity on the part of the vertebrate adaptive immune system may be an\nimportant element of coevolutionary dynamics between pathogens and their\nvertebrate hosts.\n",
        "  Machine comprehension plays an essential role in NLP and has been widely\nexplored with dataset like MCTest. However, this dataset is too simple and too\nsmall for learning true reasoning abilities. \\cite{hermann2015teaching}\ntherefore release a large scale news article dataset and propose a deep LSTM\nreader system for machine comprehension. However, the training process is\nexpensive. We therefore try feature-engineered approach with semantics on the\nnew dataset to see how traditional machine learning technique and semantics can\nhelp with machine comprehension. Meanwhile, our proposed L2R reader system\nachieves good performance with efficiency and less training data.\n",
        "  Wu has shown that if a link or a knot $L$ in $S^3$ in thin position has thin\nspheres, then the thin sphere of lowest width is an essential surface in the\nlink complement. In this paper we show that if we further assume that $L\n\\subset S^3$ is prime, then the thin sphere of lowest width also does not have\nany vertical cut-disks. We also prove the result for a specific kind of tangles\nin $S^2 \\times [-1,1]$.\n",
        "  Diversity patterns of tree species in a tropical forest community are\napproached by a simple lattice model and investigated by Monte Carlo\nsimulations using a backtracking method. Our spatially explicit neutral model\nis based on a simple statistical physics process, namely the diffusion of\nseeds. The model has three parameters: the speciation rate, the size of the\nmeta-community in which the studied tree-community is embedded, and the average\nsurviving time of the seeds. By extensive computer simulations we aim the\nreproduction of relevant statistical measures derived from the experimental\ndata of the Barro Colorado Island tree census in year 1995. The first two\nparameters of the model are fixed to known values, characteristic of the\nstudied community, thus obtaining a model with only one freely adjustable\nparameter. As a result of this, the average number of species in the considered\nterritory, the relative species abundance distribution, the species-area\nrelationship and the spatial auto-correlation function of the individuals in\nabundant species are simultaneously fitted with only one parameter which is the\naverage surviving time of the seeds.\n",
        "  We show that the problem of recognizing that a knot diagram represents a\nspecific torus knot, or any torus knot at all, is in the complexity class ${\\sf\nNP} \\cap {\\sf co\\text{-}NP}$, assuming the generalized Riemann hypothesis. We\nalso show that satellite knot detection is in ${\\sf NP}$ under the same\nassumption, and that cabled knot detection and composite knot detection are\nunconditionally in ${\\sf NP}$. Our algorithms are based on recent work of\nKuperberg and of Lackenby on detecting knottedness.\n",
        "  Workload management for cloud databases must deal with the tasks of resource\nprovisioning, query placement and query scheduling in a manner that meets the\napplication's performance goals while minimizing the cost of using cloud\nresources. Existing solutions have approached these three challenges in\nisolation, and with only a particular type of performance goal in mind. In this\npaper, we introduce WiSeDB, a learning-based framework for generating holistic\nworkload management solutions customized to application-defined performance\nmetrics and workload characteristics. Our approach relies on supervised\nlearning to train cost-effective decision tree models for guiding query\nplacement, scheduling, and resource provisioning decisions. Applications can\nuse these models for both batch and online scheduling of incoming workloads. A\nunique feature of our system is that it can adapt its offline model to\nstricter/looser performance goals with minimal re-training. This allows us to\npresent alternative workload management strategies that address the typical\nperformance vs. cost trade-off of cloud services. Experimental results show\nthat our approach has very low training overhead while offering low cost\nstrategies for a variety of performance goals and workload characteristics.\n",
        "  Skip-gram (word2vec) is a recent method for creating vector representations\nof words (\"distributed word representations\") using a neural network. The\nrepresentation gained popularity in various areas of natural language\nprocessing, because it seems to capture syntactic and semantic information\nabout words without any explicit supervision in this respect. We propose\nSubGram, a refinement of the Skip-gram model to consider also the word\nstructure during the training process, achieving large gains on the Skip-gram\noriginal test set.\n",
        "  Blooms Taxonomy (BT) have been used to classify the objectives of learning\noutcome by dividing the learning into three different domains; the cognitive\ndomain, the effective domain and the psychomotor domain. In this paper, we are\nintroducing a new approach to classify the questions and learning outcome\nstatements (LOS) into Blooms taxonomy (BT) and to verify BT verb lists, which\nare being cited and used by academicians to write questions and (LOS). An\nexperiment was designed to investigate the semantic relationship between the\naction verbs used in both questions and LOS to obtain more accurate\nclassification of the levels of BT. A sample of 775 different action verbs\ncollected from different universities allows us to measure an accurate and\nclear-cut cognitive level for the action verb. It is worth mentioning that\nnatural language processing techniques were used to develop our rules as to\ninduce the questions into chunks in order to extract the action verbs. Our\nproposed solution was able to classify the action verb into a precise level of\nthe cognitive domain. We, on our side, have tested and evaluated our proposed\nsolution using confusion matrix. The results of evaluation tests yielded 97%\nfor the macro average of precision and 90% for F1. Thus, the outcome of the\nresearch suggests that it is crucial to analyse and verify the action verbs\ncited and used by academicians to write LOS and classify their questions based\non blooms taxonomy in order to obtain a definite and more accurate\nclassification.\n",
        "  Sentiment analysis is a key component in various text mining applications.\nNumerous sentiment classification techniques, including conventional and deep\nlearning-based methods, have been proposed in the literature. In most existing\nmethods, a high-quality training set is assumed to be given. Nevertheless,\nconstructing a high-quality training set that consists of highly accurate\nlabels is challenging in real applications. This difficulty stems from the fact\nthat text samples usually contain complex sentiment representations, and their\nannotation is subjective. We address this challenge in this study by leveraging\na new labeling strategy and utilizing a two-level long short-term memory\nnetwork to construct a sentiment classifier. Lexical cues are useful for\nsentiment analysis, and they have been utilized in conventional studies. For\nexample, polar and privative words play important roles in sentiment analysis.\nA new encoding strategy, that is, $\\rho$-hot encoding, is proposed to alleviate\nthe drawbacks of one-hot encoding and thus effectively incorporate useful\nlexical cues. We compile three Chinese data sets on the basis of our label\nstrategy and proposed methodology. Experiments on the three data sets\ndemonstrate that the proposed method outperforms state-of-the-art algorithms.\n",
        "  The galaxy M49 (NGC 4472) is the brightest early-type galaxy in the Virgo\nCluster. It is located in Subcluster B and has an unusually blue, metal-poor\nouter halo. Planetary nebulae (PNe) are excellent tracers of diffuse galaxy and\nintragroup light. We present a photometric survey of PNe in the galaxy's\nextended halo to characterise its PN population, as well as the surrounding\nintragroup light (IGL) of the Subcluster B. PNe were identified based on their\nbright [OIII]5007 \\AA\\ emission and absence of a broad-band continuum. We\nidentify 738 PNe out to a radius of 155 kpc from M49's centre from which we\ndefine a complete sample of 624 PNe within a limiting magnitude of m_5007=28.8.\nComparing the PN number density to the broad-band stellar surface brightness\nprofile, we find a variation of the PN-specific frequency (alpha-parameter)\nwith radius. The outer halo beyond 60 kpc has a 3.2 times higher\nalpha-parameter compared to the main galaxy halo, which is likely due to\ncontribution from the surrounding blue IGL. We use the Planetary Nebulae\nLuminosity Function (PNLF) as an indicator of distance and stellar population.\nIts slope, which correlates empirically with galaxy type, varies within the\ninner halo. In the eastern quadrant of M49, the PNLF slope is shallower,\nindicating an additional localised, bright PN population following an accretion\nevent, likely that of the dwarf irregular galaxy VCC1249. We also determined a\ndistance modulus of mu = 31.29+/-0.08 for M49, corresponding to a physical\ndistance of 18.1+/-0.6 Mpc, which agrees with a recent surface-brightness\nfluctuations distance. The PN populations in the outer halo of M49 are\nconsistent with the presence of a main Sersic galaxy halo with a slight (B-V)\ncolour gradient of 10${}^{-4}$ mag/arcsec surrounded by intragroup light with a\nvery blue colour of (B-V)=0.25 and a constant surface brightness mu_V=28.0\nmag/arcsec${}^2$.\n",
        "  With the advents of high-speed networks, fast commodity hardware, and the\nweb, distributed data sources have become ubiquitous. The third edition of the\n\\\"Ozsu-Valduriez textbook Principles of Distributed Database Systems [10]\nreflects the evolution of distributed data management and distributed database\nsystems. In this new edition, the fundamental principles of distributed data\nmanagement could be still presented based on the three dimensions of earlier\neditions: distribution, heterogeneity and autonomy of the data sources. In\nretrospect, the focus on fundamental principles and generic techniques has been\nuseful not only to understand and teach the material, but also to enable an\ninfinite number of variations. The primary application of these generic\ntechniques has been obviously for distributed and parallel DBMS versions.\nToday, to support the requirements of important data-intensive applications\n(e.g. social networks, web data analytics, scientific applications, etc.), new\ndistributed data management techniques and systems (e.g. MapReduce, Hadoop,\nSciDB, Peanut, Pig latin, etc.) are emerging and receiving much attention from\nthe research community. Although they do well in terms of\nconsistency/flexibility/performance trade-offs for specific applications, they\nseem to be ad-hoc and might hurt data interoperability. The key questions I\ndiscuss are: What are the fundamental principles behind the emerging solutions?\nIs there any generic architectural model, to explain those principles? Do we\nneed new foundations to look at data distribution?\n",
        "  In this article, we have introduced the first parallel corpus of Persian with\nmore than 10 other European languages. This article describes primary steps\ntoward preparing a Basic Language Resources Kit (BLARK) for Persian. Up to now,\nwe have proposed morphosyntactic specification of Persian based on\nEAGLE/MULTEXT guidelines and specific resources of MULTEXT-East. The article\nintroduces Persian Language, with emphasis on its orthography and\nmorphosyntactic features, then a new Part-of-Speech categorization and\northography for Persian in digital environments is proposed. Finally, the\ncorpus and related statistic will be analyzed.\n",
        "  Context: A complete study of the molecular and ionized gas in the environs of\nthe nebula RCW 78 around WR 55 is presented. Aims: The aim of this work is to\ninvestigate the spatial distribution, physical characteristics, and kinematical\nproperties of the molecular gas linked to the galactic nebula RCW 78 to achieve\na better understanding of its interaction with the star and with the ionized\ngas. Methods: This study was based on 12CO(1-0) fully sampled observations of a\nregion of ~0.45{\\deg} in size around the star WR 55 and the nebula RCW 78\nobtained with the 4-m NANTEN telescope, radio continuum archival data at 1.4\nand 4.85 GHz, obtained from SGPS and PMNRAO Southern Radio Survey,\nrespectively, and available infrared MIPSGAL images at 24 microns. Results: A\nmolecular gas component in the velocity range from ~ -58 to -45 km s-1,\ncompatible with the velocity of the ionized gas, was found to be associated\nwith the optical nebula. Adopting a distance of ~ 5 kpc, the mass of this\nmolecular component is about 3.4 x 10^4 Msun. The analysis of the molecular\ndata revealed the presence of a velocity gradient, in agreement with the Halpha\nline. New radiocontinuum flux density determinations confirm the thermal nature\nof RCW 78. This indicates that the ionized gas in RCW 78 arises from\nphotoionization of the molecular gas component in the velocity range from -58\nkm s-1 to -45 km s-1. A molecular concentration at a velocity of -56.1 km s-1\n(identified as C1) is very likely associated with the star HD 117797 and with a\ncollection of candidate YSOs, lying at a distance of 3.9 kpc, while the rest of\nthe molecular gas at velocities between -56 and -46 km s-1 constitute an\nincomplete ring-like structure which expands around WR 55 at a velocity of\nabout ~ 5 km s-1. Mechanical energy and time requirements indicate that WR 55\nis very capable of sustaining the expansion of the nebula.\n",
        "  Superparamagnetic iron oxide nanoparticles have recently been investigated\nfor their potential to kill cancer cells with promising results, owing to their\nability to be targeted and heated by magnetic fields. In this study, novel\nhydrogel, chitosan Fe3O4 magnetic nanoparticles were synthesized to induce\nmagnetic hyperthermia, and targeted delivering of chemotherapeutics in the\ncancer microenvironment. The characteristic properties of synthesized bare and\nCS-MNPs were analyzed by various analytical methods: X-ray diffraction, Fourier\ntransformed infrared spectroscopy, Scanning electron microscopy and\nThermo-gravimetric analysis/differential thermal analysis. Magnetic\nnanoparticles were successfully synthesized using the co-precipitation method.\nThis synthesis technique resulted in nanoparticles with an average particle\nsize of 16 nm. The pure obtained nanoparticles were then successfully\nencapsulated with 4-nm-thick chitosan coating. The formation of chitosan on the\nsurface of nanoparticles was confirmed by physicochemical analyses. Heating\nexperiments at safe magnetic field (f = 100 kHz, H =10-20 kA m-1) revealed that\nthe maximum achieved temperature of water stable chitosan-coated nanoparticles\n(50 mg ml-1) is fully in agreement with cancer therapy and biomedical\napplications.\n",
        "  Beams of $^{4}$He and $^{16}$O nuclei are considered for ion-beam cancer\ntherapy as alternative options to protons and $^{12}$C nuclei. Spread-out Bragg\npeak (SOBP) distributions of physical dose and relative biological\neffectiveness for 10% survival are calculated by means of our Geant4-based\nMonte Carlo model for Heavy Ion Therapy (MCHIT) and the modified\nmicrodosimetric kinetic model. The depth distributions of cell survival\nfractions are calculated for $^{1}$H, $^{4}$He, $^{12}$C and $^{16}$O for\ntissues with normal (HSG cells), low and high radiosensitivity. In each case\nthe cell survival fractions were compared separately for the target volume,\nbehind and in front of it. In the case of normal radiosensitivity $^{4}$He and\n$^{12}$C better spare tissues in the entrance channel compared to protons and\n$^{16}$O. The cell survival fractions calculated, respectively, for the\nentrance channel and target volume are similar for $^{4}$He and $^{12}$C. When\nit is important to spare healthy tissues located after the distal edge of the\nSOBP plateau, $^{4}$He can be recommended due to reduced nuclear fragmentation\nof these projectiles. No definite advantages of $^{16}$O with respect to\n$^{12}$C were found, with the except of an enhanced impact of these heavier\nprojectiles on radioresistant tumors.\n",
        "  Classifications and phylogenetic inferences of organismal groups change in\nlight of new insights. Over time these changes can result in an imperfect\ntracking of taxonomic perspectives through the re-/use of Code-compliant or\ninformal names. To mitigate these limitations, we introduce a novel approach\nfor aligning taxonomies through the interaction of human experts and logic\nreasoners. We explore the performance of this approach with the Perelleschus\nuse case of Franz & Cardona-Duque (2013). The use case includes six taxonomies\npublished from 1936 to 2013, 54 taxonomic concepts (i.e., circumscriptions of\nnames individuated according to their respective source publications), and 75\nexpert-asserted Region Connection Calculus articulations (e.g., congruence,\nproper inclusion, overlap, or exclusion). An Open Source reasoning toolkit is\nused to analyze 13 paired Perelleschus taxonomy alignments under heterogeneous\nconstraints and interpretations. The reasoning workflow optimizes the logical\nconsistency and expressiveness of the input and infers the set of maximally\ninformative relations among the entailed taxonomic concepts. The latter are\nthen used to produce merge visualizations that represent all congruent and\nnon-congruent taxonomic elements among the aligned input trees. In this small\nuse case with 6-53 input concepts per alignment, the information gained through\nthe reasoning process is on average one order of magnitude greater than in the\ninput. The approach offers scalable solutions for tracking provenance among\nsucceeding taxonomic perspectives that may have differential biases in naming\nconventions, phylogenetic resolution, ingroup and outgroup sampling, or\nostensive (member-referencing) versus intensional (property-referencing)\nconcepts and articulations.\n",
        "  As the chief informational molecule of life, DNA is subject to extensive\nphysical manipulations. The energy required to deform double-helical DNA\ndepends on sequence, and this mechanical code of DNA influences gene\nregulation, such as through nucleosome positioning. Here we examine the\nsequence-dependent flexibility of DNA in bacterial transcription\nfactor-mediated looping, a context for which the role of sequence remains\npoorly understood. Using a suite of synthetic constructs repressed by the Lac\nrepressor and two well-known sequences that show large flexibility differences\nin vitro, we make precise statistical mechanical predictions as to how DNA\nsequence influences loop formation and test these predictions using in vivo\ntranscription and in vitro single-molecule assays. Surprisingly,\nsequence-dependent flexibility does not affect in vivo gene regulation. By\ntheoretically and experimentally quantifying the relative contributions of\nsequence and the DNA-bending protein HU to DNA mechanical properties, we reveal\nthat bending by HU dominates DNA mechanics and masks intrinsic\nsequence-dependent flexibility. Such a quantitative understanding of how\nmechanical regulatory information is encoded in the genome will be a key step\ntowards a predictive understanding of gene regulation at single-base pair\nresolution.\n",
        "  Given any generating set of any pseudo-Anosov-containing subgroup of the\nmapping class group of a surface, we construct a pseudo-Anosov with word length\nbounded by a constant depending only on the surface. More generally, in any\nsubgroup G we find an element f with the property that the minimal subsurface\nsupporting a power of f is as large as possible for elements of G; the same\nconstant bounds the word length of f. Along the way we find new examples of\nconvex cocompact free subgroups of the mapping class group.\n",
        "  By generalizing the Kuperberg sl(3) bracket, we construct a graph-valued\nanalogue of the Homflypt sl(3) invariant for virtual knots. The restriction of\nthis invariant for classical knots coincides with the usual Homflypt sl(3)\ninvariant, and for virtual knots and graphs it provides new information that\nallows one to prove minimality theorems and to construct new invariants for\nfree knots. We formulate this new invariant for virtual braids as well, and\nshow that it leads to the construction of a trace function on the virtual Hecke\nalgebra. Finally, we show that the Penrose coloring bracket is a special case\nof the Kuperberg bracket, and we raise new questions about the extension of the\npresent work.\n",
        "  In this second paper of T$h$e role of $E$nvironment in shaping $L$ow-mass\n$E$arly-type $N$earby g$a$laxies (hELENa) series we study [Mg/Fe] abundance\ndistribution trends of early-type galaxies observed with the SAURON integral\nfield unit, spanning a wide range in mass and local environment densities: 20\nlow-mass early-types (dEs) of Sybilska et al. (2017) and 258 massive early\ntypes (ETGs) of the $ATLAS^{3D}$ project, all homogeneously reduced and\nanalyzed. We show that the [Mg/Fe] ratios scale with velocity dispersion\n($\\sigma$) at fixed [Fe/H] and that they evolve with [Fe/H] along similar paths\nfor all early-types, grouped in bins of increasing local and global {$\\sigma$},\nas well as the second velocity moment $V_{rms}$, indicating a common inside-out\nformation pattern. We then place our dEs on the [Mg/Fe] $vs.$ [Fe/H] diagram of\nLocal Group galaxies and show that dEs occupy the same region and show a\nsimilar trend line slope in the diagram as the high-metallicity stars of the\nMilky Way and the Large Magellanic Cloud. This finding extends the similar\ntrend found for dwarf spheroidal $vs.$ dwarf irregular galaxies and supports\nthe notion that dEs have evolved from late-type galaxies that have lost their\ngas at a point of their evolution, which likely coincided with them entering\ndenser environments.\n",
        "  This paper proposes an approach to environmental accounting useful for\nstudying the feasibility of socio-economic systems in relation to the external\nconstraints posed by ecological compatibility. The approach is based on a\nmulti-scale analysis of the metabolic pattern of ecosystems and societies and\nit provides an integrated characterization of the resulting interaction. The\ntext starts with a theoretical part explaining (i) the implicit epistemological\nrevolution implied by the notion of ecosystem metabolism and the fund-flow\nmodel developed by Georgescu-Roegen applied to environmental accounting, and\n(ii) the potentials of this approach to create indicators to assess ecological\nintegrity and environmental impacts. This revolution also makes it possible to\ncarry out a multi-scale integrated assessment of ecosystem and societal\nmetabolisms at the territorial level. In the second part, two applications of\nthis approach using an indicator of the negentropic cost show the possibility\nto characterize in quantitative and qualitative terms degrees of alteration\n(crop cultivation, tree plantations)for different biomes (tropical and boreal\nforests). Also, a case study for land use scenarios has been included. The\nproposed approach represents an integrated multi-scale tool for the analysis of\nnature conservation scenarios and strategies.\n",
        "  Herbaria worldwide are housing a treasure of 100s of millions of herbarium\nspecimens, which are increasingly being digitized in recent years and thereby\nmade more easily accessible to the scientific community. At the same time, deep\nlearning algorithms are rapidly improving pattern recognition from images and\nthese techniques are more and more being applied to biological objects. We are\nusing digital images of herbarium specimens in order to identify taxa and\ntraits of these collection objects by applying convolutional neural networks\n(CNN). Images of the 1000 species most frequently documented by herbarium\nspecimens on GBIF have been downloaded and combined with morphological trait\ndata, preprocessed and divided into training and test datasets for species and\ntrait recognition. Good performance in both domains is promising to use this\napproach in future tools supporting taxonomy and natural history collection\nmanagement.\n",
        "  In this paper we present a model to estimate the density of aedes mosquitoes\nin a community affected by dengue. The model is based on the fitting of a\ncontinuous function to the incidence of dengue infections, from which the\ndensity of infected mosquitoes is derived straightforwardly. Further\nderivations allows the calculation of the latent and susceptible mosquitoes'\ndensities, the sum of the three equals the total mosquitoes' density. The model\nis illustrated with the case of the risk of urban yellow fever resurgence in\ndengue infested areas but the same methods apply for other aedes-transmitted\ninfections like Zika and chikungunya viruses.\n",
        "  Despite high vaccine coverage, pertussis has re-emerged as a public health\nconcern in many countries. One hypothesis posed for re-emergence is the waning\nof immunity. In some disease systems, the process of waning immunity can be\nnon-linear, involving a complex relationship between the duration of immunity\nand subsequent boosting of immunity through asymptomatic re-exposure.\n  We present and analyse a model of infectious disease transmission to examine\nthe interplay between infection and immunity. By allowing the duration of\ninfection-acquired immunity to differ from that of vaccine-acquired immunity,\nwe explore the impact of the difference in durations on long-term disease\npatterns and prevalence of infection.\n  Our model demonstrates that vaccination may induce cyclic behaviour, and its\nability to reduce the infection prevalence increases with both the duration of\ninfection-acquired immunity and duration of vaccine-acquired immunity. We find\nthat increasing vaccine coverage, while capable of leading to an increase in\noverall transmission, always results in a reduction in prevalence of primary\ninfections, with epidemic cycles characterised by a longer interepidemic period\nand taller peaks.\n  Our results show that the epidemiological patterns of an infectious disease\nmay change considerably when the duration of vaccine-acquired immunity differs\nfrom that of infection-acquired immunity. Our study highlights that for any\nparticular disease and associated vaccine, a detailed understanding of the\nduration of protection and how that duration is influenced by infection\nprevalence is important as we seek to optimise vaccination strategies.\n",
        "  Comprehending lyrics, as found in songs and poems, can pose a challenge to\nhuman and machine readers alike. This motivates the need for systems that can\nunderstand the ambiguity and jargon found in such creative texts, and provide\ncommentary to aid readers in reaching the correct interpretation. We introduce\nthe task of automated lyric annotation (ALA). Like text simplification, a goal\nof ALA is to rephrase the original text in a more easily understandable manner.\nHowever, in ALA the system must often include additional information to clarify\nniche terminology and abstract concepts. To stimulate research on this task, we\nrelease a large collection of crowdsourced annotations for song lyrics. We\nanalyze the performance of translation and retrieval models on this task,\nmeasuring performance with both automated and human evaluation. We find that\neach model captures a unique type of information important to the task.\n",
        "  We present an algorithm for fast and accurate computation of the local dose\ndistribution in MeV beams of protons, carbon ions or other heavy-charged\nparticles. It uses compound Poisson-process modelling of track interaction and\nsuccesive convolutions for fast computation. It can handle mixed particle\nfields over a wide range of fluences. Since the local dose distribution is the\nessential part of several approaches to model detector efficiency or cellular\nresponse it has potential use in ion-beam dosimetry and radiotherapy.\n",
        "  \\item[Purpose] A recent study revealed that polyethylene (PE) would cause\nextra carbon-ion attenuation per range shift by 0.45\\%/cm due to compositional\ndifferences in nuclear interactions. The present study aims to assess the\ninfluence of PE range compensators on tumor dose in carbon-ion radiotherapy.\n\\item[Methods] Carbon-ion radiation was modeled to be composed of primary\ncarbon ions and secondary particles, for each of which the dose and the\nrelative biological effectiveness (RBE) were estimated at a tumor depth in the\nmiddle of spread-out Bragg peak. Assuming exponential behavior for attenuation\nand yield of these components with depth, the PE effect on dose was calculated\nfor clinical carbon-ion beams and was partly tested by experiment. The\ntwo-component model was integrated into a treatment-planning system and the PE\neffect was estimated in two clinical cases. \\item[Results] The attenuation per\nrange shift by PE was 0.1\\%--0.3\\%/cm in dose and 0.2\\%--0.4\\%/cm in\nRBE-weighted dose, depending on energy and range-modulation width. This\ntranslates into reduction of RBE-weighted dose by up to 3\\% in extreme cases.\nIn the treatment-planning study, however, the effect on RBE-weighted dose to\ntumor was typically within 1\\% reduction. \\item[Conclusions] The extra\nattenuation of primary carbon ions in PE was partly compensated by increased\nsecondary particles for tumor dose. In practical situations, the PE range\ncompensators would normally cause only marginal errors as compared to intrinsic\nuncertainties in treatment planning, patient setup, beam delivery, and clinical\nresponse.\n",
        "  Fiber-like features are an important aspect of breast imaging. Vessels and\nducts are present in all breast images, and spiculations radiating from a mass\ncan indicate malignancy. Accordingly, fiber objects are one of the three types\nof signals used in the American College of Radiology digital mammography\n(ACR-DM) accreditation phantom. This work focuses on the image properties of\nfiber-like structures in digital breast tomosynthesis (DBT) and how image\nreconstruction can affect their appearance. The impact of DBT image\nreconstruction algorithm and regularization strength on the conspicuity of\nfiber-like signals of various orientations is investigated in simulation. A\nmetric is developed to characterize this orientation dependence and allow for\nquantitative comparison of algorithms and associated parameters in the context\nof imaging fiber signals. The imaging properties of fibers, characterized in\nsimulation, are then demonstrated in detail with physical DBT data of the\nACR-DM phantom. The characterization of imaging of fiber signals is used to\nexplain features of an actual clinical DBT case. For the algorithms\ninvestigated, at low regularization setting, the results show a striking\nvariation in conspicuity as a function of orientation in the viewing plane. In\nparticular, the conspicuity of fibers nearly aligned with the plane of the\nX-ray source trajectory is decreased relative to more obliquely oriented\nfibers. Increasing regularization strength mitigates this orientation\ndependence at the cost of increasing depth blur of these structures.\n",
        "  Many fundamental problems in natural language processing rely on determining\nwhat entities appear in a given text. Commonly referenced as entity linking,\nthis step is a fundamental component of many NLP tasks such as text\nunderstanding, automatic summarization, semantic search or machine translation.\nName ambiguity, word polysemy, context dependencies and a heavy-tailed\ndistribution of entities contribute to the complexity of this problem.\n  We here propose a probabilistic approach that makes use of an effective\ngraphical model to perform collective entity disambiguation. Input mentions\n(i.e.,~linkable token spans) are disambiguated jointly across an entire\ndocument by combining a document-level prior of entity co-occurrences with\nlocal information captured from mentions and their surrounding context. The\nmodel is based on simple sufficient statistics extracted from data, thus\nrelying on few parameters to be learned.\n  Our method does not require extensive feature engineering, nor an expensive\ntraining procedure. We use loopy belief propagation to perform approximate\ninference. The low complexity of our model makes this step sufficiently fast\nfor real-time usage. We demonstrate the accuracy of our approach on a wide\nrange of benchmark datasets, showing that it matches, and in many cases\noutperforms, existing state-of-the-art methods.\n",
        "  The pairing mechanism in the iron pnictides remains unresolved yet. One of\nthe central issues is the structure of the superconducting order parameter\nwhich classifies the community into two different and highly disputed camps. On\none hand the picture of pairing based on the magnetic origin predicts a sign\nreversal gap on the electron and hole Fermi pockets, leading to the S+-\npairing. On the other hand, a more conventional S++ pairing gap was suggested\nbased on the phonon or orbital fluctuation mediated pairing. In the\nsuperconducting state, the impurities may generate a unique pattern of local\ndensity of states in space and energy, which are regarded as the fingerprints\nfor checking the structure of the pairing gap. In this study, we successfully\nidentified the non-magnetic and magnetic impurities in Na(Fe0.97-xCo0.03Tx)As\n(T=Cu, Mn) and investigated the spatial resolved scanning tunneling\nspectroscopy. We present clear evidence of the in-gap quasiparticle states\ninduced by the nonmagnetic Cu impurities, giving decisive evidence of the S+-\npairing. This is corroborated by the consistency between the experimental data\nand the first-principles calculations based on the S+- pairing gap with a\nscalar scattering potential.\n",
        "  We present a detailed study of Australia Telescope Compact Array (ATCA)\nobservations ($\\lambda$ = 20, 13, 6 & 3~cm) of supernova remnant (SNR)\nJ0509--6731 in the Large Magellanic Cloud (LMC). The remnant has a ring\nmorphology with brightened regions towards the south-western limb. We also find\na second brightened inner ring which is only seen in the radio-continuum. The\nSNR is almost circular, with a diameter ranging from 7 to 8~pc, and a steep\nradio spectral index between 36 and 3~cm of $\\alpha=-0.73\\pm0.02$, which is\ncharacteristic of younger SNRs. We also report detection of radially orientated\npolarisation across the remnant at 6~cm, with a mean fractional polarisation\nlevel of $P\\cong$~(26~$\\pm$~13)%. We find the magnetic field ($\\sim$168~$\\mu$G)\nand $\\Sigma - D$ ($\\Sigma = $ $1.1\\times 10^{-19}$~W\nm$^{-2}$~Hz$^{-1}$~sr$^{-1}$ , $D=$ 7.35~pc) to be consistent with other young\nremnants.\n",
        "  We describe the performance of amplifiers in the 4 GHz--8 GHz range using\nDirect Current Superconducting Quantum Interference Devices(DC SQUIDs) in a\nlumped element configuration. We have used external impedance transformers to\ncouple power into and out of the DC SQUIDs. By choosing appropriate values for\ncoupling capacitors, resonator lengths and output component values, we have\ndemonstrated useful gains in several frequency ranges with different\nbandwidths, showing over 27 GHz of power gain-bandwidth product. In this work,\nwe describe our design for the 4 GHz--8 GHz range and present data\ndemonstrating gain, bandwidth, dynamic range, and drift characteristics.\n",
        "  Due to the low X-ray photon utilization efficiency and low measurement\nsensitivity of the electron multiplying charge coupled device (EMCCD) camera\nsetup, the collimator based narrow beam X-ray luminescence computed tomography\n(XLCT) usually requires a long measurement time. In this paper, we, for the\nfirst time, report a focused X-ray beam based XLCT imaging system with\nmeasurements by a single optical fiber bundle and a photomultiplier tube (PMT).\nAn X-ray tube with a polycapillary lens was used to generate a focused X-ray\nbeam whose X-ray photon density is 1200 times larger than a collimated X-ray\nbeam. An optical fiber bundle was employed to collect and deliver the emitted\nphotons on the phantom surface to the PMT. The total measurement time was\nreduced to 12.5 minutes. For numerical simulations of both single and six fiber\nbundle cases, we were able to reconstruct six targets successfully. For the\nphantom experiment, two targets with an edge-to-edge distance of 0.4 mm and a\ncenter-to-center distance of 0.8 mm were successfully reconstructed by the\nmeasurement setup with a single fiber bundle and a PMT.\n",
        "  Entity resolution (ER) is a key data integration problem. Despite the efforts\nin 70+ years in all aspects of ER, there is still a high demand for\ndemocratizing ER - humans are heavily involved in labeling data, performing\nfeature engineering, tuning parameters, and defining blocking functions. With\nthe recent advances in deep learning, in particular distributed representation\nof words (a.k.a. word embeddings), we present a novel ER system, called DeepER,\nthat achieves good accuracy, high efficiency, as well as ease-of-use (i.e.,\nmuch less human efforts). For accuracy, we use sophisticated composition\nmethods, namely uni- and bi-directional recurrent neural networks (RNNs) with\nlong short term memory (LSTM) hidden units, to convert each tuple to a\ndistributed representation (i.e., a vector), which can in turn be used to\neffectively capture similarities between tuples. We consider both the case\nwhere pre-trained word embeddings are available as well the case where they are\nnot; we present ways to learn and tune the distributed representations. For\nefficiency, we propose a locality sensitive hashing (LSH) based blocking\napproach that uses distributed representations of tuples; it takes all\nattributes of a tuple into consideration and produces much smaller blocks,\ncompared with traditional methods that consider only a few attributes. For\nease-of-use, DeepER requires much less human labeled data and does not need\nfeature engineering, compared with traditional machine learning based\napproaches which require handcrafted features, and similarity functions along\nwith their associated thresholds. We evaluate our algorithms on multiple\ndatasets (including benchmarks, biomedical data, as well as multi-lingual data)\nand the extensive experimental results show that DeepER outperforms existing\nsolutions.\n",
        "  A two-species spatially extended system of hosts and parasitoids is studied.\nThere are two distinct kinds of coexistence; one with populations distributed\nhomogeneously in space and another one with spatiotemporal patterns. In the\nlatter case, there are noise-sustained oscillations in the population\ndensities, whereas in the former one the densities are essentially constants in\ntime with small fluctuations. We introduce several metrics to characterize the\npatterns and onset thereof. We also build a consistent sequence of corrections\nto the mean-field equations using a posteriori knowledge from simulations.\nThese corrections both lead to better description of the dynamics and connect\nthe patterns to it. The analysis is readily applicable to realistic systems,\nwhich we demonstrate by an example using an empirical metapopulation landscape.\n",
        "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
        "  This thesis introduces DescribeX, a powerful framework that is capable of\ndescribing arbitrarily complex XML summaries of web collections, providing\nsupport for more efficient evaluation of XPath workloads. DescribeX permits the\ndeclarative description of document structure using all axes and language\nconstructs in XPath, and generalizes many of the XML indexing and summarization\napproaches in the literature. DescribeX supports the construction of\nheterogeneous summaries where different document elements sharing a common\nstructure can be declaratively defined and refined by means of path regular\nexpressions on axes, or axis path regular expression (AxPREs). DescribeX can\nsignificantly help in the understanding of both the structure of complex,\nheterogeneous XML collections and the behaviour of XPath queries evaluated on\nthem.\n  Experimental results demonstrate the scalability of DescribeX summary\nrefinements and stabilizations (the key enablers for tailoring summaries) with\nmulti-gigabyte web collections. A comparative study suggests that using a\nDescribeX summary created from a given workload can produce query evaluation\ntimes orders of magnitude better than using existing summaries. DescribeX's\nlight-weight approach of combining summaries with a file-at-a-time XPath\nprocessor can be a very competitive alternative, in terms of performance, to\nconventional fully-fledged XML query engines that provide DB-like functionality\nsuch as security, transaction processing, and native storage.\n",
        "  Clustering is one of the major tasks in data mining. In the last few years,\nClustering of spatial data has received a lot of research attention. Spatial\ndatabases are components of many advanced information systems like geographic\ninformation systems VLSI design systems. In this thesis, we introduce several\nefficient algorithms for clustering spatial data. First, we present a\ngrid-based clustering algorithm that has several advantages and comparable\nperformance to the well known efficient clustering algorithm. The algorithm has\nseveral advantages. The algorithm does not require many input parameters. It\nrequires only three parameters, the number of the points in the data space, the\nnumber of the cells in the grid and a percentage. The number of the cells in\nthe grid reflects the accuracy that should be achieved by the algorithm. The\nalgorithm is capable of discovering clusters of arbitrary shapes. The\ncomputational complexity of the algorithm is comparable to the complexity of\nthe most efficient clustering algorithm. The algorithm has been implemented and\ntested against different ranges of database sizes. The performance results show\nthat the running time of the algorithm is superior to the most well known\nalgorithms (CLARANS [23]). The results show also that the performance of the\nalgorithm do not degrade as the number of the data points increases.\n",
        "  For links with vanishing pairwise linking numbers, the link components bound\npairwise disjoint surfaces in $B^{4}$. In this paper, we describe the set of\ngenera of such surfaces in terms of the $h$-function, which is a link invariant\nfrom Heegaard Floer homology. In particular, we use the $h$-function to give\nlower bounds for the 4-genus of the link. For $L$-space links, the $h$-function\nis explicitly determined by Alexander polynomials of the link and sublinks. We\nshow some $L$-space links where the lower bounds are sharp, and also describe\nall possible genera of disjoint surfaces bounded by such links.\n",
        "  The aim of this article is to present an overview of the major XML\nwarehousing approaches from the literature, as well as the existing approaches\nfor performing OLAP analyses over XML data (which is termed XML-OLAP or XOLAP;\nWang et al., 2005). We also discuss the issues and future trends in this area\nand illustrate this topic by presenting the design of a unified, XML data\nwarehouse architecture and a set of XOLAP operators expressed in an XML\nalgebra.\n",
        "  The existence of the mysterious pseudo-gap state in the phase diagram of\ncopper oxide superconductors and its interplay with unconventional {\\it d-wave}\nsuperconductivity has been a long standing issue for more than a decade. There\nis now a growing number of experimental indications that the pseudo-gap phase\nactually corresponds to a symmetry breaking state. In his theory for cuprates,\nC. M. Varma proposes that the pseudo-gap is a new state of matter associated\nwith the spontaneous appearance of circulating current loops within $\\rm CuO_2$\nunit cell. This intra-unit-cell order breaks time reversal symmetry, but\npreserves lattice translation invariance. Polarized elastic neutron scattering\nmeasurements provide evidence for an intra-unit-cell magnetic order inside the\npseudo-gap state. This order could be produced by the orbital-like magnetic\nmoments induced by the circulating current loops. The magnetic order displays\nthe same characteristic features in $\\rm HgBa_2CuO_{4+\\delta}$, $\\rm\nYBa_2Cu_3O_{6+x}$ and $\\rm Bi_2Sr_2CaCu_2O_{8+\\delta}$ demonstrating that this\ngenuine phase is ubiquitous of the pseudo-gap of high temperature copper oxide\nmaterials. We review the main properties characterizing this intra-unit-cell\nmagnetic order and discuss its interplay or competition with other spin and\ncharge instabilities.\n",
        "  Traditionally evolution is seen as a process where from a pool of possible\nvariations of a population (e.g. biological species or industrial goods) a few\nvariations get selected which survive and proliferate, whereas the others\nvanish. Survival probability is typically associated with the 'fitness' of a\nparticular variation. In this paper we argue that the notion of fitness is an a\nposteriori concept, in the sense that one can assign higher fitness to species\nthat survive but one can generally not derive or even measure fitness - or\nfitness landscapes - per se. For this reason we think that in a 'physical'\ntheory of evolution such notions should be avoided. In this spirit, here we\npropose a random matrix model of evolution where selection mechanisms are\nencoded in the interaction matrices of species. We are able to recover some key\nfacts of evolution dynamics, such as punctuated equilibrium, i.e. the existence\nof intrinsic large extinctions events, and, at the same time, periods of\ndramatic diversification, as known e.g. from fossil records. Further, we\ncomment on two fundamental technical problems of a 'physics of evolution', the\nnon-closedness of its phase space and the problem of co-evolving boundary\nconditions, apparent in all systems subject to evolution.\n",
        "  A combination of x-ray diffraction, magnetization, and 75As nuclear magnetic\nresonance (NMR) experiments were performed on single-crystal EuFe1.9Co0.1As2.\nThe strength of the hyperfine interaction between the 75As nuclei and the\nEu^(2+) 4f states suggests a strong coupling between the Eu^(2+) moments and\nthe Fe1.9Co0.1As2 layers. Such a strong interlayer coupling may be due to an\nindirect exchange interaction between the localized Eu^(2+) 4f moments,\nmediated by the Fe 3d conduction electrons. Magnetic susceptibility as well as\n75As-NMR measurements reveal a decrease of the SDW transition temperature to\nT_SDW = 120 K as a result of Co doping. A change of the slope in the\ntemperature dependence of the NMR frequency of the 75As lower-satellite line\nwas observed at 225 K. At the same temperature also a change of the satellite\nline shape was found. These changes of the NMR spectra may be caused by the\nformation of a nematic phase below 225 K in EuFe1.9Co0.1As2.\n",
        "  Big data benchmarking is particularly important and provides applicable\nyardsticks for evaluating booming big data systems. However, wide coverage and\ngreat complexity of big data computing impose big challenges on big data\nbenchmarking. How can we construct a benchmark suite using a minimum set of\nunits of computation to represent diversity of big data analytics workloads?\nBig data dwarfs are abstractions of extracting frequently appearing operations\nin big data computing. One dwarf represents one unit of computation, and big\ndata workloads are decomposed into one or more dwarfs. Furthermore, dwarfs\nworkloads rather than vast real workloads are more cost-efficient and\nrepresentative to evaluate big data systems. In this paper, we extensively\ninvestigate six most important or emerging application domains i.e. search\nengine, social network, e-commerce, multimedia, bioinformatics and astronomy.\nAfter analyzing forty representative algorithms, we single out eight dwarfs\nworkloads in big data analytics other than OLAP, which are linear algebra,\nsampling, logic operations, transform operations, set operations, graph\noperations, statistic operations and sort.\n",
        "  Word alignment is an important natural language processing task that\nindicates the correspondence between natural languages. Recently, unsupervised\nlearning of log-linear models for word alignment has received considerable\nattention as it combines the merits of generative and discriminative\napproaches. However, a major challenge still remains: it is intractable to\ncalculate the expectations of non-local features that are critical for\ncapturing the divergence between natural languages. We propose a contrastive\napproach that aims to differentiate observed training examples from noises. It\nnot only introduces prior knowledge to guide unsupervised learning but also\ncancels out partition functions. Based on the observation that the probability\nmass of log-linear models for word alignment is usually highly concentrated, we\npropose to use top-n alignments to approximate the expectations with respect to\nposterior distributions. This allows for efficient and accurate calculation of\nexpectations of non-local features. Experiments show that our approach achieves\nsignificant improvements over state-of-the-art unsupervised word alignment\nmethods.\n",
        "  Functional dependencies (FDs) specify the intended data semantics while\nviolations of FDs indicate deviation from these semantics. In this paper, we\nstudy a data cleaning problem in which the FDs may not be completely correct,\ne.g., due to data evolution or incomplete knowledge of the data semantics. We\nargue that the notion of relative trust is a crucial aspect of this problem: if\nthe FDs are outdated, we should modify them to fit the data, but if we suspect\nthat there are problems with the data, we should modify the data to fit the\nFDs. In practice, it is usually unclear how much to trust the data versus the\nFDs. To address this problem, we propose an algorithm for generating\nnon-redundant solutions (i.e., simultaneous modifications of the data and the\nFDs) corresponding to various levels of relative trust. This can help users\ndetermine the best way to modify their data and/or FDs to achieve consistency.\n",
        "  We investigate the cosmic evolution of the internal structure of massive\nearly-type galaxies over half of the age of the Universe. We perform a joint\nlensing and stellar dynamics analysis of a sample of 81 strong lenses from the\nSL2S and SLACS surveys and combine the results with a hierarchical Bayesian\ninference method to measure the distribution of dark matter mass and stellar\nIMF across the population of massive early-type galaxies. Lensing selection\neffects are taken into account. We find that the dark matter mass projected\nwithin the inner 5 kpc increases for increasing redshift, decreases for\nincreasing stellar mass density, but is roughly constant along the evolutionary\ntracks of early-type galaxies. The average dark matter slope is consistent with\nthat of an NFW profile, but is not well constrained. The stellar IMF\nnormalization is close to a Salpeter IMF at $\\log{M_*} = 11.5$ and scales\nstrongly with increasing stellar mass. No dependence of the IMF on redshift or\nstellar mass density is detected. The anti-correlation between dark matter mass\nand stellar mass density supports the idea of mergers being more frequent in\nmore massive dark matter halos.\n",
        "  We examine the generic phase behavior of high-Tc cuprate superconductors in\nterms a universal van Hove singularity in the strongly overdoped region. Using\na rigid ARPES-derived dispersion we solve the BCS gap equation and show that\nthe pairing interaction or pairing energy cutoff must be a rapidly declining\nfunction of doping. This result is prejudicial to a phonon-based pairing\ninteraction and more consistent with a magnetic or magnetically enhanced\ninteraction.\n",
        "  The total angular momentum associated with the edge mass current flowing at\nthe boundary in the superfluid $^3$He A-phase confined in a disk is proved to\nbe $L=N\\hbar/2$, consisting of $L^{\\rm MJ}=N\\hbar$ from the Majorana\nquasi-particles (QPs) and $L^{\\rm cont}=-N\\hbar/2$ from the continuum state. We\nshow it based on an analytic solution of the chiral order parameter for\nquasi-classical Eilenberger equation. Important analytic expressions are\nobtained for mass current, angular momentum, and density of states (DOS).\nNotably the DOS of the Majorana QPs is exactly $N_0/2$ ($N_0$: normal state\nDOS) responsible for the factor 2 difference between $L^{\\rm MJ}$ and $L^{\\rm\ncont}$. The current decreases as $E^{-3}$ against the energy $E$, and $L(T)\n\\propto -T^2$. This analytic solution is fully backed up by numerically solving\nthe Eilenberger equation. We touch on the so-called intrinsic angular momentum\nproblem.\n",
        "  We have identified the four most significant features in the UV velocity\ndistribution of solar neighborhood stars: H1, H2 in the Hercules stream and W1,\nW2 in the Wolf 630 stream. We have formulated the problem of determining\nseveral characteristics of the central Galactic bar independently from each of\nthe identified features by assuming that the Hercules and Wolf 630 streams are\nof a bar-induced dynamical nature. The problem has been solved by constructing\n2:1 resonant orbits in the rotating bar frame for each star in these streams.\nAnalysis of the resonant orbits found has shown that the bar pattern speed is\n45-55 km/s/kpc, while the bar angle lies within the range 40-60 degrees. The\nresults obtained are consistent with the view that the Hercules and Wolf 630\nstreams could be formed by a long-term influence of the Galactic bar leading to\na characteristic bimodal splitting of the UV velocity plane.\n",
        "  We report measurements of the magnetic susceptibility of twinned single\ncrystals of YBa$_{2}$Cu$_{3}$O$_{6+x}$ from just above their superconducting\ntransition temperatures to 300 K with magnetic fields of up to 5 T applied\nparallel and perpendicular to the CuO$_2$ planes at 7 values of $x$.\nAppropriate analysis allows the relatively small, but still important, Curie\nterms to be separated from other contributions to the susceptibility. Our data\nsupport a picture in which the Curie terms arise from oxygen disorder in the\nCu-O chains. This agrees with published work on polycrystalline samples where\nthe sample cooling rate was varied, but here we show that the Curie plots\nflatten out above 200 K. We identify small effects of charge density wave (CDW)\ninstabilities in the temperature ($T$) derivative of the in-plane\nsusceptibility $d\\chi_{ab}(T)/dT$ and discuss their $x$-dependence. For\n$x=$0.67 we make a detailed comparison with published high energy X-ray\ndiffraction data using a minimal model involving Fermi arcs, thereby obtaining\nvalues for the CDW energy gap and the Helmholtz free energy in a coherence\nvolume. At 80 and 100 K the latter is comparable with, or smaller than $k_BT$\nrespectively, highlighting the probable importance of thermal fluctuations. We\nnote that the effect of the Lorentz force on charge carriers in the Fermi arcs\ncould provide a simple mechanism for enhancing the CDWs in high magnetic\nfields, as suggested by recent experiments.\n",
        "  This work investigates the dose-response curves of GAFCHROMIC EBT, EBT2, and\nEBT3 radiochromic films using synchrotron-produced monochromatic x-ray beams.\nEBT2 film is being utilized for dose verification in photoactivated Auger\nelectron therapy at the Louisiana State University CAMD synchrotron facility.\nMonochromatic beams of 25, 30, and 35 keV were generated on the tomography\nbeamline at CAMD. Ion chamber depth-dose measurements were used to determine\nthe dose delivered to films irradiated at depths from 0.7 to 8.5 cm in a\n10x10x10-cm3 PMMA phantom. AAPM TG-61 protocol was applied to convert measured\nionization into dose. Films were digitized using an Epson 1680 Professional\nflatbed scanner and analyzed using the net optical density (NOD) derived from\nthe red channel. A dose-response curve was obtained at 35 keV for EBT film, and\nat 25, 30, and 35 keV for EBT2 and EBT3 films. Calibrations of films for 4 MV\nx-rays were obtained for comparison using a radiotherapy accelerator at Mary\nBird Perkins Cancer Center. The sensitivity (NOD per unit dose) of EBT film at\n35 keV relative to that for 4-MV x-rays was 0.73 and 0.76 for doses 50 and 100\ncGy, respectively. The sensitivity of EBT2 film at 25, 30, and 35 keV relative\nto that for 4-MV x-rays varied from 1.09 - 1.07, 1.23 - 1.17, and 1.27 - 1.19\nfor doses 50 - 200 cGy, respectively. For EBT3 film the relative sensitivity\nwas within 3% of unity for all three monochromatic x-ray beams. EBT and EBT2\nfilm sensitivity showed strong energy dependence over an energy range of 25 keV\n- 4 MV, although this dependence becomes weaker for larger doses. EBT3 film\nshows weak energy dependence, indicating that it would be a better dosimeter\nfor kV x-ray beams where beam hardening effects can result in large changes in\nthe effective energy.\n",
        "  Evaluating query predicates on data samples is the only way to estimate their\nselectivity in certain scenarios. Finding a guaranteed optimal query plan is\nnot a reasonable optimization goal in those cases as it might require an\ninfinite number of samples. We therefore introduce probably approximately\noptimal query optimization (PAO) where the goal is to find a query plan whose\ncost is near-optimal with a certain probability. We will justify why PAO is a\nsuitable formalism to model scenarios in which predicate sampling and\noptimization need to be interleaved.\n  We present the first algorithm for PAO. Our algorithm is non-intrusive and\nuses standard query optimizers and sampling components as sub-functions. It is\ngeneric and can be applied to a wide range of scenarios. Our algorithm is\niterative and calculates in each iteration a query plan together with a region\nin the selectivity space where the plan has near-optimal cost. It determines\nthe confidence that the true selectivity values fall within the aforementioned\nregion and chooses the next samples to take based on the current state if the\nconfidence does not reach the threshold specified as problem input. We devise\ndifferent algorithm variants and analyze their complexity. We experimentally\ncompare them in terms of the number of optimizer invocations, samples, and\niterations over many different query classes.\n",
        "  This paper present several refinements of the Datalog +/- framework based on\nresolution and Datalog-rewriting. We first present a resolution algorithm which\nis complete for arbitrary sets of tgds and egds. We then show that a technique\nof saturation can be used to achieve completeness with respect to First-Order\n(FO) query rewriting. We then investigate the class of guarded tgds (with a\nloose definition of guardedness), and show that every set of tgds in this class\ncan be rewritten into an equivalent set of standard Datalog rules. On the\nnegative side, this implies that Datalog +/- has (only) the same expressive\npower as standard Datalog in terms of query answering. On the positive side\nhowever, this mean that known results and existing optimization techniques\n(such as Magic-Set) may be applied in the context of Datalog +/- despite its\nricher syntax.\n",
        "  The goal of this paper is to construct distinct trisections of the same genus\non a fixed 4-manifold. For every $k \\geq 2$, we construct $2^{k}-1$\nnon-diffeomorphic $(3k,k)$-trisections on infinitely many 4-manifolds. Here,\nthe manifolds are spun Seifert fiber spaces and the trisections come from\nMeier's spun trisections. The technique used to distinguish the trisections\nparallels an established technique for distinguishing Heegaard splittings. In\nparticular, we show that the Nielsen classes of the generators of the\nfundamental group, obtained from spines of the 4-dimensional 1-handlebodies of\nthe trisection, are isotopy invariants of the trisection. If we additionally\nconsider the action of the automorphism group on the Nielsen classes, we obtain\ndiffeomorphism invariants of trisections.\n",
        "  By measuring the magnetization hysteresis loops of superconducting\nBa0.6K0.4Fe2As2 single crystals, we obtained the high upper critical field and\nlarge current carrying ability, which point to optimistic applications. The\nfishtail (or second peak) effect is also found in the material, and the\nposition of the vortex pinning force shows a maximum at 1/3 of the reduced\nfield, being consistent with the picture of vortex pinning by small size normal\ncores in the sample. Together with the resistive measurements, for the first\ntime the vortex phase diagram is obtained for superconductor Ba0.6K0.4Fe2As2.\n",
        "  The dynamics of a mosquito population depends heavily on climatic variables\nsuch as temperature and precipitation. Since climate change models predict that\nglobal warming will impact on the frequency and intensity of rainfall, it is\nimportant to understand how these variables affect the mosquito populations. We\npresent a model of the dynamics of a {\\it Culex quinquefasciatus} mosquito\npopulation that incorporates the effect of rainfall and use it to study the\ninfluence of the number of rainy days and the mean monthly precipitation on the\nmaximum yearly abundance of mosquitoes $M_{max}$. Additionally, using a\nfracturing process, we investigate the influence of the variability in daily\nrainfall on $M_{max}$. We find that, given a constant value of monthly\nprecipitation, there is an optimum number of rainy days for which $M_{max}$ is\na maximum. On the other hand, we show that increasing daily rainfall\nvariability reduces the dependence of $M_{max}$ on the number of rainy days,\nleading also to a higher abundance of mosquitoes for the case of low mean\nmonthly precipitation. Finally, we explore the effect of the rainfall in the\nmonths preceding the wettest season, and we obtain that a regimen with high\nprecipitations throughout the year and a higher variability tends to advance\nslightly the time at which the peak mosquito abundance occurs, but could\nsignificantly change the total mosquito abundance in a year.\n",
        "  The meaning of null in relational databases is a major source of confusion\nnot only among database users but also among database textbook writers. The\npurpose of this article is to examine what database nulls could mean and to\nmake some modest suggestions about how to reduce the confusion.\n",
        "  While neural machine translation (NMT) is making good progress in the past\ntwo years, tens of millions of bilingual sentence pairs are needed for its\ntraining. However, human labeling is very costly. To tackle this training data\nbottleneck, we develop a dual-learning mechanism, which can enable an NMT\nsystem to automatically learn from unlabeled data through a dual-learning game.\nThis mechanism is inspired by the following observation: any machine\ntranslation task has a dual task, e.g., English-to-French translation (primal)\nversus French-to-English translation (dual); the primal and dual tasks can form\na closed loop, and generate informative feedback signals to train the\ntranslation models, even if without the involvement of a human labeler. In the\ndual-learning mechanism, we use one agent to represent the model for the primal\ntask and the other agent to represent the model for the dual task, then ask\nthem to teach each other through a reinforcement learning process. Based on the\nfeedback signals generated during this process (e.g., the language-model\nlikelihood of the output of a model, and the reconstruction error of the\noriginal sentence after the primal and dual translations), we can iteratively\nupdate the two models until convergence (e.g., using the policy gradient\nmethods). We call the corresponding approach to neural machine translation\n\\emph{dual-NMT}. Experiments show that dual-NMT works very well on\nEnglish$\\leftrightarrow$French translation; especially, by learning from\nmonolingual data (with 10% bilingual data for warm start), it achieves a\ncomparable accuracy to NMT trained from the full bilingual data for the\nFrench-to-English translation task.\n",
        "  Age structure is incorporated in many types of epidemic model. Often it is\nconvenient to assume that such models converge to early asymptotic behaviour\nquickly, before the susceptible population has been appreciably depleted. We\nmake use of dynamical systems theory to show that for some reasonable parameter\nvalues, this convergence can be slow. Such a possibility should therefore be\nconsidered when parameterising age-structured epidemic models.\n",
        "  In todays world there is a wide availability of huge amount of data and thus\nthere is a need for turning this data into useful information which is referred\nto as knowledge. This demand for knowledge discovery process has led to the\ndevelopment of many algorithms used to determine the association rules. One of\nthe major problems faced by these algorithms is generation of candidate sets.\nThe FP Tree algorithm is one of the most preferred algorithms for association\nrule mining because it gives association rules without generating candidate\nsets. But in the process of doing so, it generates many CP trees which\ndecreases its efficiency. In this research paper, an improvised FP tree\nalgorithm with a modified header table, along with a spare table and the MFI\nalgorithm for association rule mining is proposed. This algorithm generates\nfrequent item sets without using candidate sets and CP trees.\n",
        "  We calculated the polarization degree of hydrogen Balmer broad emission lines\nfrom a number of active galactic nuclei (AGNs) with determined virial factors.\nThe objects were selected from the sample presented by Decarli et al.(2008). In\nour calculations, we used the model of the flattened disc-like structure of the\nbroad-line emission region (BLR). In this model, the expression for the virial\nfactor makes it possible to determine the inclination angle for the flattened\nBLR, which in turn yields the polarization degree of the broad emission lines.\nAs a result, we obtained the direct relation between the polarization degree\nand the virial factor. We also compared the determined values of the\npolarization degree with those obtained in polarimetric observations.\n",
        "  The real part of the in-plane optical self-energy data in underdoped\nBi$_{2}$Sr$_{2}$CaCu$_{2}$O$_{8+\\delta}$ (Bi-2212) and ortho II\nYBa$_{2}$Cu$_{3}$O$_{6.5}$ contains new and important information on the\npseudogap. Using a theoretical model approach we find that the density of state\nlost below the pseudogap $\\Delta_{pg}$ is accompanied with a pileup just above\nthis energy. The pileup along with a sharp mode in the bosonic spectral\nfunction leads to an unusually rapid increase in the optical scattering rate\nand a characteristically sloped peak in the real part of the optical\nself-energy. These features are not found in optimally doped and overdoped\nsamples and represent a clearest signature so far of the opening of a pseudogap\nin the in-plane optical conductivity.\n",
        "  User preference queries are very important in spatial databases. With the\nhelp of these queries, one can found best location among points saved in\ndatabase. In many situation users evaluate quality of a location with its\ndistance from its nearest neighbor among a special set of points. There has\nbeen less attention about evaluating a location with its distance to nearest\nneighbors in spatial user preference queries. This problem has application in\nmany domains such as service recommendation systems and investment planning.\nRelated works in this field are based on top-k queries. The problem with top-k\nqueries is that user must set weights for attributes and a function for\naggregating them. This is hard for him in most cases. In this paper a new type\nof user preference queries called spatial nearest neighbor skyline queries will\nbe introduced in which user has some sets of points as query parameters. For\neach point in database attributes are its distances to the nearest neighbors\nfrom each set of query points. By separating this query as a subset of dynamic\nskyline queries N2S2 algorithm is provided for computing it. This algorithm has\ngood performance compared with the general branch and bound algorithm for\nskyline queries.\n",
        "  Purpose: The purpose of the work was experimental investigations of the\nbreast dose distributions with adaptive filtration. Adaptive filtration reduces\ndetector dynamic range and improves image quality. The adaptive filter with\npredetermined shape is placed at the x-ray beam such that the x-ray intensity\nat the detector surface is flat. However, adaptive filter alters the mean dose\nto the breast, as well as volume distribution of the dose. Methods: The dose\nwas measured using a 14 cm diameter cylindrical acrylic breast phantom. An\nacrylic adaptive filter was fabricated to match the 14 cm diameter of the\nphantom. The dose was measured using ion chamber inserted into holes\ndistributed along the radius of the phantom from the center to the edge. The\nradial distribution of dose was measured and fitted by an analytical function\nand the volume distribution and mean value of dose was calculated. The\nmeasurements were performed at 40, 60, 90, and 120 kVp tube voltages and 6.6\nmGy air kerma. Results: The adaptive filter decreased mean breast dose by a\nfactor of 1.6-2.2 depending on the tube voltage. To maintain image quality, the\ntube output was increased by these factors when adaptive filter was used. The\nadaptive filter provided uniform volume distribution of dose at 40 kVp and 60\nkVp tube voltages, but the distribution was non-uniform at 90 kVp and 120 kVp\ntube voltages. Conclusion: While adaptive filtration helps with detector\ndynamic range, CT number uniformity, and CT noise uniformity, the volume\nnon-uniformity of the dose with adaptive filter depends on tube voltage. On the\nother hand, at the same average breast dose, volume non-uniformity of the dose\nwith adaptive filter was not higher that that without adaptive filter. The\nresults allowed also to determine necessary dose increase factor to compensate\nfor x-ray absorption in the adaptive filter.\n",
        "  We first observe a potential weakness of continuous vector representations of\nsymbols in neural machine translation. That is, the continuous vector\nrepresentation, or a word embedding vector, of a symbol encodes multiple\ndimensions of similarity, equivalent to encoding more than one meaning of the\nword. This has the consequence that the encoder and decoder recurrent networks\nin neural machine translation need to spend substantial amount of their\ncapacity in disambiguating source and target words based on the context which\nis defined by a source sentence. Based on this observation, in this paper we\npropose to contextualize the word embedding vectors using a nonlinear\nbag-of-words representation of the source sentence. Additionally, we propose to\nrepresent special tokens (such as numbers, proper nouns and acronyms) with\ntyped symbols to facilitate translating those words that are not well-suited to\nbe translated via continuous vectors. The experiments on En-Fr and En-De reveal\nthat the proposed approaches of contextualization and symbolization improves\nthe translation quality of neural machine translation systems significantly.\n",
        "  Laser cooling of the atomic motion paved the way for remarkable achievements\nin the fields of quantum optics and atomic physics, including Bose-Einstein\ncondensation and the trapping of atoms in optical lattices. More recently\nsuperconducting qubits were shown to act as artificial two-level atoms,\ndisplaying Rabi oscillations, Ramsey fringes, and further quantum effects.\nCoupling such qubits to resonators brought the superconducting circuits into\nthe realm of quantum electrodynamics (circuit QED). It opened the perspective\nto use superconducting qubits as micro-coolers or to create a population\ninversion in the qubit to induce lasing behavior of the resonator. Furthering\nthese analogies between quantum optical and superconducting systems we\ndemonstrate here Sisyphus cooling of a low frequency LC oscillator coupled to a\nnear-resonantly driven superconducting qubit. In the quantum optics setup the\nmechanical degrees of freedom of an atom are cooled by laser driving the atom's\nelectronic degrees of freedom. Here the roles of the two degrees of freedom are\nplayed by the LC circuit and the qubit's levels, respectively. We also\ndemonstrate the counterpart of the Sisyphus cooling, namely Sisyphus\namplification. Parallel to the experimental demonstration we analyze the system\ntheoretically and find quantitative agreement, which supports the\ninterpretation and allows us to estimate system parameters.\n",
        "  The abundance of predicted and mined but uncertain biological data show huge\nneeds for massive, efficient and scalable curation efforts. The human expertise\nwarranted by any successful curation enterprize is often economically\nprohibitive especially for speculative end user queries that may not ultimately\nbear fruit. So the challenge remains in devising a low cost engine capable of\ndelivering fast but tentative annotation and curation of a set of data items\nthat can be authoritatively validated by experts later demanding significantly\nsmall investment. The aim thus is to make a large volume of predicted data\navailable for use as early as possible with an acceptable degree of confidence\nin their accuracy while the curation continues. In this paper, we present a\nnovel approach to annotation and curation of biological database contents using\ncrowd computing. The technical contribution is in the identification and\nmanagement of trust of mechanical turks, and support for ad hoc declarative\nqueries, both of which are leveraged to support reliable analytics using noisy\npredicted interactions.\n",
        "  This paper presents a tree-to-tree transduction method for sentence\ncompression. Our model is based on synchronous tree substitution grammar, a\nformalism that allows local distortion of the tree topology and can thus\nnaturally capture structural mismatches. We describe an algorithm for decoding\nin this framework and show how the model can be trained discriminatively within\na large margin framework. Experimental results on sentence compression bring\nsignificant improvements over a state-of-the-art model.\n",
        "  Optimised population synthesis provides an empirical method to extract the\nrelative mix of stellar evolutionary stages and the distribution of atmospheric\nparameters within unresolved stellar systems, yet a robust validation of this\nmethod is still lacking. We here provide a calibration of population synthesis\nvia non-linear bound-constrained optimisation of stellar populations based upon\noptical spectra of mock stellar systems and observed Galactic Globular Clusters\n(GGCs). The MILES stellar library is used as a basis for mock spectra as well\nas templates for the synthesis of deep GGC spectra from Schiavon et al. (2005).\nOptimised population synthesis applied to mock spectra recovers mean\nlight-weighted stellar atmospheric parameters to within a mean uncertainty of\n240 K, 0.04 dex, and 0.03 dex for T_eff, log(g), and [Fe/H], respectively.\nDecompositions of both mock and GGC spectra confirm the method's ability to\nrecover the expected mean light-weighted metallicity in dust-free conditions\n(E[B-V] < 0.15) with uncertainties comparable to evolutionary population\nsynthesis methods. Dustier conditions require either appropriate dust-modelling\nwhen fitting to the full spectrum, or fitting only to select spectral features.\nWe derive light-weighted fractions of stellar evolutionary stages from our\npopulation synthesis fits to GGCs, yielding on average a combined 25+/-6 per\ncent from main sequence and turnoff dwarfs, 64+/-7 per cent from subgiant, red\ngiant and asymptotic giant branch stars, and 15+/-7 per cent from horizontal\nbranch stars and blue stragglers. Excellent agreement is found between these\nfractions and those estimated from deep HST/ACS CMDs. Overall, optimised\npopulation synthesis remains a powerful tool for understanding the stellar\npopulations within the integrated light of galaxies and globular clusters.\n",
        "  Linearly polarized Galactic synchrotron emission provides valuable\ninformation about the properties of the Galactic magnetic field and the\ninterstellar magneto-ionic medium, when Faraday rotation along the line of\nsight is properly taken into account. We aim to survey the Galactic plane at 6\ncm including linear polarization. At such a short wavelength Faraday rotation\neffects are in general small and the Galactic magnetic field properties can be\nprobed to larger distances than at long wavelengths. The Urumqi 25-m telescope\nis used for a sensitive 6 cm survey in total and polarized intensities. WMAP\nK-band (22.8 GHz) polarization data are used to restore the absolute zero-level\nof the Urumqi U and Q maps by extrapolation. Total intensity and polarization\nmaps are presented for a Galactic plane region of 129 degree < l < 230 degree\nand |b| < 5 degree in the anti-centre with an angular resolution of 9'5 and an\naverage sensitivity of 0.6 mK and 0.4 mK Tb in total and polarized intensity,\nrespectively. We briefly discuss the properties of some extended Faraday\nScreens detected in the 6 cm polarization maps. The Sino-German 6 cm\npolarization survey provides new information about the properties of the\nmagnetic ISM. The survey also adds valuable information for discrete Galactic\nobjects and is in particular suited to detect extended Faraday Screens with\nlarge rotation measures hosting strong regular magnetic fields.\n",
        "  Functional brain imaging allows measuring dynamic functionality in all brain\nregions. It is broadly used in clinical cognitive neuroscience as, well as in\nresearch. It will allow the observation of neural activities in the brain\nsimultaneously. From the beginning when functional brain imaging was initiated\nby the mapping of brain functions proposed by phrenologists, many scientists\nwere asking why we need to image brain functionality since we have already\nstructural information. Simply, their important question was including a great\nanswer. Functional information of the human brain would definitely complement\nstructural information, helping to have a better understanding of what is\nhappening in the brain. This paper, which could be useful to those who have an\ninterest in functional brain imaging, such as engineers, will present a quick\nreview of modalities used in functional brain imaging. We will concentrate on\nthe most used techniques in functional imaging which are functional magnetic\nresonance imaging (fMRI) and functional optical imaging, which is one of\nnovelties in this area of study.\n",
        "  A mutator is an allele that increases the mutation rate throughout the genome\nby disrupting some aspect of DNA replication or repair. Mutators that increase\nthe mutation rate by the order of 100 fold have been observed to spontaneously\nemerge and achieve high frequencies in natural populations and in long-term\nlaboratory evolution experiments with \\textit{E. coli}. In principle, the\nfixation of mutator alleles is limited by (i) competition with mutations in\nwild-type backgrounds, (ii) additional deleterious mutational load, and (iii)\nrandom genetic drift. Using a multiple locus model and employing both\nsimulation and analytic methods, we investigate the effects of these three\nfactors on the fixation probability $P_{fix}$ of an initially rare mutator as a\nfunction of population size $N$, beneficial and deleterious mutation rates, and\nthe strength of mutations $s$. Our diffusion based approximation for $P_{fix}$\nsuccessfully captures effects (ii) and (iii) when selection is fast compared to\nmutation ($\\mu/s \\ll 1$). This enables us to predict the conditions under which\nmutators will be evolutionarily favored. Surprisingly, our simulations show\nthat effect (i) is typically small for strong-effect mutators. Our results\nagree semi-quantitatively with existing laboratory evolution experiments and\nsuggest future experimental directions.\n",
        "  The purpose of this article is two-fold: We first give a more elementary\nproof of a recent theorem of Korkmaz, Monden, and the author, which states that\nthe commutator length of the n-th power of a Dehn twist along a boundary\nparallel curve on a surface with boundary S of genus g at least two is the\nfloor of (|n|+3)/2 in the mapping class group of S. The alternative proof we\nprovide goes through push maps and Morita's use of Milnor-Wood inequalities, in\nparticular it does not appeal to gauge theory. In turn, we produce infinite\nfamilies of pairwise non-homotopic 4-manifolds admitting genus g surface\nbundles over genus h surfaces with distinguished sections which are flat but\nadmit no flat connections for which the sections are flat, for every fixed\npairs of integers g and h at least two. The latter result generalizes a theorem\nof Bestvina, Church, and Souto, and allows us to obtain a simple proof of\nMorita's non-lifting theorem (for an infinite family of non-conjugate\nsubgroups) in the case of marked surfaces.\n",
        "  One dimensional hybrid systems play an important role in the search for\ntopological superconductivity. Nevertheless, all one dimensional hybrid systems\nso far have been externally defined. Here we show that one-dimensional domain\nwall in a nematic superconductor can serve as an emergent hybrid system in the\npresence of spin-orbit coupling. As a concrete setting we study the domain wall\nbetween nematic domains in FeSe, which is well established to be a nematic\nsuperconductor. We first show on the symmetry grounds that spin-triplet pairing\ncan be induced at the domain wall by constructing a Ginzburg-Landau theory. We\nthen demonstrate using Bogoliubov-de Gennes approach that such nematic domain\nwall supports zero energy bound states which would satisfy Majorana condition.\nWell-known existence of these domain walls at relatively high temperatures,\nwhich can in principle be located and investigated with scanning tunneling\nmicroscopy, presents new opportunities for a search for realization of Majorana\nbound states.\n",
        "  Let $F$ be a leafwise hyperbolic taut foliation of a closed 3-manifold $M$\nand let $L$ be the leaf space of the pullback of $F$ to the universal cover of\n$M$. We show that if $F$ has branching, then the natural action of $\\pi_1(M)$\non $L$ is faithful. We also show that if $F$ has a finite branch locus $B$\nwhose stabilizer acts on $B$ nontrivially, then the stabilizer is an infinite\ncyclic group generated by an indivisible element of $\\pi_1(M)$.\n",
        "  Observational studies of halo stars during the last two decades have placed\nsome limits on the quantity and nature of accreted dwarf galaxy contributions\nto the Milky Way stellar halo by typically utilizing stellar phase-space\ninformation to identify the most recent halo accretion events. In this study we\ntested the prospects of using 2-D chemical abundance ratio distributions\n(CARDs) found in stars of the stellar halo to determine its formation history.\nFirst, we used simulated data from eleven \"MW-like\" halos to generate satellite\ntemplate sets of 2-D CARDs of accreted dwarf satellites which are comprised of\naccreted dwarfs from various mass regimes and epochs of accretion. Next, we\nrandomly drew samples of $\\sim10^{3-4}$ mock observations of stellar chemical\nabundance ratios ([$\\alpha$/Fe], [Fe/H]) from those eleven halos to generate\nsamples of the underlying densities for our CARDs to be compared to our\ntemplates in our analysis. Finally, we used the expectation-maximization\nalgorithm to derive accretion histories in relation to the satellite template\nset (STS) used and the sample size. For certain STS used we typically can\nidentify the relative mass contributions of all accreted satellites to within a\nfactor of 2. We also find that this method is particularly sensitive to older\naccretion events involving low-luminous dwarfs e.g. ultra-faint dwarfs -\nprecisely those events that are too ancient to be seen by phase-space studies\nof stars and too faint to be seen by high-z studies of the early Universe.\nSince our results only exploit two chemical dimensions and near-future surveys\npromise to provide $\\sim6-9$ dimensions, we conclude that these new\nhigh-resolution spectroscopic surveys of the stellar halo will allow us to\nrecover its accretion history - and the luminosity function of infalling dwarf\ngalaxies - across cosmic time.\n",
        "  We present Herschel Space Observatory photometric observations of the unique,\nlong-period eclipsing binary star Epsilon Aurigae. Its extended spectral energy\ndistribution is consistent with our previously published cool (550 K) dust disk\nmodel. We also present an archival infrared spectral energy distribution of the\nside of the disk facing the bright F-type star in the binary, which is\nconsistent with a warmer (1150 K) disk model. The lack of strong molecular\nemission features in the Herschel bands suggests that the disk has a low\ngas-to-dust ratio. The spectral energy distribution and Herschel images imply\nthat the 250 GHz radio detection reported by Altenhoff et al. is likely\ncontaminated by infrared-bright, extended background emission associated with a\nnearby nebular region and should be considered an upper limit to the true flux\ndensity of Epsilon Aur.\n",
        "  Virophages are viruses that rely on the replication machinery of other\nviruses to reproduce within eukaryotic hosts. Two different modes of\ncoinfection have been posited based on experimental observation. In one mode,\nthe virophage and virus enter the host independently. In the other mode, the\nvirophage adheres to the virus so both virophage and virus enter the host\ntogether. Here we ask: what are the ecological effects of these different modes\nof coinfection? In particular, what ecological effects are common to both\ninfection modes, and what are the differences particular to each mode? We\ndevelop a pair of biophysically motivated ODE models of viral-host population\ndynamics, corresponding to dynamics arising from each mode of infection. We\nfind both modes of coinfection allow for the coexistence of the virophage,\nvirus, and host either at a stable fixed point or through cyclical dynamics. In\nboth models, virophage tend to be the most abundant population and their\npresence always reduces the viral abundance and increases the host abundance.\nHowever, we do find qualitative differences between models. For example, via\nextensive sampling of biologically relevant parameter space, we only observe\nbistability when the virophage and virus enter the host together. We discuss\nhow such differences may be leveraged to help identify modes of infection in\nnatural environments from population level data.\n",
        "  We explore the possibility that the ionic electron polarizabilities of the\noxygen ions in the cuprates and the bismutates and the polarizabilities of As\nand Se ions in the iron pnictides contribute to charge carrier pairing leading\nto high Tc superconductivity. Using the fact that the ionic polarization\nresponse to a change in the electric field is practically instantaneous we\nfind, that the inter carrier electrostatic potential is attractive in a limited\ndistance range. This potential is used to calculate quantum mechanically the\ncooper-like pairing energy and wavefunction and the gap energy showing they are\nconsistent with pairing and gap energies of high Tc superconductors.\nQualitative considerations suggest that this model may explain a number of\nimportant features of high Tc superconductors.\n",
        "  The introduction of a exponential or power law gradient in the interstellar\nmedium (ISM) allows to produce an asymmetric evolution of the supernova remnant\n(SNR) when the framework of the thin layer approximation is adopted.\nUnfortunately both the exponential and power law gradients for the ISM do not\nhave a well defined physical meaning. The physics conversely is well\nrepresented by an isothermal self-gravitating disk of particles whose velocity\nis everywhere Maxwellian. . We derived a law of motion in the framework of the\nthin layer approximation with a control parameter of the swept mass. The\nphoton's losses ,that are often neglected in the thin layer approximation, are\nmodeled trough a velocity dependence. The developed framework is applied to SNR\n1987A and the three observed rings are simulated.\n",
        "  We consider the single particle spectral function for a two-dimensional clean\nsuperconductor in a regime of strong critical thermal phase fluctuations. In\nthe limit where the maximum of the superconducting gap is much smaller than the\nFermi energy we obtain an exact expression for the spectral function integrated\nover the momentum component perpendicular to the Fermi surface.\n",
        "  We have developed a novel Monte Carlo method for simulating the dynamical\nevolution of stellar systems in arbitrary geometry. The orbits of stars are\nfollowed in a smooth potential represented by a basis-set expansion and\nperturbed after each timestep using local velocity diffusion coefficients from\nthe standard two-body relaxation theory. The potential and diffusion\ncoefficients are updated after an interval of time that is a small fraction of\nthe relaxation time, but may be longer than the dynamical time. Thus our\napproach is a bridge between the Spitzer's formulation of the Monte Carlo\nmethod and the temporally smoothed self-consistent field method. The primary\nadvantages are the ability to follow the secular evolution of shape of the\nstellar system, and the possibility of scaling the amount of two-body\nrelaxation to the necessary value, unrelated to the actual number of particles\nin the simulation. Possible future applications of this approach in galaxy\ndynamics include the problem of consumption of stars by a massive black hole in\na non-spherical galactic nucleus, evolution of binary supermassive black holes,\nand the influence of chaos on the shape of galaxies, while for globular\nclusters it may be used for studying the influence of rotation.\n",
        "  We present SubMillimeter-Array observations of a Keplerian disk around the\nClass I protobinary system L1551 NE in 335 GHz continuum emission and\nsubmillimeter line emission in 13CO (J=3-2) and C18O (J=3-2) at a resolution of\n~120 x 80 AU. The 335-GHz dust-continuum image shows a strong central peak\nclosely coincident with the binary protostars and likely corresponding to\ncircumstellar disks, surrounded by a ~600 x 300 AU feature elongated\napproximately perpendicular to the [Fe II] jet from the southern protostellar\ncomponent suggestive of a circumbinary disk. The 13CO and C18O images confirm\nthat the circumbinary continuum feature is indeed a rotating disk; furthermore,\nthe C18O channel maps can be well modeled by a geometrically-thin disk\nexhibiting Keplerian rotation. We estimate a mass for the circumbinary disk of\n~0.03-0.12 Msun, compared with an enclosed mass of ~0.8 Msun that is dominated\nby the protobinary system. Compared with several other Class I protostars known\nto exhibit Keplerian disks, L1551 NE has the lowest bolometric temperature (~91\nK), highest envelope mass (~0.39 Msun), and the lowest ratio in stellar mass to\nenvelope + disk + stellar mass (~0.65). L1551 NE may therefore be the youngest\nprotostellar object so far found to exhibit a Keplerian disk. Our observations\npresent firm evidence that Keplerian disks around binary protostellar systems,\n``Keplerian circumbinary disks', can exist. We speculate that tidal effects\nfrom binary companions could transport angular momenta toward the inner edge of\nthe circumbinary disk and create the Keplerian circumbinary disk.\n",
        "  The Hyper Suprime-Cam Subaru Strategic Program (HSC SSP) is an excellent\nsurvey for the search for strong lenses, thanks to its area, image quality and\ndepth. We use three different methods to look for lenses among 43,000 luminous\nred galaxies from the Baryon Oscillation Spectroscopic Survey (BOSS) sample\nwith photometry from the S16A internal data release of the HSC SSP. The first\nmethod is a newly developed algorithm, named YATTALENS, which looks for\narc-like features around massive galaxies and then estimates the likelihood of\nan object being a lens by performing a lens model fit. The second method,\nCHITAH, is a modeling-based algorithm originally developed to look for lensed\nquasars. The third method makes use of spectroscopic data to look for emission\nlines from objects at a different redshift from that of the main galaxy. We\nfind 15 definite lenses, 36 highly probable lenses and 282 possible lenses.\nAmong the three methods, YATTALENS, which was developed specifically for this\nproblem, performs best in terms of both completeness and purity. Nevertheless\nfive highly probable lenses were missed by YATTALENS but found by the other two\nmethods, indicating that the three methods are highly complementary. Based on\nthese numbers we expect to find $\\sim$300 definite or probable lenses by the\nend of the HSC SSP.\n",
        "  In many epidemiological models a nonlinear transmission function is used in\nthe form of power law relationship. It is constantly argued that such form\nreflects population heterogeneities including differences in the mixing\npattern, susceptibility, and spatial patchiness, although the function itself\nis considered phenomenological. Comparison with large-scale simulations show\nthat models with this transmission function accurately approximate data from\nhighly heterogeneous sources. In this note we provide a mechanistic derivation\nof the power law transmission function, starting with a simple heterogeneous\nsusceptibles--infectives (SI) model, which is based on a standard mass action\nassumption. We also consider the simplest SI model with separable mixing and\ncompare our results with known results from the literature.\n",
        "  EasyPET is a new concept of a Positron Emission Tomography (PET) scanner\nusing an innovative acquisition method based on two rotation axes for the\nmovement of detector pairs. Due to its simplicity, it is suitable for education\npurposes, to teach students about the PET technology and its basic concepts,\nfrom the radiation detecting and analogue pulse analysis to the coincidence\nsorting and image reconstruction. The concept allows achieving high and uniform\nposition resolution over the whole field of view (FoV), by eliminating parallax\nerrors due to the depth of interaction (DoI), which are typical of ring-based\nPET systems, so quality images are obtained even without state-of-the-art image\nreconstruction algorithms. The technology developed at the University of Aveiro\nwith a patent-pending, is licensed to CAEN S.p.A, and included in the\neducational catalogue of the company. In this work, a simulation toolkit based\nin the Edugate platform was developed to simulate the EasyPET system. It can\nsimulate all the physical aspects of the product, such us the scanning range,\nvariable Field-of-View (FOV), scintillator energy resolution, coincidence time\nand energy window, among others. A simple image reconstruction algorithm based\non Filtered-back-projection (FBP) is implemented. The toolkit allows a quick\nanalysis in classroom of the simulation results. The platform was also used to\nstudy the new EasyPET 3D version, and a simulation of a NEMA NU 4-2008 IQ\nphantom was performed, demonstrating the capability of the platform not only\nfor education purposes but also for research. Patent Universidade de Aveiro:\nPCT/IB2016/051487\n",
        "  To optimize telecom service management, it is necessary that information\nabout telecom services is highly related to the most popular telecom service.\nTo this end, we propose an algorithm for mining target-oriented fuzzy\ncorrelation rules. In this paper, we show that by using the fuzzy statistics\nanalysis and the data mining technology, the target-oriented fuzzy correlation\nrules can be obtained from a given database. We conduct an experiment by using\na sample database from a telecom service provider in Taiwan. Our work can be\nused to assist the telecom service provider in providing the appropriate\nservices to the customers for better customer relationship management.\n",
        "  The Linguistic Annotation Framework (LAF) provides a general, extensible\nstand-off markup system for corpora. This paper discusses LAF-Fabric, a new\ntool to analyse LAF resources in general with an extension to process the\nHebrew Bible in particular. We first walk through the history of the Hebrew\nBible as text database in decennium-wide steps. Then we describe how LAF-Fabric\nmay serve as an analysis tool for this corpus. Finally, we describe three\nanalytic projects/workflows that benefit from the new LAF representation:\n  1) the study of linguistic variation: extract cooccurrence data of common\nnouns between the books of the Bible (Martijn Naaijer); 2) the study of the\ngrammar of Hebrew poetry in the Psalms: extract clause typology (Gino Kalkman);\n3) construction of a parser of classical Hebrew by Data Oriented Parsing:\ngenerate tree structures from the database (Andreas van Cranenburgh).\n",
        "  Since the release of human genome sequences, one of the most important\nresearch issues is about indexing the genome sequences, and the suffix tree is\nmost widely adopted for that purpose. The traditional suffix tree construction\nalgorithms have severe performance degradation due to the memory bottleneck\nproblem. The recent disk-based algorithms also have limited performance\nimprovement due to random disk accesses. Moreover, they do not fully utilize\nthe recent CPUs with multiple cores. In this paper, we propose a fast algorithm\nbased on 'divide-and-conquer' strategy for indexing the human genome sequences.\nOur algorithm almost eliminates random disk accesses by accessing the disk in\nthe unit of contiguous chunks. In addition, our algorithm fully utilizes the\nmulti-core CPUs by dividing the genome sequences into multiple partitions and\nthen assigning each partition to a different core for parallel processing.\nExperimental results show that our algorithm outperforms the previous fastest\nDIGEST algorithm by up to 3.5 times.\n",
        "  We report ALMA Cycle 3 observations in CO isotopes toward a dense core,\nMC27/L1521F in Taurus, which is considered to be at an early stage of multiple\nstar formation in a turbulent environment. Although most of the high-density\nparts of this core are considered to be as cold as $\\sim$10 K, high-angular\nresolution ($\\sim$20 au) observations in $^{12}$CO ($J$ = 3--2) revealed\ncomplex warm ($>$15--60 K) filamentary/clumpy structures with the sizes from a\nfew tens of au to $\\sim$1,000 au. The interferometric observations of $^{13}$CO\nand C$^{18}$O show that the densest part with arc-like morphologies associated\nwith the previously identified protostar and condensations are slightly\nredshifted from the systemic velocity of the core. We suggest that the warm CO\nclouds may be consequences of shock heating induced by interactions among the\ndifferent density/velocity components that originated from the turbulent\nmotions in the core. However, such a small-scale and fast turbulent motion does\nnot correspond to a simple extension of the line-width-size relation (i.e.,\nLarson'{}s law), and thus the actual origin remains to be studied. The\nhigh-angular resolution CO observations are expected to be essential in\ndetecting small-scale turbulent motions in dense cores and to investigate\nprotostar formation therein.\n",
        "  Repeated games have a long tradition in the behavioral sciences and\nevolutionary biology. Recently, strategies were discovered that permit an\nunprecedented level of control over repeated interactions by enabling a player\nto unilaterally enforce linear constraints on payoffs. Here, we extend this\ntheory of \"zero-determinant\" (or, more generally, \"autocratic\") strategies to\nalternating games, which are often biologically more relevant than traditional\nsynchronous games. Alternating games naturally result in asymmetries between\nplayers because the first move matters or because players might not move with\nequal probabilities. In a strictly-alternating game with two players, $X$ and\n$Y$, we give conditions for the existence of autocratic strategies for player\n$X$ when (i) $X$ moves first and (ii) $Y$ moves first. Furthermore, we show\nthat autocratic strategies exist even for (iii) games with randomly-alternating\nmoves. Particularly important categories of autocratic strategies are\nextortionate and generous strategies, which enforce unfavorable and favorable\noutcomes for the opponent, respectively. We illustrate these strategies using\nthe continuous Donation Game, in which a player pays a cost to provide a\nbenefit to the opponent according to a continuous cooperative investment level.\nAsymmetries due to alternating moves could easily arise from dominance\nhierarchies, and we show that they can endow subordinate players with more\nautocratic strategies than dominant players.\n",
        "  We present images of 29 post-starburst quasars (PSQs) from a Hubble Space\nTelescope (\\emph{HST}) Advanced Camera for Surveys (ACS) Wide Field Channel\nSnapshot program. These broad-lined active galactic nuclei (AGN) possess the\nspectral signatures of massive ($M_{burst} \\sim 10^{10} M_{\\odot}$),\nmoderate-aged stellar populations (hundreds of Myrs). Thus, their composite\nnature provides insight into the AGN-starburst connection. We measure\nquasar-to-host galaxy light contributions via semi-automated two-dimensional\nlight profile fits of PSF-subtracted images. We examine the host morphologies,\nas well as, model the separate bulge and disk components. The\n\\emph{HST}/ACS-F606W images reveal an equal number of spiral (13/29) and\nearly-type (13/29) hosts, with the remaining three hosts having indeterminate\nclassifications. AGNs hosted by early-type galaxies have on average greater\nluminosity than those hosted by spiral galaxies. Disturbances, such as tidal\ntails, shells, star-forming knots, and asymmetries are seen as signposts of\ninteraction/merger activity. Disturbances such as these were found in 17 of the\n29 objects and are evenly distributed among early-type and spiral galaxies. Two\nof these systems are clearly merging with their companions. Compared to other\nAGN of similar luminosity and redshift these PSQs have a higher fraction of\nearly-type hosts and disturbances. Our most luminous objects with disturbed\nearly-type host galaxies appear to be consistent with merger products. Thus,\nthese luminous disturbed galaxies may represent a phase in an evolutionary\nscenario for merger driven activity and of hierarchical galaxy evolution. Our\nless luminous objects appear to be consistent with Seyfert galaxies not\nrequiring triggering by major mergers. Many of these Seyferts are barred spiral\ngalaxies.\n",
        "  To investigate the superconducting (SC) state near a charge instability, we\nperformed ^{13}C NMR experiments on the molecular superconductor\nbeta\"-(BEDT-TTF)_{4}[(H_{3}O)Ga(C_{2}O_{4})_{3}]C_{6}H_{5}NO_{2}, which\nexhibits a charge anomaly at 100 K. The Knight shift which we measured in the\nSC state down to 1.5 K demonstrates that Cooper pairs are in spin-singlet\nstate. Measurements of the nuclear spin-lattice relaxation time reveal strong\nelectron-electron correlations in the normal state. The resistivity increase\nobserved below 10 K indicates that the enhanced fluctuation has an electric\norigin. We discuss the possibility of charge-fluctuation-induced\nsuperconductivity.\n",
        "  We present a quantitative spectroscopic study of twenty-seven red supergiants\nin the Sculptor Galaxy NGC 300. J-band spectra were obtained using KMOS on the\nVLT and studied with state of the art synthetic spectra including NLTE\ncorrections for the strongest diagnostic lines. We report a central metallicity\nof [Z]= -0.03 +/- 0.05 with a gradient of -0.083 +/- 0.014 [dex/kpc], in\nagreement with previous studies of blue supergiants and H II-region auroral\nline measurements. This result marks the first application of the J-band\nspectroscopic method to a population of individual red supergiant stars beyond\nthe Local Group of galaxies and reveals the great potential of this technique.\n",
        "  A knot in the three-sphere is doubly slice if it is the cross-section of an\nunknotted two-sphere in the four-sphere. For low-crossing knots, the most\ncomplete work to date gives a classification of doubly slice knots through 9\ncrossings. We extend that work through 12 crossings, resolving all but four\ncases among the 2,977 prime knots in that range. The techniques involved in\nthis analysis include considerations of the Alexander module and signature\nfunctions as well as calculations of the twisted Alexander polynomials for\nhigher-order branched covers. We give explicit illustrations of the double\nslicing for each of the 20 knots shown to be smoothly doubly slice. We place\nthe study of doubly slice knots in a larger context by introducing the double\nslice genus of a knot.\n",
        "  The transition temperature Tc~26 K of the recently discovered superconductor\nLaFeAs(O,F) has been demonstrated to be extremely sensitive to the lanthanide\nion, reaching 55 K for the Sm containing oxypnictides. Therefore, it is\nimportant to determine how the moment on the lanthanide affects the overall\nmagnetism in these systems. Here we report a neutron diffraction study of the\nNd oxypnictides. Long ranged antiferromagnetic order is apparent in NdFeAsO\nbelow 1.96 K. Rietveld refinement shows that both Fe and Nd magnetic ordering\nare required to describe the observed data with the staggered moment 1.55(4)\nBohr magneton per Nd and 0.9(1) Bohr magneton per Fe at 0.3 K. The other\nstructural properties such as the tetragonal-orthorhombic distortion are found\nto be very similar to those in LaFeAsO. Neither the magnetic ordering nor the\nstructural distortion occur in the superconducting sample NdFeAsO0.80F0.20 at\nany temperatures down to 1.5 K.\n",
        "  Evolutionary game theory is a powerful framework for studying evolution in\npopulations of interacting individuals. A common assumption in evolutionary\ngame theory is that interactions are symmetric, which means that the players\nare distinguished by only their strategies. In nature, however, the microscopic\ninteractions between players are nearly always asymmetric due to environmental\neffects, differing baseline characteristics, and other possible sources of\nheterogeneity. To model these phenomena, we introduce into evolutionary game\ntheory two broad classes of asymmetric interactions: ecological and genotypic.\nEcological asymmetry results from variation in the environments of the players,\nwhile genotypic asymmetry is a consequence of the players having differing\nbaseline genotypes. We develop a theory of these forms of asymmetry for games\nin structured populations and use the classical social dilemmas, the Prisoner's\nDilemma and the Snowdrift Game, for illustrations. Interestingly, asymmetric\ngames reveal essential differences between models of genetic evolution based on\nreproduction and models of cultural evolution based on imitation that are not\napparent in symmetric games.\n",
        "  Previous studies indicated that the physical state of cellular water could be\nsignificantly different from pure liquid water. To experimentally investigate\nthis possibility, we conducted a series of spin-echo NMR measurements on water\nprotons in rat skeletal muscle. Our result indicated that the spin-lattice\nrelaxation time and the spin-spin relaxation time of cellular water protons are\nboth significantly shorter than that of pure water (by 4.3-fold and 34-fold,\nrespectively). Furthermore, the spin diffusion coefficient of water proton is\nalmost 1/2 of that of pure water. These data suggest that cellular water is in\na more ordered state in comparison to pure water.\n",
        "  The results from muon spin relaxation experiments on the non-centrosymmetric\nintermetallic superconductor LaNiC$_2$ are reported. We find that the onset of\nsuperconductivity coincides with the appearance of spontaneous magnetic fields,\nimplying that in the superconducting state time reversal symmetry is broken. An\nanalysis of the possible pairing symmetries suggests only four triplet states\ncompatible with this observation, all of which are non-unitary. They include\nthe intriguing possibility of triplet pairing with the full point group\nsymmetry of the crystal, which is only possible in a non-centrosymmetric\nsuperconductor.\n",
        "  We present spectroscopy of 880 galaxies within a 2-degree field around the\nmassive, merging cluster Abell 3266. This sample, which includes 704 new\nmeasurements, was combined with the existing redshifts measurements to generate\na sample of over 1300 spectroscopic redshifts; the largest spectroscopic sample\nin the vicinity of A3266 to date. We define a cluster sub-sample of 790\nredshifts which lie within a velocity range of 14,000 to 22,000 kms$^{-1}$ and\nwithin 1 degree of the cluster centre. A detailed structural analysis finds\nA3266 to have a complex dynamical structure containing six groups and filaments\nto the north of the cluster as well as a cluster core which can be decomposed\ninto two components split along a northeast-southwest axis, consistent with\nprevious X-ray observations. The mean redshift of the cluster core is found to\nbe $0.0594 \\pm 0.0005$ and the core velocity dispersion is given as\n$1462^{+99}_{-99}$ kms$^{-1}$. The overall velocity dispersion and redshift of\nthe entire cluster and related structures are $1337^{+67}_{-67}$ kms$^{-1}$ and\n$0.0596 \\pm 0.0002$, respectively, though the high velocity dispersion does not\nrepresent virialised motions but rather is due to relative motions of the\ncluster components. We posit A3266 is seen following a merger along the\nnortheast southwest axis, however, the rich substructure in the rest of the\ncluster suggests that the dynamical history is more complex than just a simple\nmerger with a range of continuous dynamical interactions taking place. It is\nthus likely that turbulence in A3266 is very high, even for a merging cluster.\n",
        "  We study the relation between stellar ages and vertical velocity dispersion\n(the age-velocity relation, or AVR) in a sample of seven simulated disc\ngalaxies. In our simulations, the shape of the AVR for stars younger than 9 Gyr\ndepends strongly on the merger history at low redshift, with even 1:10 - 1:15\nmergers being able to create jumps in the AVR (although these jumps might not\nbe detectable if the errors on stellar ages are on the order of 30%). For\ngalaxies with a quiescent history at low redshift, we find that the vertical\nvelocity dispersion rises smoothly for ages up to 8-9 Gyr, following a power\nlaw with a slope of ~0.5, similar to what is observed in the solar\nneighbourhood by the Geneva-Copenhagen Survey. For these galaxies, we show that\nthe slope of the AVR is not imprinted at birth, but is the result of subsequent\nheating. By contrast, in all our simulations, the oldest stars form a\nsignificantly different population, with a high velocity dispersion. These\nstars are usually born kinematically hot in a turbulent phase of intense\nmergers at high redshift, and also include some stars accreted from satellites.\nThis maximum in velocity dispersion is strongly decreased when age errors are\nincluded, suggesting that observations can easily miss such a jump with the\ncurrent accuracy of age measurements.\n",
        "  In this paper, we give a parameterization of the SU(2,1) representation space\nof the Brieskorn homology spheres using the trace coordinates. As applications,\nwe give an example which shows that the orbifold Toledo invariant in\n\\cite{krebs} does not distinguish the connected components of the PU(2,1)\nrepresentation space.\n",
        "  In the present paper a general setup for determination of imperfect geometry\nof radiotherapeutic devices has been proposed that base on geometric algebra\nframework. To account for this imperfect geometry, two methods of a calibration\nwere presented, consisting of determining for each angular position of a gantry\na correction shift which must be applied to the origin of a laboratory frame of\nreference to place it along a radiation axis for this angular position. Closed\nform solutions for these corrections are provided.\n",
        "  Ultrasound computed tomography (USCT) holds great promise for improving the\ndetection and management of breast cancer. Because they are based on the\nacoustic wave equation, waveform inversion-based reconstruction methods can\nproduce images that possess improved spatial resolution properties over those\nproduced by ray-based methods. However, waveform inversion methods are\ncomputationally demanding and have not been applied widely in USCT breast\nimaging. In this work, source encoding concepts are employed to develop an\naccelerated USCT reconstruction method that circumvents the large computational\nburden of conventional waveform inversion methods. This method, referred to as\nthe waveform inversion with source encoding (WISE) method, encodes the\nmeasurement data using a random encoding vector and determines an estimate of\nthe sound speed distribution by solving a stochastic optimization problem by\nuse of a stochastic gradient descent algorithm. Both computer-simulation and\nexperimental phantom studies are conducted to demonstrate the use of the WISE\nmethod. The results suggest that the WISE method maintains the high spatial\nresolution of waveform inversion methods while significantly reducing the\ncomputational burden.\n",
        "  We study the long-range triplet Josephson current in a clean junction\ncomposed of two s-wave superconductors and a\nnormal-metal/ferromagnet/normal-metal trilayer. Through applying the bias\nvoltages on the metal regions by two antiparallel half-metal electrodes, we\nshow that the amplitude and direction of this long-range current can be\ncontrolled easily and flexibly. Such current arises from the fact that the\napplied voltage can produce a nonequilibrium spin-dependent quasiparticle\ndistribution in the metal regions so that the Cooper pairs acquire an extra\nmomenta, which will lead to a spin-flip processes in the metal regions. This\nprocesses can produce the parallel spin triplet pairs in the central\nferromagnet layer. In particular, if the voltage is applied only on one metal\nregion, we further find that the recently discovered long-range superharmonic\nJosephson current will appear because of the transport of an even number of\nparallel spin triplet pairs.\n",
        "  The Balmer H alpha emission line in the stationary spectrum of SS 433 has a\ncomponent originating in the wind above the accretion disk. The Doppler motion\nof this line is a blurred representation of the motion of the compact object\naccreting. I show how this may be understood in terms of emission lasting over\na few days, like radiation from the jet bolides.\n",
        "  Commutative Replicated Data-Type (CRDT) is a new class of algorithms that\nensures scalable consistency of replicated data. It has been successfully\napplied to collaborative editing of texts without complex concurrency control.\nIn this paper, we present a CRDT to edit XML data. Compared to existing\napproaches for XML collaborative editing, our approach is more scalable and\nhandles all the XML editing aspects : elements, contents, attributes and undo.\nIndeed, undo is recognized as an important feature for collaborative editing\nthat allows to overcome system complexity through error recovery or\ncollaborative conflict resolution.\n",
        "  In this paper, we discuss how globular clusters (GCs) structural and\nobservational properties can be used to infer the presence of a black hole\nsystem (BHS) inhabiting their inner regions. We propose a novel way to identify\nthe BHS size, defined as the GC radius containing a mass contributed equally\nfrom stars and stellar BHs. Using this definition, similar to the well-known\nconcept of \"influence radius\", we found a \"fundamental plane\" connecting the\nBHS typical density with the GC central surface density profile, total\nluminosity and observational half-mass radius. Our approach allows us to define\na unique way to connect the observational GCs parameters with their dark\ncontent. Comparing our results with observed Milky Way GCs, we found that many\nof them likely host, at the present time, as many as several hundreds of BHs.\nThese BHS are characterized by a relatively low typical density, $\\rho_\\bhs\n\\sim 10-10^5\\Ms$ pc$^{-3}$ and composed of relatively massive BHs, with average\nmasses in the range $m_\\bhs = 14-22\\Ms$. We also show that a similar approach\ncan be used to find Milky Way GCs potentially hosting an intermediate-mass\nblack hole.\n",
        "  Recent observations indicate a remarkable similarity in the properties of\nevolving galaxies at fixed mass and redshift, prompting us to consider the\npossibility that most galaxies may evolve with a common history encompassing\nstar formation, quasar accretion, and eventual quiescence. We quantify this by\ndefining a \"synchronization timescale\" for galaxies as a function of mass and\nredshift that characterizes the extent to which different galaxies of a common\nmass are evolving in the same manner at various cosmic epochs. We measure this\nsynchronization timescale using 9 different star-forming galaxy observations\nfrom the literature and SDSS quasar observations spanning $0 < z \\lesssim 6$.\nSurprisingly, this synchronization timescale is a constant, approximately 1.5\nGyr for all combinations of mass and time. We also find that the ratio between\nthe stellar mass of galaxies turning off star formation and black hole mass of\nturnoff quasars is approximately 30:1, much lower than the 500:1 for quiescent\ngalaxies at low redshift. As a result, we propose a model in which the\nstar-forming \"main sequence\", analogous quasar behavior, and other observations\nform a galactic evolution \"main sequence\", in which star formation occurs\nearliest, followed by supermassive black hole accretion, and feedback between\nthe two are dominated by deterministic rather than stochastic processes.\n",
        "  The Seebeck and Nernst coefficients $S$ and $\\nu$ of the cuprate\nsuperconductor YBa$_2$Cu$_3$O$_y$ (YBCO) were measured in a single crystal with\ndoping $p = 0.12$ in magnetic fields up to H = 28 T. Down to T=9 K, $\\nu$\nbecomes independent of field by $H \\simeq 30$ T, showing that superconducting\nfluctuations have become negligible. In this field-induced normal state, $S/T$\nand $\\nu/T$ are both large and negative in the $T \\to 0$ limit, with the\nmagnitude and sign of $S/T$ consistent with the small electron-like Fermi\nsurface pocket detected previously by quantum oscillations and the Hall effect.\nThe change of sign in $S(T)$ at $T \\simeq 50$ K is remarkably similar to that\nobserved in La$_{2-x}$Ba$_x$CuO$_4$, La$_{2-x-y}$Nd$_y$Sr$_x$CuO$_4$ and\nLa$_{2-x-y}$Eu$_y$Sr$_x$CuO$_4$, where it is clearly associated with the onset\nof stripe order. We propose that a similar density-wave mechanism causes the\nFermi surface reconstruction in YBCO.\n",
        "  An adaptive agent predicting the future state of an environment must weigh\ntrust in new observations against prior experiences. In this light, we propose\na view of the adaptive immune system as a dynamic Bayesian machinery that\nupdates its memory repertoire by balancing evidence from new pathogen\nencounters against past experience of infection to predict and prepare for\nfuture threats. This framework links the observed initial rapid increase of the\nmemory pool early in life followed by a mid-life plateau to the ease of\nlearning salient features of sparse environments. We also derive a modulated\nmemory pool update rule in agreement with current vaccine response experiments.\nOur results suggest that pathogenic environments are sparse and that memory\nrepertoires significantly decrease infection costs even with moderate sampling.\nThe predicted optimal update scheme maps onto commonly considered competitive\ndynamics for antigen receptors.\n",
        "  A lot of prior work on event extraction has exploited a variety of features\nto represent events. Such methods have several drawbacks: 1) the features are\noften specific for a particular domain and do not generalize well; 2) the\nfeatures are derived from various linguistic analyses and are error-prone; and\n3) some features may be expensive and require domain expert. In this paper, we\ndevelop a Chinese event extraction system that uses word embedding vectors to\nrepresent language, and deep neural networks to learn the abstract feature\nrepresentation in order to greatly reduce the effort of feature engineering. In\naddition, in this framework, we leverage large amount of unlabeled data, which\ncan address the problem of limited labeled corpus for this task. Our\nexperiments show that our proposed method performs better compared to the\nsystem using rich language features, and using unlabeled data benefits the word\nembeddings. This study suggests the potential of DNN and word embedding for the\nevent extraction task.\n",
        "  We report synthesis, structural, magnetic, specific heat and Density\nFunctional Theory (DFT) studies on MgCNi3 superconductor. Polycrystalline\nMgCNi3 samples are synthesized through standard solid state reaction route and\nfound to crystallize in cubic perovskite structure with space group Pm3m,\nwithout any detectable trace of Ni impurity. Both AC and DC magnetization\nexhibited superconducting transition (Tc) at around 7.25 K. The lower critical\nfield (Hc1) and irreversibility field (Hirr) are around 140 Oe and 11 kOe\nrespectively at 2 K. The upper critical field (Hc2) being determined from\nin-field AC susceptibility measurements is 11.6 kOe and 91.70 kOe with 50% and\n90% diamagnetism criteria respectively. Heat capacity (Cp) measurements are\ncarried out under applied field of up to 140 kOe and down to 2 K. The\nSommerfeld constant ({\\gamma}) and Debye temperature ({\\Theta}D) as determined\nfrom low temperature fitting of Cp(T) data to Sommerfeld-Debye model are 36.13\nmJ/mole-K2 and 263.13 K respectively. The Bardeen-Cooper-Schrieffer (BCS)\nparameter (2{\\Delta}/KBTc) is around 3.62, suggesting MgCNi3 to be an\nintermediate coupled BCS superconductor with value {\\lambda} = 0.69. Although\nthe density functional theory (DFT) calculations exhibited the compound to be\nnon-magnetic but with spin fluctuations, the experimental isothermal\nmagnetization MH loops at 20, 50, 100, 200 and 300 K showed some ferromagnetic\nnature in this compound with coercive field (Hc) of around 50 Oe at 20 K. The\nNi3d states play dominant roles near the Fermi levels and there is strong\nhybridization between Ni3d and C2p states. It seems that MgCNi3 is\nsuperconducting in close proximity of ferromagnetism.\n",
        "  This paper focuses on some dosimetry aspects of proton therapy and pencil\nbeam scanning based on the experience accumulated at Paul Scherrer\nInstitute(PSI). The basic formalism for absolute dosimetry in proton therapy is\noutlined and the two main techniques and equipment to perform the primary beam\nmonitor chamber calibration are presented. Depth-dose curve and lateral beam\nwidth measurements are exposed and discussed in detail, with particular\nattention to the size of the ionization chamber and the characteristic of\nscintillating-CCD dosimetry systems, respectively. It is also explained how the\nangular-spatial distribution of individual pencil beams can be determined in\npractice. The equipment and the techniques for performing\nregularmachine-specific quality checks are focused on (i)output constancy\nchecks, (ii)pencil beam position and size checks and (iii)beam energy checks.\nFinally, patient-specific verification is addressed.\n",
        "  Data mining is the practice to search large amount of data to discover data\npatterns. Data mining uses mathematical algorithms to group the data and\nevaluate the future events. Association rule is a research area in the field of\nknowledge discovery. Many data mining researchers had improved upon the quality\nof association rule for business development by incorporating influential\nfactors like utility, number of items sold and for the mining of association\ndata patterns. In this paper, we propose an efficient algorithm to find maximal\nfrequent itemset first. Most of the association rule algorithms used to find\nminimal frequent item first, then with the help of minimal frequent itemsets\nderive the maximal frequent itemsets, these methods consume more time to find\nmaximal frequent itemsets. To overcome this problem, we propose a new approach\nto find maximal frequent itemset directly using the concepts of subsets. The\nproposed method is found to be efficient in finding maximal frequent itemsets.\n",
        "  In iron-based superconductors, a unique tri-layer Fe-As (Se, Te, P) plays an\nessential role in controlling the electronic properties, especially the Cooper\npairing interaction. Here we use scanning tunneling microscopy/spectroscopy\n(STM/S) to investigate the role of arsenic atom in superconducting\nBa0.4K0.6Fe2As2 by directly breaking and restoring the Fe-As structure at\natomic scale. After the up-As-layer peeled away, the tunneling spectrum of the\nexposed iron surface reveals a shallow incoherent gap, indicating a severe\nsuppression of superconductivity without arsenic covering. When a pair of\narsenic atoms is placed on such iron surface, a localized topographic feature\nis formed due to Fe-As orbital hybridization, and the superconducting coherent\npeaks recover locally with the gap magnitude the same as that on the iron-layer\nfully covered by arsenic. These observations unravel the Fe-As interactions on\nan atomic scale and imply its essential roles in the iron-based\nsuperconductivity.\n",
        "  -Purpose: A neural network estimator to process x-ray spectral measurements\nfrom photon counting detectors with pileup. The estimator is used with an\nexpansion of the attenuation coefficient as a linear combination of functions\nof energy multiplied by coefficients that depend on the material composition at\npoints within the object [R.E. Alvarez and A. Macovski, Phys. Med. Biol., 1976,\n733-744]. The estimator computes the line integrals of the coefficients from\nmeasurements with different spectra. Neural network estimators are trained with\nmeasurements of a calibration phantom with the clinical x-ray system. One\nestimator uses low noise training data and another network is trained with data\ncomputed by adding random noise to the low noise data. The performance of the\nestimators is compared to each other and to the Cramer-Rao lower bound (CRLB).\nMethods: The estimator performance is measured using a Monte Carlo simulation\nwith an idealized model of a photon counting detector that includes only pileup\nand quantum noise. Transmitted x-ray spectra are computed for a calibration\nphantom. The transmitted spectra are used to compute random data for photon\ncounting detectors with pileup. Detectors with small and large dead times are\nconsidered. Neural network training data with extremely low noise are computed\nby averaging the random detected data with pileup for a large numbers of\nexposures of the phantom. Each exposure is equivalent to a projection image or\none projection of a computed tomography scan. Training data with high noise are\ncomputed by using data from one exposure. Finally, training data are computed\nby adding random data to the low noise data. The added random data are\nmultivariate normal with zero mean and covariance equal to the sample\ncovariance of data for an object with properly chosen attenuation. To test the\nestimators, random data are computed for different thicknesses of three test\nobjects with different compositions. These are used as inputs to the neural\nnetwork estimators. The mean squared errors (MSE), variance and square of the\nbias of the neural networks' outputs with the random object data are each\ncompared to the CRLB. Results: The MSE for a network trained with low noise\ndata and added noise is close to the CRLB for both the low and high pileup\ncases. Networks trained with very low noise data have low bias but large\nvariance for both pileup cases. ralvarez@aprendtech.com Networks trained with\nhigh noise data have both large bias and large variance. Conclusion: With a\nproperly chosen level of added training data noise, a neural network estimator\nfor photon counting data with pileup can have variance close to the CRLB with\nnegligible bias.\n",
        "  Malaria remains endemic in tropical areas, especially in Africa. For the\nevaluation of new tools and to further ourunderstanding of host-parasite\ninteractions, knowing the environmental risk of transmission-even at a very\nlocal scale-isessential. The aim of this study was to assess how malaria\ntransmission is influenced and can be predicted by local climaticand\nenvironmental factors. As the entomological part of a cohort study of 650\nnewborn babies in nine villages in the ToriBossito district of Southern Benin\nbetween June 2007 and February 2010, human landing catches were performed to\nassessthe density of malaria vectors and transmission intensity. Climatic\nfactors as well as household characteristics were recordedthroughout the study.\nStatistical correlations between Anopheles density and environmental and\nclimatic factors weretested using a three-level Poisson mixed regression model.\nThe results showed both temporal variations in vector density(related to season\nand rainfall), and spatial variations at the level of both village and house.\nThese spatial variations could belargely explained by factors associated with\nthe house's immediate surroundings, namely soil type, vegetation index andthe\nproximity of a watercourse. Based on these results, a predictive regression\nmodel was developed using a leave-one-outmethod, to predict the spatiotemporal\nvariability of malaria transmission in the nine villages. This study points up\ntheimportance of local environmental factors in malaria transmission and\ndescribes a model to predict the transmission risk ofindividual children, based\non environmental and behavioral characteristics.\n",
        "  Rooted phylogenetic networks are often constructed by combining trees,\nclusters, triplets or characters into a single network that in some\nwell-defined sense simultaneously represents them all. We review these four\nmodels and investigate how they are related. In general, the model chosen\ninfluences the minimum number of reticulation events required. However, when\none obtains the input data from two binary trees, we show that the minimum\nnumber of reticulations is independent of the model. The number of\nreticulations necessary to represent the trees, triplets, clusters (in the\nsoftwired sense) and characters (with unrestricted multiple crossover\nrecombination) are all equal. Furthermore, we show that these results also hold\nwhen not the number of reticulations but the level of the constructed network\nis minimised. We use these unification results to settle several complexity\nquestions that have been open in the field for some time. We also give explicit\nexamples to show that already for data obtained from three binary trees the\nmodels begin to diverge.\n",
        "  We have fabricated the Fe(Se, Te) superconducting wire by a special process\nbased on a powder-in-tube method. The pure Fe tube plays the role of not only\nthe sheath but also the raw material for synthesizing the superconducting\nphases. We succeeded in observing zero resistivity current on the\ncurrent-voltage measurements for the Fe(Se, Te) wire. Introduction of the\npinning centers and fabricating a multi-core wire will enhance the critical\ncurrent density for the next step.\n",
        "  Technical Note describing studies, in 2011, of time resolved diode dosimetry\nin an anthropomorphic phantom at the Francis H. Burr Proton Therapy Center,\nMassachusetts General Hospital, Boston, MA. This technique measures the water\nequivalent path length (WEPL) to a diode dosimeter with sub-millimeter\nprecision, using very little dose. It may allow one to use the proton stopping\npoint (otherwise too uncertain) to cut between tissue to be treated and tissue\nto be spared, in cases (such as the prostate) where a diode can be placed in a\ncavity (the rectum) distal to the target.\n  A potential problem is range mixing, where (because of scattering) protons\nmay arrive at a diode with different energy-loss histories. The WEPL reported\nby such a diode is meaningless and must be ignored. We therefore use an array\nof diodes, of which some are likely to be at more favorable locations. In\naddition to the overall analysis of time resolved data, this Note describes in\ndetail how range mixed diodes can be identified by looking at the skewness and\nkurtosis of the time dependent signal.\n",
        "  Andreev levels deplete energy states above the superconductive gap, which\nleads to the peculiar nonmonotonous crossover in the local density of states of\nmesoscopic superconductor/normal-metal/superconductor junctions. This effect is\nespecially pronounced in the case when the normal metal bridge length is small\ncompared to the superconductive coherence length. Remarkable property of the\ncrossover function is that it vanishes not only at the proximity induced gap\nbut also at the superconductive gap. Analytical expressions for the density of\nstates at the both gap edges, as well as general structure of the crossover are\ndiscussed.\n",
        "  The four-year oscillations of the number of spawning sockeye salmon\n(Oncorhynchus nerka) that return to their native stream within the Fraser River\nbasin in Canada are a striking example of population oscillations. The period\nof the oscillation corresponds to the dominant generation time of these fish.\nVarious - not fully convincing - explanations for these oscillations have been\nproposed, including stochastic influences, depensatory fishing, or genetic\neffects. Here, we show that the oscillations can be explained as a stable\ndynamical attractor of the population dynamics, resulting from a strong\nresonance near a Neimark Sacker bifurcation. This explains not only the\nlong-term persistence of these oscillations, but also reproduces correctly the\nempirical sequence of salmon abundance within one period of the oscillations.\nFurthermore, it explains the observation that these oscillations occur only in\nsockeye stocks originating from large oligotrophic lakes, and that they are\nusually not observed in salmon species that have a longer generation time.\n",
        "  In this note, we complete the classification of quasi-alternating Montesinos\nlinks. We show that the quasi-alternating Montesinos links are precisely those\nidentified independently by Qazaqzeh-Chbili-Qublan and Champanerkar-Ording. A\nconsequence of our proof is that a Montesinos link $L$ is quasi-alternating if\nand only if its double branched cover is an L-space, and bounds both a positive\ndefinite and a negative definite 4-manifold with vanishing first homology.\n",
        "  The chromium arsenides BaCr2As2 and BaCrFeAs2 with ThCr2Si2 type structure\n(space group I4/mmm; also adopted by '122' iron arsenide superconductors) have\nbeen suggested as mother compounds for possible new superconductors. DFT-based\ncalculations of the electronic structure evidence metallic antiferromagnetic\nground states for both compounds. By powder neutron diffraction we confirm for\nBaCr2As2 a robust ordering in the antiferromagnetic G-type structure at T_N =\n580 K with mu_Cr = 1.9 mu_B at T = 2K. Anomalies in the lattice parameters\npoint to magneto-structural coupling effects. In BaCrFeAs2 the Cr and Fe atoms\nrandomly occupy the transition-metal site and G-type order is found below 265 K\nwith mu_Cr/Fe = 1.1 mu_B. 57Fe Moessbauer spectroscopy demonstrates that only a\nsmall ordered moment is associated with the Fe atoms, in agreement with\nelectronic structure calculations with mu_Fe ~ 0. The temperature dependence of\nthe hyperfine field does not follow that of the total moments. Both compounds\nare metallic but show large enhancements of the linear specific heat\ncoefficient gamma with respect to the band structure values. The metallic state\nand the electrical transport in BaCrFeAs2 is dominated by the atomic disorder\nof Cr and Fe and partial magnetic disorder of Fe. Our results indicate that\nNeel-type order is unfavorable for the Fe moments and thus it is destabilized\nwith increasing iron content.\n",
        "  The prevalent approach to neural machine translation relies on bi-directional\nLSTMs to encode the source sentence. In this paper we present a faster and\nsimpler architecture based on a succession of convolutional layers. This allows\nto encode the entire source sentence simultaneously compared to recurrent\nnetworks for which computation is constrained by temporal dependencies. On\nWMT'16 English-Romanian translation we achieve competitive accuracy to the\nstate-of-the-art and we outperform several recently published results on the\nWMT'15 English-German task. Our models obtain almost the same accuracy as a\nvery deep LSTM setup on WMT'14 English-French translation. Our convolutional\nencoder speeds up CPU decoding by more than two times at the same or higher\naccuracy as a strong bi-directional LSTM baseline.\n",
        "  We are addressing the sites of isolated low mass star formation in the solar\nneighbourhood, i.e. small cloud cores within one kiloparsec. We aim at\ndetermining the physical parameters of the cores, i.e., temperature, volume\ndensity, column density and (radial) velocity fields, and the status of star\nformation, i.e., whether embedded objects are present within the cores.\nSurveying small dark clouds in both celestial hemispheres we study the physical\nconditions of low-mass star formation for detectable core masses M>0.01Msun.\nThe target list is drawn from catalogues of optically selected dark cloud\ncores, where the visual extinction exceeds 5 magnitudes. The selected probe is\nthe CS molecule that needs high densities for excitation of its rotational\nlevels. To gauge the state of excitation, the cores were observed in two\ntransitions. In a limited number of cases, optical depths were derived from\ncomplementing lines of the rarer isotopologue C34S for the (2-1) and (3-2)\ntransitions. Making small (3arcmin by 3arcmin) maps, the 471 optically selected\ncores were searched for CS(2-1) and 315 (67%) were detected (T_A*>3sigma). In\ngeneral, the position of peak CS emission does not coincide with the optically\ndetermined centre of the cores. The cores appear cold (T<10K) and, in the\nmajority of cases, the CS emission is optically thin (tau<1). On the arcminute\nscales of the observations, the median column density of carbon monosulfide is\nN(CS)=7.E12/cm2. For an average abundance of N(CS)/N(H2)=1.E-8, the median mass\nof the detected cores is 1.0Msun. The line shapes are most often Gaussian with\nwidths exceeding that due to thermal broadening of <0.1km/s. The observed\nmedian FWHM=0.7km/s, i.e. non-thermal turbulence contributes dominantly to the\nline widths\n",
        "  It is important to study absorption spectrum in film dosimetry because the\nspectral absorbance of the film relates to the film's total absorption dose. We\ninvestigated the absorption spectra of Gafchromic EBT2 film with various\nrotational angles in a visible wavelength band. The film was irradiated with 6\nMV photon beams and a total dose of 300 cGy. Absorption spectra were taken\nunder different rotational angles after 24 h after irradiation and we fitted\nthe spectra using Lorentzian functions. There were two dominant absorption\npeaks at approximately 586 nm (green) and 634 nm (red). The measured spectrum\nwas decomposed 542 nm, 558 nm, 578 nm, 586 nm, 626 nm, 634 nm, and 641 nm. The\nmaximum total area of the red band absorption spectrum was at\n45{\\deg}(225{\\deg}) and the minimum at 90{\\deg}(270{\\deg}). As the angle of\nrotation changed, the intensity and integrated area of the blue and green peaks\nalso changed with 180{\\deg} period, with minima at 90{\\deg} and 270{\\deg}, and\nmaxima at 0{\\deg} and 180{\\deg}, although the overall absorbance is very low.\nThe spectral peak wavelengths remained constant within 2.4 nm for all angles.\nThere was no hysteresis of absorption spectrum of the film; spectra taken at\n0{\\deg} and 360{\\deg}were substantially the same and showed similar behavior\nfor all rotational angles. The change of absorbance with rotational angle of\nthe film affected the dosimetric properties, resulting in rotationalvariations\nof film dosimetry in each red-green-blue channel.\n",
        "  Pathogen transmission and virulence are main evolutionary variables broadly\nassumed to be linked through trade-offs. In well-mixed populations, these\ntrade-offs are often ascribed to physiological restrictions, while populations\nwith spatial self-structuring might evolve emergent trade-offs. Here, we\nreexamine a model of the latter kind proposed by Ballegooijen and Boerlijst\nwith the aim of characterising the mechanisms causing the emergence of the\ntrade-off and its structural robustness. Using invadability criteria, we\nestablish the conditions under which an evolutionary feedback between\ntransmission and virulence mediated by pattern formation can poise the system\nto a critical boundary separating a disordered state (without emergent\ntrade-off) from a self-structured phase (where the trade-off emerges), and\nanalytically calculate the functional shape of the boundary in a certain\napproximation. Beyond evolutionary parameters, the success of an invasion\ndepends on the size and spatial structure of the invading and invaded\npopulations. Spatial self-structuring is often destroyed when hosts are mobile,\nchanging the evolutionary dynamics to those of a well-mixed population. In a\nmetapopulation scenario, the systematic extinction of the pathogen in the\ndisordered phase may counteract the disruptive effect of host mobility, favour\npattern formation and therefore recover the emergent trade-off.\n",
        "  The bag-of-words (BOW) model is the common approach for classifying\ndocuments, where words are used as feature for training a classifier. This\ngenerally involves a huge number of features. Some techniques, such as Latent\nSemantic Analysis (LSA) or Latent Dirichlet Allocation (LDA), have been\ndesigned to summarize documents in a lower dimension with the least semantic\ninformation loss. Some semantic information is nevertheless always lost, since\nonly words are considered. Instead, we aim at using information coming from\nn-grams to overcome this limitation, while remaining in a low-dimension space.\nMany approaches, such as the Skip-gram model, provide good word vector\nrepresentations very quickly. We propose to average these representations to\nobtain representations of n-grams. All n-grams are thus embedded in a same\nsemantic space. A K-means clustering can then group them into semantic\nconcepts. The number of features is therefore dramatically reduced and\ndocuments can be represented as bag of semantic concepts. We show that this\nmodel outperforms LSA and LDA on a sentiment classification task, and yields\nsimilar results than a traditional BOW-model with far less features.\n",
        "  In this paper, we give definitions of three kinds of minimal charts, and we\ninvestigate properties of minimal charts and establish fundamental theorems\ncharacterizing minimal charts. To classify charts with two or three crossings\nwe use the fundamental theorems. In the future paper, we give an numeration of\nthe charts with two crossings.\n",
        "  Grammatical error correction (GEC) is the task of detecting and correcting\ngrammatical errors in texts written by second language learners. The\nstatistical machine translation (SMT) approach to GEC, in which sentences\nwritten by second language learners are translated to grammatically correct\nsentences, has achieved state-of-the-art accuracy. However, the SMT approach is\nunable to utilize global context. In this paper, we propose a novel approach to\nimprove the accuracy of GEC, by exploiting the n-best hypotheses generated by\nan SMT approach. Specifically, we build a classifier to score the edits in the\nn-best hypotheses. The classifier can be used to select appropriate edits or\nre-rank the n-best hypotheses. We apply these methods to a state-of-the-art GEC\nsystem that uses the SMT approach. Our experiments show that our methods\nachieve statistically significant improvements in accuracy over the best\npublished results on a benchmark test dataset on GEC.\n",
        "  In this work I communicate the detection of a new Galactic Wolf-Rayet star\n(WR60a) in Centaurus. The H- and K-band spectra of WR60a, show strong carbon\nnear-infrared emission lines, characteristic of Wolf-Rayet stars of the WC5-7\nsub-type. Adopting mean absolute magnitude M$_K$ and mean intrinsic ($J-K_S$)\nand ($H-K_S$) colours, it was found that WR60a suffer a mean visual extinction\nof 3.8$\\pm$1.3 magnitudes, being located at a probable heliocentric distance of\n5.2$\\pm$0.8 Kpc, which for the related Galactic longitude (l=312) puts this\nstar probably in the Carina-Sagittarius arm at about 5.9 kpc from the Galactic\ncenter. I searched for clusters in the vicinity of WR60a, and in principle\nfound no previously known clusters in a search radius region of several tens\narc-minutes. The detection of a well isolated WR star induced us to seek for\nsome still unknown cluster, somewhere in the vicinity of WR60a. From inspection\nof 5.8$\\mu$m and 8.0$\\mu$m Spitzer/IRAC GLIMPSE images of the region around the\nnew WR star, it was found strong mid-infrared extended emission at about 13.5\narcmin south-west of WR60a. The study of the the H-K$_S$ colour distribution of\npoint sources associated with the extended emission, reveals the presence of a\nnew Galactic cluster candidate probably formed by at least 85 stars.\n",
        "  Using VLT/FLAMES optical IFU observations, we present a detailed study of UM\n448, a nearby Blue Compact Galaxy (BCG) previously reported to have an\nanomalously high N/O abundance ratio. NTT/SuSI2 images reveal a morphology\nsuggestive of a merger of two systems of contrasting colour, whilst our H-alpha\nemission maps resolve UM 448 into three separate regions that do not coincide\nwith the stellar continuum peaks. UM 448 exhibits complex emission line\nprofiles, with lines consisting of a narrow, central component, an underlying\nbroad component and a third, narrow blue-shifted component. Radial velocity\nmaps show signs of solid body rotation across UM 448, with a projected rotation\naxis that correlates with the continuum morphology of the galaxy. A\nspatially-resolved, chemodynamical analysis is presented. Whilst the eastern\ntail of UM 448 has electron temperatures (Te) that are typical of BCGs, we find\na region within the main body of the galaxy where the narrow and broad [O III]\n4363 line components trace temperatures differing by 5000K and oxygen\nabundances differing by 0.4 dex. We measure spatially resolved and integrated\nionic and elemental abundances for O, N, S and Ne throughout UM 448, and find\nthey do not agree, possibly due the flux-weighting of Te from the integrated\nspectrum. This has significant implications for abundances derived from\nlong-slit and integrated spectra of star-forming galaxies in the nearby and\ndistant universe. A region of enhanced N/O ratio is indeed found, extended over\na ~0.6 kpc^2 region within the main body of the galaxy. Contrary to previous\nstudies, however, we do not find evidence for a large Wolf-Rayet population,\nand conclude that WR stars alone cannot be responsible for producing the\nobserved N/O excess. Instead, the location and disturbed morphology of the\nN-enriched region suggests that interaction-induced inflow of metal-poor gas\nmay be responsible.\n",
        "  Recent research has taken advantage of Wikipedia's multilingualism as a\nresource for cross-language information retrieval and machine translation, as\nwell as proposed techniques for enriching its cross-language structure. The\navailability of documents in multiple languages also opens up new opportunities\nfor querying structured Wikipedia content, and in particular, to enable answers\nthat straddle different languages. As a step towards supporting such queries,\nin this paper, we propose a method for identifying mappings between attributes\nfrom infoboxes that come from pages in different languages. Our approach finds\nmappings in a completely automated fashion. Because it does not require\ntraining data, it is scalable: not only can it be used to find mappings between\nmany language pairs, but it is also effective for languages that are\nunder-represented and lack sufficient training samples. Another important\nbenefit of our approach is that it does not depend on syntactic similarity\nbetween attribute names, and thus, it can be applied to language pairs that\nhave distinct morphologies. We have performed an extensive experimental\nevaluation using a corpus consisting of pages in Portuguese, Vietnamese, and\nEnglish. The results show that not only does our approach obtain high precision\nand recall, but it also outperforms state-of-the-art techniques. We also\npresent a case study which demonstrates that the multilingual mappings we\nderive lead to substantial improvements in answer quality and coverage for\nstructured queries over Wikipedia content.\n",
        "  Superconductivity is commonly described as a macroscopic quantum phenomenon.\nHowever, it arises from microscopic mechanisms occurring at the nanometer scale\nas illustrated, for example, by the non-trivial pairing in unconventional\nsuperconductors. More recently, also local interactions with superconductors in\nthe context of Majorana fermions became of interest. A very direct way to study\nthe atomic scale properties of superconductors is given by the combination of\nthe Josephson effect with scanning tunneling microscopy (STM), also referred to\nas JSTM. Here, the critical Josephson current serves as a direct local probe of\nthe superconducting ground state and may reveal valuable information that is\noften inaccessible when studying quasi-particle excitation spectra. We show\nthat we can extract local values of the critical Josephson current from JSTM\nmeasurements in the dynamical Coulomb blockade regime. Furthermore, we\nexperimentally determine the regime of sequential Cooper pair tunneling, which\nis in accordance to theoretical predictions. Our study presents new insights on\nthe tunneling mechanisms in Josephson junctions and lays the basis for the\nimplementation of JSTM as a versatile probe for superconductivity.\n",
        "  One of the most popular scenarios for the superconductivity in Fe-based\nsuperconductors (FeBSC) posits that the bosons responsible for electronic\npairing are spin-fluctuations with a wave vector spanning the hole Fermi\nsurfaces (FSs) near $\\Gamma$ and the electron FSs near M points. So far all\nFeBSC for which neutron data are available do demonstrate such excitations, and\nthe band structure calculations so far were finding quasi-nested FSs in all\nFeBSC, providing for a peak in the spin susceptibility at the desired wave\nvectors. However, the newest addition to the family, Sr$_{2}$VO$_{3}$FeAs, has\nbeen calculated to have a very complex FS with no visible quasi-nesting\nfeatures. It was argued therefore that this material does not fall under the\nexisting paradigm and calls for revisiting our current ideas about what is the\nlikely cause of superconductivity in FeBSC. In this paper, I show that the\nvisible complexity of the FS is entirely due to the V-derived electronic\nstates. Assuming that superconductivity in Sr$_{2}$VO$_{3}$FeAs, as in the\nother FeBSC, originates in the FeAs layers, and the superconducting electrons\nare sensitive to the susceptibility of the FeAs electronic subsystem, I\nrecalculate the bare susceptibility, weighting the electronic states with their\nFe character, and obtain a susceptibility that fully supports the existing\nquasi-nesting model.\n",
        "  The goal in the NER task is to classify proper nouns of a text into classes\nsuch as person, location, and organization. This is an important preprocessing\nstep in many NLP tasks such as question-answering and summarization. Although\nmany research studies have been conducted in this area in English and the\nstate-of-the-art NER systems have reached performances of higher than 90\npercent in terms of F1 measure, there are very few research studies for this\ntask in Persian. One of the main important causes of this may be the lack of a\nstandard Persian NER dataset to train and test NER systems. In this research we\ncreate a standard, big-enough tagged Persian NER dataset which will be\ndistributed for free for research purposes. In order to construct such a\nstandard dataset, we studied standard NER datasets which are constructed for\nEnglish researches and found out that almost all of these datasets are\nconstructed using news texts. So we collected documents from ten news websites.\nLater, in order to provide annotators with some guidelines to tag these\ndocuments, after studying guidelines used for constructing CoNLL and MUC\nstandard English datasets, we set our own guidelines considering the Persian\nlinguistic rules.\n",
        "  We report direction-dependent susceptibility and resistivity measurements on\nLa$_{2-x}$Sr$_{x}$CuO$_{4}$ single crystals. These crystals have rectangular\nneedle-like shapes with the crystallographic \"c\" direction parallel or\nperpendicular to the needle axis,which, in turn, is in the applied field\ndirection. At optimal doping we find finite diamagnetic susceptibility above\n$T_{c}$, namely fluctuating superconductivity (FSC), only when the field is\nperpendicular to the planes. In underdoped samples we could find FSC in both\nfield directions. We provide a phase diagram showing the FSC region, although\nit is sample dependent in the underdoped cases. The variations in the\nsusceptibility data suggest a different origin for the FSC between underdoping\n(below 10%) and optimal doping. Finally, our data indicates that the\nspontaneous vortex diffusion constant above $T_c$ is anomalously high.\n",
        "  A striking feature of the marine ecosystem is the regularity in its size\nspectrum: the abundance of organisms as a function of their weight\napproximately follows a power law over almost ten orders of magnitude. We\ninterpret this as evidence that the population dynamics in the ocean is\napproximately scale-invariant. We use this invariance in the construction and\nsolution of a size-structured dynamical population model. Starting from a\nMarkov model encoding the basic processes of predation, reproduction,\nmaintenance respiration and intrinsic mortality, we derive a partial\nintegro-differential equation describing the dependence of abundance on weight\nand time. Our model represents an extension of the jump-growth model and hence\nalso of earlier models based on the McKendrick--von Foerster equation. The\nmodel is scale-invariant provided the rate functions of the stochastic\nprocesses have certain scaling properties. We determine the steady-state power\nlaw solution, whose exponent is determined by the relative scaling between the\nrates of the density-dependent processes (predation) and the rates of the\ndensity-independent processes (reproduction, maintenance, mortality). We study\nthe stability of the steady-state against small perturbations and find that\ninclusion of maintenance respiration and reproduction in the model has astrong\nstabilising effect. Furthermore, the steady state is unstable against a change\nin the overall population density unless the reproduction rate exceeds a\ncertain threshold.\n",
        "  The study of geometric group theory has suggested several theorems related to\nsubdivision tilings that have a natural hyperbolic structure. However, few\nexamples exist. We construct subdivision tilings for the complement of every\nnonsingular, prime alternating link. These tilings define a combinatorial space\nat infinity, similar to the space at infinity for word hyperbolic groups.\n",
        "  A virtual knot that has a homologically trivial representative $\\mathscr{K}$\nin a thickened surface $\\Sigma \\times [0,1]$ is said to be an almost classical\n(AC) knot. $\\mathscr{K}$ then bounds a Seifert surface $F\\subset \\Sigma \\times\n[0,1]$. Seifert surfaces of AC knots are useful for computing concordance\ninvariants and slice obstructions. However, Seifert surfaces in $\\Sigma \\times\n[0,1]$ are difficult to construct. Here we introduce virtual Seifert surfaces\nof AC knots. These are planar figures representing $F \\subset \\Sigma \\times\n[0,1]$. An algorithm for constructing a virtual Seifert surface from a Gauss\ndiagram is given. This is applied to computing signatures and Alexander\npolynomials of AC knots. A canonical genus of AC knots is also studied. It is\nshown to be distinct from the virtual canonical genus of\nStoimenow-Tchernov-Vdovina.\n",
        "  Because of their complex Fermi surfaces, the identification of the physical\nphenomena contributing to electronic scattering in the Fe-based superconductors\nis a difficult task. Here, we report on the electrical resistivity,\nmagnetoresistance, and Hall effect in two series of BaFe$_{2-x}$T$_x$As$_2$ (T\n= Co, Ni) crystals with different values of $x$. The T contents were chosen so\nthat the majority of the investigated samples present an intermediate\nmagnetically ordered state and a superconducting ground state. We interpret the\nobtained results in terms of scattering of charge carriers by magnetic\nexcitations instead of describing them as resulting uniquely from effects\nrelated to multiple-band conduction. Our samples are single crystals from the\nstructural point of view and their overall magnetotransport properties are\ndominated by a single magnetic state.\n",
        "  While the Sersic profile family provide adequate fits for the surface\nbrightness profiles of observed galaxies, the physical origin is unknown. We\nshow that, if the cosmological density field are seeded by random gaussian\nfluctuations, as in the standard cold dark matter model, galaxies with steep\ncentral profiles have simultaneously extended envelopes of shallow profiles in\nthe outskirts, whereas galaxies with shallow central profiles are accompanied\nby steep density profiles in the outskirts. These properties are in accord with\nthose of the Sersic profile family. Moreover, galaxies with steep central\nprofiles form their central regions in smaller denser subunits that possibly\nmerge subsequently, which naturally leads to formation of bulges. In contrast,\ngalaxies with shallow central profiles form their central regions in a coherent\nfashion without significant substructure, a necessary condition for disk galaxy\nformation. Thus, the scenario is self-consistent with respect to the\ncorrelation between observed galaxy morphology and Sersic index. We predict\nfurther that clusters of galaxies should display a similar trend, which should\nbe verifiable observationally.\n",
        "  In this paper, we investigate the application of text classification methods\nto predict the law area and the decision of cases judged by the French Supreme\nCourt. We also investigate the influence of the time period in which a ruling\nwas made over the textual form of the case description and the extent to which\nit is necessary to mask the judge's motivation for a ruling to emulate a\nreal-world test scenario. We report results of 96% f1 score in predicting a\ncase ruling, 90% f1 score in predicting the law area of a case, and 75.9% f1\nscore in estimating the time span when a ruling has been issued using a linear\nSupport Vector Machine (SVM) classifier trained on lexical features.\n",
        "  This paper describes a Hierarchical Composition Recurrent Network (HCRN)\nconsisting of a 3-level hierarchy of compositional models: character, word and\nsentence. This model is designed to overcome two problems of representing a\nsentence on the basis of a constituent word sequence. The first is a\ndata-sparsity problem in word embedding, and the other is a no usage of\ninter-sentence dependency. In the HCRN, word representations are built from\ncharacters, thus resolving the data-sparsity problem, and inter-sentence\ndependency is embedded into sentence representation at the level of sentence\ncomposition. We adopt a hierarchy-wise learning scheme in order to alleviate\nthe optimization difficulties of learning deep hierarchical recurrent network\nin end-to-end fashion. The HCRN was quantitatively and qualitatively evaluated\non a dialogue act classification task. Especially, sentence representations\nwith an inter-sentence dependency are able to capture both implicit and\nexplicit semantics of sentence, significantly improving performance. In the\nend, the HCRN achieved state-of-the-art performance with a test error rate of\n22.7% for dialogue act classification on the SWBD-DAMSL database.\n",
        "  In this paper, we reformulated the spell correction problem as a machine\ntranslation task under the encoder-decoder framework. This reformulation\nenabled us to use a single model for solving the problem that is traditionally\nformulated as learning a language model and an error model. This model employs\nmulti-layer recurrent neural networks as an encoder and a decoder. We\ndemonstrate the effectiveness of this model using an internal dataset, where\nthe training data is automatically obtained from user logs. The model offers\ncompetitive performance as compared to the state of the art methods but does\nnot require any feature engineering nor hand tuning between models.\n",
        "  We describe a generic framework for representing and reasoning with annotated\nSemantic Web data, a task becoming more important with the recent increased\namount of inconsistent and non-reliable meta-data on the web. We formalise the\nannotated language, the corresponding deductive system and address the query\nanswering problem. Previous contributions on specific RDF annotation domains\nare encompassed by our unified reasoning formalism as we show by instantiating\nit on (i) temporal, (ii) fuzzy, and (iii) provenance annotations. Moreover, we\nprovide a generic method for combining multiple annotation domains allowing to\nrepresent, e.g. temporally-annotated fuzzy RDF. Furthermore, we address the\ndevelopment of a query language -- AnQL -- that is inspired by SPARQL,\nincluding several features of SPARQL 1.1 (subqueries, aggregates, assignment,\nsolution modifiers) along with the formal definitions of their semantics.\n",
        "  We present abundance measurements of elements O, C, Si and Fe for three\ngas-rich galaxies at z~5 using observations from the Very Large Telescope (VLT)\nand the Keck telescope in order to better constrain the early chemical\nenrichment of gas-rich galaxies. These galaxies show strong Lyman-{\\alpha}\nabsorption in the spectra of background quasars, with neutral hydrogen column\ndensities log N${_H{}_I}$(cm$^{-2}$)=20.10$\\pm$0.15, 20.10$\\pm$0.15, and\n20.80$\\pm$0.15. Using the undepleted element O, we find the metallicities [O/H]\nto be in the range of -2.51 to -2.05 dex. Our study has doubled the existing\nsample of measurements of undepleted elements at z>4.5. Combining our\nmeasurements with those from the literature, we find that the\nN${_H{}_I}$-weighted mean metallicity of z~5 absorbers is consistent with the\nprediction based on z<4.5 DLAs. Thus, we find no significant evidence of a\nsudden drop in metallicity at z>4.7 as reported by some prior studies. We also\ndetermine the extent of dust depletion using a combination of both the volatile\nelement O and the refractory elements Si and/or Fe. Some of the absorbers show\nevidence of depletion of elements on dust grains, e.g. low [Si/O] or [Fe/O].\nThe relative abundances of these absorbers along with other z~5 absorbers from\nthe literature show some peculiarities, e.g. low [C/O] in several absorbers and\nhigh [Si/O] in one absorber. Finally, we find that the metallicity vs. velocity\ndispersion relation of z~5 absorbers may be different from that of\nlower-redshift absorbers.\n",
        "  The problem of identification of statistically significant patterns in a\nsequence of data has been applied to many domains such as intrusion detection\nsystems, financial models, web-click records, automated monitoring systems,\ncomputational biology, cryptology, and text analysis. An observed pattern of\nevents is deemed to be statistically significant if it is unlikely to have\noccurred due to randomness or chance alone. We use the chi-square statistic as\na quantitative measure of statistical significance. Given a string of\ncharacters generated from a memoryless Bernoulli model, the problem is to\nidentify the substring for which the empirical distribution of single letters\ndeviates the most from the distribution expected from the generative Bernoulli\nmodel. This deviation is captured using the chi-square measure. The most\nsignificant substring (MSS) of a string is thus defined as the substring having\nthe highest chi-square value. Till date, to the best of our knowledge, there\ndoes not exist any algorithm to find the MSS in better than O(n^2) time, where\nn denotes the length of the string. In this paper, we propose an algorithm to\nfind the most significant substring, whose running time is O(n^{3/2}) with high\nprobability. We also study some variants of this problem such as finding the\ntop-t set, finding all substrings having chi-square greater than a fixed\nthreshold and finding the MSS among substrings greater than a given length. We\nexperimentally demonstrate the asymptotic behavior of the MSS on varying the\nstring size and alphabet size. We also describe some applications of our\nalgorithm on cryptology and real world data from finance and sports. Finally,\nwe compare our technique with the existing heuristics for finding the MSS.\n",
        "  We study the propagation of nucleons and nuclei in tissue-like media within a\nMonte Carlo Model for Heavy-ion Therapy (MCHIT) based on the GEANT4 toolkit\n(version 8.2). The model takes into account fragmentation of projectile nuclei\nand secondary interactions of produced nuclear fragments. Model predictions are\nvalidated with available experimental data obtained for water and PMMA phantoms\nirradiated by monoenergetic carbon-ion beams. The MCHIT model describes well\n(1) the depth-dose distributions in water and PMMA, (2) the doses measured for\nfragments of certain charge, (3) the distributions of positron emitting nuclear\nfragments produced by carbon-ion beams, and (4) the energy spectra of secondary\nneutrons measured at different angles to the beam direction. Radial dose\nprofiles for primary nuclei and for different projectile fragments are\ncalculated and discussed as possible input for evaluation of biological dose\ndistributions. It is shown that at the periphery of the transverse dose profile\nclose to the Bragg peak the dose from secondary nuclear fragments is comparable\nto the dose from primary nuclei.\n",
        "  Using Heegaard Floer homology, we construct a numerical invariant for any\nsmooth, oriented $4$-manifold $X$ with the homology of $S^1 \\times S^3$.\nSpecifically, we show that for any smoothly embedded $3$-manifold $Y$\nrepresenting a generator of $H_3(X)$, a suitable version of the Heegaard Floer\n$d$ invariant of $Y$, defined using twisted coefficients, is a diffeomorphism\ninvariant of $X$. We show how this invariant can be used to obstruct embeddings\nof certain types of $3$-manifolds, including those obtained as a connected sum\nof a rational homology $3$-sphere and any number of copies of $S^1 \\times S^2$.\nWe also give similar obstructions to embeddings in certain open $4$-manifolds,\nincluding exotic $\\mathbb{R}^4$s.\n",
        "  We review methods for determination of \"quasi-periods\" (or \"cycle length\") of\nsignals of low coherence. Such type of variability was called \"cyclic\" for\nsemi-regular red variables, or \"quasi-periodic oscillations\" (QPO) for fast\nvariability in cataclysmic variables and related objects. Methods are\nillustrated by application to AF Cygni.\n",
        "  We define an operation on homology ${B}^4$ which we call an $n$-twist annulus\nmodification. We give a new construction of smoothly slice knots and exotically\nslice knots via $n$-twist annulus modifications. As an application, we present\na new example of a smoothly slice knot with non-slice derivatives. Such\nexamples were first discovered by Cochran and Davis. Also, we relate $n$-twist\nannulus modifications to $n$-fold annulus twists which was first introduced by\nOsoinach, then has been used by Abe and Tange to construct smoothly slice\nknots. Furthermore we consider $n$-twist annulus modifications in more general\nsetting to show that any exotically slice knot can be obtained by the image of\nthe unknot in the boundary of a smooth $4$-manifold homeomorphic to ${B}^4$\nafter an annulus modification.\n",
        "  Since about 10 years ago, University of Applied Science and Technology (UAST)\nin Iran has admitted students in discontinuous associate degree by modular\nmethod, so that almost 100,000 students are accepted every year. Although the\nfirst aim of holding such courses was to improve scientific and skill level of\nemployees, over time a considerable group of unemployed people have been\ninterested to participate in these courses. According to this fact, in this\npaper, we mine and analyze a sample data of accepted candidates in modular 2008\nand 2009 courses by using unsupervised and supervised learning paradigms. In\nthe first step, by using unsupervised paradigm, we grouped (clustered) set of\nmodular accepted candidates based on their student status and labeled data sets\nby three classes so that each class somehow shows educational and student\nstatus of modular accepted candidates. In the second step, by using supervised\nand unsupervised algorithms, we generated predicting models in 2008 data sets.\nThen, by making a comparison between performances of generated models, we\nselected predicting model of association rules through which some rules were\nextracted. Finally, this model is executed for Test set which includes accepted\ncandidates of next course then by evaluation of results, the percentage of\ncorrectness and confidentiality of obtained results can be viewed.\n",
        "  We give formulas expressing Milnor invariants of an n-component link L in the\n3-sphere in terms of the HOMFLYPT polynomial as follows. If the Milnor\ninvariant \\bar{\\mu}_J(L) vanishes for any sequence J with length at most k,\nthen any Milnor \\bar{\\mu}-invariant \\bar{\\mu}_I(L) with length between 3 and\n2k+1 can be represented as a combination of HOMFLYPT polynomial of knots\nobtained from the link by certain band sum operations. In particular, the\n`first non vanishing' Milnor invariants can be always represented as such a\nlinear combination.\n",
        "  This work presents a systematic theoretical and empirical comparison of the\nmajor algorithms that have been proposed for learning Harmonic and Optimality\nTheory grammars (HG and OT, respectively). By comparing learning algorithms, we\nare also able to compare the closely related OT and HG frameworks themselves.\nExperimental results show that the additional expressivity of the HG framework\nover OT affords performance gains in the task of predicting the surface word\norder of Czech sentences. We compare the perceptron with the classic Gradual\nLearning Algorithm (GLA), which learns OT grammars, as well as the popular\nMaximum Entropy model. In addition to showing that the perceptron is\ntheoretically appealing, our work shows that the performance of the HG model it\nlearns approaches that of the upper bound in prediction accuracy on a held out\ntest set and that it is capable of accurately modeling observed variation.\n",
        "  Compressed bitmap indexes are used in databases and search engines. Many\nbitmap compression techniques have been proposed, almost all relying primarily\non run-length encoding (RLE). However, on unsorted data, we can get superior\nperformance with a hybrid compression technique that uses both uncompressed\nbitmaps and packed arrays inside a two-level tree. An instance of this\ntechnique, Roaring, has recently been proposed. Due to its good performance, it\nhas been adopted by several production platforms (e.g., Apache Lucene, Apache\nSpark, Apache Kylin and Druid).\n  Yet there are cases where run-length encoded bitmaps are smaller than the\noriginal Roaring bitmaps---typically when the data is sorted so that the\nbitmaps contain long compressible runs. To better handle these cases, we build\na new Roaring hybrid that combines uncompressed bitmaps, packed arrays and RLE\ncompressed segments. The result is a new Roaring format that compresses better.\n  Overall, our new implementation of Roaring can be several times faster (up to\ntwo orders of magnitude) than the implementations of traditional RLE-based\nalternatives (WAH, Concise, EWAH) while compressing better. We review the\ndesign choices and optimizations that make these good results possible.\n",
        "  It has recently been noted that the relative prevalence of the various kinds\nof epistasis varies along an adaptive walk. This has been explained as a result\nof mean regression in NK model fitness landscapes. Here we show that this\nphenomenon occurs quite generally in fitness landscapes. We propose a simple\nand general explanation for this phenomemon, confirming the role of mean\nregression. We provide support for this explanation with simulations, and\ndiscuss the empirical relevance of our findings.\n",
        "  Methanol (CH$_3$OH) is found to be abundant and widespread towards the\nCentral Molecular Zone, the inner few hundred parsecs of our Galaxy. Its origin\nis, however, not fully understood. It was proposed that the high cosmic ray\nionisation rate in this region could lead to a more efficient non-thermal\ndesorption of this species formed on grain surfaces, but it would also mean\nthat this species is destroyed in a relatively short timescale. In a first\nstep, we run chemical models with a high cosmic ray ionisation rate and find\nthat this scenario can only reproduce the lowest abundances of methanol derived\nin this region ($\\sim$10$^{-9}$-10$^{-8}$). In a second step, we investigate\nanother scenario based on episodic explosions of grain mantles. We find a good\nagreement between the predicted abundances of methanol and the observations. We\nfind that the dominant route for the formation of methanol is through\nhydrogenation of CO on the grains followed by the desorption due to the grain\nmantle explosion. The cyclic aspect of this model can explain the widespread\npresence of methanol without requiring any additional mechanism. We also model\nsilicon monoxide (SiO), another species detected in several molecular clouds of\nthe Galactic Centre. An agreement is found with observations for a high\ndepletion of Si (Si/H $\\sim$ 10$^{-8}$) with respect to the solar abundance.\n",
        "  The transformations, analyses and interpretations of data in scientific\nworkflows are vital for the repeatability and reliability of scientific\nworkflows. This provenance of scientific workflows has been effectively carried\nout in Grid based scientific workflow systems. However, recent adoption of\nCloud-based scientific workflows present an opportunity to investigate the\nsuitability of existing approaches or propose new approaches to collect\nprovenance information from the Cloud and to utilize it for workflow\nrepeatability in the Cloud infrastructure. The dynamic nature of the Cloud in\ncomparison to the Grid makes it difficult because resources are provisioned\non-demand unlike the Grid. This paper presents a novel approach that can assist\nin mitigating this challenge. This approach can collect Cloud infrastructure\ninformation along with workflow provenance and can establish a mapping between\nthem. This mapping is later used to re-provision resources on the Cloud. The\nrepeatability of the workflow execution is performed by: (a) capturing the\nCloud infrastructure information (virtual machine configuration) along with the\nworkflow provenance, and (b) re-provisioning the similar resources on the Cloud\nand re-executing the workflow on them. The evaluation of an initial prototype\nsuggests that the proposed approach is feasible and can be investigated\nfurther.\n",
        "  We define a new strategy for population games based on techniques from\nmachine learning and statistical inference that is essentially uninvadable and\ncan successfully invade (significantly more likely than a neutral mutant)\nessentially all known memory-one strategies for the prisoner's dilemma and\nother population games, including ALLC (always cooperate), ALLD (always\ndefect), tit-for-tat (TFT), win-stay-lose-shift (WSLS), and zero determinant\n(ZD) strategies, including extortionate and generous strategies. We will refer\nto a player using this strategy as an \"information player\" and the specific\nimplementation as $IP_0$. Such players use the history of play to identify\nopponent's strategies and respond accordingly, and naturally learn to cooperate\nwith each other.\n",
        "  We show there exists a linear function w: N->N with the following property.\nLet K be a hyperbolic knot in a hyperbolic 3-manifold M admitting a\nnon-longitudinal S^3 surgery. If K is put into thin position with respect to a\nstrongly irreducible, genus g Heegaard splitting of M then K intersects a thick\nlevel at most 2w(g) times. Typically, this shows that the bridge number of K\nwith respect to this Heegaard splitting is at most w(g), and the tunnel number\nof K is at most w(g) + g-1.\n",
        "  In this paper we describe VIG, a data scaler for benchmarks in the context of\nontology-based data access (OBDA). Data scaling is a relatively recent\napproach, proposed in the database community, that allows for quickly scaling\nup an input data instance to s times its size, while preserving certain\napplication-specific characteristics. The advantage of the approach is that the\nuser is not required to manually input the characteristics of the data to be\nproduced, making it particularly suitable for OBDA benchmarks, where the\ncomplexity of database schemas might pose a challenge for manual input (e.g.,\nthe NPD benchmark contains 70 tables with some containing more than 60\ncolumns). As opposed to a traditional data scaler, VIG includes domain\ninformation provided by the OBDA mappings and the ontology in order to produce\ndata. VIG is currently used in the NPD benchmark, but it is not NPD-specific\nand can be seeded with any data instance. The distinguishing features of VIG\nare (1) its simple and clear generation strategy; (2) its efficiency, as each\nvalue is generated in constant time, without accesses to the disk or to RAM to\nretrieve previously generated values; (3) and its generality, as the data is\nexported in CSV files that can be easily imported by any RDBMS system. VIG is a\njava implementation licensed under Apache 2.0, and its source code is available\non GitHub (https://github.com/ontop/vig) in the form of a Maven project. The\ncode is being maintained since two years by the -ontop- team at the Free\nUniversity of Bozen-Bolzano.\n",
        "  In vivo mapping of the neurite density with diffusion MRI (dMRI) is a high\nbut challenging aim. First, it is unknown whether all neurites exhibit\ncompletely anisotropic ('stick-like') diffusion. Second, the 'density' of\ntissue components may be confounded by non-diffusion properties such as T2\nrelaxation. Third, the domain of validity for the estimated parameters to serve\nas indices of neurite density is incompletely explored. We investigated these\nchallenges by acquiring data with 'b-tensor encoding' and multiple echo times\nin both healthy brain and white matter lesions. Results showed that microscopic\nanisotropy from b-tensor data is associated with myelinated axons but not with\ndendrites. Furthermore, b-tensor and multi-echo data showed that unbiased\ndensity estimates in white matter lesions require data-driven estimates of\ncompartment-specific T2 times. Finally, the 'stick' fractions of different\nbiophysical models could generally not serve as neurite density indices across\nthe healthy brain and white matter lesions, where outcomes of comparisons\ndepended on the choice of constraints. In particular, constraining\ncompartment-specific T2 times was ambiguous in the healthy brain and had a\nlarge impact on estimated values. In summary, estimating neurite density may\nrequire accounting for different diffusion and/or T2 properties between axons\nand dendrites. Constrained 'index' parameters could be valid within limited\ndomains that should be delineated by future studies.\n",
        "  In this paper, we present a set of computational methods to identify the\nlikeliness of a word being borrowed, based on the signals from social media. In\nterms of Spearman correlation coefficient values, our methods perform more than\ntwo times better (nearly 0.62) in predicting the borrowing likeliness compared\nto the best performing baseline (nearly 0.26) reported in literature. Based on\nthis likeliness estimate we asked annotators to re-annotate the language tags\nof foreign words in predominantly native contexts. In 88 percent of cases the\nannotators felt that the foreign language tag should be replaced by native\nlanguage tag, thus indicating a huge scope for improvement of automatic\nlanguage identification systems.\n",
        "  In the real world datasets (e.g.,DBpedia query log), queries built on\nwell-designed patterns containing only AND and OPT operators (for short,\nWDAO-patterns) account for a large proportion among all SPARQL queries. In this\npaper, we present a plugin-based framework for all SELECT queries built on\nWDAO-patterns, named PIWD. The framework is based on a parse tree called\n\\emph{well-designed AND-OPT tree} (for short, WDAO-tree) whose leaves are basic\ngraph patterns (BGP) and inner nodes are the OPT operators. We prove that for\nany WDAO-pattern, its parse tree can be equivalently transformed into a\nWDAO-tree. Based on the proposed framework, we can employ any query engine to\nevaluate BGP for evaluating queries built on WDAO-patterns in a convenient way.\nTheoretically, we can reduce the query evaluation of WDAO-patterns to subgraph\nhomomorphism as well as BGP since the query evaluation of BGP is equivalent to\nsubgraph homomorphism. Finally, our preliminary experiments on gStore and\nRDF-3X show that PIWD can answer all queries built on WDAO-patterns effectively\nand efficiently.\n",
        "  The need for accurate SQL progress estimation in the context of decision\nsupport administration has led to a number of techniques proposed for this\ntask. Unfortunately, no single one of these progress estimators behaves\nrobustly across the variety of SQL queries encountered in practice, meaning\nthat each technique performs poorly for a significant fraction of queries. This\npaper proposes a novel estimator selection framework that uses a statistical\nmodel to characterize the sets of conditions under which certain estimators\noutperform others, leading to a significant increase in estimation robustness.\nThe generality of this framework also enables us to add a number of novel\n\"special purpose\" estimators which increase accuracy further. Most importantly,\nthe resulting model generalizes well to queries very different from the ones\nused to train it. We validate our findings using a large number of industrial\nreal-life and benchmark workloads.\n",
        "  The effect of the quality of starting powders on the microstructure and\nsuperconducting properties of in-situ processed Fe-sheathed MgB2 tapes has been\ninvestigated. Three different types of commercial atomized spherical magnesium\npowder and two different purities of amorphous boron powder were employed. When\nusing the 10-micrometre magnesium as precursor powders, the Mg reacted with\nboron more uniformly and quickly, thus the uniformity of the fabricated MgB2\nwas improved and the grain size of the MgB2 was decreased, hence significant\ncritical current density (Jc) enhancements were achieved for MgB2 tapes. Jc at\n4.2 K for MgB2 tapes made from the 10 um Mg and high purity boron powders was\nat least a factor of ten higher than values measured for MgB2 samples made from\nall other starting powders. At 20 K, 5 T, the typical Jc values of the tapes\nwere over 1.0x10^4 A/cm^2 and were much better than those of tape samples\nreported recently.\n",
        "  In this paper, we enhance the traditional confusion network system\ncombination approach with an additional model trained by a neural network. This\nwork is motivated by the fact that the commonly used binary system voting\nmodels only assign each input system a global weight which is responsible for\nthe global impact of each input system on all translations. This prevents\nindividual systems with low system weights from having influence on the system\ncombination output, although in some situations this could be helpful. Further,\nwords which have only been seen by one or few systems rarely have a chance of\nbeing present in the combined output. We train a local system voting model by a\nneural network which is based on the words themselves and the combinatorial\noccurrences of the different system outputs. This gives system combination the\noption to prefer other systems at different word positions even for the same\nsentence.\n",
        "  This work provides an overall characterization of the kinematic behavior of\nthe ionized gas of the galaxies included in the Calar Alto Legacy Integral\nfield Area (CALIFA), offering kinematic clues to potential users of this survey\nfor including kinematical criteria for specific studies. From the first 200\ngalaxies observed by CALIFA, we present the 2D kinematic view of the 177\ngalaxies satisfying a gas detection threshold. After removing the stellar\ncontribution, we used the cross-correlation technique to obtain the radial\nvelocity of the dominant gaseous component. The main kinematic parameters were\ndirectly derived from the radial velocities with no assumptions on the internal\nmotions. Evidence of the presence of several gaseous components with different\nkinematics were detected by using [OIII] profiles. Most objects in the sample\nshow regular velocity fields, although the ionized-gas kinematics are rarely\nconsistent with simple coplanar circular motions. 35% of the objects present\nevidence of a displacement between the photometric and kinematic centers larger\nthan the original spaxel radii. Only 17% of the objects in the sample exhibit\nkinematic lopsidedness when comparing receding and approaching sides of the\nvelocity fields, but most of them are interacting galaxies exhibiting nuclear\nactivity. Early-type galaxies in the sample present clear photometric-kinematic\nmisaligments. There is evidence of asymmetries in the emission line profiles\nsuggesting the presence of kinematically distinct gaseous components at\ndifferent distances from the nucleus. This work constitutes the first\ndetermination of the ionized gas kinematics of the galaxies observed in the\nCALIFA survey. The derived velocity fields, the reported kinematic\npeculiarities and the identification of the presence of several gaseous\ncomponents might be used as additional criteria for selecting galaxies for\nspecific studies.\n",
        "  We report a study of the stellar content of the Near-infrared cluster\n[DBS2003]\\,157 embedded in the extended H\\,{\\sc ii} region GAL\\,331.31-00.34,\nwhich is associated with the IRAS source 16085-5138. $JHK$ photometry was\ncarried out in order to identify potential ionizing candidates, and the\nfollow-up NIR spectroscopy allowed the spectral classification of some sources,\nincluding two O-type stars. A combination of NIR photometry and spectroscopy\ndata was used to obtain the distance of these two stars, with the method of\nspectroscopic parallax: IRS\\,298 (O6\\,{\\sc V}, $3.35 \\pm 0.61$\\,kpc) and\nIRS\\,339 (O9\\,{\\sc V}, $3.24 \\pm 0.56$\\,kpc). Adopting the average distance of\n$3.29 \\pm 0.58$\\,kpc and comparing the Lyman continuum luminosity of these\nstars with that required to account for the radio continuum flux of the H\\,{\\sc\nii} region, we conclude that these two stars are the ionizing sources of\nGAL\\,331.31-00.34. Young stellar objects (YSOs) were searched by using our NIR\nphotometry and MIR data from the GLIMPSE survey. The analysis of NIR and MIR\ncolour-colour diagrams resulted in 47 YSO candidates.\n  The GLIMPSE counterpart of IRAS\\,16085-5138, which presents IRAS colour\nindices compatible with an ultra-compact H\\,{\\sc ii} region, has been\nidentified. The analysis of its spectral energy distribution between 2 and\n$100\\,\\mu$m revealed that this source shows a spectral index $\\alpha = 3.6$\nbetween 2 and $25\\,\\mu$m, which is typical of a YSO immersed in a protostellar\nenvelope. Lower limits to the bolometric luminosity and the mass of the\nembedded protostar have been estimated as $L=7.7\\times10^3L_{\\sun}$ and $M =\n10\\,M_{\\sun}$, respectively, which corresponds to a B0--B1\\,{\\sc V} ZAMS star.\n",
        "  All dynamical systems of biological interest--be they food webs, regulation\nof genes, or contacts between healthy and infectious individuals--have complex\nnetwork structure. Wigner's semicircular law and Girko's circular law describe\nthe eigenvalues of systems whose structure is a fully connected network.\nHowever, these laws fail for systems with complex network structure. Here we\nshow that in these cases the eigenvalues are described by superellipses. We\nalso develop a new method to analytically estimate the dominant eigenvalue of\ncomplex networks.\n",
        "  We study an undocumented large translucent cloud, detected by means of its\nenhanced radiation on the SHASSA (Southern H-Alpha Sky Survey Atlas) survey. We\nconsider whether its excess surface brightness can be explained by light\nscattered off the dust grains in the cloud, or whether emission from in situ\nionized gas is required. In addition, we aim to determine the temperature of\ndust, the mass of the cloud, and its possible star formation activity. We\ncompare the observed H-alpha surface brightness of the cloud with predictions\nof a radiative transfer model. We use the WHAM (Wisconsin H-Alpha Mapper)\nsurvey as a source for the Galactic H-alpha interstellar radiation field\nilluminating the cloud. Visual extinction through the cloud is derived using\n2MASS J, H, and K band photometry. We use far-IR ISOSS (ISO Serendipitous\nSurvey), IRAS, and DIRBE data to study the thermal emission of dust. The LAB\n(The Leiden/Argentine/Bonn Galactic HI Survey) is used to study 21cm HI\nemission associated with the cloud. Radiative transfer calculations of the\nGalactic diffuse H-alpha radiation indicate that the surface brightness of the\ncloud can be explained solely by radiation scattered off dust particles in the\ncloud. The maximum visual extinction through the cloud is about 1.2mag. The\ncloud is found to be associated with 21cm HI emission at a velocity of about -9\nkm/s. The total mass of the cloud is about 550-1000 solar masses. There is no\nsign of star formation in this cloud. The distance of the cloud is estimated\nfrom the Hipparcos data to be about 100 pc.\n",
        "  In order to identify the pairing symmetry with chirality, we study\nsite-selective NMR in chiral p-wave superconductors. We calculate local nuclear\nrelaxation rate 1/T_1 in the vortex lattice state by Eilenberger theory,\nincluding the applied magnetic field dependence. We find that 1/T_1 in the NMR\nresonance line shape is different between two chiral states\np_{pm}(=p_x{pm}ip_y), depending on whether the chirality is parallel or\nanti-parallel to the vorticity. Anomalous suppression of 1/T_1 occurs around\nthe vortex core in the chiral p_- wave due to the negative coherence term\ncoming from the odd-frequency s-wave Cooper pair induced around the vortex with\nMajorana state.\n",
        "  We present the Runaways and Isolated O-Type Star Spectroscopic Survey of the\nSMC (RIOTS4), a spatially complete survey of uniformly selected field OB stars\nthat covers the entire star-forming body of the SMC. Using the IMACS multislit\nspectrograph and MIKE echelle spectrograph on the Magellan telescopes, we\nobtained spectra of 374 early-type field stars that are at least 28 pc from any\nother OB candidates. We also obtained spectra of an additional 23 field stars\nin the SMC bar identified from slightly different photometric criteria. Here,\nwe present the observational catalog of stars in the RIOTS4 survey, including\nspectral classifications and radial velocities. For three multi-slit fields\ncovering 8% of our sample, we carried out monitoring observations over 9-16\nepochs to study binarity, finding a spectroscopic, massive binary frequency of\nat least $\\sim$60% in this subsample. Classical Oe/Be stars represent a large\nfraction of RIOTS4 (42%), occurring at much higher frequency than in the\nGalaxy, consistent with expectation at low metallicity. RIOTS4 confirmed a\nsteep upper IMF in the field, apparently caused by the inability of the most\nmassive stars to form in the smallest clusters. Our survey also yields evidence\nfor in-situ field OB star formation, and properties of field emission-line star\npopulations, including sgB[e] stars and classical Oe/Be stars. We also discuss\nthe radial velocity distribution and its relation to SMC kinematics and runaway\nstars. RIOTS4 presents a first quantitative characterization of field OB stars\nin an external galaxy, including the contributions of sparse, but normal, star\nformation; runaway stars; and candidate isolated star formation.\n",
        "  This article describes an exclusively resource-based method of morphological\nannotation of written Korean text. Korean is an agglutinative language. Our\nannotator is designed to process text before the operation of a syntactic\nparser. In its present state, it annotates one-stem words only. The output is a\ngraph of morphemes annotated with accurate linguistic information. The\ngranularity of the tagset is 3 to 5 times higher than usual tagsets. A\ncomparison with a reference annotated corpus showed that it achieves 89% recall\nwithout any corpus training. The language resources used by the system are\nlexicons of stems, transducers of suffixes and transducers of generation of\nallomorphs. All can be easily updated, which allows users to control the\nevolution of the performances of the system. It has been claimed that\nmorphological annotation of Korean text could only be performed by a\nmorphological analysis module accessing a lexicon of morphemes. We show that it\ncan also be performed directly with a lexicon of words and without applying\nmorphological rules at annotation time, which speeds up annotation to 1,210\nword/s. The lexicon of words is obtained from the maintainable language\nresources through a fully automated compilation process.\n",
        "  Based on recent work by Futer, Kalfagianni and Purcell, we prove that the\nvolume of sufficiently complicated positive braid links is proportional to the\nsignature defect $\\Delta \\sigma=2g-\\sigma$.\n",
        "  Data describing the historical growth of human population global and regional\n(Western Europe, Eastern Europe, Asia, former USSR, Africa and Latin America)\nare analysed. Results are in harmony with the earlier analysis of the\nhistorical growth of the world population in the past 12,000 years and with a\nsimilar but limited study carried out over 50 years ago. This analysis is also\nin harmony with the study of the historical economic growth. Within the range\nof analysable data, there was no Malthusian stagnation. Takeoffs from\nstagnation to growth, postulated by the Unified Growth Theory never happened.\nThere were no escapes from the Malthusian trap because there was no trap. This\nanalysis and the earlier studies of the Gross Domestic Product lead to the\nconclusion that there were also no takeoffs in the income per capita\ndistributions, claimed by the Unified Growth Theory. Consequently, the claimed\nin this theory differential timing in takeoffs never happened. Unified Growth\nTheory is contradicted yet again by the mathematical analysis of the same data,\nwhich were used, but never analysed, during the formulation of this theory.\nHowever, this study, as well as the earlier publications on the related topics,\nshows also that some fundamental postulates used in the economic and\ndemographic research are repeatedly contradicted by the mathematical analysis\nof data.\n",
        "  We explore the factors influencing the dependence of single sentences on\ntheir larger textual context in order to automatically identify candidate\nsentences for language learning exercises from corpora which are presentable in\nisolation. An in-depth investigation of this question has not been previously\ncarried out. Understanding this aspect can contribute to a more efficient\nselection of candidate sentences which, besides reducing the time required for\nitem writing, can also ensure a higher degree of variability and authenticity.\nWe present a set of relevant aspects collected based on the qualitative\nanalysis of a smaller set of context-dependent corpus example sentences.\nFurthermore, we implemented a rule-based algorithm using these criteria which\nachieved an average precision of 0.76 for the identification of different\nissues related to context dependence. The method has also been evaluated\nempirically where 80% of the sentences in which our system did not detect\ncontext-dependent elements were also considered context-independent by human\nraters.\n",
        "  Given two collections of set objects $R$ and $S$, the $R \\bowtie_{\\subseteq}\nS$ set containment join returns all object pairs $(r, s) \\in R \\times S$ such\nthat $r \\subseteq s$. Besides being a basic operator in all modern data\nmanagement systems with a wide range of applications, the join can be used to\nevaluate complex SQL queries based on relational division and as a module of\ndata mining algorithms. The state-of-the-art algorithm for set containment\njoins (PRETTI) builds an inverted index on the right-hand collection $S$ and a\nprefix tree on the left-hand collection $R$ that groups set objects with common\nprefixes and thus, avoids redundant processing. In this paper, we present a\nframework which improves PRETTI in two directions. First, we limit the prefix\ntree construction by proposing an adaptive methodology based on a cost model;\nthis way, we can greatly reduce the space and time cost of the join. Second, we\npartition the objects of each collection based on their first contained item,\nassuming that the set objects are internally sorted. We show that we can\nprocess the partitions and evaluate the join while building the prefix tree and\nthe inverted index progressively. This allows us to significantly reduce not\nonly the join cost, but also the maximum memory requirements during the join.\nAn experimental evaluation using both real and synthetic datasets shows that\nour framework outperforms PRETTI by a wide margin.\n",
        "  We use data from the Multi-Unit Spectroscopic Explorer (MUSE), recently\ncommissioned at the Very Large Telescope (VLT), to study the kinematics and\nstellar population content of NGC 4371, an early-type massive barred galaxy in\nthe core of the Virgo cluster. We integrate this study with a detailed\nstructural analysis using imaging data from the Hubble and Spitzer space\ntelescopes, which allows us to perform a thorough investigation of the physical\nproperties of the galaxy. We show that the rotationally supported inner\ncomponents in NGC 4371, an inner disc and a nuclear ring - which, according to\nthe predominant scenario, are built with stars formed from gas brought to the\ninner region by the bar - are vastly dominated by stars older than 10 Gyr. Our\nresults thus indicate that the formation of the bar occurred at a redshift of\nabout $z=1.8^{+0.5}_{-0.4}$ (error bars are derived from 100 Monte Carlo\nrealisations). NGC 4371 thus testifies to the robustness of bars. In addition,\nthe mean stellar age of the fraction of the major disc of the galaxy covered by\nour MUSE data is above 7 Gyr, with a small contribution from younger stars.\nThis suggests that the quenching of star formation in NGC 4371, likely due to\nenvironmental effects, was already effective at a redshift of about\n$z=0.8^{+0.2}_{-0.1}$. Our results point out that bar-driven secular evolution\nprocesses may have an extended impact in the evolution of galaxies, and thus on\nthe properties of galaxies as observed today, not necessarily restricted to\nmore recent cosmic epochs.\n",
        "  We find that Koschorke's $\\beta$-invariant and the triple $\\mu$-invariant of\nlink maps in the critical dimension can be computed as degrees of certain maps\nof configuration spaces - just like the linking number. Both formulas admit\ngeometric interpretations in terms of Vassiliev's ornaments via new operations\nakin to the Jin suspension, and both were unexpected for the author, because\nthe only known direct ways to extract $\\mu$ and $\\beta$ from invariants of maps\nbetween configuration spaces involved some homotopy theory (Whitehead products\nand the stable Hopf invariant, respectively).\n",
        "  We present results based on $YJK_{\\rm s}$ photometry of star clusters in the\nLarge Magellanic Cloud (LMC), distributed throughout the central part of the\ngalaxy's bar and the 30 Doradus region. We analysed the field-star\ndecontaminated colour--magnitude diagrams of 313 clusters to estimate their\nreddening values and ages. The clusters are affected by a mean reddening of\n$E(B-V) \\in [0.2,0.3]$ mag, where the average internal LMC reddening amounts to\n$\\sim$ 0.1--0.2 mag. The region covering 30 Doradus includes clusters with\nreddening values in excess of $E(B-V)$ = 0.4 mag. Our cluster sample spans the\nage range $7.0 \\le \\log(t$ yr$^{-1}) < 9.0$, represents an increase of 30 per\ncent in terms of the number of clusters with robust age estimates and comprises\na statistically complete sample in the LMC regions of interest here. The\nresulting cluster frequencies suggest that the outermost regions of the LMC bar\nfirst experienced enhanced cluster formation -- $\\log(t$ yr$^{-1}) \\in\n[8.5,9.0]$ -- before the activity proceeded, although in a patchy manner, to\nthe innermost regions, for $\\log(t$ yr$^{-1}) < 7.7$. Cluster frequencies in\nthe 30 Doradus region show that the area is dominated by very recent cluster\nformation. The derived star-formation frequencies suggest that the cluster and\nfield-star populations do not seem to have fully evolved as fully coupled\nsystems during the last $\\sim$ 100 Myr.\n",
        "  End-to-end task-oriented dialog systems usually suffer from the challenge of\nincorporating knowledge bases. In this paper, we propose a novel yet simple\nend-to-end differentiable model called memory-to-sequence (Mem2Seq) to address\nthis issue. Mem2Seq is the first neural generative model that combines the\nmulti-hop attention over memories with the idea of pointer network. We\nempirically show how Mem2Seq controls each generation step, and how its\nmulti-hop attention mechanism helps in learning correlations between memories.\nIn addition, our model is quite general without complicated task-specific\ndesigns. As a result, we show that Mem2Seq can be trained faster and attain the\nstate-of-the-art performance on three different task-oriented dialog datasets.\n",
        "  Parallel hole collimators which are currently routinely used in SPECT imaging\nwere designed a few decades ago, when filtered backprojection (FBP) was used\nfor tomographic reconstruction. Statistical reconstruction methods with precise\nmodeling of the projection measurement offer a different standard, and the\nquestion of optimizing resolution - sensitivity tradeoff by choosing\nappropriate collimator aperture needs to be revised. In this paper we search\nfor a parallel hole collimator which offers best performance in detection of\nhot lesions on a uniform background. To evaluate the image quality we use\nstandard quantitative measures like the contrast recovery coefficient,\ncoefficient of variation, and contrast to noise ratio. We also performed signal\ndetection by human observers followed by Receiver Operating Characteristics\n(ROC) analysis for a few lesions close to the limit of visibility. Our results\nconsistently indicate that optimal performance is achieved with collimators of\nsensitivity 6 - 9 times that of the conventional high resolution (HR)\ncollimator. When the optimized collimator is applied, the image quality of the\nconventional image obtained with the HR can be achieved at scanning time or\nactivity dose reduced by factor of 3.\n",
        "  Despite 25 years of research in academia, approximate query processing (AQP)\nhas had little industrial adoption. One of the major causes of this slow\nadoption is the reluctance of traditional vendors to make radical changes to\ntheir legacy codebases, and the preoccupation of newer vendors (e.g.,\nSQL-on-Hadoop products) with implementing standard features. Additionally, the\nfew AQP engines that are available are each tied to a specific platform and\nrequire users to completely abandon their existing databases---an unrealistic\nexpectation given the infancy of the AQP technology. Therefore, we argue that a\nuniversal solution is needed: a database-agnostic approximation engine that\nwill widen the reach of this emerging technology across various platforms.\n  Our proposal, called VerdictDB, uses a middleware architecture that requires\nno changes to the backend database, and thus, can work with all off-the-shelf\nengines. Operating at the driver-level, VerdictDB intercepts analytical queries\nissued to the database and rewrites them into another query that, if executed\nby any standard relational engine, will yield sufficient information for\ncomputing an approximate answer. VerdictDB uses the returned result set to\ncompute an approximate answer and error estimates, which are then passed on to\nthe user or application. However, lack of access to the query execution layer\nintroduces significant challenges in terms of generality, correctness, and\nefficiency. This paper shows how VerdictDB overcomes these challenges and\ndelivers up to 171$\\times$ speedup (18.45$\\times$ on average) for a variety of\nexisting engines, such as Impala, Spark SQL, and Amazon Redshift, while\nincurring less than 2.6% relative error. VerdictDB is open-sourced under Apache\nLicense.\n",
        "  In this paper we propose to exploit the automatic Quality Estimation (QE) of\nASR hypotheses to perform the unsupervised adaptation of a deep neural network\nmodeling acoustic probabilities. Our hypothesis is that significant\nimprovements can be achieved by: i)automatically transcribing the evaluation\ndata we are currently trying to recognise, and ii) selecting from it a subset\nof \"good quality\" instances based on the word error rate (WER) scores predicted\nby a QE component. To validate this hypothesis, we run several experiments on\nthe evaluation data sets released for the CHiME-3 challenge. First, we operate\nin oracle conditions in which manual transcriptions of the evaluation data are\navailable, thus allowing us to compute the \"true\" sentence WER. In this\nscenario, we perform the adaptation with variable amounts of data, which are\ncharacterised by different levels of quality. Then, we move to realistic\nconditions in which the manual transcriptions of the evaluation data are not\navailable. In this case, the adaptation is performed on data selected according\nto the WER scores \"predicted\" by a QE component. Our results indicate that: i)\nQE predictions allow us to closely approximate the adaptation results obtained\nin oracle conditions, and ii) the overall ASR performance based on the proposed\nQE-driven adaptation method is significantly better than the strong, most\nrecent, CHiME-3 baseline.\n",
        "  We performed a numerical experiment designed for core formation in a\nself-gravitating, magnetically supercritical, supersonically turbulent,\nisothermal cloud. A density probability distribution function (PDF) averaged\nover a converged turbulent state before turning self-gravity on is well-fitted\nwith a lognormal distribution. However, after turning self-gravity on, the\nvolume fractions of density PDFs at a high density tail, compared with the\nlognormal distribution, increase as time goes on. In order to see the effect of\nself-gravity on core formation rates, we compared the core formation rate per\nfree-fall time (CFR$_{\\rm ff}$) from the theory based on the lognormal\ndistribution and the one from our numerical experiment. For our fiducial value\nof a critical density, 100, normalised with an initial value, the latter\nCFR$_{\\rm ff}$ is about 30 times larger the former one. Therefore, self-gravity\nplays an important role in significantly increasing CFR$_{\\rm ff}$. This result\nimplies that core (star) formation rates or core (stellar) mass functions\npredicted from theories based on the lognormal density PDF need some\nmodifications. Our result of the increased volume fraction of density PDFs\nafter turning self-gravity on is consistent with power-law like tails commonly\nobserved at higher ends of visual extinction PDFs of active star-forming\nclouds.\n",
        "  Superfluidity and superconductivity have many elements in common. However, I\nargue that their most important commonality has been overlooked: that both are\nkinetic energy driven. Clear evidence that superfluidity in $^4He$ is kinetic\nenergy driven is the shape of the $\\lambda$ transition and the negative thermal\nexpansion coefficient below $T_\\lambda$. Clear evidence that superconductivity\nis kinetic energy driven is the Meissner effect: I argue that otherwise the\nMeissner effect would not take place. Associated with this physics I predict\nthat superconductors expel negative charge from the interior to the surface and\nthat a spin current exists in the ground state of superconductors (spin\nMeissner effect). I propose that this common physics of superconductors and\nsuperfluids originates in rotational zero point motion. This view of\nsuperconductivity and superfluidity implies that rotational zero-point motion\nis a fundamental property of the quantum world that is missed in the current\nunderstanding.\n",
        "  Existing benchmarks for analytical database systems such as TPC-DS and TPC-H\nare designed for static reporting scenarios. The main metric of these\nbenchmarks is the performance of running individual SQL queries over a\nsynthetic database. In this paper, we argue that such benchmarks are not\nsuitable for evaluating database workloads originating from interactive data\nexploration (IDE) systems where most queries are ad-hoc, not based on\npredefined reports, and built incrementally. As a main contribution, we present\na novel benchmark called IDEBench that can be used to evaluate the performance\nof database systems for IDE workloads. As opposed to traditional benchmarks for\nanalytical database systems, our goal is to provide more meaningful workloads\nand datasets that can be used to benchmark IDE query engines, with a particular\nfocus on metrics that capture the trade-off between query performance and\nquality of the result. As a second contribution, this paper evaluates and\ndiscusses the performance results of selected IDE query engines using our\nbenchmark. The study includes two commercial systems, as well as two research\nprototypes (IDEA, approXimateDB/XDB), and one traditional analytical database\nsystem (MonetDB).\n",
        "  We show how to construct a Kirby diagram for a large class of finite volume\nhyperbolic 4-manifolds constructed by J. Ratcliffe and S. Tschantz.\n",
        "  We present a method for measuring the Sun's motion using the proper motions\nof Galactic halo star streams. The method relies on the fact that the motion of\nthe stars perpendicular to a stream from a low-mass progenitor is close to zero\nwhen viewed from a non-rotating frame at rest with respect to the Galaxy, and\nthat the deviation from zero is due to the reflex motion of the observer. The\nprocedure we implement here has the advantage of being independent of the\nGalactic mass distribution. We run a suite of simulations to test the algorithm\nwe have developed, and find that we can recover the input Solar motion to good\naccuracy with data of the quality that will soon become available from the\nESA/Gaia mission.\n",
        "  We present for the first time extended stellar density and/or surface\nbrightness radial profiles for almost all the known Large Magellanic Cloud\n(LMC) old globular clusters (GCs). These were built from DECam images and reach\nout to ~ 4 times the GCs' tidal radii. The background subtracted radial\nprofiles reveal that the GCs located closer than ~ 5 kpc from the LMC centre\ncontain an excess of stars in their outermost regions with respect to the\nstellar density expected from a King profile. Such a residual amount of stars -\nnot seen in GCs located farther than ~ 5 kpc from the LMC centre-, as well as\nthe GCs' dimensions, show a clear dependence with the GCs' positions in the\ngalaxy, in the sense that, the farther the GC from the centre of the LMC, the\nlarger both the excess of stars in its outskirts and size. Although the masses\nof GCs located inside and outside ~ 5 kpc are commensurate, the outermost\nregions of GCs located closer than ~ 5 kpc from the LMC centre appear to have\ndynamically evolved more quickly. These outcomes can be fully interpreted in\nthe light of the known GC radial velocity disc-like kinematics, from which GCs\nhave been somehow mostly experiencing the influence of the LMC gravitational\nfield at their respective mean distances from the LMC centre.\n",
        "  The Reeb graph $\\mathcal{R}(f) $ is one of the fundamental invariants of a\nsmooth function $f\\colon M\\to \\mathbb{R} $ with isolated critical points. It is\ndefined as the quotient space $M/_{\\!\\sim}$ of the closed manifold $M$ by a\nrelation that depends on $f$. Here we construct a $1$-dimensional complex\n$\\Gamma(f)$ embedded into $M$ which is homotopy equivalent to $\\mathcal{R}(f)$.\nAs a consequence we show that for every function $f$ on a manifold with finite\nfundamental group, the Reeb graph of $f$ is a tree. If $\\pi_1(M)$ is an abelian\ngroup, or more general, a discrete amenable group, then $\\mathcal{R}(f)$\ncontains at most one loop. Finally we prove that the number of loops in the\nReeb graph of every function on a surface $M_g$ is estimated from above by $g$,\nthe genus of $M_g$.\n",
        "  In this paper we study the relation between two diagrammatic representations\nof links in lens spaces: the disk diagram and the grid diagram and we find how\nto pass from one to the other. We also investigate whether the HOMFLY-PT\ninvariant and the Link Floer Homology are essential invariants, that is, we try\nto understand if these invariants are able to distinguish links in $L(p,q)$\ncovered by the same link in $\\mathbf{S}^3$. In order to do so, we generalize\nthe combinatorial definition of Knot Floer Homology in lens spaces to the case\nof links and we analyze how both the invariants change when we switch the\norientation of the link.\n",
        "  According to the unified model of active galactic nuclei, Seyfert 2 galaxies\nare physically the same as Seyfert 1 objects and they possess a broad-line\nregion (BLR), but it is hidden from the observer due to their orientation. In\nthe past few years, various authors reported that not all Seyfert 2 galaxies\nharbor a BLR. We compiled a sample of 38 Seyfert 2 galaxies to find non-hidden\nbroad-line region (non-HBLR) objects. Using the theory of Nicastro et al. which\nsuggests the existence of a critical value of the Eddington ratio below which\nBLR can't be formed, we found 26 non-HBLR Seyfert 2 candidates. We found also\nthat 5 of these 26 non-HBLR objects could be low-ionization nuclear\nemission-line regions (LINERs).\n",
        "  We present high-resolution ($\\sim 1\"$), 1.5 GHz continuum observations of the\nbrightest cluster galaxies (BCGs) of 13 CLASH (Cluster Lensing And Supernova\nsurvey with Hubble) clusters at $0.18<z<0.69$ with the Karl G. Jansky Very\nLarge Array (JVLA). Radio emission is clearly detected and characterized for 11\nBCGs, while for two of them we obtain only upper limits to their radio flux\n($<0.1$ mJy at 5$\\sigma$ confidence level). We also consider five additional\nclusters whose BCG is detected in FIRST or NVSS. We find radio powers in the\nrange from $2\\times 10^{23}$ to $\\sim 10^{26}$ $W~Hz^{-1}$ and radio spectral\nindices $\\alpha_{1.5}^{30}$ (defined as the slope between 1.5 and 30 GHz)\ndistributed from $\\sim -1$ to $-0.25$ around the central value $\\langle \\alpha\n\\rangle= - 0.68$. The radio emission from the BCGs is resolved in three cases\n(Abell 383, MACS J1931, and RX J2129), and unresolved or marginally resolved in\nthe remaining eight cases observed with JVLA. In all the cases the BCGs are\nconsistent with being powered by active galactic nuclei (AGN). The radio power\nshows a positive correlation with the BCG star formation rate, and a negative\ncorrelation with the central entropy of the surrounding intracluster medium\n(ICM) except in two cases (MACS J1206 and CL J1226). Finally, over the\nrestricted range in radio power sampled by the CLASH BCGs, we observe a\nsignificant scatter between the radio power and the average mechanical power\nstored in the ICM cavities.\n",
        "  The interstellar abundances of refractory elements indicate a substantial\ndepletion from the gas phase, that increases with gas density. Our recent model\nof dust evolution, based on hydrodynamic simulations of the lifecycle of giant\nmolecular clouds (GMCs) proves that the observed trend for [Si$_{gas}$/H] is\ndriven by a combination of dust growth by accretion in the cold diffuse\ninterstellar medium (ISM) and efficient destruction by supernova (SN) shocks\n(Zhukovska et al. 2016). With an analytic model of dust evolution, we\ndemonstrate that even with optimistic assumptions for the dust input from stars\nand without destruction of grains by SNe it is impossible to match the observed\n[Si$_{gas}$/H]$-n_H$ relation without growth in the ISM. We extend the\nframework developed in our previous work for silicates to include the evolution\nof iron grains and address a long-standing conundrum: ``Where is the\ninterstellar iron?'. Much higher depletion of Fe in the warm neutral medium\ncompared to Si is reproduced by the models, in which a large fraction of\ninterstellar iron (70%) is locked as inclusions in silicate grains, where it is\nprotected from sputtering by SN shocks. The slope of the observed\n[Fe$_{gas}$/H]$-n_H$ relation is reproduced if the remaining depleted iron\nresides in a population of metallic iron nanoparticles with sizes in the range\nof 1-10nm. Enhanced collision rates due to the Coulomb focusing are important\nfor both silicate and iron dust models to match the observed slopes of the\nrelations between depletion and density and the magnitudes of depletion at high\ndensity.\n",
        "  We derive two types of linearity conditions for mapping class groups of\norientable surfaces: one for once-punctured surface, and the other for closed\nsurface, respectively. For the once-punctured case, the condition is described\nin terms of the action of the mapping class group on the deformation space of\nlinear representations of the fundamental group of the corresponding closed\nsurface. For the closed case, the condition is described in terms of the vector\nspace generated by the isotopy classes of essential simple closed curves on the\ncorresponding surface. The latter condition also describes the linearity for\nthe mapping class group of compact orientable surface with boundary, up to\ncenter.\n",
        "  We construct the universal sl(2)-tangle cohomology using an approach with\nwebs and dotted foams. This theory depends on two parameters, and for the case\nof links it is a categorification of the unnormalized Jones polynomial of the\nlink.\n",
        "  Our present understanding of high-mass star formation still remains very\nschematic. In particular, it is not yet clear how much of the difference\nbetween low-mass and high-mass star formation occurs during the earliest star\nformation phases. The chemical characteristics of massive cold clumps, and the\ncomparison with those of their low-mass counterparts, could provide crucial\nclues about the exact role that chemistry plays in differentiating the early\nphases of low-mass and high-mass star formation. Water, in particular, is a\nunique probe of physical and chemical conditions in star-forming regions. Using\nthe HIFI instrument of Herschel we have observed the ortho-NH3 (1_0-0_0)\n(572GHz), ortho-H2O (1_10-1_01) (557GHz) and N2H+ (6-5) (559GHz) lines toward a\nsample of high-mass starless and proto-stellar clumps selected from the\n\"Herschel} Infrared Galactic Plane Survey\" (Hi-GAL). We compare our results to\nprevious studies of low-mass and high-mass proto-stellar objects. At least one\nof the three molecular lines was detected in 4 (out of 35) and 7 (out of 17)\nobjects in the l=59deg and l=30deg galactic regions, respectively. All detected\nsources are proto-stellar. The water spectra are complex and consist of several\nkinematic components, identified through a Gaussian decomposition, and in a few\nsources inverse and regular P-Cygni profiles have been detected. All water line\nprofiles of the l=59deg region are dominated by a broad Gaussian emission\nfeature, indicating that the bulk of the water emission arises in outflows. No\nsuch broad emission is detected toward the l=30deg objects. The ammonia line in\nsome cases also shows line wings and an inverse P-Cygni profile, thus\nconfirming that NH3 rotational transitions can be used to probe the dynamics of\nhigh-mass star forming regions. Both bolometric and water line luminosity\nincrease with the continuum temperature.\n",
        "  We give criteria for Morin singularities for germs of maps into lower\ndimensions. As an application, we study the bifurcation of Lefschetz\nsingularities.\n",
        "  Dose calculation for radiotherapy with protons and heavier ions deals with a\nlarge volume of path integrals involving a scattering power of body tissue.\nThis work provides a simple model for such demanding applications. There is an\napproximate linearity between RMS end-point displacement and range of incident\nparticles in water, empirically found in measurements and detailed\ncalculations. This fact was translated into a simple linear formula, from which\nthe scattering power that is only inversely proportional to residual range was\nderived. The simplicity enabled analytical formulation for ions stopping in\nwater, which was designed to be equivalent with the extended Highland model and\nagreed with measurements within 2% or 0.02 cm in RMS displacement. The\nsimplicity will also improve the efficiency of numerical path integrals in the\npresence of heterogeneity.\n",
        "  We make use of the action of $H_1(Y)$ in Heegaard Floer homology to\ngeneralize the Ozsv\\'ath-Szab\\'o correction terms for $3$-manifolds with\nstandard $\\operatorname{HF}^\\infty$. We establish the basic properties of these\ninvariants: conjugation invariance, behavior under orientation reversal,\nadditivity, and spin$^c$ rational homology cobordism invariance.\n",
        "  The angular resolution (~10\") achieved by the Herschel Space Observatory\n~3.5m telescope at FIR wavelengths allowed us to roughly separate the emission\ntoward the inner parsec of the galaxy (the central cavity) from that of the\nsurrounding circumnuclear disk (the CND). The FIR spectrum toward SgrA* is\ndominated by intense [Oiii], [Oi], [Cii], [Niii], [Nii], and [Ci]\nfine-structure lines (in decreasing order of luminosity) arising in gas\nirradiated by the strong UV field from the central stellar cluster. The high-J\nCO rotational line intensities observed at the interface between the inner CND\nand the central cavity are consistent with a hot isothermal component at\nT~10^{3.1} K and n(H_2)~10^4 cm^{-3}. They are also consistent with a\ndistribution of lower temperatures at higher gas density, with most CO at T~300\nK. The hot CO component (either the bulk of the CO column density or just a\nsmall fraction depending on the above scenario) likely results from a\ncombination of UV and shock-driven heating. Although this component is beam\ndiluted in our FIR observations, it may be resolved at much higher angular\nresolution. An ALMA project using different molecular tracers to characterize\nUV-irradiated shocks in the innermost layers of the CND is ongoing.\n",
        "  In this paper, we present new data with interstellar C2 (Phillips bands A-X),\nfrom observations made with the Ultraviolet-Visual Echelle Spectrograph of the\nEuropean Southern Observatory. We have determined the interstellar column\ndensities and excitation temperatures of C2 for nine Galactic lines. For seven\nof these, C2 has never been observed before, so in this case the still small\nsample of interstellar clouds (26 lines of sight), where a detailed analysis of\nC2 excitation has been made, has increased significantly. This paper is a\ncontinuation of previous works where interstellar molecules (C2 and diffuse\ninterstellar bands) have been analysed. Because the sample of interstellar\nclouds with C2 has increased, we can show that the width and shape of the\nprofiles of some diffuse interstellar bands (6196 and 5797 A) apparently depend\non the gas kinetic and rotational temperatures of C2; the profiles are broader\nbecause of the higher values of the gas kinetic and rotational temperatures of\nC2. There are also diffuse interstellar bands (4964 and 5850 A) for which this\neffect does not exist.\n",
        "  We present the Stanford Question Answering Dataset (SQuAD), a new reading\ncomprehension dataset consisting of 100,000+ questions posed by crowdworkers on\na set of Wikipedia articles, where the answer to each question is a segment of\ntext from the corresponding reading passage. We analyze the dataset to\nunderstand the types of reasoning required to answer the questions, leaning\nheavily on dependency and constituency trees. We build a strong logistic\nregression model, which achieves an F1 score of 51.0%, a significant\nimprovement over a simple baseline (20%). However, human performance (86.8%) is\nmuch higher, indicating that the dataset presents a good challenge problem for\nfuture research.\n  The dataset is freely available at https://stanford-qa.com\n",
        "  In the paper, we analyze the distribution of complexities in the Vai script,\nan indigenous syllabic writing system from Liberia. It is found that the\nuniformity hypothesis for complexities fails for this script. The models using\nPoisson distribution for the number of components and hyper-Poisson\ndistribution for connections provide good fits in the case of the Vai script.\n",
        "  [abridged] Energy relaxation around a massive black hole (MBH) is key to\nestablishing the dynamical state of galactic nuclei, and the nature of close\nstellar interactions with the MBH. The standard description of relaxation as\ndiffusion provides a perturbative 2nd-order solution in the weak two-body\ninteraction limit. We run N-body simulations and find that this solution fails\nto describe the non-Gaussian relaxation on short timescale, which is strongly\ninfluenced by extreme events even in the weak limit, and is thus difficult to\ncharacterize and measure. We derive a non-perturbative solution for relaxation\nas an anomalous diffusion process, and develop a robust estimation technique to\nmeasure it in simulations. These enable us to analyze and model our numerical\nresults, and validate in detail, for the first time, this model of energy\nrelaxation around an MBH on all timescales. We derive the relation between the\nenergy diffusion time, t_E, and the time for a small perturbation to return to\nsteady state, t_r, in a relaxed, single mass cusp around a MBH. We constrain\nthe contribution of strong encounters, measure that of the weakest encounters,\ndetermine the value of the Coulomb logarithm, and provide a robust analytical\nestimate for t_E in a finite nuclear stellar cusp. We find that t_r ~ 10t_E\n~(5/32)Q^2P_h/N_h log Q, where Q=M_bh/M_* is the MBH to star mass ratio, the\norbital period P_h and number of stars N_h are evaluated at the energy scale\ncorresponding to the MBH's sphere of influence, E_h=sigma_inf^2, where\nsigma_inf is the velocity dispersion far from the MBH. We conclude, using the\nobserved cosmic M_bh/sigma correlation, that cusps around lower-mass MBHs\n(M_bh<10^7 Mo), which evolved passively over a Hubble time, should be relaxed.\nWe consider the effects of anomalous energy diffusion on orbital perturbations\nof stars observed near the Galactic MBH.\n",
        "  A polystore system is a database management system (DBMS) composed of\nintegrated heterogeneous database engines and multiple programming languages.\nBy matching data to the storage engine best suited to its needs, complex\nanalytics run faster and flexible storage choices helps improve data\norganization. BigDAWG (Big Data Working Group) is our reference implementation\nof a polystore system. In this paper, we describe the current BigDAWG software\nrelease which supports PostgreSQL, Accumulo and SciDB. We describe the overall\narchitecture, API and initial results of applying BigDAWG to the MIMIC II\nmedical dataset.\n",
        "  Question generation from a knowledge base (KB) is the task of generating\nquestions related to the domain of the input KB. We propose a system for\ngenerating fluent and natural questions from a KB, which significantly reduces\nthe human effort by leveraging massive web resources. In more detail, a seed\nquestion set is first generated by applying a small number of hand-crafted\ntemplates on the input KB, then more questions are retrieved by iteratively\nforming already obtained questions as search queries into a standard search\nengine, before finally questions are selected by estimating their fluency and\ndomain relevance. Evaluated by human graders on 500 random-selected triples\nfrom Freebase, questions generated by our system are judged to be more fluent\nthan those of \\newcite{serban-EtAl:2016:P16-1} by human graders.\n",
        "  There is a growing need for distributed graph processing systems that are\ncapable of gracefully scaling to very large graph datasets. Unfortunately, this\nchallenge has not been easily met due to the intense memory pressure imposed by\nprocess-centric, message passing designs that many graph processing systems\nfollow. Pregelix is a new open source distributed graph processing system that\nis based on an iterative dataflow design that is better tuned to handle both\nin-memory and out-of-core workloads. As such, Pregelix offers improved\nperformance characteristics and scaling properties over current open source\nsystems (e.g., we have seen up to 15x speedup compared to Apache Giraph and up\nto 35x speedup compared to distributed GraphLab), and makes more effective use\nof available machine resources to support Big(ger) Graph Analytics.\n",
        "  We consider the relationship between hyperbolic cone-manifold structures on\nsurfaces, and algebraic representations of the fundamental group into a group\nof isometries. A hyperbolic cone-manifold structure on a surface, with all\ninterior cone angles being integer multiples of $2\\pi$, determines a holonomy\nrepresentation of the fundamental group. We ask, conversely, when a\nrepresentation of the fundamental group is the holonomy of a hyperbolic\ncone-manifold structure. In this paper we build upon previous work with\npunctured tori to prove results for higher genus surfaces. Our techniques\nconstruct fundamental domains for hyperbolic cone-manifold structures, from the\ngeometry of a representation. Central to these techniques are the Euler class\nof a representation, the group $\\widetilde{PSL_2\\R}$, the twist of hyperbolic\nisometries, and character varieties. We consider the action of the outer\nautomorphism and related groups on the character variety, which is\nmeasure-preserving with respect to a natural measure derived from its\nsymplectic structure, and ergodic in certain regions. Under various hypotheses,\nwe almost surely or surely obtain a hyperbolic cone-manifold structure with\nprescribed holonomy.\n",
        "  Proton scattering in some water and tissue equivalent phantom materials was\nmeasured to evaluate their simulation accuracy of water and respective human\nbiological tissues. The measurements were performed on the medical facility of\nthe ITEP synchrotron, proton energy was 219 MeV, a narrow beam was formed by a\n3 mm collimator. A stack of plastic slabs was set closely to the collimator\nhole as a scatterer. Three types of Plastic Water (PW, PW LR and PW DT), lung,\ncortical bone, adipose and muscle plastics (CIRS Inc., USA) were used in the\nexperiments as the substitutes under investigation and liquid water and PMMA\nslabs as reference materials. Dose (intensity) profiles were measured for each\nsample by two orthogonal strips of the Gafchromic EBT film. A total thickness\nof the plastic slab was from 4 to 16 cm depending on the material. The\nGafchromic film response nonlinearity was taken into account by an additional\ncalibration vs. absorbed dose in a wide proton beam, the temporal\nirradiation-to-scanning dependence was also accounted. The central part of each\nangular distribution was fitted by the Gaussian function and compared with the\nrespective parameters calculated for the simulated medium by Monte Carlo\ntechnique with the IThMC code.\n",
        "  We propose that one of the sources in the recently detected system CR7 by\nSobral et al. (2015) through spectro-photometric measurements at $z = 6.6$\nharbors a direct collapse blackhole (DCBH). We argue that the LW radiation\nfield required for direct collapse in source A is provided by sources B and C.\nBy tracing the LW production history and star formation rate over cosmic time\nfor the halo hosting CR7 in a $\\Lambda$CDM universe, we demonstrate that a DCBH\ncould have formed at $z\\sim 20$. The spectrum of source A is well fit by\nnebular emission from primordial gas around a BH with MBH $\\sim 4.4 \\times 10^6\n\\ \\rm M_{\\odot}$ accreting at a 40 % of the Eddington rate, which strongly\nsupports our interpretation of the data. Combining these lines of evidence, we\nargue that CR7 might well be the first DCBH candidate.\n",
        "  Nowadays, huge amounts of data are naturally collected in distributed sites\ndue to different facts and moving these data through the network for extracting\nuseful knowledge is almost unfeasible for either technical reasons or policies.\nFurthermore, classical par- allel algorithms cannot be applied, specially in\nloosely coupled environments. This requires to develop scalable distributed\nalgorithms able to return the global knowledge by aggregating local results in\nan effective way. In this paper we propose a distributed algorithm based on\nindependent local clustering processes and a global merging based on minimum\nvariance increases and requires a limited communication overhead. We also\nintroduce the notion of distributed sub-clusters perturbation to improve the\nglobal generated distribution. We show that this algorithm improves the quality\nof clustering compared to classical local centralized ones and is able to find\nreal global data nature or distribution.\n",
        "  Superconducting devices based on the Josephson effect are effectively used\nfor the implementation of qubits and quantum gates. The manipulation of\nsuperconducting qubits is generally performed by using microwave pulses with\nfrequencies from 5 to 15 GHz, obtaining a typical operating clock from 100MHz\nto 1GHz. A manipulation based on simple pulses in the absence of microwaves is\nalso possible. In our system a magnetic flux pulse modifies the potential of a\ndouble SQUID qubit from a symmetric double well to a single deep well\ncondition. By using this scheme with a Nb/AlOx/Nb system we obtained coherent\noscillations with sub-nanosecond period (tunable from 50ps to 200ps), very fast\nwith respect to other manipulating procedures, and with a coherence time up to\n10ns, of the order of what obtained with similar devices and technologies but\nusing microwave manipulation. We introduce the ultrafast manipulation\npresenting experimental results, new issues related to this approach (such as\nthe use of a feedback procedure for cancelling the effect of \"slow\"\nfluctuations), and open perspectives, such as the possible use of RSFQ logic\nfor the qubit control.\n",
        "  We present a short elementary proof of an existence theorem of certain\nCAT(-1)-surfaces in open hyperbolic 3-manifolds. The main construction lemma in\nCalegari and Gabai's proof of Marden's Tameness Conjecture can be replaced by\nan applicable version of our theorem.\n",
        "  Chemistry plays an important role in the structure and evolution of\nprotoplanetary disks, with implications for the composition of comets and\nplanets. This is the first of a series of papers based on data from DISCS, a\nSubmillimeter Array survey of the chemical composition of protoplanetary disks.\nThe six Taurus sources in the program (DM Tau, AA Tau, LkCa 15, GM Aur, CQ Tau\nand MWC 480) range in stellar spectral type from M1 to A4 and offer an\nopportunity to test the effects of stellar luminosity on the disk chemistry.\nThe disks were observed in 10 different lines at ~3\" resolution and an rms of\n~100 mJy beam-1 at ~0.5 km s-1. The four brightest lines are CO 2-1, HCO+ 3-2,\nCN 2_3-1_2 and HCN 3-2 and these are detected toward all sources (except for\nHCN toward CQ Tau). The weaker lines of CN 2_2-1_1, DCO+ 3-2, N2H+ 3-2, H2CO\n3_03-2_02 and 4_14-3_13 are detected toward two to three disks each, and DCN\n3-2 only toward LkCa 15. CH3OH 4_21-3_12 and c-C3H2 are not detected. There is\nno obvious difference between the T Tauri and Herbig Ae sources with regard to\nCN and HCN intensities. In contrast, DCO+, DCN, N2H+ and H2CO are detected only\ntoward the T Tauri stars, suggesting that the disks around Herbig Ae stars lack\ncold regions for long enough timescales to allow for efficient deuterium\nchemistry, CO freeze-out, and grain chemistry.\n",
        "  We first present a minimal feature set for transition-based dependency\nparsing, continuing a recent trend started by Kiperwasser and Goldberg (2016a)\nand Cross and Huang (2016a) of using bi-directional LSTM features. We plug our\nminimal feature set into the dynamic-programming framework of Huang and Sagae\n(2010) and Kuhlmann et al. (2011) to produce the first implementation of\nworst-case O(n^3) exact decoders for arc-hybrid and arc-eager transition\nsystems. With our minimal features, we also present O(n^3) global training\nmethods. Finally, using ensembles including our new parsers, we achieve the\nbest unlabeled attachment score reported (to our knowledge) on the Chinese\nTreebank and the \"second-best-in-class\" result on the English Penn Treebank.\n",
        "  Definition of the clinical target volume (CTV) is one of the weakest links in\nthe radiation therapy chain. In particular, inability to account for\nuncertainties is a severe limitation in the traditional CTV delineation\napproach. Here, we introduce and test a new concept for tumor target\ndefinition, the clinical target distribution (CTD). The CTD is a continuous\ndistribution of the probability of voxels to be tumorous. We describe an\napproach to incorporate the CTD in treatment plan optimization algorithms, and\nimplement it in a commercial treatment planning system. We test the approach in\ntwo synthetic and two clinical cases, a sarcoma and a glioblastoma case. The\nCTD is straightforward to implement in treatment planning and comes with\nseveral advantages. It allows one to find the most suitable tradeoff between\ntarget coverage and sparing of surrounding healthy organs at the treatment\nplanning stage, without having to modify or redraw a CTV. Owing to the variable\nprobabilities afforded by the CTD, a more flexible and more clinically\nmeaningful sparing of critical structure becomes possible. Finally, the CTD is\nexpected to reduce the inter-user variability of defining the traditional CTV.\n",
        "  We describe efforts towards getting better resources for English-Arabic\nmachine translation of spoken text. In particular, we look at movie subtitles\nas a unique, rich resource, as subtitles in one language often get translated\ninto other languages. Movie subtitles are not new as a resource and have been\nexplored in previous research; however, here we create a much larger bi-text\n(the biggest to date), and we further generate better quality alignment for it.\nGiven the subtitles for the same movie in different languages, a key problem is\nhow to align them at the fragment level. Typically, this is done using\nlength-based alignment, but for movie subtitles, there is also time\ninformation. Here we exploit this information to develop an original algorithm\nthat outperforms the current best subtitle alignment tool, subalign. The\nevaluation results show that adding our bi-text to the IWSLT training bi-text\nyields an improvement of over two BLEU points absolute.\n",
        "  Institute Dante Pazzanese of Cardiology (IDPC) develops Ventricular Assist\nDevices (VAD) that can stabilize the hemodynamics of patients with severe heart\nfailure before, during and/or after the medical practice; can be temporary or\npermanent. The ADV's centrifugal basically consist of a rotor suspended for\nsystem pivoting bearing; the PIVOT is the axis with movement of rotational and\nthe bearing is the bearing surface. As a whole system of an implantable VAD\nshould be made of long-life biomaterial so that there is no degradation or\ndeformation during application time; surface modification techniques have been\nwidely studied and implemented to improve properties such as biocompatibility\nand durability of applicable materials. The Chemical Vapour Deposition\ntechnique allows substrates having melting point higher than 300 {\\deg}C to be\ncoated, encapsulated, with a diamond like carbon film (DLC); The test simulated\nthe actual conditions in which the system of support remains while applying a\nADV. The results have been prepared on a comparative basis; where you select\nthe pivoting assembly which showed less deformation by abrasive wear.\n",
        "  This paper presents a novel method for quantifying axonal density and\ndiameter using simple multiple gradient echo (MGE) MRI\n",
        "  Recent improvements in positioning technology has led to a much wider\navailability of massive moving object data. A crucial task is to find the\nmoving objects that travel together. Usually, these object sets are called\nspatio-temporal patterns. Due to the emergence of many different kinds of\nspatio-temporal patterns in recent years, different approaches have been\nproposed to extract them. However, each approach only focuses on mining a\nspecific kind of pattern. In addition to being a painstaking task due to the\nlarge number of algorithms used to mine and manage patterns, it is also time\nconsuming. Moreover, we have to execute these algorithms again whenever new\ndata are added to the existing database. To address these issues, we first\nredefine spatio-temporal patterns in the itemset context. Secondly, we propose\na unifying approach, named GeT_Move, which uses a frequent closed itemset-based\nspatio-temporal pattern-mining algorithm to mine and manage different\nspatio-temporal patterns. GeT_Move is implemented in two versions which are\nGeT_Move and Incremental GeT_Move. To optimize the efficiency and to free the\nparameters setting, we also propose a Parameter Free Incremental GeT_Move\nalgorithm. Comprehensive experiments are performed on real datasets as well as\nlarge synthetic datasets to demonstrate the effectiveness and efficiency of our\napproaches.\n",
        "  Identifying novel functional materials with desired key properties is an\nimportant part of bridging the gap between fundamental research and\ntechnological advancement. In this context, high-throughput calculations\ncombined with data-mining techniques highly accelerated this process in\ndifferent areas of research during the past years. The strength of a\ndata-driven approach for materials prediction lies in narrowing down the search\nspace of thousands of materials to a subset of prospective candidates.\nRecently, the open-access organic materials database OMDB was released\nproviding electronic structure data for thousands of previously synthesized\nthree-dimensional organic crystals. Based on the OMDB, we report about the\nimplementation of a novel density of states similarity search tool which is\ncapable of retrieving materials with similar density of states to a reference\nmaterial. The tool is based on the approximate nearest neighbor algorithm as\nimplemented in the ANNOY library and can be applied via the OMDB web interface.\nThe approach presented here is wide-ranging and can be applied to various\nproblems where the density of states is responsible for certain key properties\nof a material. As the first application, we report about materials exhibiting\nelectronic structure similarities to the aromatic hydrocarbon p-terphenyl which\nwas recently discussed as a potential organic high-temperature superconductor\nexhibiting a transition temperature in the order of 120~K under strong\npotassium doping. Although the mechanism driving the remarkable transition\ntemperature remains under debate, we argue that the density of states,\nreflecting the electronic structure of a material, might serve as a crucial\ningredient for the observed high-$T_\\mathrm{c}$. To provide candidates which\nmight exhibit comparable properties, we present 15 purely organic materials\nwith similar features to p-terphenyl...\n",
        "  In this paper, we investigate the emergence of a predator-prey system with\nIvlev-type functional response and reaction-diffusion. We study how diffusion\naffects the stability of predator-prey coexistence equilibrium and derive the\nconditions for Hopf and Turing bifurcation in the spatial domain. Based on the\nbifurcation analysis, we give the spatial pattern formation, the evolution\nprocess of the system near the coexistence equilibrium point, via numerical\nsimulation. We find that pure Hopf instability leads to the formation of spiral\npatterns and pure Turing instability destroys the spiral pattern and leads to\nthe formation of chaotic spatial pattern. Furthermore, we perform three\ncategories of initial perturbations which predators are introduced in a small\ndomain to the coexistence equilibrium point to illustrate the emergence of\nspatiotemporal patterns, we also find that in the beginning of evolution of the\nspatial pattern, the special initial conditions have an effect on the formation\nof spatial patterns, though the effect is less and less with the more and more\niterations. This indicates that for prey-dependent type predator-prey model,\npattern formations do depend on the initial conditions, while for\npredator-dependent type they do not. Our results show that modeling by\nreaction-diffusion equations is an appropriate tool for investigating\nfundamental mechanisms of complex spatiotemporal dynamics.\n",
        "  High-throughput sequencing techniques such as metagenomic and\nmetatranscriptomic technologies allow cataloging of functional characteristics\nof microbial community members as well as their taxonomic identity. Such\nstudies have found that a community's composition in terms of ecologically\nrelevant functional traits or guilds can be conserved more strictly across\nvarying settings than taxonomic composition is. I use a standard ecological\nresource-consumer model to examine the dynamics of traits relevant to resource\nconsumption, and analyze determinants of functional composition. This model\ndemonstrates that interaction with essential resources can regulate the\ncommunity-wide abundance of ecologically relevant traits, keeping them at\nconsistent levels despite large changes in the abundances of the species\nhousing those traits in response to changes in the environment, and across\nvariation between communities in species composition. Functional composition is\nshown to be able to track differences in environmental conditions faithfully\nacross differences in community composition. Mathematical conditions on\nconsumers' vital rates and functional responses sufficient to produce\nconservation of functional community structure across taxonomic differences are\npresented.\n",
        "  Phylogenetic invariants are not the only constraints on site-pattern\nfrequency vectors for phylogenetic trees. A mutation matrix, by its definition,\nis the exponential of a matrix with non-negative off-diagonal entries; this\npositivity requirement implies non-trivial constraints on the site-pattern\nfrequency vectors. We call these additional constraints ``edge-parameter\ninequalities.'' In this paper, we first motivate the edge-parameter\ninequalities by considering a pathological site-pattern frequency vector\ncorresponding to a quartet tree with a negative internal edge. This\nsite-pattern frequency vector nevertheless satisfies all of the constraints\ndescribed up to now in the literature. We next describe two complete sets of\nedge-parameter inequalities for the group-based models; these constraints are\nsquare-free monomial inequalities in the Fourier transformed coordinates. These\ninequalities, along with the phylogenetic invariants, form a complete\ndescription of the set of site-pattern frequency vectors corresponding to\n\\emph{bona fide} trees. Said in mathematical language, this paper explicitly\npresents two finite lists of inequalities in Fourier coordinates of the form\n``monomial $\\leq 1$,'' each list characterizing the phylogenetically relevant\nsemialgebraic subsets of the phylogenetic varieties.\n",
        "  The Taylor-Frank method for making kin selection models when fitness is a\nnonlinear function of a continuous phenotype requires this function to be\ndifferentiable. This assumption sometimes fails for biologically important\nfitness functions, for instance in microbial data and the theory of repeated\nn-person games, even when fitness functions are smooth and continuous. In these\ncases, the Taylor-Frank methodology cannot be used, and a more general form of\ndirect fitness must replace the standard one to account for kin selection, even\nunder weak selection.\n",
        "  Massive stars shape the surrounding ISM by emitting ionizing photons and\nejecting material through stellar winds. To study the impact of the momentum\nfrom the wind of a massive star on the surrounding neutral or ionized material,\nwe implemented a new HEALPix-based momentum conserving wind scheme in the\nSmoothed Particle Hydrodynamics (SPH) code SEREN. A qualitative study of the\nimpact of the feedback from an O7.5-like star on a self gravitating sphere\nshows that, on its own, the transfer of momentum from a wind onto cold\nsurrounding gas has both a compressing and dispersing effect. It mostly affects\ngas at low and intermediate densities. When combined with a stellar source's\nionizing UV radiation, we find the momentum driven wind to have little direct\neffect on the gas. We conclude that, during a massive star's main sequence, the\nUV ionizing radiation is the main feedback mechanism shaping and compressing\nthe cold gas. Overall, the wind's effects on the dense gas dynamics and on the\ntriggering of star formation are very modest. The structures formed in the\nionization-only simulation and in the combined feedback simulation are\nremarkably similar. However, in the combined feedback case, different SPH\nparticles end up being compressed. This indicates that the microphysics of gas\nmixing differ between the two feedback simulations and that the winds can\ncontribute to the localized redistribution and reshuffling of gas.\n",
        "  This study presents a Long Short-Term Memory (LSTM) neural network approach\nto Japanese word segmentation (JWS). Previous studies on Chinese word\nsegmentation (CWS) succeeded in using recurrent neural networks such as LSTM\nand gated recurrent units (GRU). However, in contrast to Chinese, Japanese\nincludes several character types, such as hiragana, katakana, and kanji, that\nproduce orthographic variations and increase the difficulty of word\nsegmentation. Additionally, it is important for JWS tasks to consider a global\ncontext, and yet traditional JWS approaches rely on local features. In order to\naddress this problem, this study proposes employing an LSTM-based approach to\nJWS. The experimental results indicate that the proposed model achieves\nstate-of-the-art accuracy with respect to various Japanese corpora.\n",
        "  For a finite simplicial graph $\\Gamma$, let $G(\\Gamma)$ denote the\nright-angled Artin group on the complement graph of $\\Gamma$. In this article,\nwe introduce the notions of \"induced path lifting property\" and \"semi-induced\npath lifting property\" for immersions between graphs, and obtain graph\ntheoretical criteria for the embedability between right-angled Artin groups. We\nrecover the result of S.-h.{} Kim and T.{} Koberda that an arbitrary\n$G(\\Gamma)$ admits a quasi-isometric group embedding into $G(T)$ for some\nfinite tree $T$. The upper bound on the number of vertices of $T$ is improved\nfrom $2^{2^{(m-1)^2}}$ to $m2^{m-1}$, where $m$ is the number of vertices of\n$\\Gamma$. We also show that the upper bound on the number of vertices of $T$ is\nat least $2^{m/4}$. Lastly, we show that $G(C_m)$ embeds in $G(P_n)$ for\n$n\\geqslant 2m-2$, where $C_m$ and $P_n$ denote the cycle and path graphs on\n$m$ and $n$ vertices, respectively.\n",
        "  Surgery triangles are an important computational tool in Floer homology.\nGiven a connected oriented surface $\\Sigma$, we consider the abelian group\n$K(\\Sigma)$ generated by bordered 3-manifolds with boundary $\\Sigma$, modulo\nthe relation that the three manifolds involved in any surgery triangle sum to\nzero. We show that $K(\\Sigma)$ is a finitely generated free abelian group and\ncompute its rank. We also construct an explicit basis and show that it\ngenerates all bordered 3-manifolds in a certain stronger sense. Our basis is\nstrictly contained in another finite generating set which was constructed\npreviously by Baldwin and Bloom. As a byproduct we confirm a conjecture of\nBlokhuis and Brouwer on spanning sets for the binary symplectic dual polar\nspace.\n",
        "  The segregation of plasmids in a bacterial population is investigated.\nHereby, a dynamical model is formulated in terms of a size-structured\npopulation using a hyperbolic partial differential equation incorporating\nnon-local terms (the fragmentation equation). For a large class of parameter\nfunctions this PDE can be re-written as an infinite system of ordinary\ndifferential equations for the moments of its solution. We investigate the\ninfluence of different plasmid production modes, kinetic parameters, and\nplasmid segregation modes on the equilibrium plasmid distribution. In\nparticular, at small plasmid numbers the distribution is strongly influenced by\nthe production mode, while the kinetic parameters (cell growth rate resp. basic\nplasmid reproduction rate) influence the distribution mainly at large plasmid\nnumbers. The plasmid transmission characteristics only gradually influence the\ndistribution, but may become of importance for biologically relevant cases. We\ncompare the theoretical findings with experimental results.\n",
        "  The \\emph{action dimension} of a discrete group $G$ is the minimum dimension\nof a contractible manifold, which admits a proper $G$-action. In this paper, we\nstudy the action dimension of general Artin groups. The main result is that the\naction dimension of an Artin group with the nerve $L$ of dimension $n$ for $n\n\\ne 2$ is less than or equal to $(2n + 1)$ if the Artin group satisfies the\n$K(\\pi, 1)$-Conjecture and the top cohomology group of $L$ with\n$\\mathbb{Z}$-coefficients is trivial. For $n = 2$, we need one more condition\non $L$ to get the same inequality; that is the fundamental group of $L$ is\ngenerated by $r$ elements where $r$ is the rank of $H_1(L, \\mathbb{Z})$.\n",
        "  The quality of data is context dependent. Starting from this intuition and\nexperience, we propose and develop a conceptual framework that captures in\nformal terms the notion of \"context-dependent data quality\". We start by\nproposing a generic and abstract notion of context, and also of its uses, in\ngeneral and in data management in particular. On this basis, we investigate\n\"data quality assessment\" and \"quality query answering\" as context-dependent\nactivities. A context for the assessment of a database D at hand is modeled as\nan external database schema, with possibly materialized or virtual data, and\nconnections to external data sources. The database D is put in context via\nmappings to the contextual schema, which produces a collection C of alternative\nclean versions of D. The quality of D is measured in terms of its distance to\nC. The class C} is also used to define and do \"quality query answering\". The\nproposed model allows for natural extensions, like the use of data quality\npredicates, the optimization of the access by the context to external data\nsources, and also the representation of contexts by means of more expressive\nontologies.\n",
        "  Some implants have approximately a lifetime of 15 years. The femoral stem,\nfor example, should be made of 316L/316LN stainless steel. Fretting corrosion,\nfriction under small displacements, should occur during human gait, due to\nrepeated loadings and un-loadings, between stainless steel and bone for\ninstance. Some experimental investigations of fretting corrosion have been\npracticed. As well known, metallic alloys and especially stainless steels are\ncovered with a passive film that prevents from the corrosion and degradation.\nThis passive layer of few nanometers, at ambient temperature, is the key of our\ncivilization according to some authors. This work is dedicated to predict the\npassive layer thicknesses of stainless steel under fretting corrosion with a\nspecific emphasis on the role of proteins. The model is based on the Point\nDefect Model (micro scale) and an update of the model on the friction process\n(micro-macro scale). Genetic algorithm was used for finding solution of the\nproblem. The major results are, as expected from experimental results, albumin\nprevents from degradation at the lowest concentration of chlorides; an\nincubation time is necessary for degrading the passive film; under fretting\ncorrosion and high concentration of chlorides the passive behavior is\nannihilated.\n  Les implants orthop\\'ediques de hanche ont une dur\\'ee de vie d'environ 15\nans. Par exemple, la tige f\\'emorale d'un tel implant peut \\^etre r\\'ealis\\'ee\nen acier inoxydable 316L ou 316LN. Le fretting corrosion, frottement sous\npetits d\\'eplacements, peut se produire pendant la marche humaine en raison des\nchargements r\\'ep\\'et\\'es entre le m\\'etal de la proth\\`ese et l'os. Plusieurs\ninvestigations exp\\'erimentales du fretting corrosion ont \\'et\\'e entreprises.\nCette couche passive de quelques nanom\\`etres, \\`a temp\\'erature ambiante, est\nle point clef sur lequel repose le d\\'eveloppement de notre civilisation, selon\ncertains auteurs. Ce travail vise \\`a pr\\'edire les \\'epaisseurs de cette\ncouche passive de l'acier inoxydable soumis au fretting corrosion, avec une\nattention sp\\'ecifique sur le r\\^ole des prot\\'eines. Le mod\\`ele utilis\\'e est\nbas\\'e sur le Point Defect Model, PDM (\\`a une \\'echelle microscopique) et une\nam\\'elioration de ce mod\\`ele en prenant en compte le processus de frottement\nsous petits d\\'ebattements. L'algorithme g\\'en\\'etique a \\'et\\'e utilis\\'e pour\noptimiser la convergence du probl\\`eme. Les r\\'esultats les plus importants\nsont, comme d\\'emontr\\'e avec les essais exp\\'erimentaux, que l'albumine, la\nprot\\'eine \\'etudi\\'ee, emp\\^eche les d\\'egradations de l'acier inoxydable aux\nplus faibles concentrations d'ions chlorure ; ensuite, aux plus fortes\nconcentrations de chlorures, un temps d'incubation est n\\'ecessaire pour\nd\\'etruire le film passif.\n",
        "  Spatiotemporal fractionation schemes, that is, treatments delivering\ndifferent dose distributions in different fractions, may lower treatment side\neffects without compromising tumor control. This is achieved by\nhypofractionating parts of the tumor while delivering approximately uniformly\nfractionated doses to the healthy tissue. Optimization of such treatments is\nbased on biologically effective dose (BED), which leads to computationally\nchallenging nonconvex optimization problems. Current optimization methods yield\nonly locally optimal plans, and it has been unclear whether these are close to\nthe global optimum. We present an optimization model to compute rigorous bounds\non the normal tissue BED reduction achievable by such plans.\n  The approach is demonstrated on liver tumors, where the primary goal is to\nreduce mean liver BED without compromising other treatment objectives. First a\nuniformly fractionated reference plan is computed using convex optimization.\nThen a nonconvex quadratically constrained quadratic programming model is\nsolved to local optimality to compute a spatiotemporally fractionated plan that\nminimizes mean liver BED subject to the constraints that the plan is no worse\nthan the reference plan with respect to all other planning goals. Finally, we\nderive a convex relaxation of the second model in the form of a semidefinite\nprogramming problem, which provides a lower bound on the lowest achievable mean\nliver BED.\n  The method is presented on 5 cases with distinct geometries. The computed\nspatiotemporal plans achieve 12-35% mean liver BED reduction over the reference\nplans, which corresponds to 79-97% of the gap between the reference mean liver\nBEDs and our lower bounds. This indicates that spatiotemporal treatments can\nachieve substantial reduction in normal tissue BED, and that local optimization\nprovides plans that are close to realizing the maximum potential benefit.\n",
        "  We describe a new mm-wave molecular-line mapping survey of the southern\nGalactic Plane and its first data releases. The Three-mm Ultimate Mopra Milky\nWay Survey (ThrUMMS) maps a 60{\\deg}x2{\\deg} sector of our Galaxy's fourth\nquadrant, using a combination of fast mapping techniques with the Mopra radio\ntelescope, simultaneously in the J=1-0 lines of $^{12}$CO, $^{13}$CO,\nC$^{18}$O, and CN near 112 GHz at ~arcminute and ~0.3 km s$^{-1}$ resolution,\nwith ~2 K channel$^{-1}$ sensitivity for $^{12}$CO and ~1 K channel$^{-1}$ for\nthe other transitions. The calibrated data cubes from these observations are\nmade available to the community after processing through our pipeline. Here, we\ndescribe the motivation for ThrUMMS, the development of new observing\ntechniques for Mopra, and how these techniques were optimised to the objectives\nof the survey. We showcase some sample data products and describe the first\nscience results on CO-isotopologue line ratios. These vary dramatically across\nthe Galactic Plane, indicating a very wide range of optical depth and\nexcitation conditions, from warm and translucent to cold and opaque. The\npopulation of cold clouds in particular have optical depths for $^{12}$CO\neasily exceeding 100. We derive a new, nonlinear conversion law from $^{12}$CO\nintegrated intensity to column density, which suggests that the molecular mass\ntraced by CO in the Galactic disk may have been substantially underestimated.\nThis further suggests that some global relationships in disk galaxies, such as\nstar formation laws, may need to be recalibrated. The large ThrUMMS team is\nproceeding with several other science investigations.\n",
        "  Sadrzadeh et al (2013) present a compositional distributional analysis of\nrelative clauses in English in terms of the Frobenius algebraic structure of\nfinite dimensional vector spaces. The analysis relies on distinct type\nassignments and lexical recipes for subject vs object relativisation. The\nsituation for Dutch is different: because of the verb final nature of Dutch,\nrelative clauses are ambiguous between a subject vs object relativisation\nreading. Using an extended version of Lambek calculus, we present a\ncompositional distributional framework that accounts for this derivational\nambiguity, and that allows us to give a single meaning recipe for the relative\npronoun reconciling the Frobenius semantics with the demands of Dutch\nderivational syntax.\n",
        "  We aim to study the excitation conditions of the molecular gas in the\nrotating disk of the Red Rectangle, the only post-Asymptotic-Giant-Branch\nobject in which the existence of an equatorial rotating disk has been\ndemonstrated. For this purpose, we developed a complex numerical code that\naccurately treats radiative transfer in 2-D, adapted to the study of molecular\nlines from rotating disks.\n  We present far-infrared Herschel/HIFI observations of the 12CO and 13CO\nJ=6-5, J=10-9, and J=16-15 transitions in the Red Rectangle. We also present\nour code in detail and discuss the accuracy of its predictions, from comparison\nwith well-tested codes. Theoretical line profiles are compared with the\nempirical data to deduce the physical conditions in the disk by means of model\nfitting.\n  We conclude that our code is very efficient and produces reliable results.\nThe comparison of the theoretical predictions with our observations reveals\nthat the temperature of the Red Rectangle disk is typically ~ 100-150 K, about\ntwice as high as previously deduced from mm-wave observations of lower-J lines.\nWe discuss the relevance of these new temperature estimates for understanding\nthe thermodynamics and dynamics of this prototype object, as well as for\ninterpreting observations of other rarely studied post-AGB disks. Despite our\nsophisticated treatment of the line formation, our model cannot explain the\nrelatively strong line-wing emission for intermediate-J transitions. We argue\nthat a model including a rotating disk only cannot reproduce these data and\nsuggest that there is an additional extended (probably bipolar) structure\nexpanding at about 7--15 km/s.\n",
        "  Generally, genotypes and phenotypes are expected to be spatially congruent,\nhowever, in widespread species complexes with few barriers to dispersal,\nmultiple contact zones, and limited reproductive isolation, discordance between\nphenotypes and phylogeographic groups is more probable. Wagtails (Aves:\nMotacilla) are a genus of birds with striking plumage pattern variation across\nEurasia. Up to 13 subspecies are recognized within a single species, yet\nprevious studies using mitochondrial DNA have supported phylogeographic groups\nthat are inconsistent with subspecies plumage characteristics. In this study,\nwe investigate the link between phenotypes and genotype by comparing\npopulations thought to be at different stages along the speciation continuum.\nWe take a phylogeographic approach by estimating population structure, testing\nfor isolation by distance, conducting demographic modeling, and estimating the\nfirst time-calibrated species tree for the genus. Our study provides strong\nevidence for species-level patterns of differentiation in wagtails, however\npopulation-level differentiation is less pronounced. We find evidence that\nthree of four widespread Eurasian species exhibit an east-west divide that\ncontradicts both subspecies taxonomy and phenotypic variation. Both the\ngeographic location of this divide and time estimates from demographic models\nare overlapping in two sympatric species, indicating that coincident\nPleistocene events shaped their histories.\n",
        "  Nodal angle resolved photoemission spectra taken on overdoped\nLa$_{1.77}$Sr$_{0.23}$CuO$_4$ are presented and analyzed. It is proven that the\nlow-energy excitations are true Landau Fermi-liquid quasiparticles. We show\nthat momentum and energy distribution curves can be analyzed self-consistently\nwithout quantitative knowledge of the bare band dispersion. Finally, by\nimposing Kramers-Kronig consistency on the self-energy $\\Sigma$, insight into\nthe quasiparticle residue is gained. We conclude by comparing our results to\nquasiparticle properties extracted from thermodynamic, magneto-resistance, and\nhigh-field quantum oscillation experiments on overdoped\nTl$_2$Ba$_2$CuO$_{6+\\delta}$.\n",
        "  Although attention-based Neural Machine Translation (NMT) has achieved\nremarkable progress in recent years, it still suffers from issues of repeating\nand dropping translations. To alleviate these issues, we propose a novel\nkey-value memory-augmented attention model for NMT, called KVMEMATT.\nSpecifically, we maintain a timely updated keymemory to keep track of attention\nhistory and a fixed value-memory to store the representation of source sentence\nthroughout the whole translation process. Via nontrivial transformations and\niterative interactions between the two memories, the decoder focuses on more\nappropriate source word(s) for predicting the next target word at each decoding\nstep, therefore can improve the adequacy of translations. Experimental results\non Chinese=>English and WMT17 German<=>English translation tasks demonstrate\nthe superiority of the proposed model.\n",
        "  Interstellar dust is a key element in our understanding of the interstellar\nmedium and star formation. The manner in which dust populations evolve with the\nexcitation and the physical conditions is a first step in the comprehension of\nthe evolution of inter- stellar dust. Within the framework of the Evolution of\ninterstellar dust Herschel key program, we have acquired PACS and SPIRE spec-\ntrophotometric observations of various photodissociation regions, to\ncharacterise this evolution. The aim of this paper is to trace the evolution of\ndust grains in the Orion Bar photodissociation region. We use Herschel/PACS (70\nand 160 mic) and SPIRE (250, 350 and 500 mic) together with Spitzer/IRAC\nobservations to map the spatial distribution of the dust populations across the\nBar. Brightness profiles are modelled using the DustEM model coupled with a\nradiative transfer code. Thanks to Herschel, we are able to probe finely the\ndust emission of the densest parts of the Orion Bar with a resolution from 5.6\"\nto 35.1\". These new observations allow us to infer the temperature of the\nbiggest grains at different positions in the Bar, which reveals a gradient from\n\\sim 80 K to 40 K coupled with an increase of the spectral emissivity index\nfrom the ionization front to the densest regions. Combining Spitzer/IRAC\nobservations, which are sensitive to the dust emission from the surface, with\nHerschel maps, we have been able to measure the Orion Bar emission from 3.6 to\n500 mic. We find a stratification in the different dust components which can be\nre- produced quantitatively by a simple radiative transfer model without dust\nevolution. However including dust evolution is needed to explain the brightness\nin each band. PAH abundance variations, or a combination of PAH abundance\nvariations with an emissivity enhancement of the biggest grains due to\ncoagulation give good results.\n",
        "  We present a computational analysis of cognate effects on the spontaneous\nlinguistic productions of advanced non-native speakers. Introducing a large\ncorpus of highly competent non-native English speakers, and using a set of\ncarefully selected lexical items, we show that the lexical choices of\nnon-natives are affected by cognates in their native language. This effect is\nso powerful that we are able to reconstruct the phylogenetic language tree of\nthe Indo-European language family solely from the frequencies of specific\nlexical items in the English of authors with various native languages. We\nquantitatively analyze non-native lexical choice, highlighting cognate\nfacilitation as one of the important phenomena shaping the language of\nnon-native speakers.\n",
        "  We derive various inequalities involving the intersection number of the\ncurves contained in geodesics and tight geodesics in the curve graph. While\nthere already exist such inequalities on tight geodesics, our method applies in\nthe setting of geodesics. Furthermore, the method gives inequalities with a\nuniform constant depending only on the topology of the surface.\n",
        "  We investigate the evolution of the mass-metallicity (MZ) relation with a\nlarge sample of 53,444 star-forming galaxies (SFGs) at $0.04<z<0.12$, selected\nfrom the catalog of MPA-JHU emission-line measurements for the Sloan Digital\nSky Survey (SDSS) Data Release 7. Regarding the sample of SFGs, we correct the\nobservational bias and raise the aperture covering fractions to check the\nreliability of the metallicity evolution. We show that (1) the redshift\nevolution of log($\\rm H\\alpha$) and log(O III) luminosities is displayed in our\nsample; (2) we find the metallicity evolution of $\\sim 0.15$ dex at $\\rm log\n(M_*/M_\\odot)\\sim9.3$ in SFGs at $0.04<z<0.12$; (3) after applying the\nluminosity thresholds of log$(L_{\\rm H \\alpha})>41.0$ and log$(L_{\\rm O\\\nIII})>39.7$, we find that metallicity evolution is shown well, and that SFR\nevolution still is shown well under the latter luminosity threshold, but the\nevolution is not observed under the former one; (4) the evolution of the MZ\nrelation seems to disappear at about $\\rm log(M_{*}/M_\\odot)>10.0$ after\napplying the luminosity threshold of log$(L_{\\rm H \\alpha})>41.0$ or\nlog$(L_{\\rm O\\ III})>39.7$; (5) we find $\\alpha =0.09$ and $\\alpha =0.07$ in\nthe equation ($\\mu={\\rm log}M_{*}-\\alpha \\rm log(SFR)$) for log$(L_{\\rm H\n\\alpha})>41.0$ and log$(L_{\\rm O\\ III})>39.7$ samples, respectively, and these\nimply that the evolution of the MZ relation may have a weaker dependence on SFR\nin our sample.\n",
        "  Interstellar dust plays decisive roles in the conversion of neutral to\nmolecular hydrogen (H_2), the thermodynamical evolution of interstellar medium\n(ISM), and the modification of spectral energy distributions (SEDs) of\ngalaxies. These important roles of dust have not been self-consistently\nincluded in previous numerical simulations of galaxy formation and evolution.\nWe have therefore developed a new model by which one can investigate whether\nand how galaxy formation and evolution can be influenced by dust-related\nphysical processes such as photo-electric heating, H_2 formation on dust, and\nstellar radiation pressure on dust in detail. A novel point of the model is\nthat different dust species in a galaxy are represented by `live dust'\nparticles (i.e., not test particles). Therefore, dust particles in a galaxy not\nonly interact gravitationally with all four components of the galaxy (i.e.,\ndark matter, stars, gas, and dust) but also are grown and destroyed through\nphysical processes of ISM. First we describe a way to include dust-related\nphysical processes in Nbody+hydrodynamical simulations of galaxy evolution in\ndetail. Then we show some preliminary results of dust-regulated galaxy\nevolution. The preliminary results suggest that the evolution of dust\ndistributions driven by radiation pressure of stars is very important for the\nevolution of star formation rates, chemical abundances, H_2 fractions, and gas\ndistributions in galaxies.\n",
        "  Seven-core Ag/Fe sheathed Sr0.6K0.4Fe2As2 (Sr-122) superconducting wires were\nproduced by the ex situ powder-in-tube (PIT) method. The relationship between\nthe cold-work deformation process and the superconducting properties of wires\nwere systematically studied. It was found that flat rolling can efficiently\nincrease the density of the superconducting core and largely improve the\ntransport critical current density (Jc) of as-drawn wires. The Jc of the best\nsample achieved 21.1 kA/cm^2 at 4.2 K in self field, and showed very weak\nmagnetic field dependence in high fields. Our result suggested a promising\nfuture of multifilamentary iron-based superconductors in practical application.\n",
        "  The study of the Tip of the Tongue phenomenon (TOT) provides valuable clues\nand insights concerning the organisation of the mental lexicon (meaning, number\nof syllables, relation with other words, etc.). This paper describes a tool\nbased on psycho-linguistic observations concerning the TOT phenomenon. We've\nbuilt it to enable a speaker/writer to find the word he is looking for, word he\nmay know, but which he is unable to access in time. We try to simulate the TOT\nphenomenon by creating a situation where the system knows the target word, yet\nis unable to access it. In order to find the target word we make use of the\nparadigmatic and syntagmatic associations stored in the linguistic databases.\nOur experiment allows the following conclusion: a tool like SVETLAN, capable to\nstructure (automatically) a dictionary by domains can be used sucessfully to\nhelp the speaker/writer to find the word he is looking for, if it is combined\nwith a database rich in terms of paradigmatic links like EuroWordNet.\n",
        "  In this study it was shown that it is possible to determine radiation doses\nfrom external beam therapy both directly and retrospectively from a human blood\nsample. To the best of our knowledge no other studies exist on the direct\nmeasurement of doses received by a person from external beam therapy. Optically\nstimulated luminescence counts from a healthy blood sample exposed to an\nexternal radiation source were measured. Blood aliquots were given 0, 1, 2, 3,\n4, 5, 10, 15, 20, 25, 50, 100 and 200Gy beta doses and their decay and\ndose-response curves were plotted. While the luminescence intensities were\nfound to be relatively low for the doses smaller than 10Gy, they were measured\nconsiderably higher for doses greater than 10Gy. The dose received by the blood\naliquots was determined by interpolating the luminescence counts of 10Gy to the\ndose-response curve. This study has important ramifications for healthcare,\nmedicine and radiation protection\n",
        "  Apriori Algorithm is one of the most important algorithm which is used to\nextract frequent itemsets from large database and get the association rule for\ndiscovering the knowledge. It basically requires two important things: minimum\nsupport and minimum confidence. First, we check whether the items are greater\nthan or equal to the minimum support and we find the frequent itemsets\nrespectively. Secondly, the minimum confidence constraint is used to form\nassociation rules. Based on this algorithm, this paper indicates the limitation\nof the original Apriori algorithm of wasting time and space for scanning the\nwhole database searching on the frequent itemsets, and present an improvement\non Apriori.\n",
        "  This thesis is aimed at studying mutations, understood as trajectories in the\nDNA configuration space. An evolutive model of mutations in terms of Levy\nflights is proposed. The parameters of the model are estimated by means of data\nfrom the Long-Term Evolution Experiment (LTEE) with {\\it E. Coli} bacteria. The\nresults of simulations on competition of clones, mean fitness, etc are compared\nwith experimental data. We discuss the qualitative analogy found between the\nbacterial mutator phenotype and the cancerous cells. The role of radiation as\nsource of mutations is analyzed. We focus on the case of Radon's decay in the\nlungs in breathing.\n",
        "  We study an individual-based model in which two spatially-distributed\nspecies, characterized by different diffusivities, compete for resources. We\nconsider three different ecological settings. In the first, diffusing faster\nhas a cost in terms of reproduction rate. In the second case, resources are not\nuniformly distributed in space. In the third case, the two species are\ntransported by a fluid flow. In all these cases, at varying the parameters, we\nobserve a transition from a regime in which diffusing faster confers an\neffective selective advantage to one in which it constitutes a disadvantage. We\nanalytically estimate the magnitude of this advantage (or disadvantage) and\ntest it by measuring fixation probabilities in simulations of the\nindividual-based model. Our results provide a framework to quantify\nevolutionary pressure for increased or decreased dispersal in a given\nenvironment.\n",
        "  We present a magnitude and proper motion limited catalogue of ~10,000 white\ndwarf candidates, obtained from the SuperCOSMOS Sky Survey by means of reduced\nproper motion selection. This catalogue extends to magnitudes r~19.75 and\nproper motions as low as mu ~0.05\"/yr, and covers nearly three quarters of the\nsky. Photometric parallaxes provide distance estimates accurate to ~50%. This\ncatalogue is used to measure the luminosity functions for disk and spheroid\nwhite dwarfs, using strict velocity cuts to isolate subsamples belonging to\neach population. Disk luminosity functions measured in this manner are really a\nconglomerate of thin and thick disk objects, due to the significant velocity\noverlap between these populations. We introduce a new statistical approach to\nthe stellar luminosity function for nearby objects that succesfully untangles\nthe contributions from the different kinematic populations, without the need\nfor stringent velocity cuts. This improves the statistical power by allowing\nall stars to contribute to the luminosity function, even at tangential\nvelocities where the populations are indistinguishable. This method is\nparticularly suited to white dwarfs, for which population discrimination by\nchemical tagging is not possible. We use this technique to obtain the first\nmeasurement of the thick disk white dwarf luminosity function, while also\nimproving constraint on both the thin disk and spheroid luminosity functions.\nWe find that the thin disk, thick disk and spheroid populations contribute to\nthe local white dwarf density in roughly 79%/16%/5% proportions.\n",
        "  The data warehousing and OLAP technologies are now moving onto handling\ncomplex data that mostly originate from the Web. However, intagrating such data\ninto a decision-support process requires their representation under a form\nprocessable by OLAP and/or data mining techniques. We present in this paper a\ncomplex data warehousing methodology that exploits XML as a pivot language. Our\napproach includes the integration of complex data in an ODS, under the form of\nXML documents; their dimensional modeling and storage in an XML data warehouse;\nand their analysis with combined OLAP and data mining techniques. We also\naddress the crucial issue of performance in XML warehouses.\n",
        "  A thorough review of the q-space technique is presented starting from a\ndiscussion of Fick's laws. The work presented here is primarily conceptual,\ntheoretical and hopefully pedagogical. We offered the notion of molecular\nconcentration to unify Fick's laws and diffusion MRI within a coherent\nconceptual framework. The fundamental relationship between diffusion MRI and\nthe Fick's laws are carefully established. The conceptual and theoretical basis\nof the q-space technique is investigated from first principles.\n",
        "  We generalize the Blonder-Tinkham-Klapwijk theory considering non-diagonal\nboundary conditions in the Bogoliubov-de Gennes scattering problem, to describe\nanomalous conductance features often reported for normal-metal/superconductor\ncontacts. We calculate the differential conductance spectra showing that\nconductance dips, not expected in the standard formulation, are explained in\nterms of phase \\pi-shift, between the bulk and the interface order parameter,\npossibly induced by a localized magnetic moment. A discretized model is used to\ngive quantitative evaluation of the physical conditions, namely the\npolarization and transparency of the interface, needed to realize the phase\ngradient.\n",
        "  This paper has been withdrawn due to a missing hypothesis in the main\nstatement.\n",
        "  In this paper we study the susceptible-infectious (SI) epidemiological model\nusing dynamical graphs. Dynamical structures have been recently applied in many\nareas including complex systems. Dynamical structures include the mutual\ninteraction between the structure topology and the characteristics of its\nmembers. Dynamical graphs applied to epidemics consider generally that the\nnodes are individuals and the links represent different classes of\nrelationships between individuals with the potential to transmit the disease.\nThe main aim in this article is to study the evolution of the SI\nepidemiological model and the creation of subgraphs due to the dynamic behavior\nof the individuals trying to avoid the contagious of the disease. The proposed\ndynamical graph model uses a single parameter which reflects the probability of\nrewire that represent actions to avoid the disease. This parameter includes\nalso information regarding the infectivity of the disease. The numerical\nsimulations using Monte Carlo method show that the dynamical behavior of\nindividuals affects the evolution of the subgraphs. Furthermore, it is shown\nthat the connectivity degree of the graphs can change the arise of subgraphs\nand the asymptotic state of the infectious diseases.\n",
        "  To counterbalance the views presented here by Suzana Moss de Oliveira, we\nexplain here the truth: How men are oppressed by Mother Nature, who may have\nmade an error inventing us, and by living women, who could get rid of most of\nus. Why do women live longer than us? Why is the Y chromosome for men so small?\nWhat are the dangers of marital fidelity? In an appendix we mention the\ndemographic challenges of the future with many old and few young people.\n",
        "  We describe a collection of acoustic and language modeling techniques that\nlowered the word error rate of our English conversational telephone LVCSR\nsystem to a record 6.6% on the Switchboard subset of the Hub5 2000 evaluation\ntestset. On the acoustic side, we use a score fusion of three strong models:\nrecurrent nets with maxout activations, very deep convolutional nets with 3x3\nkernels, and bidirectional long short-term memory nets which operate on FMLLR\nand i-vector features. On the language modeling side, we use an updated model\n\"M\" and hierarchical neural network LMs.\n",
        "  We construct a locally hyperbolic 3-manifold $M_\\infty$ such that $\\pi_\n1(M_\\infty)$ has no divisible subgroup. We then show that $M_\\infty$ is not\nhomeomorphic to any complete hyperbolic manifold. This answers a question of\nAgol [DHM06,Mar07].\n",
        "  In a previous paper Kobayashi and Rieck defined the growth rate of the tunnel\nnumber of a knot $K$, a knot invariant that measures the asymptotic behavior of\nthe tunnel number under iterated connected sum of $K$. We denote the growth\nrate by $\\mbox{gr}_t(K)$.\n  In this paper we construct, for any $\\epsilon > 0$, a hyperbolic knots $K\n\\subset S^{3}$ for which $1 - \\epsilon < \\mbox{gr}_t(K) < 1$. This is the first\nproof that the spectrum of the growth rate of the tunnel number is infinite.\n",
        "  We show that in presence of an applied external field the two-component order\nparameter superconductor falls in two categories of ground states, namely, in\nthe traditional Abrikosov ground state or in a new ground state fitted to\ndescribe a superconducting layer with texture, that is, patched regions\nseparated by a phase difference of $\\pi$. The existence of these two kinds of\nground states follows from the sole assumption that the total supercurrent is\nthe sum of the two individual supercurrents and is independent of any\nconsideration about the free energy expansion. Uniquely defined relations\nbetween the current density and the superfluid density hold for these two\nground states, which also determine the magnetization in terms of average\nvalues of the order parameters. Because these ground state conditions are also\nBogomolny equations we construct the free energy for the two-component\nsuperconductor which admits the Bogomolny solution at a special coupling value.\n",
        "  We have combined the wide-area Herschel-ATLAS far-IR survey with\nspectroscopic redshifts from GAMA and SDSS to define a sample of 21\nlow--redshift ($z_{\\rm spec} < 0.5$) analogs of submm galaxies (SMGs). These\nhave been selected because their dust temperatures and total IR luminosities\nare similar to those for the classical high-redshift SMG population. As well as\npresenting the sample, in this paper we report $^{12}$CO(2-1) and\n$^{12}$CO(1-0) observations of 16 low-redshift analogs of SMGs taken with the\nIRAM-30m telescope. We have obtained that low-redshift analogs of SMGs\nrepresent a very diverse population, similar to what has been found for\nhigh-redshift SMGs. A large variety in the molecular gas excitation or\n$^{12}$CO(2-1)/$^{12}$CO(1-0) line ratio is seen, meaning that extrapolations\nfrom $J \\geq 2$ CO lines can result in very uncertain molecular gas mass\ndeterminations. Our sources with $^{12}$CO(1-0) detections follow the dust--gas\ncorrelation found in previous work at different redshifts and luminosities. The\nmolecular gas mass of low-redshift SMGs has an average value of $M_{\\rm H_2}\n\\sim 1.6 \\times 10^{10}\\,M_\\odot$ and will be consumed in $\\sim 100 \\, {\\rm\nMyr}$ . We also find a wide range of molecular gas fractions, with the highest\nvalues being compatible with those found in high-redshift SMGs with\n$^{12}$CO(1-0) detections, which are only the most luminous. Low-redshift SMGs\noffer a unique opportunity to study the properties of extreme star formation in\na detail not possible at higher redshifts.\n",
        "  Phylogenetic diversity (PD) depends on sampling intensity, which complicates\nthe comparison of PD between samples of different depth. One approach to\ndealing with differing sample depth for a given diversity statistic is to\nrarefy, which means to take a random subset of a given size of the original\nsample. Exact analytical formulae for the mean and variance of species richness\nunder rarefaction have existed for some time but no such solution exists for\nPD. We have derived exact formulae for the mean and variance of PD under\nrarefaction. We show that these formulae are correct by comparing exact\nsolution mean and variance to that calculated by repeated random (Monte Carlo)\nsubsampling of a dataset of stem counts of woody shrubs of Toohey Forest,\nQueensland, Australia. We also demonstrate the application of the method using\ntwo examples: identifying hotspots of mammalian diversity in Australasian\necoregions, and characterising the human vaginal microbiome. There is a very\nhigh degree of correspondence between the analytical and random subsampling\nmethods for calculating mean and variance of PD under rarefaction, although the\nMonte Carlo method requires a large number of random draws to converge on the\nexact solution for the variance. Rarefaction of mammalian PD of ecoregions in\nAustralasia to a common standard of 25 species reveals very different rank\norderings of ecoregions, indicating quite different hotspots of diversity than\nthose obtained for unrarefied PD. The application of these methods to the\nvaginal microbiome shows that a classical score used to quantify bacterial\nvaginosis is correlated with the shape of the rarefaction curve. The analytical\nformulae for the mean and variance of PD under rarefaction are both exact and\nmore efficient than repeated subsampling. Rarefaction of PD allows for many\napplications where comparisons of samples of different depth is required.\n",
        "  In outside the body HIFU treatment that focused ultrasound beams hit severely\nwith cancer tissue layer especially the soft one, at the time of passage of the\nbody different layers as long as they want to reach tumor, put their own way\ncomponents under mechanical and even thermal influence and they can cause skin\nlesions. To reduce this effect a specific mechanical model can be used that\nmeans body tissue is considered as a mechanical model, it is affected when\npassing sound mechanical waves through it and each layer has an average heat.\nGradually sound intensity decreases through every layer passage, finally in one\ndirection a decreased intensity sound reach tumor tissue. If sound propagated\ndirections increase, countless waves with decreased intensity are gathered upon\nthe tumor tissue that causes a lot of heat focus on tumor tissue. Depending on\nthe kind and mechanical properties of the tissue, intensity of each sound wave\nwhen it passes through tissue can be controlled to reduce damages outside the\ntumor tissue.\n",
        "  A detailed survey of the current trends and recent advances in rotary\nmechanical circulatory support systems is presented in this paper. Rather than\nclinical reports, the focus is on technological aspects of these rehabilitating\ndevices as a reference for engineers and biomedical researchers. Existing\ntrends in flow regimes, flow control, and bearing mechanisms are summarized.\nSystem specifications and applications of the most prominent continuous-flow\nventricular assistive devices are provided. Based on the flow regime, pumps are\ncategorized as axial flow, centrifugal flow, and mixed flow. Unique\ncharacteristics of each system are unveiled through an examination of the\nstructure, bearing mechanism, impeller design, flow rate, and biocompatibility.\nA discussion on the current limitations is provided to invite more studies and\nfurther improvements.\n",
        "  The recent detection of Fast Radio Bursts (FRBs) has generated strong\ninterest in identifying the origin of these bright, non-repeating, highly\ndispersed pulses. The principal limitation in understanding the origin of these\nbursts is the lack of reliable distance estimates; their high dispersion\nmeasures imply that they may be at cosmological distances ($0.1 < z < 1.0$).\nHere we discuss new distance constraints to the FRB010621 (a.k.a J1852$-$08)\nfirst reported by Keane. We use velocity resolved $H\\alpha$ and $H\\beta$\nobservations of diffuse ionised gas toward the burst to calculate an\nextinction-corrected emission measure along the line of sight. We combine this\nemission measure with models of Galactic rotation and of electron distribution\nto derive a 90% probability of the pulse residing in the Galaxy. However, we\ncannot differentiate between the two Galactic interpretations of Keane: a\nneutron star with unusual pulse amplitude distribution or Galactic black hole\nannihilation.\n",
        "  Optoacoustic tomography (OAT), also known as photoacoustic tomography, is an\nemerging computed biomedical imaging modality that exploits optical contrast\nand ultrasonic detection principles. Iterative image reconstruction algorithms\nthat are based on discrete imaging models are actively being developed for OAT\ndue to their ability to improve image quality by incorporating accurate models\nof the imaging physics, instrument response, and measurement noise. In this\nwork, we investigate the use of discrete imaging models based on Kaiser-Bessel\nwindow functions for iterative image reconstruction in OAT. A closed-form\nexpression for the pressure produced by a Kaiser-Bessel function is calculated,\nwhich facilitates accurate computation of the system matrix.\nComputer-simulation and experimental studies are employed to demonstrate the\npotential advantages of Kaiser-Bessel function-based iterative image\nreconstruction in OAT.\n",
        "  Stars form in molecular complexes that are visible as giant clouds ($\\sim\n10^{5-6} \\mathrm{M}_\\odot$) in nearby galaxies and as giant clumps ($\\sim\n10^{8-9}\\mathrm{M}_\\odot$) in galaxies at redshifts $z\\approx1$$-$$3$.\nTheoretical inferences on the origin and evolution of these complexes often\nrequire robust measurements of their characteristic size, which is hard to\nmeasure at limited resolution and often ill-defined due to overlap and\nquasi-fractal substructure. We show that maximum and luminosity-weighted sizes\nof clumps seen in star formation maps (e.g.\\ H$\\alpha$) can be recovered\nstatistically using the two-point correlation function (2PCF), if an\napproximate stellar surface density map is taken as the normalizing random\nfield. After clarifying the link between Gaussian clumps and the 2PCF\nanalytically, we design a method for measuring the diameters of Gaussian clumps\nwith realistic quasi-fractal substructure. This method is tested using mock\nimages of clumpy disk galaxies at different spatial resolutions and perturbed\nby Gaussian white noise. We find that the 2PCF can recover the input clump\nscale at $\\sim20\\%$ accuracy, as long as this scale is larger than the spatial\nresolution. We apply this method to the local spiral galaxy NGC 5194, as well\nas to three clumpy turbulent galaxies from the DYNAMO-HST sample. In both\ncases, our statistical H$\\alpha$-clump size measurements agree with previous\nmeasurements and with the estimated Jeans lengths. However, the new\nmeasurements are free from subjective choices when fitting individual clumps.\n",
        "  Date and Darwen have proposed a theory of types, the latter forms the basis\nof a detailed presentation of a panoply of simple and complex types. However,\nthis proposal has not been structured in a formal system. Specifically, Date\nand Darwen haven't indicated the formalism of the type system that corresponds\nto the type theory established. In this paper, we propose a pseudo-algorithmic\nand grammatical description of a system of types for Date and Darwen's model.\nOur type system is supposed take into account null values; for such intention,\nwe introduce a particular type noted #, which expresses one or more occurrences\nof incomplete information in a database. Our algebraic grammar describes in\ndetail the complete specification of an inheritance model and the subryping\nrelation induced, thus the different definitions of related concepts.\n",
        "  In the small phylogeny problem we, are given a phylogenetic tree and gene\norders of the extant species and our goal is to reconstruct all of the\nancestral genomes so that the number of evolutionary operations is minimized.\nAlgorithms for reconstructing evolutionary history from gene orders are usually\nbased on repeatedly computing medians of genomes at neighbouring vertices of\nthe tree. We propose a new, more general approach, based on an iterative local\noptimization procedure. In each step, we propose candidates for ancestral\ngenomes and choose the best ones by dynamic programming. We have implemented\nour method and used it to reconstruct evolutionary history of 16 yeast mtDNAs\nand 13 Campanulaceae cpDNAs.\n",
        "  Semantic Web applications require querying available RDF Data with high\nperformance and reliability. However, ensuring both data availability and\nperformant SPARQL query execution in the context of public SPARQL servers are\nchallenging problems. Queries could have arbitrary execution time and unknown\narrival rates. In this paper, we propose SaGe, a preemptive server-side SPARQL\nquery engine. SaGe relies on a preemptable physical query execution plan and\npreemptable physical operators. SaGe stops query execution after a given slice\nof time, saves the state of the plan and sends the saved plan back to the\nclient with retrieved results. Later, the client can continue the query\nexecution by resubmitting the saved plan to the server. By ensuring a fair\nquery execution, SaGe maintains server availability and provides high query\nthroughput. Experimental results demonstrate that SaGe outperforms the state of\nthe art SPARQL query engines in terms of query throughput, query timeout and\nanswer completeness.\n",
        "  This is a self-published methodological note distributed under the Creative\nCommons Attribution License (http://creativecommons.org/licenses/by/4.0/),\nwhich permits unrestricted use, distribution, and reproduction in any medium,\nprovided the original work is properly cited. The note contains an original\nreasoning of mine and the goal to share thoughts and methodologies, not\nresults. Therefore before using the contents of these notes, everyone is\ninvited to verify the accuracy of the assumptions and conclusions.\n",
        "  We study the population dynamics of lytic viruses which replicate slowly in\ndividing host cells within an organism or cell culture, and find a range of\nviral replication rates that allows viruses to persist, avoiding extinction of\nhost cells or dilution of viruses at too rapid or too slow viral replication.\nFor the within-host competition between multiple viral strains, a strain with a\n\"stable\" replication rate could outcompete another strain with a higher or\nlower replication rate, therefore natural selection of viruses stabilizes the\nviral persistence. However, when strains with higher and lower than the\n\"stable\" value replication rates are both present, competition between strains\ndoes not result in dominance of one strain, but in their coexistence.\n",
        "  We present an explicit unified stochastic model of fluctuations in population\nsize due to random birth, death, density-dependent competition and\nenvironmental fluctuations. Stochastic dynamics provide insight into small\npopulations, including processes such as extinction, that cannot be correctly\ntreated by deterministic methods. We present exact analytical and\nsimulation-based results for extinction times of our stochastic model and\ncompare the different effects of environmental stochasticity and intrinsic\ndemographic stochasticity. We use both the discrete master equation approach\nand an exact mapping to a Fokker-Planck equation (the Poisson method) and\nstochastic equation, showing they are precisely equivalent. We also calculate\napproximate extinction times using a steepest descent method. This model can\nreadily be extended to accommodate metapopulation structure and genetic\nvariation in the population and thus represents a step towards a\nmicroscopically explicit synthesis of population dynamics and population\ngenetics.\n",
        "  We assess the astrometric detectability of intermediate-mass black holes\npopulating the inner parsec of the Milky Way Galaxy. The presence of these\nobjects induces dynamical effects on Sgr A* and the star S2, which could be\ndetected by next generation astrometric instruments that enable micro-arcsecond\nastrometry. An allowed population of ten $10^4~M_{\\odot}$ IMBHs within one\nparsec induces an angular shift of about 65 $\\mu$as yr$^{-1}$ on the position\nof Sgr A*, corresponding to a perpendicular velocity component magnitude of 1.6\nkm s$^{-1}$. It also induces changes in the orbit of S2 that surpass those\ninduced by general relativity but lie within observational constraints,\ngenerating a mean angular shift in periapse and apoapse of 62 $\\mu$as and 970\n$\\mu$as respectively.\n",
        "  We report ALMA long-baseline observations of Orion Source I (SrcI) with\nresolution 0.03-0.06\" (12-24 AU) at 1.3 and 3.2 mm. We detect both continuum\nand spectral line emission from SrcI's disk. We also detect a central weakly\nresolved source that we interpret as a hot spot in the inner disk, which may\nindicate the presence of a binary system. The high angular resolution and\nsensitivity of these observations allow us to measure the outer envelope of the\nrotation curve of the H$_2$O $5_{5,0}-6_{4,3}$ line, which gives a mass\n$M_I\\approx15\\pm2$ Msun. We detected several other lines that more closely\ntrace the disk, but were unable to identify their parent species. Using\ncentroid-of-channel methods on these other lines, we infer a similar mass.\nThese measurements solidify SrcI as a genuine high-mass protostar system and\nsupport the theory that SrcI and the Becklin Neugebauer Object were ejected\nfrom the dynamical decay of a multiple star system $\\sim$500 years ago, an\nevent that also launched the explosive molecular outflow in Orion.\n",
        "  To constrain the giant pulse (GP) emission mechanism and test the model of\nLyutikov (2007) for GP emission, we have carried out a campaign of simultaneous\nobservations of the Crab pulsar at gamma-ray (Fermi) and radio (Green Bank\nTelescope) wavelengths. Over 10 hours of simultaneous observations we obtained\na sample of 2.1x10^4 giant pulses, observed at a radio frequency of 9 GHz, and\n77 Fermi photons, with energies between 100 MeV and 5 GeV. The majority of GPs\ncame from the interpulse (IP) phase window. We found no change in the GP\ngeneration rate within 10-120 s windows at lags of up to +-40 min of observed\ngamma-ray photons. The 95% upper limit for a gamma-ray flux enhancement in\npulsed emission phase window around all GPs is 4 times the average pulsed\ngamma-ray flux from the Crab. For the subset of IP GPs, the enhancement upper\nlimit, within the IP emission window, is 12 times the average pulsed gamma-ray\nflux. These results suggest that GPs, at least high-frequency IP GPs, are due\nto changes in coherence of radio emission rather than an overall increase in\nthe magnetospheric particle density.\n",
        "  In this study, finite element analysis is used to simulate the surgical\ndeployment procedure of a bifurcated stent-graft on a real patient's arterial\ngeometry. The stent-graft is modeled using realistic constitutive properties\nfor both the stent and most importantly for the graft. The arterial geometry is\nobtained from pre-operative imaging exam. The obtained results are in good\nagreement with the post-operative imaging data. As the whole computational time\nwas reduced to less than 2 hours, this study constitutes an essential step\ntowards predictive planning simulations of aneurysmal endovascular surgery\n",
        "  The recently discovered superconducting - spin density wave materials,\ncontaining Fe and As, have raised huge interest. However most materials\nprepared to date, suffer from a varying degree of content of foreign Fe-As\nphases, Fe2As, FeAs2 and FeAs, which can lead to wrong conclusions concerning\nthe properties of these materials. We show here that Mossbauer Spectroscopy is\nable to determine quite easily the relative content of the foreign phases. This\nprocedure is demonstrated by a study of seven samples of superconducting or\nspin density wave materials, prepared in three different laboratories.\n",
        "  In parallel magnetic resonance imaging (pMRI) reconstruction without using\nestimation of coil sensitivity functions, one group of algorithms reconstruct\nsensitivity encoded images of the coils first followed by the magnitude only\nimage reconstruction, e.g. GRAPPA, and another group of algorithms jointly\ncompute the image and sensitivity functions by regularized optimization which\nis a non-convex problem with local only solutions. For the magnitude only image\nreconstruction, this paper derives a reconstruction formulation, which is\nlinear in the magnitude image, and an associated convex hull in the solution\nspace of the formulated equation containing the magnitude of the image. As a\nresult, the magnitude only image reconstruction for pMRI is formulated into a\ntwo-step convex optimization problem, which has a globally optimal solution. An\nalgorithm based on split-bregman and nuclear norm regularized optimizations is\nproposed to implement the two-step convex optimization and its applications to\nphantom and in-vivo MRI data sets result in superior reconstruction performance\ncompared with other algorithms.\n",
        "  This paper addresses automatic quality assessment of spoken language\ntranslation (SLT). This relatively new task is defined and formalized as a\nsequence labeling problem where each word in the SLT hypothesis is tagged as\ngood or bad according to a large feature set. We propose several word\nconfidence estimators (WCE) based on our automatic evaluation of transcription\n(ASR) quality, translation (MT) quality, or both (combined ASR+MT). This\nresearch work is possible because we built a specific corpus which contains\n6.7k utterances for which a quintuplet containing: ASR output, verbatim\ntranscript, text translation, speech translation and post-edition of\ntranslation is built. The conclusion of our multiple experiments using joint\nASR and MT features for WCE is that MT features remain the most influent while\nASR feature can bring interesting complementary information. Our robust quality\nestimators for SLT can be used for re-scoring speech translation graphs or for\nproviding feedback to the user in interactive speech translation or\ncomputer-assisted speech-to-text scenarios.\n",
        "  It is demonstrated that in the type I and II superconductors with\nlow-transparent twinning planes (TP) the penetration of external parallel\nmagnetic field into the region of the twinning plane can be energetically\nfavorable. In the type I superconductors the twinning planes become similar to\nJosephson junctions and the magnetic field penetrates into the center of the TP\nin the form of soft Josephson-like vortices. This leads to increase in the\ncritical magnetic field values. The corresponding phase diagram in the\nparameter plane \"temperature - magnetic field\" essentially differs from the one\nobtained without taking the finite value of the magnetic field near the TP into\naccount. Comparison between obtained phase diagrams and experimental data for\ndifferent type I superconductors can allow to estimate the value of the TP\ntransparency, which is the only fitting parameter in our theory.\n",
        "  Purpose: To compare two methods that use x-ray spectral information to image\nexternally administered contrast agents: K-edge subtraction and basis-function\ndecomposition (the A-space method), Methods: The K-edge method uses narrow band\nx-ray spectra with energies infinitesimally below and above the contrast\nmaterial K-edge energy. The A-space method uses a broad spectrum x-ray tube\nsource and measures the transmitted spectrum with photon counting detectors\nwith pulse height analysis. The methods are compared by their signal to noise\nratio (SNR) divided by the patient dose for an imaging task to decide whether\ncontrast material is present in a soft tissue background. The performance with\niodine or gadolinium containing contrast material is evaluated as a function of\nobject thickness and the x-ray tube voltage of the A-space method. Results: For\na tube voltages above 60 kV and soft tissue thicknesses from 5 to 25 g/cm^2,\nthe A-space method has a larger SNR per dose than the K-edge subtraction method\nfor either iodine or gadolinium containing contrast agent. Conclusion: Even\nwith the unrealistic spectra assumed for the K-edge method, the A-space method\nhas a substantially larger SNR per patient dose.\n",
        "  Competition among gametes for fertilization imposes strong selection. For\nexternal fertilizers, this selective pressure extends to eggs for which\nspawning conditions can range from sperm limitation (competition among eggs) to\nsexual conflict (overabundance of competing sperm toxic to eggs). Yet existing\nfertilization models ignore dynamics that can alter the functional nature of\ngamete interactions. These factors include attraction of sperm to eggs, egg\ncrowding effects or other nonlinearities in per capita rates of sperm-egg\ninteraction. Such processes potentially allow egg concentrations to drastically\naffect viable fertilization probabilities. I experimentally tested whether such\negg effects occur using the urchin $\\textit{Strongylocentrotus purpuratus}$ and\nparameterized a newly derived model of fertilization dynamics and existing\nmodels modified to include such interactions. The experiments revealed that at\nlow sperm concentrations, eggs compete for sperm while at high sperm\nconcentrations eggs cooperatively reduce abnormal fertilization (a proxy for\npolyspermy). I show that these observations are consistent with declines in the\nper capita rate at which sperm and eggs interact as eggs increase in density.\nThe results suggest a fitness trade-off of egg release during spawning: as\nsperm range from scarce to superabundant, interactions among eggs transition\nfrom highly competitive to cooperative in terms of viable fertilization\nprobabilities.\n",
        "  Open data platforms such as data.gov or opendata.socrata. com provide a huge\namount of valuable information. Their free-for-all nature, the lack of\npublishing standards and the multitude of domains and authors represented on\nthese platforms lead to new integration and standardization problems. At the\nsame time, crowd-based data integration techniques are emerging as new way of\ndealing with these problems. However, these methods still require input in form\nof specific questions or tasks that can be passed to the crowd. This paper\ndiscusses integration problems on Open Data Platforms, and proposes a method\nfor identifying and ranking integration hypotheses in this context. We will\nevaluate our findings by conducting a comprehensive evaluation using on one of\nthe largest Open Data platforms.\n",
        "  In this paper we present the functionality of a currently under development\ndatabase programming methodology called ODRA (Object Database for Rapid\nApplication development) which works fully on the object oriented principles.\nThe database programming language is called SBQL (Stack based query language).\nWe discuss some concepts in ODRA for e.g. the working of ODRA, how ODRA runtime\nenvironment operates, the interoperability of ODRA with .net and java .A view\nof ODRA's working with web services and xml. Currently the stages under\ndevelopment in ODRA are query optimization. So we present the prior work that\nis done in ODRA related to Query optimization and we also present a new fusion\nalgorithm of how ODRA can deal with joins based on collections like set, lists,\nand arrays for query optimization.\n",
        "  We describe an inventory of semantic relations that are expressed by\nprepositions. We define these relations by building on the word sense\ndisambiguation task for prepositions and propose a mapping from preposition\nsenses to the relation labels by collapsing semantically related senses across\nprepositions.\n",
        "  Stereotactic radiosurgery is an effective technique to treat brain tumors for\nwhich several inverse planning methods may be appropriate. We propose an\ninteger programming model to simultaneous sector duration and isocenter\noptimization (SDIO) problem for Leksell Gamma Knife{\\textregistered}\nIcon{\\texttrademark} (Elekta, Stockholm, Sweden) to tractably incorporate\ntreatment time. We devise a Benders decomposition scheme to solve the SDIO\nproblem to optimality. The performances of our approaches are assessed using\nanonymized data from eight previously treated cases, and obtained treatment\nplans are compared against each other and against the clinical plans. The plans\ngenerated by our SDIO model all meet or exceed clinical guidelines while\ndemonstrating high conformity.\n",
        "  In today's Web and social network environments, query workloads include ad\nhoc and OLAP queries, as well as iterative algorithms that analyze data\nrelationships (e.g., link analysis, clustering, learning). Modern DBMSs support\nad hoc and OLAP queries, but most are not robust enough to scale to large\nclusters. Conversely, \"cloud\" platforms like MapReduce execute chains of batch\ntasks across clusters in a fault tolerant way, but have too much overhead to\nsupport ad hoc queries.\n  Moreover, both classes of platform incur significant overhead in executing\niterative data analysis algorithms. Most such iterative algorithms repeatedly\nrefine portions of their answers, until some convergence criterion is reached.\nHowever, general cloud platforms typically must reprocess all data in each\nstep. DBMSs that support recursive SQL are more efficient in that they\npropagate only the changes in each step -- but they still accumulate each\niteration's state, even if it is no longer useful. User-defined functions are\nalso typically harder to write for DBMSs than for cloud platforms.\n  We seek to unify the strengths of both styles of platforms, with a focus on\nsupporting iterative computations in which changes, in the form of deltas, are\npropagated from iteration to iteration, and state is efficiently updated in an\nextensible way. We present a programming model oriented around deltas, describe\nhow we execute and optimize such programs in our REX runtime system, and\nvalidate that our platform also handles failures gracefully. We experimentally\nvalidate our techniques, and show speedups over the competing methods ranging\nfrom 2.5 to nearly 100 times.\n",
        "  Our dictionary-based lemmatizer for the Bulgarian language presented here is\ndistributed as free software, publicly available to download and use under the\nGPL v3 license. The presented software is written entirely in Java and is\ndistributed as a GATE plugin. To our best knowledge, at the time of writing\nthis article, there are not any other free lemmatization tools specifically\ntargeting the Bulgarian language. The presented lemmatizer is a work in\nprogress and currently yields an accuracy of about 95% in comparison to the\nmanually annotated corpus BulTreeBank-Morph, which contains 273933 tokens.\n",
        "  The first algorithm for sampling the space of thick equilateral knots, as a\nfunction of thickness, will be described. This algorithm is based on previous\nalgorithms of applying random reflections.\n  To prove the existence of the algorithm, we describe a method for turning any\nknot into the regular planar polygon using only thickness non-decreasing moves.\nThis approach ensures that the algorithm has a positive probability of\nconnecting any two knots with the required thickness constraint and so is\nergodic. This ergodic sampling unlocks the ability to analyze the effects of\nthickness on properties of the geometric knot such as radius of gyration.\n  This algorithm will be shown to be faster than previous methods for\ngenerating thick knots, and the data from this algorithm shows that the radius\nof gyration increases strongly with thickness and that the growth exponent for\nradius of gyration increases with thickness.\n",
        "  We show that a specific superposition principle is valid for \\emph{nonlinear}\nJosephson plasma waves in layered superconductors. We study theoretically the\nreflection and transmission of terahertz waves through a finite-size\nsuperconducting slab placed inside a rectangular waveguide with ideal-metal\nwalls. We assume that the superconducting layers are parallel to the waveguide\naxis. We show that there exist two specific mutually-orthogonal polarizations\nfor waves which, in spite of the nonlinearity, reflect and transmit through the\nsuperconductor \\emph{independently}. The wave of the first polarization causes\na strong shielding current along the crystallographic \\textbf{ab}-plane of the\nsuperconductor. Therefore, this wave reflects nearly completely from the\nsuperconductor and excites only an evanescent mode inside it. The wave of the\nother polarization does not contain the electric field component parallel to\nboth the sample surface and the crystallographic \\textbf{ab}-plane, and excites\nmuch weaker shielding currents. Therefore, it partially reflects and partially\ntransmits through the sample. Moreover, this wave excites the nonlinear mode in\nthe layered superconductor, and the transmission coefficient of the\nsuperconductor depends on the amplitude of the incident wave of this\npolarization.On the basis of the discussed superposition principle, we suggest\na new method for solving nonlinear problems of waves interaction in layered\nsuperconductors. Namely, it is reasonably to represent incident, reflected, and\ntransmitted waves of any polarizations as superpositions of the modes with the\ntwo specific polarizations considered here, and then solve the problem\nseparately for these modes. We apply this method to the case of nonlinear\ninteraction and mutual transformation of the transverse electric and transverse\nmagnetic modes in layered superconductors.\n",
        "  The circumgalactic medium (CGM) remains one of the least constrained\ncomponents of galaxies and as such has significant potential for advancing\ngalaxy formation theories. In this work, we vary the extragalactic ultraviolet\nbackground for a high-resolution cosmological simulation of a Milky Way-like\ngalaxy and examine the effect on the absorption and emission properties of\nmetals in the CGM. We find that a reduced quasar background brings the column\ndensity predictions into better agreement with recent data. Similarly, when the\nobservationally derived physical properties of the gas are compared to the\nsimulation, we find that the simulation gas is always at temperatures\napproximately 0.5 dex higher. Thus, similar column densities can be produced\nfrom fundamentally different gas. However, emission maps can provide\ncomplementary information to the line-of-sight column densities to better\nderive gas properties. From the simulations, we find that the brightest\nemission is less sensitive to the extragalactic background and that it closely\nfollows the fundamental filamentary structure of the halo. This becomes\nincreasingly true as the galaxy evolves from z=1 to z=0 and the majority of the\ngas transitions to a hotter, more diffuse phase. For the brightest ions (CIII,\nCIV, OVI), detectable emission can extend as far as 120 kpc at z=0. Finally,\nresolution is a limiting factor for the conclusions we can draw from emission\nobservations but with moderate resolution and reasonable detection limits,\nupcoming instrumentation should place constraints on the physical properties of\nthe CGM.\n",
        "  With the advent of informal electronic communications such as social media,\ncolloquial languages that were historically unwritten are being written for the\nfirst time in heavily code-switched environments. We present a method for\ninducing portions of translation lexicons through the use of expert knowledge\nin these settings where there are approximately zero resources available other\nthan a language informant, potentially not even large amounts of monolingual\ndata. We investigate inducing a Moroccan Darija-English translation lexicon via\nFrench loanwords bridging into English and find that a useful lexicon is\ninduced for human-assisted translation and statistical machine translation.\n",
        "  Objective: High-intensity focused ultrasound (HIFU) therapy can be used for\nnon-invasive treatment of kidney (renal) cancer, but the clinical outcomes have\nbeen variable. In this study, the efficacy of renal HIFU therapy was studied\nusing nonlinear acoustic and thermal simulations in three patients. Methods:\nThe acoustic simulations were conducted with and without refraction in order to\ninvestigate its effect on the shape, size and pressure distribution at the\nfocus. The values for the attenuation, sound speed, perfusion and thermal\nconductivity of the kidney were varied over the reported ranges to determine\nthe effect of variability on heating. Furthermore, the phase aberration was\nstudied in order to quantify the underlying phase shifts using a second order\npolynomial function. Results: The ultrasound field intensity was found to drop\non average 11.1 dB with refraction and 6.4 dB without refraction. Reflection at\ntissue interfaces was found to result in a loss less than 0.1 dB. Focal point\nsplitting due to refraction significantly reduced the heating efficacy.\nPerfusion did not have a large effect on heating during short sonication\ndurations. Small changes in temperature were seen with varying attenuation and\nthermal conductivity, but no visible changes were present with sound speed\nvariations. The aberration study revealed an underlying trend in the spatial\ndistribution of the phase shifts. Conclusion: The results show that the\nefficacy of HIFU therapy in the kidney could be improved with aberration\ncorrection. Significance: A method is proposed by which patient specific\npre-treatment calculations could be used to overcome the aberration and\ntherefore make ultrasound treatment possible.\n",
        "  We study online graph queries that retrieve nearby nodes of a query node from\na large network. To answer such queries with high throughput and low latency,\nwe partition the graph and process the data in parallel across a cluster of\nservers. State-of-the-art distributed graph querying systems place each graph\npartition on a separate server, where query answering over that partition takes\nplace. This design has two major disadvantages. First, the router needs to\nmaintain a fixed routing table. Hence, these systems are less flexible with\nrespect to query routing, fault tolerance, and graph updates. Second, the graph\ndata must be partitioned such that the workload across the servers is balanced,\nand the inter-machine communication is minimized. In addition, it is required\nto update the existing partitions based on workload changes over graph nodes.\nHowever, graph partitioning, online monitoring of workloads, and dynamically\nupdating the graph partitions are expensive. In this work, we mitigate both\nthese problems by decoupling graph storage from query processors, and by\ndeveloping smart routing strategies that improve the cache locality in query\nprocessors. Since a query processor is no longer assigned any fixed part of the\ngraph, it is equally capable of handling any request, thus facilitating load\nbalancing and fault tolerance. On the other hand, due to our smart routing\nstrategies, query processors can effectively leverage their cache contents,\nreducing the overall impact of how the graph is partitioned across storage\nservers. A detailed experimental evaluation with several real-world, large\ngraph datasets demonstrates that our proposed framework, gRouting - even with\nsimple hash partitioning of the data - achieves up to an order of magnitude\nbetter query throughput compared to existing graph querying systems that employ\nexpensive graph partitioning and re-partitioning strategies.\n",
        "  This chapter outlines some of the challenges and opportunities associated\nwith adopting provenance principles and standards in a variety of disciplines,\nincluding data publication and reuse, and information sciences.\n",
        "  Superconducting and epitaxially grown LaFeAsOF thin films were successfully\nprepared on (001)-oriented LaAlO3 substrates using pulsed laser deposition. The\nprepared thin films show exclusively a single in-plane orientation with\nepitaxial relation (001)[100] parallel to (001)[100] and a FWHM value of 1deg.\nFurthermore, resistive measurement of the superconducting transition\ntemperature revealed a Tc90 of 25K with a high residual resistive ratio of 6.8.\nThe applied preparation technique, standard thin film pulsed laser deposition\nat room temperature in combination with a subsequent post annealing process, is\nsuitable for fabrication of high quality LaFeAsO1-xFx thin films. A high upper\ncritical field of 76.2 T was evaluated for magnetic fields applied\nperpendicular to the c-axis and the anisotropy was calculated to be 3.3\nassuming single band superconductivity.\n",
        "  This paper explores the novel and unconventional idea of implementing an\nanalytical RDBMS in pure JavaScript so that it runs completely inside a browser\nwith no external dependencies. Our prototype, called Afterburner, generates\ncompiled query plans that exploit typed arrays and asm.js, two relatively\nrecent advances in JavaScript. On a few simple queries, we show that\nAfterburner achieves comparable performance to MonetDB running natively on the\nsame machine. This is an interesting finding in that it shows how far\nJavaScript has come as an efficient execution platform. Beyond a mere technical\ncuriosity, we discuss how our techniques could support ubiquitous in-browser\ninteractive analytics (potentially integrating with browser-based notebooks)\nand also present interesting opportunities for split-execution strategies where\nquery operators are distributed between the browser and backend servers.\n",
        "  This paper describes a method for the automatic inference of structural\ntransfer rules to be used in a shallow-transfer machine translation (MT) system\nfrom small parallel corpora. The structural transfer rules are based on\nalignment templates, like those used in statistical MT. Alignment templates are\nextracted from sentence-aligned parallel corpora and extended with a set of\nrestrictions which are derived from the bilingual dictionary of the MT system\nand control their application as transfer rules. The experiments conducted\nusing three different language pairs in the free/open-source MT platform\nApertium show that translation quality is improved as compared to word-for-word\ntranslation (when no transfer rules are used), and that the resulting\ntranslation quality is close to that obtained using hand-coded transfer rules.\nThe method we present is entirely unsupervised and benefits from information in\nthe rest of modules of the MT system in which the inferred rules are applied.\n",
        "  Abstract Meaning Representation (AMR) annotation efforts have mostly focused\non English. In order to train parsers on other languages, we propose a method\nbased on annotation projection, which involves exploiting annotations in a\nsource language and a parallel corpus of the source language and a target\nlanguage. Using English as the source language, we show promising results for\nItalian, Spanish, German and Chinese as target languages. Besides evaluating\nthe target parsers on non-gold datasets, we further propose an evaluation\nmethod that exploits the English gold annotations and does not require access\nto gold annotations for the target languages. This is achieved by inverting the\nprojection process: a new English parser is learned from the target language\nparser and evaluated on the existing English gold standard.\n",
        "  We examine two positions, ON1 and ON2, within the Ophiuchus cloud LDN 1688\nusing observations made with the ISOPHOT instrument aboard the ISO satellite.\nThe data include mid-IR spectra (~6-12{\\mu}m) and several photometric bands up\nto 200{\\mu}m. The data probe the emission from molecular PAH-type species,\ntransiently-heated Very Small Grains (VSGs), and large classical dust grains.\nWe compare the observations to earlier studies, especially those carried out\ntowards an isolated translucent cloud in Chamaeleon (Paper I). The spectra\ntowards the two LDN 1688 positions are very similar to each other, in spite of\nposition ON1 having a larger column density and probably being subjected to a\nstronger radiation field. The ratios of the mid-IR features are similar to\nthose found in other diffuse and translucent clouds. Compared to paper I, the\n7.7/11.3{\\mu}m band ratios are lower, ~2.0, at both LDN 1688 positions. A\ncontinuum is detected in the ~10{\\mu}m region. This is stronger towards the\nposition ON1 but still lower than on any of the sightlines in Paper I. The\nfar-infrared opacities are higher than for diffuse medium. The value of the\nposition ON2, {\\tau}200/N(H) = 3.9 x 10^{-25} cm^2/H, is twice the value found\nfor ON1. The radiation field of LDN 1688 is dominated by the two embedded B\ntype double stars, {\\rho} Oph AB and HD 147889, with an additional contribution\nfrom the Upper Sco OB association. The strong heating is reflected in the high\ncolour temperature, ~24 K, of the large grain emission. Radiative transfer\nmodelling confirms a high level of the radiation field and points to an\nincreased abundance of PAH grains. However, when the hardening of the radiation\nfield caused by the local B-stars is taken into account, the observations can\nbe fitted with almost no change to the standard dust models. However, all the\nexamined models underestimate the level of the mid-IR continuum.\n",
        "  Distributed representation learned with neural networks has recently shown to\nbe effective in modeling natural languages at fine granularities such as words,\nphrases, and even sentences. Whether and how such an approach can be extended\nto help model larger spans of text, e.g., documents, is intriguing, and further\ninvestigation would still be desirable. This paper aims to enhance neural\nnetwork models for such a purpose. A typical problem of document-level modeling\nis automatic summarization, which aims to model documents in order to generate\nsummaries. In this paper, we propose neural models to train computers not just\nto pay attention to specific regions and content of input documents with\nattention models, but also distract them to traverse between different content\nof a document so as to better grasp the overall meaning for summarization.\nWithout engineering any features, we train the models on two large datasets.\nThe models achieve the state-of-the-art performance, and they significantly\nbenefit from the distraction modeling, particularly when input documents are\nlong.\n",
        "  Knowledge of users' emotion states helps improve human-computer interaction.\nIn this work, we presented EmoNet, an emotion detector of Chinese daily\ndialogues based on deep convolutional neural networks. In order to maintain the\noriginal linguistic features, such as the order, commonly used methods like\nsegmentation and keywords extraction were not adopted, instead we increased the\ndepth of CNN and tried to let CNN learn inner linguistic relationships. Our\nmain contribution is that we presented a new model and a new pipeline which can\nbe used in multi-language environment to solve sentimental problems.\nExperimental results shows EmoNet has a great capacity in learning the emotion\nof dialogues and achieves a better result than other state of art detectors do.\n",
        "  Predicting the ancestral sequences of a group of homologous sequences related\nby a phylogenetic tree has been the subject of many studies, and numerous\nmethods have been proposed to this purpose. Theoretical results are available\nthat show that when the mutation rate become too large, reconstructing the\nancestral state at the tree root is no longer feasible. Here, we also study the\nreconstruction of the ancestral changes that occurred along the tree edges. We\nshow that, depending on the tree and branch length distribution, reconstructing\nthese changes (i.e. reconstructing the ancestral state of all internal nodes in\nthe tree) may be easier or harder than reconstructing the ancestral root state.\nHowever, results from information theory indicate that for the standard Yule\ntree, the task of reconstructing internal node states remains feasible, even\nfor very high substitution rates. Moreover, computer simulations demonstrate\nthat for more complex trees and scenarios, this result still holds. For a large\nvariety of counting, parsimony-based and likelihood-based methods, the\npredictive accuracy of a randomly selected internal node in the tree is indeed\nmuch higher than the accuracy of the same method when applied to the tree root.\nMoreover, parsimony- and likelihood-based methods appear to be remarkably\nrobust to sampling bias and model mis-specification.\n",
        "  The Fe isotope effect (Fe-IE) on the transition temperature T_c and the\ncrystal structure was studied in the Fe chalcogenide superconductor FeSe_1-x by\nmeans of magnetization and neutron powder diffraction (NPD). The substitution\nof natural Fe (containing \\simeq 92% of ^{56}Fe) by its lighter ^{54}Fe isotope\nleads to a shift of T_c of 0.22(5)K corresponding to an Fe-IE exponent of\n\\alpha_Fe=0.81(15). Simultaneously, a small structural change with isotope\nsubstitution is observed by NDP which may contribute to the total Fe isotope\nshift of T_c.\n",
        "  In this paper, we present a novel approach to machine reading comprehension\nfor the MS-MARCO dataset. Unlike the SQuAD dataset that aims to answer a\nquestion with exact text spans in a passage, the MS-MARCO dataset defines the\ntask as answering a question from multiple passages and the words in the answer\nare not necessary in the passages. We therefore develop an\nextraction-then-synthesis framework to synthesize answers from extraction\nresults. Specifically, the answer extraction model is first employed to predict\nthe most important sub-spans from the passage as evidence, and the answer\nsynthesis model takes the evidence as additional features along with the\nquestion and passage to further elaborate the final answers. We build the\nanswer extraction model with state-of-the-art neural networks for single\npassage reading comprehension, and propose an additional task of passage\nranking to help answer extraction in multiple passages. The answer synthesis\nmodel is based on the sequence-to-sequence neural networks with extracted\nevidences as features. Experiments show that our extraction-then-synthesis\nmethod outperforms state-of-the-art methods.\n",
        "  Knowledge of thermal history in biological tissues during laser based\nhyperthermia is essential to achieve necrosis of tumour orcarcinoma cells. A\nsemi analytical model to predict sub surface thermal history in translucent,\nsoft, bio tissue mimics has been proposed. The model can accurately predict the\nspatio temporal temperature variations along depth and the anomalous thermal\nbehaviour in such media, viz. occurrence of sub surface temperature peaks.\nBased on opto thermal properties, the augmented temperature and shift of the\npeak positions in case of gold nanostructure mediated tissue phantom\nhyperthermia can be predicted. Employing inverse approach, the absorbance\ncoefficient of nano graphene infused tissue mimics is determined from the peak\ntemperature and found to provide appreciably accurate predictions along depth.\nFurthermore, a simplistic, dimensionally consistent correlation to\ntheoretically determine the position of the peak in such media is proposed and\nfound to be consistent with experiments and computations. The model shows\npromise in predicting thermal history in optically active soft matter and\ndeduction of therapeutic hyperthermia parameters, thereby providing a priori\nestimates for clinical trials.\n",
        "  Many database applications perform complex data retrieval and update tasks.\nNested queries, and queries that invoke user-defined functions, which are\nwritten using a mix of procedural and SQL constructs, are often used in such\napplications. A straight-forward evaluation of such queries involves repeated\nexecution of parameterized sub-queries or blocks containing queries and\nprocedural code.\n  An important problem that arises while optimizing nested queries as well as\nqueries with joins, aggregates and set operations is the problem of finding an\noptimal sort order from a factorial number of possible sort orders. We show\nthat even a special case of this problem is NP-Hard, and present practical\nheuristics that are effective and easy to incorporate in existing query\noptimizers.\n  We also consider iterative execution of queries and updates inside complex\nprocedural blocks such as user-defined functions and stored procedures.\nParameter batching is an important means of improving performance as it enables\nset-orientated processing. The key challenge to parameter batching lies in\nrewriting a given procedure/function to process a batch of parameter values. We\npropose a solution, based on program analysis and rewrite rules, to automate\nthe generation of batched forms of procedures and replace iterative database\ncalls within imperative loops with a single call to the batched form.\n  We present experimental results for the proposed techniques, and the results\nshow significant gains in performance.\n",
        "  The aim of the paper is to give a physical explanation of the absence of the\nfeature in the Calzetti extinction curve. We analyze the dust attenuation of a\nhomogeneous source seen through a distant inhomogeneous distant screen. The\ninhomogeneities are described through an idealized isothermal turbulent medium\nwhere the probability distribution function (PDF) of the column density is\nlog-normal. In addition it is assumed that below a certain critical column\ndensity the carriers of the extinction bump at 2175 Angstroem are being\ndestroyed by the ambient UV radiation field. Turbulence is found to be a\nnatural explanation not only of the flatter curvature of the Calzetti\nextinction curve but also of the missing bump provided the critical column\ndensity is N_H >= 10^21 cm^-2. The density contrast needed to explain both\ncharacteristics is well consistent with the Mach number of the cold neutral\nmedium of our own Galaxy which suggests a density contrast sigma_(rho/<rho>) 6.\n",
        "  One of the key questions in observational cosmology is the identification of\nthe sources responsible for ionisation of the Universe after the cosmic Dark\nAges, when the baryonic matter was neutral. The currently identified distant\ngalaxies are insufficient to fully reionise the Universe by redshift z~6, but\nlow-mass star-forming galaxies are thought to be responsible for the bulk of\nthe ionising radiation. Since direct observations at high redshift are\ndifficult for a variety of reasons, one solution is to identify local proxies\nof this galaxy population. However, starburst galaxies at low redshifts are\ngenerally opaque to their ionising radiation. This radiation with small escape\nfractions of 1-3% is directly detected only in three low-redshift galaxies.\nHere we present far-ultraviolet observations of a nearby low-mass star-forming\ngalaxy, J0925+1403, selected for its compactness and high excitation. The\ngalaxy is leaking ionising radiation, with an escape fraction of ~8%. The total\nnumber of photons emitted during the starburst phase is sufficient to ionize\nintergalactic medium material, which is about 40 times more massive than the\nstellar mass of the galaxy.\n",
        "  The ad-hoc querying process is slow and error prone due to inability of\nbusiness experts of accessing data directly without involving IT experts. The\nproblem lies in complexity of means used to query data. We propose a new\nnatural language- and semistar ontology-based ad-hoc querying approach which\nlowers the steep learning curve required to be able to query data. The proposed\napproach would significantly shorten the time needed to master the ad-hoc\nquerying and to gain the direct access to data by business experts, thus\nfacilitating the decision making process in enterprises, government\ninstitutions and other organizations.\n",
        "  Temperature-dependent inter-plane resistivity, $\\rho_c(T)$, was measured\nsystematically as a function of transition metal substitution in the\niron-arsenide superconductors Ba(Fe$_{1-x}$T$_x$)$_2$As$_2$, $T$= Ni, Pd, Rh.\nThe data are compared with the behavior found in\nBa(Fe$_{1-x}$Co$_x$)$_2$As$_2$, revealing resistive signatures of pseudogap. In\nall compounds we find resistivity crossover at a characteristic pseudogap\ntemperature $T^*$ from non-metallic to metallic temperature dependence on\ncooling. Suppression of $T^*$ proceeds very similar in cases of Ni and Pd\ndoping and much faster than in similar cases of Co and Rh doping. In cases of\nCo and Rh doping an additional minimum in the temperature-dependent $\\rho_c$\nemerges for high dopings, when superconductivity is completely suppressed.\nThese features are consistent with the existence of a charge gap covering part\nof the Fermi surface. The part of the Fermi surface affected by this gap is\nnotably larger for Ni and Pd doped compositions than in Co and Rh doped\ncompounds.\n",
        "  Let $b$ be a pseudo-Anosov braid whose permutation has a fixed point and let\n$M_b$ be the mapping torus by the pseudo-Anosov homeomorphism defined on the\ngenus $0$ fiber $F_b$ associated with $b$. This paper describes a structure of\nthe fibered cone $\\mathcal{C}$ of $F$ for $M_b$. We prove that there is a\n$2$-dimensional subcone $\\mathcal{C}_0$ contained in the fibered cone $\n\\mathcal{C}$ of $F_b$ such that the fiber $F_a$ for each primitive integral\nclass $a \\in \\mathcal{C}_0$ has genus $0$. We also give a constructive\ndescription of the monodromy $ \\phi_a: F_a \\rightarrow F_a$ of the fibration on\n$M_b$ over the circle, and consequently provide a construction of many\nsequences of pseudo-Anosov braids with small normalized entropies. As an\napplication we prove that the smallest entropy among skew-palindromic braids\nwith $n$ strands is comparable to $1/n$, and the smallest entropy among\nelements of the odd/even spin mapping class groups of genus $g$ is comparable\nto $1/g$.\n",
        "  In this paper, we describe ROOT13, a supervised system for the classification\nof hypernyms, co-hyponyms and random words. The system relies on a Random\nForest algorithm and 13 unsupervised corpus-based features. We evaluate it with\na 10-fold cross validation on 9,600 pairs, equally distributed among the three\nclasses and involving several Parts-Of-Speech (i.e. adjectives, nouns and\nverbs). When all the classes are present, ROOT13 achieves an F1 score of 88.3%,\nagainst a baseline of 57.6% (vector cosine). When the classification is binary,\nROOT13 achieves the following results: hypernyms-co-hyponyms (93.4% vs. 60.2%),\nhypernymsrandom (92.3% vs. 65.5%) and co-hyponyms-random (97.3% vs. 81.5%). Our\nresults are competitive with stateof-the-art models.\n",
        "  Based on a phenomenological model and the Kubo formula, we investigate the\nsuperfluid density $\\rho_s(T)$ and then the penetration depth $\\lambda(T)$ of\nthe iron-based superconductors in the coexistence region of the\nspin-density-wave(SDW) and superconductivity, and also in the over-doped\nregion. Our calculations show a dramatic increase of $\\lambda(0)$ with the\ndecrease of the doping concentration $x$ below $x=0.1$. This result is\nconsistent with the experimental observations. At low temperatures, $\\rho(T)$\nshows an exponential-law behavior, while at higher temperatures, the\nlinear-in-$T$ behavior is dominant before it trends to vanish. The evolution of\n$\\Delta\\lambda(T)$ can be roughly fitted by a exponential function at overdoped\nlevels while in other doping range it is a power-law function with the exponent\ndepending on the doping concentration. We show that the Uemura relation holds\nfor the iron-based superconductors only at very low doping levels.\n",
        "  High resolution spectra are used to analyze the galactic kinematics and\ndistribution of a sample of planetary nebulae with [WR] and 'wel' central star\n([WR]PN and WLPN). The circular and peculiar velocities (Vpec) of the objects\nwere derived. The results are: a) [WR]PNe are distributed mainly in the\ngalactic disk and they are more concentrated in a thinner disk than WLPNe and\nnormal PNe, which corresponds to a younger population; b) the sample was\nseparated in Peimbert's types, and it is found that Type I PNe have Vpec <50 km\ns-1, indicating young objects. Most of the [WR]PNe are of Type II showing Vpec\n<60 km s-1, although a small percentage is of Type III, with larger Vpec\nshowing that the Wolf-Rayet phenomenon in central stars can occur at any\nstellar mass and in old objects. None of our WLPNe is Type I. Thus, [WR]PNe and\nWLPNe are unrelated objects.\n",
        "  The absolute number and the density profiles of different types of stars in\nthe solar neighborhood are a fundamental anchor for studies of the initial mass\nfunction, stellar evolution, and galactic structure. Using data from the Gaia\nDR1 Tycho-Gaia Astrometric Solution, we reconstruct Gaia's selection function\nand we determine Gaia's volume completeness, the local number density, and the\nvertical profiles of different spectral types along the main sequence from\nearly A stars to late K stars as well as along the giant branch. We clearly\ndetect the expected flattening of the stellar density profile near the\nmid-plane for all stellar types: All vertical profiles are well represented by\nsech^2 profiles, with scale heights ranging from ~50 pc for A stars to ~150 pc\nfor G and K dwarfs and giants. We determine the luminosity function along the\nmain sequence for M_V < 7 (M >~ $0.72 M_\\odot$) and along the giant branch for\nM_J >~ -2.5. Converting this to a mass function, we find that the high-mass (M\n> $1\\,M_\\odot$) present-day mass function along the main sequence is d n / d M\n= 0.016 $(M/M_\\odot)^{-4.7}$ stars/pc^3/$M_\\odot$. Extrapolating below M =\n$0.72\\,M_\\odot$, we find a total mid-plane stellar density of 0.040+/-0.002\n$M_\\odot$/pc^3. Giants contribute 0.00039+/-0.00001 stars/pc^3 or about\n0.00046+/-0.00005 $M_\\odot$/pc^3. The star-formation rate surface density is\n\\Sigma(t) = 7+/-1 exp(-t/[7+/-1 Gyr]) $M_\\odot$/pc^2/Gyr. Overall, we find that\nGaia DR1's selection biases are manageable and allow a detailed new inventory\nof the solar neighborhood to be made that agrees with and extends previous\nstudies. This bodes well for mapping the Milky Way with the full Gaia data set.\n",
        "  Atherosclerosis is a disease caused due to formation of plaque into the\nartery. Increase in plaque affects the wall shear stress. The present study is\nperformed to calculate wall shear stress in different geometries of stenosed\ncarotid artery. A 2D model of different geometries is generated using CFD for\nNon- Newtonian model. After this WSS of different geometries of stenosed\narteries is calculated and compared. Wall Shear Stress (WSS) of carotid\narteries with smooth plaque, irregular plaque, cosine plaque and artery with\nblood clot is calculated. It is found that with increase of plaque in common\ncarotid artery WSS increases. Irregular plaque causes highest WSS. Wall Shear\nStress of opposite walls of carotid artery is compared where one wall is having\nblood clot into it and other one is healthy.\n",
        "  A new concept of permanent magnet systems for guiding superparamagnetic\nparticles on arbitrary trajectories is proposed. The basic concept is to use\none magnet system with a strong and homogeneous (dipolar) magnetic field to\nmagnetize and orient the particles. A second constantly graded field\n(quadrupolar) is superimposed to the first to generate a force. In this\nconfiguration the motion of the particles is driven solely by the component of\nthe gradient field which is parallel to the direction of the homogeneous field.\nThen the particles are guided with constant force in a single direction over\nthe entire volume. The direction can be adjusted by varying the angle between\nquadrupole and dipole. Since a single gradient is impossible, the other\ngradient component determines the angular deviation of the force. The latter\ncan be neglected if the homogeneous field is stronger than the local\ncontribution of the gradient field. A possible realization is a coaxial\narrangement of two Halbach cylinders. A dipole produces a strong and\nhomogeneous field to evenly magnetize the particles, while the second cylinder\nis a quadrupole to generate the force. A simple prototype was constructed to\ndemonstrate the principle on several nano-particles, which were moved along a\nrough square by manually changing the force angle. The observed velocities of\nparticles were always several orders of magnitude higher than the theoretical\nvalue. This discrepancy is attributed to the observed formation of long\nparticle chains as a result of their polarization by the homogeneous field. The\nmagnetic moment of such a chain is then the combination of that of its\nconstituents. A complete system will consist of another quadrupole (third\ncylinder) to additionally scale the strength of the force by another rotation.\nIn this configuration the device could then also be used as a simple MRI\nmachine to image the particles.\n",
        "  The run time complexity of state-of-the-art inference algorithms in\ngraph-based dependency parsing is super-linear in the number of input words\n(n). Recently, pruning algorithms for these models have shown to cut a large\nportion of the graph edges, with minimal damage to the resulting parse trees.\nSolving the inference problem in run time complexity determined solely by the\nnumber of edges (m) is hence of obvious importance.\n  We propose such an inference algorithm for first-order models, which encodes\nthe problem as a minimum spanning tree (MST) problem in an undirected graph.\nThis allows us to utilize state-of-the-art undirected MST algorithms whose run\ntime is O(m) at expectation and with a very high probability. A directed parse\ntree is then inferred from the undirected MST and is subsequently improved with\nrespect to the directed parsing model through local greedy updates, both steps\nrunning in O(n) time. In experiments with 18 languages, a variant of the\nfirst-order MSTParser (McDonald et al., 2005b) that employs our algorithm\nperforms very similarly to the original parser that runs an O(n^2) directed MST\ninference.\n",
        "  Predicting query execution time is a fundamental issue underlying many\ndatabase management tasks. Existing predictors rely on information such as\ncardinality estimates and system performance constants that are difficult to\nknow exactly. As a result, accurate prediction still remains elusive for many\nqueries. However, existing predictors provide a single, point estimate of the\ntrue execution time, but fail to characterize the uncertainty in the\nprediction. In this paper, we take a first step towards providing uncertainty\ninformation along with query execution time predictions. We use the query\noptimizer's cost model to represent the query execution time as a function of\nthe selectivities of operators in the query plan as well as the constants that\ndescribe the cost of CPU and I/O operations in the system. By treating these\nquantities as random variables rather than constants, we show that with low\noverhead we can infer the distribution of likely prediction errors. We further\nshow that the estimated prediction errors by our proposed techniques are\nstrongly correlated with the actual prediction errors.\n",
        "  We study Veech groups of covering surfaces of primitive translation surfaces.\nTherefore we define congruence subgroups in Veech groups of primitive\ntranslation surfaces using their action on the homology with entries in\n$\\mathbb{Z}/a\\mathbb{Z}$. We introduce a congruence level definition and a\nproperty of a primitive translation surface which we call property $(\\star)$.\nIt guarantees that partition stabilising congruence subgroups of this level\noccur as Veech group of a translation covering.\n  Each primitive surface with exactly one singular point has property $(\\star)$\nin every level. We additionally show that the surface glued from a regular\n$2n$-gon with odd $n$ has property $(\\star)$ in level $a$ iff $a$ and $n$ are\ncoprime. For the primitive translation surface glued from two regular $n$-gons,\nwhere $n$ is an odd number, we introduce a generalised Wohlfahrt level of\nsubgroups in its Veech group. We determine the relationship between this\nWohlfahrt level and the congruence level of a congruence group.\n",
        "  We present the compilation catalogue of redshift-independent distances\nincluded in the HyperLEDA database. It is actively maintained to be up-to-date,\nand the current version counts 6640 distance measurements for 2335 galaxies\ncompiled from 430 published articles. Each individual series is recalibrated\nonto a common distance scale based on a carefully selected set of high-quality\nmeasurements. This information together with data on HI line-width, central\nvelocity dispersion, magnitudes, diameters, and redshift is used to derive a\nhomogeneous distance estimate and physical properties of galaxies, such as\ntheir absolute magnitudes and intrinsic size.\n",
        "  We show that the figure eight knot complement admits a uniformizable\nspherical CR structure, i.e. it occurs as the manifold at infinity of a complex\nhyperbolic orbifold. The uniformization is unique provided we require the\nperipheral subgroups to have unipotent holonomy.\n",
        "  We present a sample of dwarf galaxies that suffer ongoing disruption by the\ntidal force of nearby massive galaxies. Analysing structural and stellar\npopulation properties using the archival imaging and spectroscopic data from\nthe Sloan Digital Sky Survey (SDSS), we find that they are likely a `smoking\ngun' example of the formation of early-type dwarf galaxies (dEs) in the galaxy\ngroup environment through the tidal stirring. Inner cores of these galaxies are\nfairly intact and the observed light profiles are well fitted with the Sersic\nfunctions, while the tidally stretched stellar halos are prominent in the outer\nparts. They are all located within the 50 kpc sky-projected distance from the\ncenter of host galaxies and no dwarf galaxies have relative line-of-sight\nvelocity larger than 205 km/s to their hosts. We derive the Composite Stellar\nPopulation (CSP) properties these galaxies by fitting the SDSS optical spectra\nto a multiple-burst composite stellar population model. We find that these\ngalaxies accumulate a significant fraction of stellar mass within the last 1\nGyr, while they contain a majority stellar population of intermediate age of 2\nto 4 Gyr. With these evidences, we argue that tidal stirring, particularly\nthrough the galaxy-galaxy interaction, might have an important role in the\nformation and evolution of dEs in the group environment, where the influence of\nother gas stripping mechanism might be limited.\n",
        "  We focus on knowledge base construction (KBC) from richly formatted data. In\ncontrast to KBC from text or tabular data, KBC from richly formatted data aims\nto extract relations conveyed jointly via textual, structural, tabular, and\nvisual expressions. We introduce Fonduer, a machine-learning-based KBC system\nfor richly formatted data. Fonduer presents a new data model that accounts for\nthree challenging characteristics of richly formatted data: (1) prevalent\ndocument-level relations, (2) multimodality, and (3) data variety. Fonduer uses\na new deep-learning model to automatically capture the representation (i.e.,\nfeatures) needed to learn how to extract relations from richly formatted data.\nFinally, Fonduer provides a new programming model that enables users to convert\ndomain expertise, based on multiple modalities of information, to meaningful\nsignals of supervision for training a KBC system. Fonduer-based KBC systems are\nin production for a range of use cases, including at a major online retailer.\nWe compare Fonduer against state-of-the-art KBC approaches in four different\ndomains. We show that Fonduer achieves an average improvement of 41 F1 points\non the quality of the output knowledge base---and in some cases produces up to\n1.87x the number of correct entries---compared to expert-curated public\nknowledge bases. We also conduct a user study to assess the usability of\nFonduer's new programming model. We show that after using Fonduer for only 30\nminutes, non-domain experts are able to design KBC systems that achieve on\naverage 23 F1 points higher quality than traditional machine-learning-based KBC\napproaches.\n",
        "  Electrohydrodynamic (EHD) generation, a commonly used method in BioMEMS,\nplays a significant role in the pulsed-release drug delivery system for a\ndecade. In this paper, an EHD based drug delivery system is well designed,\nwhich can be used to generate a single drug droplet as small as 2.83 nL in 8.5\nms with a total device of 2x2x3 mm^3, and an external supplied voltage of 1500\nV. Theoretically, we derive the expressions for the size and the formation time\nof a droplet generated by EHD method, while taking into account the drug supply\nrate, properties of liquid, gap between electrodes, nozzle size, and charged\ndroplet neutralization. This work proves a repeatable, stable and controllable\ndroplet generation and delivery system based on EHD method.\n",
        "  Purpose: For more than a decade that plastic optical fiber based dosimeters\nhave been developed for medical applications. The feasibility of dosimeters\nusing optical fibers that are almost Cherenkov light free has been demonstrated\nin some prototypes, particularly suitable for photon high-energy beams. In the\nenergy range up to a few hundred keV, where the production of Cherenkov light\nby secondary electrons is negligible or small, the largest source of background\nare the fluorescence mechanisms. Methods: In recent years our group has\ndeveloped an optical fiber dosimeter with photodiode readout named DosFib,\nwhich has small energy dependence in the range below 100 keV relevant for\nradiology. Photodiodes are robust photodetectors, presenting good stability\nover time and enough sensitivity to allow the use of an electrometer as a\nmeasuring device without extra electronics. Results: In-vitro tests using a\nHigh Dose Rate 192Ir source have demonstrated its suitability for brachytherapy\napplications using this important radioactive source. Attenuation curves have\nbeen obtained in water with the DosFib dosimeter and an ionization chamber and\nthe depth dose profiles shown to be identical. Conclusions: This new dosimeter\ncan thus be calibrated and used in clinical measurements with good competitive\nadvantages over other dosimeters.\n",
        "  Given a finite collection P of convex n-polytopes in RP^n (n>1), we consider\na real projective manifold M which is obtained by gluing together the polytopes\nin P along their facets in such a way that the union of any two adjacent\npolytopes sharing a common facet is convex. We prove that the real projective\nstructure on M is (1) convex if P contains no triangular polytope, and (2)\nproperly convex if, in addition, P contains a polytope whose dual polytope is\nthick. Triangular polytopes and polytopes with thick duals are defined as\nanalogues of triangles and polygons with at least five edges, respectively.\n",
        "  An important question in molecular evolution is whether an amino acid that\noccurs at a given position makes an independent contribution to fitness, or\nwhether its effect depends on the state of other loci in the organism's genome,\na phenomenon known as epistasis. In a recent letter to Nature, Breen et al.\n(2012) argued that epistasis must be \"pervasive throughout protein evolution\"\nbecause the observed ratio between the per-site rates of non-synonymous and\nsynonymous substitutions (dN/dS) is much lower than would be expected in the\nabsence of epistasis. However, when calculating the expected dN/dS ratio in the\nabsence of epistasis, Breen et al. assumed that all amino acids observed in a\nprotein alignment at any particular position have equal fitness. Here, we relax\nthis unrealistic assumption and show that any dN/dS value can in principle be\nachieved at a site, without epistasis. Furthermore, for all nuclear and\nchloroplast genes in the Breen et al. dataset, we show that the observed dN/dS\nvalues and the observed patterns of amino acid diversity at each site are\njointly consistent with a non-epistatic model of protein evolution.\n",
        "  Finding optimal evolutionary trees from sequence data is typically an\nintractable problem, and there is usually no way of knowing how close to\noptimal the best tree from some search truly is. The problem would seem to be\nparticularly acute when we have many taxa and when that data has high levels of\nhomoplasy, in which the individual characters require many changes to fit on\nthe best tree. However, a recent mathematical result has provided a precise\ntool to generate a short number of high-homoplasy characters for any given\ntree, so that this tree is provably the optimal tree under the maximum\nparsimony criterion. This provides, for the first time, a rigorous way to test\ntree search algorithms on homoplasy-rich data, where we know in advance what\nthe `best' tree is. In this short note we consider just one search program\n(TNT) but show that it is able to locate the globally optimal tree correctly\nfor 32,768 taxa, even though the characters in the dataset requires, on\naverage, 1148 state-changes each to fit on this tree, and the number of\ncharacters is only 57.\n",
        "  An analog of the diaelastic effect is predicted to occur in a small Josephson\ncontact with Josephson vortices manifesting itself as magnetic field induced\nsoftening of the contact shear modulus C(T,H). In addition to Fraunhofer type\nfield oscillations, C(T,H) is found to exhibit pronounced flux driven\ntemperature oscillations near T_C.\n",
        "  We present the Herschel SPIRE Fourier Transform Spectroscopy (FTS) atlas for\na complete flux limited sample of local Ultra-Luminous Infra-Red Galaxies as\npart of the HERschel ULIRG Survey (HERUS). The data reduction is described in\ndetail and was optimized for faint FTS sources with particular care being taken\nwith the subtraction of the background which dominates the continuum shape of\nthe spectra. Special treatment in the data reduction has been given to any\nobservation suffering from artefacts in the data caused by anomalous\ninstrumental effects to improve the final spectra. Complete spectra are shown\ncovering $200 - 671\\mu$m with photometry in the SPIRE bands at 250$\\mu$m,\n350$\\mu$m and 500$\\mu$m. The spectra include near complete CO ladders for over\nhalf of our sample, as well as fine structure lines from [CI] 370 $\\mu$m, [CI]\n609 $\\mu$m, and [NII] 205 $\\mu$m. We also detect H$_{2}$O lines in several\nobjects. We construct CO Spectral Line Energy Distributions (SLEDs) for the\nsample, and compare their slopes with the far-infrared colours and\nluminosities. We show that the CO SLEDs of ULIRGs can be broadly grouped into\nthree classes based on their excitation. We find that the mid-J (5$<$J$<$8)\nlines are better correlated with the total far-infrared luminosity, suggesting\nthat the warm gas component is closely linked to recent star-formation. The\nhigher J transitions do not linearly correlate with the far-infrared\nluminosity, consistent with them originating in hotter, denser gas unconnected\nto the current star-formation. {\\bf We conclude that in most cases more than\none temperature components are required to model the CO SLEDs.}\n",
        "  Vector-borne diseases with reservoir cycles are complex to understand because\nnew infections come from contacts of the vector with humans and different\nreservoirs. In this scenario, the basic reproductive number $\\mathcal{R}^h_0$\nof the system where the reservoirs are not included could turn out to be less\nthan one, yet, an endemic equilibrium be observed. Indeed, when the reservoirs\nare taken back into account, the basic reproductive number $\\mathcal{R}_0^r$,\nof only vectors and reservoirs, explains the endemic state. Furthermore,\nreservoirs cycles with a small basic reproductive number could contribute to\nreach an endemic state in the human cycle. Therefore, when controlling for the\nspread of a disease, it could not be enough to focus on specific reservoir\ncycles or only on the vector. In this work, we created a simple epidemiological\nmodel with a network of reservoirs where $\\mathcal{R}_0^r$ is a bifurcation\nparameter of the system, explaining disease endemicity in the absence of a\nstrong reservoir cycle. This simple model may help to explain transmission\ndynamics of diseases such as Chagas, Leishmaniasis and Dengue.\n",
        "  In speech-applications such as text-to-speech (TTS) or automatic speech\nrecognition (ASR), \\emph{text normalization} refers to the task of converting\nfrom a \\emph{written} representation into a representation of how the text is\nto be \\emph{spoken}. In all real-world speech applications, the text\nnormalization engine is developed---in large part---by hand. For example, a\nhand-built grammar may be used to enumerate the possible ways of saying a given\ntoken in a given language, and a statistical model used to select the most\nappropriate pronunciation in context. In this study we examine the tradeoffs\nassociated with using more or less language-specific domain knowledge in a text\nnormalization engine. In the most data-rich scenario, we have access to a\ncarefully constructed hand-built normalization grammar that for any given token\nwill produce a set of all possible verbalizations for that token. We also\nassume a corpus of aligned written-spoken utterances, from which we can train a\nranking model that selects the appropriate verbalization for the given context.\nAs a substitute for the carefully constructed grammar, we also consider a\nscenario with a language-universal normalization \\emph{covering grammar}, where\nthe developer merely needs to provide a set of lexical items particular to the\nlanguage. As a substitute for the aligned corpus, we also consider a scenario\nwhere one only has the spoken side, and the corresponding written side is\n\"hallucinated\" by composing the spoken side with the inverted normalization\ngrammar. We investigate the accuracy of a text normalization engine under each\nof these scenarios. We report the results of experiments on English and\nRussian.\n",
        "  The hunt for the benchmark topological superconductor (TSc) has been an\nextremely active research subject in condensed matter research, with quite a\nfew candidates identified or proposed. However, low transition temperatures\n(Tc) and/or strong sensitivity to disorder and dopant levels in known TSc\ncandidates have greatly hampered progress in this field. Here, we use\nAngle-resolved Photoemission Spectroscopy (ARPES) to show the presence of Dirac\nNodal Lines (DNLs) and the corresponding topological surface states (TSS's) on\nthe [010] faces of the Tc=39K s-wave BCS superconductor MgB2. Not only is this\nnearly triple the current record of superconducting Tc among all candidate\nTSc's, but the nature of these DNL states should make them highly tolerant\nagainst disorder and inadvertent doping variations. This makes MgB2 a promising\nhigh temperature platform for the study of topological superconductivity.\n",
        "  We present CARMA observations in 3.3 mm continuum and several molecular lines\nof the surroundings of N14, N22, and N74, three infrared bubbles from the\nGLIMPSE catalog. We have discovered 28 compact continuum sources and confirmed\ntheir associations with the bubbles using velocity information from HCO+ and\nHCN. We have also mapped small-scale structures of N2H+ emission in the\nvicinity of the bubbles. By combining our data with survey data from GLIMPSE,\nMIPSGAL, BGPS, and MAGPIS, we establish about half of our continuum sources as\nstar-forming cores. We also use survey data with the velocity information from\nour molecular line observations to describe the morphology of the bubbles and\nthe nature of the fragmentation. We conclude from the properties of the\ncontinuum sources that N74 likely is at the near kinematic distance, which was\npreviously unconfirmed. We also present tentative evidence of molecular clouds\nbeing more fragmented on bubble rims compared to dark clouds, suggesting that\ntriggered star formation may occur, though our findings do not conform to a\nclassic collect-and-collapse model.\n",
        "  A phase-slip flux qubit, exactly dual to a charge qubit, is composed of a\nsuperconducting loop interrupted by a phase-slip junction. Here we propose a\ntunable phase-slip flux qubit by replacing the phase-slip junction with a\ncharge-related superconducting quantum interference device (SQUID) consisting\nof two phase-slip junctions connected in series with a superconducting island.\nThis charge-SQUID acts as an effective phase-slip junction controlled by the\napplied gate voltage and can be used to tune the energy-level splitting of the\nqubit. Also, we show that a large inductance inserted in the loop can reduce\nthe inductance energy and consequently suppress the dominating flux noise of\nthe phase-slip flux qubit. This enhanced phase-slip flux qubit is exactly dual\nto a transmon qubit.\n",
        "  We present deep new GTC/OSIRIS narrow-band images and optical WHT/ISIS\nlong-slit spectroscopy of the merging system Mrk273 that show a spectacular\nextended halo of warm ionised gas out to a radius of $\\sim45$ kpc from the\nsystem nucleus. Outside of the immediate nuclear regions (r > 6 kpc), there is\nno evidence for kinematic disturbance in the ionised gas: in the extended\nregions covered by our spectroscopic slits the emission lines are relatively\nnarrow (FWHM $\\lesssim$ 350 km$\\rm s^{-1}$) and velocity shifts small\n(|$\\Delta$V| $\\lesssim{} $250 km$\\rm s^{-1}$). This is despite the presence of\npowerful near-nuclear outflows (FWHM > 1000 km$\\rm s^{-1}$; |$\\Delta$V| > 400\nkm$\\rm s^{-1}$; r < 6 kpc). Diagnostic ratio plots are fully consistent with\nSeyfert 2 photo-ionisation to the NE of the nuclear region, however to the SW\nthe plots are more consistent with low-velocity radiative shock models. The\nkinematics of the ionised gas, combined with the fact that the main structures\nare aligned with low-surface-brightness tidal continuum features, are\nconsistent with the idea that the ionised halo represents tidal debris left\nover from a possible triple-merger event, rather than a reservoir of outflowing\ngas.\n",
        "  Monte Carlo (MC) simulation is commonly considered to be the most accurate\ndose calculation method in radiotherapy. However, its efficiency still requires\nimprovement for many routine clinical applications. In this paper, we present\nour recent progress towards the development a GPU-based MC dose calculation\npackage, gDPM v2.0. It utilizes the parallel computation ability of a GPU to\nachieve high efficiency, while maintaining the same particle transport physics\nas in the original DPM code and hence the same level of simulation accuracy. In\nGPU computing, divergence of execution paths between threads can considerably\nreduce the efficiency. Since photons and electrons undergo different physics\nand hence attain different execution paths, we use a simulation scheme where\nphoton transport and electron transport are separated to partially relieve the\nthread divergence issue. High performance random number generator and hardware\nlinear interpolation are also utilized. We have also developed various\ncomponents to handle fluence map and linac geometry, so that gDPM can be used\nto compute dose distributions for realistic IMRT or VMAT treatment plans. Our\ngDPM package is tested for its accuracy and efficiency in both phantoms and\nrealistic patient cases. In all cases, the average relative uncertainties are\nless than 1%. A statistical t-test is performed and the dose difference between\nthe CPU and the GPU results is found not statistically significant in over 96%\nof the high dose region and over 97% of the entire region. Speed up factors of\n69.1 ~ 87.2 have been observed using an NVIDIA Tesla C2050 GPU card against a\n2.27GHz Intel Xeon CPU processor. For realistic IMRT and VMAT plans, MC dose\ncalculation can be completed with less than 1% standard deviation in 36.1~39.6\nsec using gDPM.\n",
        "  We study the twisted Alexander polynomial $\\Delta_{K,\\rho}$ of a knot $K$\nassociated to a non-abelian representation $\\rho$ of the knot group into\n$SL_2(\\BC)$. It is known for every knot $K$ that if $K$ is fibered, then for\nevery non-abelian representation, $\\Delta_{K,\\rho}$ is monic and has degree\n$4g(K)-2$ where $g(K)$ is the genus of $K$. Kim and Morifuji recently proved\nthe converse for 2-bridge knots. In fact they proved a stronger result: if a\n2-bridge knot $K$ is non-fibered, then all but finitely many non-abelian\nrepresentations on some component have $\\Delta_{K,\\rho}$ non-monic and degree\n$4g(K)-2$. In this paper, we consider two special families of non-fibered\n2-bridge knots including twist knots. For these families, we calculate the\nnumber of non-abelian representations where $\\Delta_{K,\\rho}$ is monic and\ncalculate the number of non-abelian representations where the degree of\n$\\Delta_{K,\\rho}$ is less than $4g(K)-2$.\n",
        "  Organisations store huge amounts of data from multiple heterogeneous sources\nin the form of Knowledge Graphs (KGs). One of the ways to query these KGs is to\nuse SPARQL queries over a database engine. Since SPARQL follows exact match\nsemantics, the queries may return too few or no results. Recent works have\nproposed query relaxation where the query engine judiciously replaces a query\npredicate with similar predicates using weighted relaxation rules mined from\nthe KG. The space of possible relaxations is potentially too large to fully\nexplore and users are typically interested in only top-k results, so such query\nengines use top-k algorithms for query processing. However, they may still\nprocess all the relaxations, many of whose answers do not contribute towards\ntop-k answers. This leads to computation overheads and delayed response times.\n  We propose Spec-QP, a query planning framework that speculatively determines\nwhich relaxations will have their results in the top-k answers. Only these\nrelaxations are processed using the top-k operators. We, therefore, reduce the\ncomputation overheads and achieve faster response times without adversely\naffecting the quality of results. We tested Spec-QP over two datasets - XKG and\nTwitter, to demonstrate the efficiency of our planning framework at reducing\nruntimes with reasonable accuracy for query engines supporting relaxations.\n",
        "  Superconductivity in Sr$_{2}$RuO$_{4}$ is unconventional, believed to be of\n$p_{x}\\pm ip_{y}$ pairing symmetry. These two degenerate order parameters allow\nthe formation of chiral domains separated by domain walls. In a Josephson\njunction formed on the edge of a single crystal of Sr$_{2}$RuO$_{4}$, the\nchiral domains can create a variation of the phase in the tunneling direction\ncausing interference effects which suppress and modulate the critical current\nof the junction. Cooling the junction in a magnetic field lifts the degeneracy\nbetween the order parameter states and induces a preferential chirality,\nsignificantly modifying the phase interference. We present experimental results\non Sr$_{2}$RuO$_{4}$/Cu/Pb Josephson junctions cooled in a magnetic field\nshowing a dramatic enhancement of their critical current, giving direct\nevidence for the presence of chiral domains and their alignment in a magnetic\nfield.\n",
        "  In this article, we show how the mathematical object tensor can be used to\nbuild a multi-paradigm model for the storage of social data in data warehouses.\nFrom an architectural point of view, our approach allows to link different\nstorage systems (polystore) and limits the impact of ETL tools performing model\ntransformations required to feed different analysis algorithms. Therefore,\nsystems can take advantage of multiple data models both in terms of query\nexecution performance and the semantic expressiveness of data representation.\nThe proposed model allows to reach the logical independence between data and\nprograms implementing analysis algorithms. With a concrete case study on\nmessage virality on Twitter during the French presidential election of 2017, we\nhighlight some of the contributions of our model.\n",
        "  It is conjectured that every cusped hyperbolic 3-manifold has a decomposition\ninto positive volume ideal hyperbolic tetrahedra (a \"geometric\" triangulation\nof the manifold). Under a mild homology assumption on the manifold we construct\ntopological ideal triangulations which admit a strict angle structure, which is\na necessary condition for the triangulation to be geometric. In particular,\nevery knot or link complement in the 3-sphere has such a triangulation. We also\ngive an example of a triangulation without a strict angle structure, where the\nobstruction is related to the homology hypothesis, and an example illustrating\nthat the triangulations produced using our methods are not generally geometric.\n",
        "  Combinatorial transgressions are secondary invariants of a space admitting\ntriangulations. They arise from subdivisions and are analogous to transgressive\nforms such as those arising in Chern-Weil theory. Unlike combinatorial\ncharacteristic classes, combinatorial transgressions have not been previously\nstudied. First, this article characterizes transgressions that are\npath-independent of subdivision sequence. The result is obtained by using a\ncohomology on posets that is shown to be equivalent to higher derived functors\nof the inverse (or projective) limit over the opposite poset. Second, a\ncanonical local formula is demonstrated for a particular combinatorial\ntransgression: namely, that relative the difference of Poincar\\'{e} duals to\nthe Euler class.\n",
        "  In this paper the task of emotion recognition from speech is considered.\nProposed approach uses deep recurrent neural network trained on a sequence of\nacoustic features calculated over small speech intervals. At the same time\nspecial probabilistic-nature CTC loss function allows to consider long\nutterances containing both emotional and neutral parts. The effectiveness of\nsuch an approach is shown in two ways. Firstly, the comparison with recent\nadvances in this field is carried out. Secondly, human performance on the same\ntask is measured. Both criteria show the high quality of the proposed method.\n",
        "  Evaluation of NLP methods requires testing against a previously vetted\ngold-standard test set and reporting standard metrics\n(accuracy/precision/recall/F1). The current assumption is that all items in a\ngiven test set are equal with regards to difficulty and discriminating power.\nWe propose Item Response Theory (IRT) from psychometrics as an alternative\nmeans for gold-standard test-set generation and NLP system evaluation. IRT is\nable to describe characteristics of individual items - their difficulty and\ndiscriminating power - and can account for these characteristics in its\nestimation of human intelligence or ability for an NLP task. In this paper, we\ndemonstrate IRT by generating a gold-standard test set for Recognizing Textual\nEntailment. By collecting a large number of human responses and fitting our IRT\nmodel, we show that our IRT model compares NLP systems with the performance in\na human population and is able to provide more insight into system performance\nthan standard evaluation metrics. We show that a high accuracy score does not\nalways imply a high IRT score, which depends on the item characteristics and\nthe response pattern.\n",
        "  We investigate the evolution of mass segregation in initially sub-structured\nyoung embedded star clusters with two different background potentials mimicking\nthe gas. Our clusters are initially in virial or sub-virial global states and\nhave different initial distributions for the most massive stars: randomly\nplaced, initially mass segregated or even inverse segregation. By means of\nN-body simulation we follow their evolution for 5 Myr. We measure the mass\nsegregation using the minimum spanning tree method Lambda_MSR and an equivalent\nrestricted method. Despite this variety of different initial conditions, we\nfind that our stellar distributions almost always settle very fast into a mass\nsegregated and more spherical configuration, suggesting that once we see a\nspherical or nearly spherical embedded star cluster, we can be sure it is mass\nsegregated no matter what the real initial conditions were. We, furthermore,\nreport under which circumstances this process can be more rapid or delayed,\nrespectively.\n",
        "  The 1144 iron arsenide (e.g. CaKFe4As4) has recently been discovered and\ninspired a tide of search for superconductors. Such far, the discovered\ncompounds are confined to iron arsenides (ABFe4As4), where A and B are either\nalkali metals or alkaline earth elements. In this work, we propose two\ndirections in searching 1144 structures: (i) using tri-valence cations for A;\n(ii) substituting the transition metal, e.g. replacing Fe by Co. Following the\ntwo directions, we employ density functional theory to study stability and\nelectronic structures of 1144 pnictides of various tri-valence cations (La, Y,\nIn, Tl, Sm and Gd), as well as cobalt arsenides. For LaAFe4As4, the 1144 phase\ncan be stabilized in three systems: LaKFe4As4, LaRbFe4As4 and LaCsFe4As4, which\nshow quasi-two-dimensional semi-metal features similar to the iron pnictide\nsuperconductors: hole-type Fermi surface at Gama point and electron-type Fermi\nsurface at M point in B.Z. In addition, LaKFe4As4 feature an extra bubble\nshaped Fermi surface sheets, distinct from the other two peers. Y does not\nsupport any 1144 phase within our search. For In and Tl, substitute Fe by Co\nand two unknown compounds of the 122 phase are stabilized: InCo2As2 and\nTlCo2As2. The two cobalt arsenides have Fermi surfaces of similar topology as\niron arsenides, but the Fermi surfaces are all electron-type, showing\npotentials to be undiscovered superconductors. Stable 1144 phases are also\nfound in InKCo4As4 and InRbCo4As4. For Sm and Gd, most 1144 and 122 iron\narsenides are found unstable.\n",
        "  Photon-counting energy resolving detectors are subject to intense research\ninterest, and there is a need for a general framework for performance\nassessment of these detectors. The commonly used linear-systems theory\nframework, which measures detector performance in terms of noise-equivalent\nquanta (NEQ) and detective quantum efficiency (DQE) is widely used for\ncharacterizing conventional X-ray detectors but does not take energy-resolving\ncapabilities into account. We extend this framework to encompass\nenergy-resolving photon-counting detectors and elucidate how the imperfect\nenergy response of real-world detectors affects imaging performance. We\ngeneralize NEQ and DQE to matrix-valued quantities as functions of spatial\nfrequency, and show how these can be calculated from simple Monte Carlo\nsimulations. To demonstrate how the new metrics can be interpreted, we compute\nthem for simplified models of fluorescence and Compton scatter in a\nphoton-counting detector and for a Monte Carlo model of a CdTe detector with\n0.5 x 0.5 mm^2 pixels. Our results show that the ideal-linear-observer\nperformance for any detection or material quantification task can be calculated\nfrom the proposed metrics. Off-diagonal elements in these matrices are shown to\nbe related to imperfect energy resolution. The Monte Carlo model of the CdTe\ndetector predicts a zero-frequency dose efficiency relative to an ideal\ndetector of 0.86 and 0.65 for detecting water and bone, respectively. When the\ntask instead is to quantify these materials, the corresponding values are 0.34\nfor water and 0.26 for bone. We have shown that the matrix-valued NEQ and DQE\nmetrics contain sufficient information for calculating the dose efficiency for\nboth detection or quantification tasks, the task having any spatial and energy\ndependence. This framework will be beneficial for the development of\nphoton-counting X-ray detectors.\n",
        "  A major problem of unstructured P2P systems is their heavy network traffic.\nThis is caused mainly by high numbers of query answers, many of which are\nirrelevant for users. One solution to this problem is to use Top-k queries\nwhereby the user can specify a limited number (k) of the most relevant answers.\nIn this paper, we present FD, a (Fully Distributed) framework for executing\nTop-k queries in unstructured P2P systems, with the objective of reducing\nnetwork traffic. FD consists of a family of algorithms that are simple but\neffec-tive. FD is completely distributed, does not depend on the existence of\ncertain peers, and addresses the volatility of peers during query execution. We\nvali-dated FD through implementation over a 64-node cluster and simulation\nusing the BRITE topology generator and SimJava. Our performance evaluation\nshows that FD can achieve major performance gains in terms of communication and\nresponse time.\n",
        "  This paper reviews the scientific support for a ballistic pressure wave\nradiating outward from a penetrating projectile and causing injury and\nincapacitation. This phenomenon is known colloquially as \"hydrostatic shock.\"\nThe idea apparently originates with Col. Frank Chamberlin, a World War II\ntrauma surgeon and wound ballistics researcher. The paper reviews claims that\nhydrostatic shock is a myth and considers supporting evidence through parallels\nwith blast, describing the physics of the pressure wave, evidence for remote\ncerebral effects, and remote effects in the spine and other internal organs.\nFinally, the review considers the levels of energy transfer required for the\nphenomenon to be readily observed.\n",
        "  The question as to why most higher organisms reproduce sexually has remained\nopen despite extensive research, and has been called \"the queen of problems in\nevolutionary biology\". Theories dating back to Weismann have suggested that the\nkey must lie in the creation of increased variability in offspring, causing\nenhanced response to selection. Rigorously quantifying the effects of assorted\nmechanisms which might lead to such increased variability, and establishing\nthat these beneficial effects outweigh the immediate costs of sexual\nreproduction has, however, proved problematic. Here we introduce an approach\nwhich does not focus on particular mechanisms influencing factors such as the\nfixation of beneficial mutants or the ability of populations to deal with\ndeleterious mutations, but rather tracks the entire distribution of a\npopulation of genotypes as it moves across vast fitness landscapes. In this\nsetting simulations now show sex robustly outperforming asex across a broad\nspectrum of finite or infinite population models. Concentrating on the additive\ninfinite populations model, we are able to give a rigorous mathematical proof\nestablishing that sexual reproduction acts as a more efficient optimiser of\nmean fitness, thereby solving the problem for this model. Some of the key\nfeatures of this analysis carry through to the finite populations case.\n",
        "  CRDT[24] Sets as implemented in Riak[6] perform poorly for writes, both as\ncardinality grows, and for sets larger than 500KB[25]. Riak users wish to\ncreate high cardinality CRDT sets, and expect better than O(n) performance for\nindividual insert and remove operations. By decomposing a CRDT set on disk, and\nemploying delta-replication[2], we can achieve far better performance than just\ndelta replication alone: relative to the size of causal metadata, not the\ncardinality of the set, and we can support sets that are 100s times the size of\nRiak sets, while still providing the same level of consistency. There is a\ntrade-off in read performance but we expect it is mitigated by enabling queries\non sets.\n",
        "  The surrogate-based analysis and optimization of thermal damage in living\nbiological tissue by laser irradiation are discussed in this paper. Latin\nHypercube Sampling (LHS) and Response Surface Model (RSM) are applied to study\nsurrogate-based optimization of thermal damage in tissue using a generalized\ndual phase lag model. Response value of high temperature as a function of input\nvariables and relationship of maximum temperature and thermal damage as a\nfunction of input variables are investigated. Comparison of SBO model and\nsimulation results for different sample sizes are examined. The results show\nthat every input variable individually has quadratic response of maximum\ntemperature and maximum thermal damage in highly absorbing tissues.\n",
        "  We perform a detailed study of temperature, bias and doping dependance of\ninterlayer transport in the layered high temperature superconductor\nBi$_2$Sr$_2$CaCu$_2$O$_{8+\\delta}$. We observe that the shape of interlayer\ncharacteristics in underdoped crystals exhibit a remarkable crossover at the\nsuperconducting transition temperature: from thermal activation-type at\n$T>T_c$, to almost $T-$independent quantum tunneling-type, at $T<T_c$. Our data\nindicates that the interlayer transport mechanism may change with doping: from\nthe conventional single quasiparticle tunneling in overdoped, to a\nprogressively increasing Cooper pair contribution in underdoped crystals.\n",
        "  Twisted links are obtained from a base link by starting with a $n$-braid\nrepresentation, choosing several ($m$) adjacent strands, and applying one or\nmore twists to the set. Various restrictions may be applied, e.g. the twists\nmay be required to be positive or full twists, or the base braid may be\nrequired to have a certain form.\n  The Jones polynomial of full $m$-twisted links have some interesting\nproperties. It is known that when sufficiently many full $m$-twists are added\nthat the coefficients break up into disjoint blocks which are independent of\nthe number of full twists. These blocks are separated by constants which\nalternate in sign. Other features are known. This paper presents the value of\nthese constants when two strands of a three-braid are twisted. It also\ndiscloses when this pattern emerges for either two or three strand twisting of\na three-braid, along with other properties.\n  Lorenz links and the equivalent T-links are positively twisted links of a\nspecial form. This paper presents the Jones polynomial for such links which\nhave braid index three. Some families of braid representations whose closures\nare identical links are given.\n",
        "  Aggregate analysis, such as comparing country-wise sales versus global market\nshare across product categories, is often complicated by the unavailability of\ncommon join attributes, e.g., category, across diverse datasets from different\ngeographies or retail chains, even after disparate data is technically ingested\ninto a common data lake. Sometimes this is a missing data issue, while in other\ncases it may be inherent, e.g., the records in different geographical databases\nmay actually describe different product 'SKUs', or follow different norms for\ncategorization. Record linkage techniques can be used to automatically map\nproducts in different data sources to a common set of global attributes,\nthereby enabling federated aggregation joins to be performed. Traditional\nrecord-linkage techniques are typically unsupervised, relying textual\nsimilarity features across attributes to estimate matches. In this paper, we\npresent an ensemble model combining minimal supervision using Bayesian network\nmodels together with unsupervised textual matching for automating such\n'attribute fusion'. We present results of our approach on a large volume of\nreal-life data from a market-research scenario and compare with a standard\nrecord matching algorithm. Finally we illustrate how attribute fusion using\nmachine learning could be included as a data-lake management feature,\nespecially as our approach also provides confidence values for matches,\nenabling human intervention, if required.\n",
        "  Morphological inflection generation is the task of generating the inflected\nform of a given lemma corresponding to a particular linguistic transformation.\nWe model the problem of inflection generation as a character sequence to\nsequence learning problem and present a variant of the neural encoder-decoder\nmodel for solving it. Our model is language independent and can be trained in\nboth supervised and semi-supervised settings. We evaluate our system on seven\ndatasets of morphologically rich languages and achieve either better or\ncomparable results to existing state-of-the-art models of inflection\ngeneration.\n",
        "  This work distinguishes between translated and original text in the UN\nprotocol corpus. By modeling the problem as classification problem, we can\nachieve up to 95% classification accuracy. We begin by deriving a parallel\ncorpus for different language-pairs annotated for translation direction, and\nthen classify the data by using various feature extraction methods. We compare\nthe different methods as well as the ability to distinguish between translated\nand original texts in the different languages. The annotated corpus is publicly\navailable.\n",
        "  In this paper we present a new family of Intensional RDBs (IRDBs) which\nextends the traditional RDBs with the Big Data and flexible and 'Open schema'\nfeatures, able to preserve the user-defined relational database schemas and all\npreexisting user's applications containing the SQL statements for a deployment\nof such a relational data. The standard RDB data is parsed into an internal\nvector key/value relation, so that we obtain a column representation of data\nused in Big Data applications, covering the key/value and column-based Big Data\napplications as well, into a unifying RDB framework. We define a query\nrewriting algorithm, based on the GAV Data Integration methods, so that each\nuser-defined SQL query is rewritten into a SQL query over this vector relation,\nand hence the user-defined standard RDB schema is maintained as an empty global\nschema for the RDB schema modeling of data and as the SQL interface to stored\nvector relation. Such an IRDB architecture is adequate for the massive\nmigrations from the existing slow RDBMSs into this new family of fast IRDBMSs\nby offering a Big Data and new flexible schema features as well.\n",
        "  We propose a mathematical model for computing drug release from multi-layer\ncapsules. The diffusion problem in such heterogeneous layer-by-layer composite\nmedium is described by a system of coupled partial differential equations,\nwhich we solve analytically using separation of variables. In addition to the\nconventional interlayer continuous mass transfer boundary conditions, we\nconsider also the case of finite mass transfer resistance, which corresponds to\nthe case of a coated capsule. The drug concentration in the core and through\nall layers, as well as in the external release medium, is then given in terms\nof a Fourier series that we compute numerically to characterize the drug\nrelease mechanism.\n",
        "  This paper describes the Duluth system that participated in SemEval-2017 Task\n6 #HashtagWars: Learning a Sense of Humor. The system participated in Subtasks\nA and B using N-gram language models, ranking highly in the task evaluation.\nThis paper discusses the results of our system in the development and\nevaluation stages and from two post-evaluation runs.\n",
        "  We have previously developed for nuclear cross-sections of therapeutic\nprotons a calculation model, which is founded on the collective model as well\nas a quantum mechanical many particle problem to derive the S matrix and\ntransition probabilities. In this communication, we show that the resonances\ncan be derived by shifted Gaussian functions, whereas the unspecific nuclear\ninteraction compounds can be represented by an error function, which also\nprovides the asymptotic behavior. The energy shifts can be interpreted in terms\nof necessary domains of energy to excite typical nuclear processes. Thus the\nnecessary formulas referring to previous calculations of nuclear cross-sections\nwill be represented in section 2. The mass number AN determines the strong\ninteraction range. The threshold energy ETh of the energy barrier is determined\nby the condition Estrong = ECoulomb. A linear combination of Gaussians, which\ncontain additional energy shifts, and an error function incorporate a possible\nrepresentation of Fermi-Dirac statistics, which is applied here to nuclear\nexcitations and reaction with release of secondary particles. The new\ncalculation formula provides a better understanding of different types of\nresonances occurring in nuclear interactions with protons. The present study is\nmainly a continuation of previous papers.\n",
        "  Although there has been much progress in understanding how galaxies evolve,\nwe still do not understand how and when they stop forming stars and become\nquiescent. We address this by applying our galaxy spectral energy distribution\nmodels, which incorporate physically motivated star formation histories (SFHs)\nfrom cosmological simulations, to a sample of quiescent galaxies at\n$0.2<z<2.1$. A total of 845 quiescent galaxies with multi-band photometry\nspanning rest-frame ultraviolet through near-infrared wavelengths are selected\nfrom the CANDELS dataset. We compute median SFHs of these galaxies in bins of\nstellar mass and redshift. At all redshifts and stellar masses, the median SFHs\nrise, reach a peak, and then decline to reach quiescence. At high redshift, we\nfind that the rise and decline are fast, as expected because the Universe is\nyoung. At low redshift, the duration of these phases depends strongly on\nstellar mass. Low-mass galaxies ($\\log(M_{\\ast}/M_{\\odot})\\sim9.5$) grow on\naverage slowly, take a long time to reach their peak of star formation\n($\\gtrsim 4$ Gyr), and the declining phase is fast ($\\lesssim 2$ Gyr).\nConversely, high-mass galaxies ($\\log(M_{\\ast}/M_{\\odot})\\sim11$) grow on\naverage fast ($\\lesssim 2$ Gyr), and, after reaching their peak, decrease the\nstar formation slowly ($\\gtrsim 3$ Gyr). These findings are consistent with\ngalaxy stellar mass being a driving factor in determining how evolved galaxies\nare, with high-mass galaxies being the most evolved at any time (i.e.,\ndownsizing). The different durations we observe in the declining phases also\nsuggest that low- and high-mass galaxies experience different quenching\nmechanisms that operate on different timescales.\n",
        "  Applications in various domains rely on processing graph streams, e.g.,\ncommunication logs of a cloud-troubleshooting system, road-network traffic\nupdates, and interactions on a social network. A labeled-graph stream refers to\na sequence of streamed edges that form a labeled graph. Label-aware\napplications need to filter the graph stream before performing a graph\noperation. Due to the large volume and high velocity of these streams, it is\noften more practical to incrementally build a lossy-compressed version of the\ngraph, and use this lossy version to approximately evaluate graph queries.\nChallenges arise when the queries are unknown in advance but are associated\nwith filtering predicates based on edge labels. Surprisingly common, and\nespecially challenging, are labeled-graph streams that have highly skewed label\ndistributions that might also vary over time. This paper introduces\nSelf-Balanced Graph Sketch (SBG-Sketch, for short), a graphical sketch for\nsummarizing and querying labeled-graph streams that can cope with all these\nchallenges. SBG-Sketch maintains synopsis for both the edge attributes (e.g.,\nedge weight) as well as the topology of the streamed graph. SBG-Sketch allows\nefficient processing of graph-traversal queries, e.g., reachability queries.\nExperimental results over a variety of real graph streams show SBG-Sketch to\nreduce the estimation errors of state-of-the-art methods by up to 99%.\n",
        "  Even the simplest organisms are too complex to have spontaneously arisen\nfully-formed, yet precursors to first life must have emerged ab initio from\ntheir environment. A watershed event was the appearance of the first entity\ncapable of evolution: the Initial Darwinian Ancestor. Here we suggest that\nnucleopeptide reciprocal replicators could have carried out this important role\nand contend that this is the simplest way to explain extant replication systems\nin a mathematically consistent way. We propose short nucleic-acid templates on\nwhich amino-acylated adapters assembled. Spatial localization drives peptide\nligation from activated precursors to generate phosphodiester-bond-catalytic\npeptides. Comprising autocatalytic protein and nucleic acid sequences, this\ndynamical system links and unifies several previous hypotheses and provides a\nplausible model for the emergence of DNA and the operational code.\n",
        "  We use a Cartesian grid to simulate the flow of gas in a barred Galactic\npotential and investigate the effects of varying the sound speed in the gas and\nthe resolution of the grid. For all sound speeds and resolutions, streamlines\nclosely follow closed orbits at large and small radii. At intermediate radii\nshocks arise and the streamlines shift between two families of closed orbits.\nThe point at which the shocks appear and the streamlines shift between orbit\nfamilies depends strongly on sound speed and resolution. For sufficiently large\nvalues of these two parameters, the transfer happens at the cusped orbit as\nhypothesised by Binney et al. over two decades ago. For sufficiently high\nresolutions the flow downstream of the shocks becomes unsteady. If this\nunsteadiness is physical, as appears to be the case, it provides a promising\nexplanation for the asymmetry in the observed distribution of CO.\n",
        "  There is a considerable body of work on sequence mining of Web Log Data. We\nare using One Pass frequent Episode discovery (or FED) algorithm, takes a\ndifferent approach than the traditional apriori class of pattern detection\nalgorithms. In this approach significant intervals for each Website are\ncomputed first (independently) and these interval used for detecting frequent\npatterns/Episode and then the Analysis is performed on Significant Intervals\nand frequent patterns That can be used to forecast the user's behavior using\nprevious trends and this can be also used for advertising purpose. This type of\napplications predicts the Website interest. In this approach, time-series data\nare folded over a periodicity (day, week, etc.) Which are used to form the\nInterval? Significant intervals are discovered from these time points that\nsatisfy the criteria of minimum confidence and maximum interval length\nspecified by the user.\n",
        "  Graph data model and graph databases are very popular in various areas such\nas bioinformatics, semantic web, and social networks. One specific problem in\nthe area is a path querying with constraints formulated in terms of formal\ngrammars. The query in this approach is written as grammar, and paths querying\nis graph parsing with respect to given grammar. There are several solutions to\nit, but how to provide structural representation of query result which is\npractical for answer processing and debugging is still an open problem. In this\npaper we propose a graph parsing technique which allows one to build such\nrepresentation with respect to given grammar in polynomial time and space for\narbitrary context-free grammar and graph. Proposed algorithm is based on\ngeneralized LL parsing algorithm, while previous solutions are based mostly on\nCYK or Earley algorithms, which reduces time complexity in some cases.\n",
        "  We study the condition for optimizing superconductivity in the iron pnictides\nfrom the lattice structure point of view. Studying the band structure of the\nhypothetical lattice structure of LaFeAsO, the hole Fermi surface multiplicity\nis found to be maximized around the Fe-As-Fe bond angle regime where the\narsenic atoms form a regular tetrahedron. Superconductivity is optimized within\nthis three hole Fermi surface regime, while the stoner factor of the\nantiferromagnetism has an overall tendency of increasing upon decreasing the\nbond angle. Combining also the effect of the varying the Fe-As bond length, we\nprovide a guiding principle for obtaining high $T_c$.\n",
        "  The use of GEANT4 code has increased in the medical field. There are various\nstudies to calculate the patient dose distributions with the GEANT4 code using\nthe imaging data. In present study, the Monte Carlo simulations based on the\nDICOM data were performed to calculate absorbed dose in the patient's body.\nVarious visualization tools were equipped in the GEANT4 code to display the\ndetector construction, however there are limitations to display the DICOM\nimages. In addition, it is difficult to display the dose distributions on the\nimaging data of the patient. Recently, gMocren code, volume visualization tool\nfor GEANT4 simulation, has been developed and used in volume visualization of\nimage files. In this study, the imaging data based absorbed dose distributions\nin patient were performed by using the gMocren code. The dosimetric evaluations\nwith TLD and film dosimetry methods were carried out to verify the calculation\nresults.\n",
        "  Nonlinear spatial encoding magnetic (SEM) fields have been studied to\ncomplement multichannel RF encoding and accelerate MRI scans. Published schemes\ninclude PatLoc, O-Space, Null Space, 4D-RIO, and others, but the large variety\nof possible approaches to exploiting nonlinear SEMs remains mostly unexplored.\nBefore, we have presented a new approach, Fast ROtary Nonlinear Spatial\nACquisition (FRONSAC) imaging, where the nonlinear fields provide a small\nrotating perturbation to standard linear trajectories. While FRONSAC encoding\ngreatly improves image quality, at the highest accelerations or weakest FRONSAC\nfields, some undersampling artifacts remain. However, the under-sampling\nartifacts that occur with FRONSAC encoding are relatively incoherent and well\nsuited to the compressed sensing (CS) reconstruction. CS provides a\nsparsity-promoting convex strategy to reconstruct images from highly\nundersampled datasets. The work presented here combines the benefits of FRONSAC\nand CS. Simulations illustrate that this combination can further improve image\nreconstruction with FRONSAC gradients of low amplitudes and frequencies.\n",
        "  We show that under certain conditions multiband superconductors with broken\ntime-reversal symmetry have a new vortex viscosity-generating mechanism which\nis different from that in conventional superconductors. It appears due to the\nexistence of mixed superfluid phase-density mode inside vortex core. This new\ncontribution is dominant near the time reversal symmetry breaking phase\ntransition. The results could be relevant for three band superconductor\n$Ba_{1-x}K_{x}Fe_2As_2$.\n",
        "  Abstraction without regret refers to the vision of using high-level\nprogramming languages for systems development without experiencing a negative\nimpact on performance. A database system designed according to this vision\noffers both increased productivity and high performance, instead of sacrificing\nthe former for the latter as is the case with existing, monolithic\nimplementations that are hard to maintain and extend. In this article, we\nrealize this vision in the domain of analytical query processing. We present\nLegoBase, a query engine written in the high-level language Scala. The key\ntechnique to regain efficiency is to apply generative programming: LegoBase\nperforms source-to-source compilation and optimizes the entire query engine by\nconverting the high-level Scala code to specialized, low-level C code. We show\nhow generative programming allows to easily implement a wide spectrum of\noptimizations, such as introducing data partitioning or switching from a row to\na column data layout, which are difficult to achieve with existing low-level\nquery compilers that handle only queries. We demonstrate that sufficiently\npowerful abstractions are essential for dealing with the complexity of the\noptimization effort, shielding developers from compiler internals and\ndecoupling individual optimizations from each other. We evaluate our approach\nwith the TPC-H benchmark and show that: (a) With all optimizations enabled,\nLegoBase significantly outperforms a commercial database and an existing query\ncompiler. (b) Programmers need to provide just a few hundred lines of\nhigh-level code for implementing the optimizations, instead of complicated\nlow-level code that is required by existing query compilation approaches. (c)\nThe compilation overhead is low compared to the overall execution time, thus\nmaking our approach usable in practice for compiling query engines.\n",
        "  We have performed an angle-resolved photoemission spectroscopy (ARPES) study\nof the iron-based superconductor PrFeAsO_{0.7} and examined the Fermi surfaces\nand band dispersions near the Fermi level. Heavily hole-doped electronic states\nhave been observed due to the polar nature of the cleaved surfaces.\nNevertheless, we have found that the ARPES spectra basically agree with band\ndispersions calculated in the local density approximation (LDA) if the\nbandwidth is reduced by a factor of ~2.5 and then the chemical potential is\nlowered by ~70 meV. Comparison with previous ARPES results on LaFePO reveals\nthat the energy positions of the d_{3z^2-r^2}- and d_{yz,zx}-derived bands are\nconsiderably different between the two materials, which we attribute to the\ndifferent pnictogen height as predicted by the LDA calculation.\n",
        "  Repeated interaction between individuals is the main mechanism for\nmaintaining cooperation in social dilemma situations. Variants of tit-for-tat\n(repeating the previous action of the opponent) and the win-stay lose-shift\nstrategy are known as strong competitors in iterated social dilemma games. On\nthe other hand, real repeated interaction generally allows plasticity (i.e.,\nlearning) of individuals based on the experience of the past. Although\nplasticity is relevant to various biological phenomena, its role in repeated\nsocial dilemma games is relatively unexplored. In particular, if\nexperience-based learning plays a key role in promotion and maintenance of\ncooperation, learners should evolve in the contest with nonlearners under\nselection pressure. By modeling players using a simple reinforcement learning\nmodel, we numerically show that learning enables the evolution of cooperation.\nWe also show that numerically estimated adaptive dynamics appositely predict\nthe outcome of evolutionary simulations. The analysis of the adaptive dynamics\nenables us to capture the obtained results as an affirmative example of the\nBaldwin effect, where learning accelerates the evolution to optimality.\n",
        "  Slot Filling (SF) aims to extract the values of certain types of attributes\n(or slots, such as person:cities\\_of\\_residence) for a given entity from a\nlarge collection of source documents. In this paper we propose an effective DNN\narchitecture for SF with the following new strategies: (1). Take a regularized\ndependency graph instead of a raw sentence as input to DNN, to compress the\nwide contexts between query and candidate filler; (2). Incorporate two\nattention mechanisms: local attention learned from query and candidate filler,\nand global attention learned from external knowledge bases, to guide the model\nto better select indicative contexts to determine slot type. Experiments show\nthat this framework outperforms state-of-the-art on both relation extraction\n(16\\% absolute F-score gain) and slot filling validation for each individual\nsystem (up to 8.5\\% absolute F-score gain).\n",
        "  A real projective orbifold has a radial end if a neighborhood of the end is\nfoliated by projective geodesics that develop into geodesics ending at a common\npoint. It has a totally geodesic end if the end can be completed to have the\ntotally geodesic boundary.\n  The purpose of this paper is to announce some partial results. A real\nprojective structure sometimes admits deformations to parameters of real\nprojective structures. We will prove a homeomorphism between the deformation\nspace of convex real projective structures on an orbifold $\\mathcal{O}$ with\nradial or totally geodesic ends with various conditions with the union of open\nsubspaces of strata of the corresponding subset of \\[ Hom(\\pi_{1}(\\mathcal{O}),\nPGL(n+1, \\mathbb{R}))/PGL(n+1, \\mathbb{R}).\\] Lastly, we will talk about the\nopenness and closedness of the properly (resp. strictly) convex real projective\nstructures on a class of orbifold with generalized admissible ends.\n",
        "  We propose an extension of the Cross Industry Standard Process for Data\nMining (CRISPDM) which addresses specific challenges of machine learning and\ndata mining for context and model reuse handling. This new general\ncontext-aware process model is mapped with CRISP-DM reference model proposing\nsome new or enhanced outputs.\n",
        "  The development of novel platforms and techniques for emerging \"Big Data\"\napplications requires the availability of real-life datasets for data-driven\nexperiments, which are however out of reach for academic research in most cases\nas they are typically proprietary. A possible solution is to use synthesized\ndatasets that reflect patterns of real ones in order to ensure high quality\nexperimental findings. A first step in this direction is to use inverse mining\ntechniques such as inverse frequent itemset mining (IFM) that consists of\ngenerating a transactional database satisfying given support constraints on the\nitemsets in an input set, that are typically the frequent ones. This paper\nintroduces an extension of IFM, called many-sorted IFM, where the schemes for\nthe datasets to be generated are those typical of Big Tables as required in\nemerging big data applications, e.g., social network analytics.\n",
        "  Many mathematical models of evolution assume that all individuals experience\nthe same environment. Here, we study the Moran process in heterogeneous\nenvironments. The population is of finite size with two competing types, which\nare exposed to a fixed number of environmental conditions. Reproductive rate is\ndetermined by both the type and the environment. We first calculate the\ncondition for selection to favor the mutant relative to the resident wild type.\nIn large populations, the mutant is favored if and only if the mutant's spatial\naverage reproductive rate exceeds that of the resident. But environmental\nheterogeneity elucidates an interesting asymmetry between the mutant and the\nresident. Specifically, mutant heterogeneity suppresses its fixation\nprobability; if this heterogeneity is strong enough, it can even completely\noffset the effects of selection (including in large populations). In contrast,\nresident heterogeneity has no effect on a mutant's fixation probability in\nlarge populations and can amplify it in small populations.\n",
        "  Context. Outflows and jets are the first signposts of ongoing star formation\nprocesses in any molecular cloud, yet their study in optical bands provides\nlimited results due to the large extinction present. Near-infrared unbiased\nwide-field observations in the H2 1-0 S(1) line at 2.122{\\mu}m alleviates the\nproblem, enabling us to detect more outflows and trace them closer to their\ndriving sources. Aims. As part of a large-scale multi-waveband study of ongoing\nstar formation in the Braid Nebula Star Formation region, we focus on a one\nsquare degree region that includes Lynds Dark Nebula 1003 and 1004. Our goal is\nto find all of the near-infrared outflows, uncover their driving sources and\nestimate their evolutionary phase. Methods. We use near-infrared wide-field\nobservations obtained with WFCAM on UKIRT, in conjunction with\npreviously-published optical and archival MM data, to search for outflows and\nidentify their driving sources; we subsequently use colour-colour analysis to\ndetermine the evolutionary phase of each source. Results. Within a one square\ndegree field we have identified 37 complex MHOs, most of which are new. After\ncombining our findings with other wide-field, multi-waveband observations of\nthe same region we were able to discern 28 outflows and at least 18 protostars.\nOur analysis suggests that these protostars are younger and/or more energetic\nthan those of the Taurus-Auriga region. The outflow data enable us to suggest\nconnection between outflow ejection and repetitive FU Ori outburst events. We\nalso find that star formation progresses from W to E across the investigated\nregion.\n",
        "  We live within an increasingly technological, information-laden environment\nfor the first time in human evolution.This subjects us, and will continue to\nsubject us in an accelerating fashion, to an unremitting exposure to meaningful\ninformation that requires action. Directly dependent upon this new environment\nare novel evolutionary pressures, which can modify existing resource allocation\nmechanisms and may eventually favor the survival of somatic cells,particularly\nneurons, at the expense of germ line cells. Here it is argued that persistent,\nstructured information-sharing in both virtual and real domains, leads to\nincreased biological complexity and functionality, which reflects upon human\nsurvival characteristics. Certain immortalisation mechanisms currently employed\nby germ cells may thus need to be downgraded in order to enable somatic cells\nto manage these new energy demands placed by our modern environment. Relevant\nconcepts from a variety of disciplines such as the evolution of complex\nadaptive systems, information theory, digital hyper-connectivity, and cell\nimmortalisation will be reviewed.\n",
        "  A series of SmFeAsO1-xFx samples were sintered in quartz tubes filled with\nair of different pressures. The effects of the sintering atmosphere on the\nsuperconductivity were systematically investigated. The SmFeAsO1-xFx system\nmaintains a transition temperature (Tc) near 50 K until the concentration of\noxygen in quartz tubes increases to a certain threshold, after which Tc\ndecreases dramatically. Fluorine losses, whether due to vaporization, reactions\nwith starting materials, and reactions with oxygen, proved to be detrimental to\nthe superconductivity of this material. The deleterious effects of the oxygen\nin the sintering atmosphere were also discussed in detail.\n",
        "  Recent observations have been discovering new ultra-faint dwarf galaxies as\nsmall as $\\sim20~{\\rm pc}$ in half-light radius and $\\sim3~{\\rm km~s^{-1}}$ in\nline-of-sight velocity dispersion. In these galaxies, dynamical friction on a\nstar against dark matter can be significant and alter their stellar density\ndistribution. The effect can strongly depend on a central density profile of\ndark matter, i.e. cusp or core. In this study, I perform computations using a\nclassical and a modern analytic formulae and $N$-body simulations to study how\ndynamical friction changes a stellar density profile and how different it is\nbetween cuspy and cored dark matter haloes. This study shows that, if a dark\nmatter halo has a cusp, dynamical friction can cause shrivelling instability\nwhich results in emergence of a stellar cusp in the central region\n$\\simeq2~{\\rm pc}$. On the other hand, if it has a constant-density core,\ndynamical friction is significantly weaker and does not generate a stellar cusp\neven if the galaxy has the same line-of-sight velocity dispersion. In such a\ncompact and low-mass galaxy, since the shrivelling instability by dynamical\nfriction is inevitable if it has a dark matter cusp, absence of a stellar cusp\nimplies that the galaxy has a dark-matter core. I expect that this could be\nused to diagnose a dark matter density profile in these compact ultra-faint\ndwarf galaxies.\n",
        "  Edge-labeled graphs are widely used to describe relationships between\nentities in a database. Given a query subgraph that represents an example of\nwhat the user is searching for, we study the problem of efficiently searching\nfor similar subgraphs in a large data graph, where the similarity is defined in\nterms of the well-known graph edit distance. We call these queries\n\"error-tolerant exemplar queries\" since matches are allowed despite small\nvariations in the graph structure and the labels. The problem in its general\ncase is computationally intractable, but efficient solutions are reachable for\nlabeled graphs under well-behaved distribution of the labels, commonly found in\nknowledge graphs. We propose two efficient exact algorithms, based on a\nfiltering-and-verification framework, for finding subgraphs in a large data\ngraph that are isomorphic to a query graph under some edit operations. Our\nfiltering scheme, which uses the neighbourhood structure around a node and the\npresence or absence of paths, significantly reduces the number of candidates\nthat are passed to the verification stage. Moreover, we analyze the costs of\nour algorithms and the conditions under which one algorithm is expected to\noutperform the other. Our analysis identifies some of the variables that affect\nthe cost, including the number and the selectivity of query edge labels and the\ndegree of nodes in the data graph, and characterizes their relationships. We\nempirically evaluate the effectiveness of our filtering schemes and queries,\nthe efficiency of our algorithms and the reliability of our cost models on real\ndatasets.\n",
        "  A detector using liquid Xenon (LXe) in the scintillation mode is studied for\nPositron Emission Tomography (PET) of small animals. Its specific design aims\nat taking full advantage of the Liquid Xenon scintillation properties. This\npaper reports on energy, time and spatial resolution capabilities of the first\nLXe prototype module equipped with a Position Sensitive Photo- Multiplier tube\n(PSPMT) operating in the VUV range (178 nm) and at 165 K. The experimental\nresults show that such a LXe PET configuration might be a promising solution\ninsensitive to any parallax effect.\n",
        "  A long genomic segment inherited by a pair of individuals from a single,\nrecent common ancestor is said to be identical-by-descent (IBD). Shared IBD\nsegments have numerous applications in genetics, from demographic inference to\nphasing, imputation, pedigree reconstruction, and disease mapping. Here, we\nprovide a theoretical analysis of IBD sharing under Markovian approximations of\nthe coalescent with recombination. We describe a general framework for the IBD\nprocess along the chromosome under the Markovian models (SMC/SMC'), as well as\nintroduce and justify a new model, which we term the renewal approximation,\nunder which lengths of successive segments are independent. Then, considering\nthe infinite-chromosome limit of the IBD process, we recover previous results\n(for SMC) and derive new results (for SMC') for the mean number of shared\nsegments longer than a cutoff and the fraction of the chromosome found in such\nsegments. We then use renewal theory to derive an expression (in Laplace space)\nfor the distribution of the number of shared segments and demonstrate\nimplications for demographic inference. We also compute (again, in Laplace\nspace) the distribution of the fraction of the chromosome in shared segments,\nfrom which we obtain explicit expressions for the first two moments. Finally,\nwe generalize all results to populations with a variable effective size.\n",
        "  For an oriented irreducible 3-manifold M with non-empty toroidal boundary, we\ndescribe how sutured Floer homology ($SFH$) can be used to determine all\nfibered classes in $H^1(M)$. Furthermore, we show that the $SFH$ of a balanced\nsutured manifold $(M,\\gamma)$ detects which classes in $H^1(M)$ admit a taut\ndepth one foliation such that the only compact leaves are the components of\n$R(\\gamma)$. The latter had been proved earlier by the first author under the\nextra assumption that $H_2(M)=0$. The main technical result is that we can\nobtain an extremal $\\text{Spin}^c$-structure $\\mathfrak{s}$ (i.e., one that is\nin a `corner' of the support of $SFH$) via a nice and taut sutured manifold\ndecomposition even when $H_2(M) \\neq 0$, assuming the corresponding group\n$SFH(M,\\gamma,\\mathfrak{s})$ has non-trivial Euler characteristic.\n",
        "  The large amount of data available in social media, forums and websites\nmotivates researches in several areas of Natural Language Processing, such as\nsentiment analysis. The popularity of the area due to its subjective and\nsemantic characteristics motivates research on novel methods and approaches for\nclassification. Hence, there is a high demand for datasets on different domains\nand different languages. This paper introduces TweetSentBR, a sentiment corpora\nfor Brazilian Portuguese manually annotated with 15.000 sentences on TV show\ndomain. The sentences were labeled in three classes (positive, neutral and\nnegative) by seven annotators, following literature guidelines for ensuring\nreliability on the annotation. We also ran baseline experiments on polarity\nclassification using three machine learning methods, reaching 80.99% on\nF-Measure and 82.06% on accuracy in binary classification, and 59.85% F-Measure\nand 64.62% on accuracy on three point classification.\n",
        "  We present sensitivity analysis for results of query executions in a\nrelational model of data extended by ordinal ranks. The underlying model of\ndata results from the ordinary Codd's model of data in which we consider\nordinal ranks of tuples in data tables expressing degrees to which tuples match\nqueries. In this setting, we show that ranks assigned to tuples are insensitive\nto small changes, i.e., small changes in the input data do not yield large\nchanges in the results of queries.\n",
        "  In many interesting cases the reconstruction of a correct phylogeny is\nblurred by high mutation rates and/or horizontal transfer events. As a\nconsequence a divergence arises between the true evolutionary distances and the\ndifferences between pairs of taxa as inferred from available data, making the\nphylogenetic reconstruction a challenging problem. Mathematically this\ndivergence translates in a loss of additivity of the actual distances between\ntaxa. In distance-based reconstruction methods, two properties of additive\ndistances were extensively exploited as antagonist criteria to drive phylogeny\nreconstruction: on the one hand a local property of quartets, i.e., sets of\nfour taxa in a tree, the four-points condition; on the other hand a recently\nproposed formula that allows to write the tree length as a function of the\ndistances between taxa, the Pauplin's formula. Here we introduce a new\nreconstruction scheme, that exploits in a unified framework both the\nfour-points condition and the Pauplin's formula. We propose, in particular, a\nnew general class of distance-based Stochastic Local Search algorithms, which\nreduces in a limit case to the minimization of the Pauplin's length. When\ntested on artificially generated phylogenies our Stochastic Big-Quartet\nSwapping algorithmic scheme significantly outperforms state-of-art\ndistance-based algorithms in cases of deviation from additivity due to high\nrate of back mutations. A significant improvement is also observed with respect\nto the state-of-art algorithms in case of high rate of horizontal transfer.\n",
        "  In this work, we present an approach based on combining string kernels and\nword embeddings for automatic essay scoring. String kernels capture the\nsimilarity among strings based on counting common character n-grams, which are\na low-level yet powerful type of feature, demonstrating state-of-the-art\nresults in various text classification tasks such as Arabic dialect\nidentification or native language identification. To our best knowledge, we are\nthe first to apply string kernels to automatically score essays. We are also\nthe first to combine them with a high-level semantic feature representation,\nnamely the bag-of-super-word-embeddings. We report the best performance on the\nAutomated Student Assessment Prize data set, in both in-domain and cross-domain\nsettings, surpassing recent state-of-the-art deep learning approaches.\n",
        "  The aim of this paper is to show how we can handle the Recognising Textual\nEntailment (RTE) task by using Description Logics (DLs). To do this, we propose\na representation of natural language semantics in DLs inspired by existing\nrepresentations in first-order logic. But our most significant contribution is\nthe definition of two novel inference tasks: A-Box saturation and subgraph\ndetection which are crucial for our approach to RTE.\n",
        "  We introduce a neural machine translation model that views the input and\noutput sentences as sequences of characters rather than words. Since word-level\ninformation provides a crucial source of bias, our input model composes\nrepresentations of character sequences into representations of words (as\ndetermined by whitespace boundaries), and then these are translated using a\njoint attention/translation model. In the target language, the translation is\nmodeled as a sequence of word vectors, but each word is generated one character\nat a time, conditional on the previous character generations in each word. As\nthe representation and generation of words is performed at the character level,\nour model is capable of interpreting and generating unseen word forms. A\nsecondary benefit of this approach is that it alleviates much of the challenges\nassociated with preprocessing/tokenization of the source and target languages.\nWe show that our model can achieve translation results that are on par with\nconventional word-based models.\n",
        "  Herbig-Haro objects are regions of shocked gas and dust which are produced\nwhen collimated outflows from a protostar interact with the surrounding dense\ngas. They have many similarities to supernova remnants which are interacting\nwith molecular clouds. 1720-MHz OH masers have been identified towards a number\nof interacting supernova remnants. Observations and models indicate that these\nmasers are shock excited and are produced behind C-type shocks. If conditions\nbehind the shock fronts of Herbig-Haro objects are similarly able to support\n1720-MHz OH masers they could be a useful diagnostic tool for star formation.\nWe therefore searched for 1720-MHz OH maser emission towards a sample of 97\nHerbig-Haro objects using the Green Bank radio telescope. We detected 1720-MHz\nOH lines in emission in 17 of them, but neither their spectral signature nor\nfollow-up observations with the Very Large Array showed any conclusive evidence\nof maser emission. We conclude that the emission detected from our single-dish\nobservations must be extended and most likely originates from thermal or\nquasi-thermal excitation processes. We also investigated the properties of\nHerbig-Haro shocks more closely and conclude that despite the overall\nsimilarities to supernova remnants, the conditions required for maser emission,\nin particular, a sufficient velocity-coherent column density, are not likely to\noccur in Herbig-Haro objects.\n",
        "  We have studied a EuFe2As2 single crystal by neutron diffraction under\nmagnetic fields up to 3.5 T and temperatures down to 2 K. A field induced spin\nreorientation is observed in the presence of a magnetic field along both the a\nand c axes, respectively. Above critical field, the ground state\nantiferromagnetic configuration of Eu$^{2+}$ moments transforms into a\nferromagnetic structure with moments along the applied field direction. The\nmagnetic phase diagram for Eu magnetic sublattice in EuFe2As2 is presented. A\nconsiderable strain ($\\sim$0.9%) is induced by the magnetic field, caused by\nthe realignment of the twinning structure. Furthermore, the realignment of the\ntwinning structure is found to be reversible with the rebound of magnetic\nfield, which suggested the existence of magnetic shape-memory effect. The Eu\nmoment ordering exhibits close relationship with the twinning structure. We\nargue that the Zeeman energy in combined with magnetic anisotropy energy is\nresponsible for the observed spin-lattice coupling.\n",
        "  Prior to the superconducting transition at Tc = 2.3 K, Mo3Sb7 undergoes a\nsymmetry-lowering, cubic-to-tetragonal structural transition at Ts = 53 K. We\nhave monitored the pressure dependence of these two transitions by measuring\nthe resistivity of Mo3Sb7 single crystals under various hydrostatic pressures\nup to 15 GPa. The application of external pressure enhances Tc but suppresses\nTs until Pc ~ 10 GPa, above which a pressure-induced first order structural\ntransition takes place and is manifested by the phase coexistence in the\npressure range 8 < P < 12 GPa. The cubic phase above 12 GPa is also found to be\nsuperconducting with a higher Tc =6 K that decreases slightly with further\nincreasing pressure. The variations with pressure of Tc and Ts satisfy the\nBilbro-McMillan equation, i.e. Tc^nTs^(1-n) = constant, thus suggesting the\ncompetition of superconductivity with the structural transition that has been\nproposed to be accompanied with a spin-gap formation at Ts. This scenario is\nsupported by our first-principles calculations which imply the plausible\nimportance of magnetism that competes with the superconductivity in Mo3Sb7.\n",
        "  We calculate the critical current of a\nsuperconductor/ferromagnetic/superconductor (S/FM/S) Josephson junction in\nwhich the FM layer has a conical magnetic structure composed of an in-plane\nrotating antiferromagnetic phase and an out-of-plane ferromagnetic component.\nIn view of the realistic electronic properties and magnetic structures that can\nbe formed when conical magnets such as Ho are grown with a polycrystalline\nstructure in thin-film form by methods such as direct current sputtering and\nevaporation, we have modeled this situation in the dirty limit with a large\nmagnetic coherence length ($\\xi_f$). This means that the electron mean free\npath is much smaller than the normalized spiral length $\\lambda/2\\pi$ which in\nturn is much smaller than $\\xi_f$ (with $\\lambda$ as the length a complete\nspiral makes along the growth direction of the FM). In this physically\nreasonable limit we have employed the linearized Usadel equations: we find that\nthe triplet correlations are short ranged and manifested in the critical\ncurrent as a rapid oscillation on the scale of $\\lambda/2\\pi$. These rapid\noscillations in the critical current are superimposed on a slower oscillation\nwhich is related to the singlet correlations. Both oscillations decay on the\nscale of $\\xi_f$. We derive an analytical solution and also describe a\ncomputational method for obtaining the critical current as a function of the\nconical magnetic layer thickness.\n",
        "  The current data explosion poses great challenges to the approximate\naggregation with an efficiency and accuracy. To address this problem, we\npropose a novel approach to calculate the aggregation answers with a high\naccuracy using only a small portion of the data. We introduce leverages to\nreflect individual differences in the samples from a statistical perspective.\nTwo kinds of estimators, the leverage-based estimator, and the sketch estimator\n(a \"rough picture\" of the aggregation answer), are in constraint relations and\niteratively improved according to the actual conditions until their difference\nis below a threshold. Due to the iteration mechanism and the leverages, our\napproach achieves a high accuracy. Moreover, some features, such as not\nrequiring recording the sampled data and easy to extend to various execution\nmodes (e.g., the online mode), make our approach well suited to deal with big\ndata. Experiments show that our approach has an extraordinary performance, and\nwhen compared with the uniform sampling, our approach can achieve high-quality\nanswers with only 1/3 of the same sample size.\n",
        "  We derive average flux corrections to the \\texttt{Model} magnitudes of the\nSloan Digital Sky Survey (SDSS) galaxies by stacking together mosaics of\nsimilar galaxies in bins of stellar mass and concentration. Extra flux is\ndetected in the outer low surface brightness part of the galaxies, leading to\ncorrections ranging from 0.05 to 0.32 mag for the highest stellar mass\ngalaxies. We apply these corrections to the MPA-JHU (Max-Planck Institute for\nAstrophysics - John Hopkins University) stellar masses for a complete sample of\nhalf a million galaxies from the SDSS survey to derive a corrected galaxy\nstellar mass function at $z=0.1$ in the stellar mass range\n$9.5<\\log(M_\\ast/M_\\odot)<12.0$. We find that the flux corrections and the use\nof the MPA-JHU stellar masses have a significant impact on the massive end of\nthe stellar mass function, making the slope significantly shallower than that\nestimated by Li \\& White (2009), but steeper than derived by Bernardi et al.\n(2013). This corresponds to a mean comoving stellar mass density of galaxies\nwith stellar masses $\\log(M_\\ast/M_\\odot) \\ge 11.0$ that is a factor of 3.36\nlarger than the estimate by Li \\& White (2009), but is 43\\% smaller than\nreported by Bernardi et al. (2013).\n",
        "  A twisted torus knot is a knot obtained from a torus knot by twisting\nadjacent strands by full twists. The twisted torus knots lie in $F$, the genus\n2 Heegaard surface for $S^3$. Primitive/primitive and primitive/Seifert knots\nlie in $F$ in a particular way. Dean gives sufficient conditions for the\nparameters of the twisted torus knots to ensure they are primitive/primitive or\nprimitive/Seifert. Using Dean's conditions, Doleshal shows that there are\ninfinitely many twisted torus knots that are fibered and that there are twisted\ntorus knots with distinct primitive/Seifert representatives with the same slope\nin $F$. In this paper, we extend Doleshal's results to show there is a four\nparameter family of positive twisted torus knots. Additionally, we provide new\nexamples of twisted torus knots with distinct representatives with the same\nsurface slope in $F$.\n",
        "  The effects of copper doping on the structural and superconducting phase\ntransitions of Ba(Ni$_{1-x}$Cu$_{x}$)$_{2}$As$_{2}$ were studied by examining\nthe resistivity, magnetic susceptibility, and specific heat. We found an abrupt\nincrease in the superconducting transition temperature $T_{\\rm c}$ from 0.6 K\nin the triclinic phase with less copper ($x$ $\\leq$ 0.16) to 2.5-3.2 K in the\ntetragonal phase with more copper ($x$ $>$ 0.16). The specific-heat data\nsuggested that doping-induced phonon softening was responsible for the enhanced\nsuperconductivity in the tetragonal phase. All of these observations exhibited\nstriking similarities to those observed in the phosphorus doping of\nBaNi$_{2}$(As$_{1-x}$P$_{x}$)$_{2}$ [K. Kudo et al., Phys. Rev. Lett. 109,\n097002 (2012).], which markedly contrast the behavior of phosphorus and copper\ndoping of the iron-based superconductor BaFe$_{2}$As$_{2}$.\n",
        "  We count orientable small covers over cubes. We also get estimates for\n$O_n/R_n$, where $O_n$ is the number of orientable small covers and $R_n$ is\nthe number of all small covers over an $n$-cube up to the Davis-Januszkiewicz\nequivalence.\n",
        "  The paper presents the design and development of English-Lithuanian-English\ndictionarylexicon tool and lexicon database management system for MT. The\nsystem is oriented to support two main requirements: to be open to the user and\nto describe much more attributes of speech parts as a regular dictionary that\nare required for the MT. Programming language Java and database management\nsystem MySql is used to implement the designing tool and lexicon database\nrespectively. This solution allows easily deploying this system in the\nInternet. The system is able to run on various OS such as: Windows, Linux, Mac\nand other OS where Java Virtual Machine is supported. Since the modern lexicon\ndatabase managing system is used, it is not a problem accessing the same\ndatabase for several users.\n",
        "  Background: Influenza A/H3N2 has been circulating in humans since 1968,\ncausing considerable morbidity and mortality. Although H3N2 incidence is highly\nseasonal, how such seasonality contributes to global phylogeographic migration\ndynamics has not yet been established.\n  Results: Incorporating seasonally varying migration rates improves the\nmodeling of migration. In our global model, windows of increased immigration\nmap to the seasonal timing of epidemic spread, while windows of increased\nemigration map to epidemic decline. Seasonal patterns also correlate with the\nprobability that local lineages go extinct and fail to contribute to long term\nviral evolution, as measured through the trunk of the phylogeny. However, the\nfraction of the trunk in each community was found to be better determined by\nits overall human population size\n  Conclusions: Seasonal migration and rapid turnover within regions is\nsustained by the invasion of 'fertile epidemic grounds' at the end of older\nepidemics. Thus, the current emphasis on connectivity, including air-travel,\nshould be complemented with a better understanding of the conditions and timing\nrequired for successful establishment.Models which account for migration\nseasonality will improve our understanding of the seasonal drivers of\ninfluenza,enhance epidemiological predictions, and ameliorate vaccine updating\nby identifying strains that not only escape immunity but also have the seasonal\nopportunity to establish and spread. Further work is also needed on additional\nconditions that contribute to the persistence and long term evolution of\ninfluenza within the human population,such as spatial heterogeneity with\nrespect to climate and seasonality\n",
        "  Given a link $L$, a representation $\\pi_1(S^{3}-L)\\to {\\rm SL}(2,\\mathbb{C})$\nis {\\it tracefree} if the image of each meridian has trace zero. We determine\nthe conjugacy classes of tracefree representations when $L$ is a Montesinos\nlink.\n",
        "  With the widespread use of GPS-enabled mobile devices, an unprecedented\namount of trajectory data is becoming available from various sources such as\nBikely, GPS-wayPoints, and Uber. The rise of innovative transportation services\nand recent break-throughs in autonomous vehicles will lead to the continued\ngrowth of trajectory data and related applications. Supporting these services\nin emerging platforms will require more efficient query processing in\ntrajectory databases. In this paper, we propose two new coverage queries for\ntrajectory databases: (i) k Maximizing Reverse Range Search on Trajectories\n(kMaxRRST); and (ii) a Maximum k Coverage Range Search on Trajectories\n(MaxkCovRST). We propose a novel index structure, the Trajectory Quadtree\n(TQ-tree) that utilizes a quadtree to hierarchically organize trajectories into\ndifferent quadtree nodes, and then applies a z-ordering to further organize the\ntrajectories by spatial locality inside each node. This structure is highly\neffective in pruning the trajectory search space, which is of independent\ninterest. By exploiting the TQ-tree data structure, we develop a\ndivide-and-conquer approach to compute the trajectory \"service value\", and a\nbest-first strategy to explore the trajectories using the appropriate upper\nbound on the service value to efficiently process a kMaxRRST query. Moreover,\nto solve the MaxkCovRST, which is a non-submodular NP-hard problem, we propose\na greedy approximation which also exploits the TQ-tree. We evaluate our\nalgorithms through an extensive experimental study on several real datasets,\nand demonstrate that our TQ-tree based algorithms outperform common baselines\nby two to three orders of magnitude.\n",
        "  Based on high-resolution spectra obtained with the MIKE spectrograph on the\nMagellan telescopes we present detailed elemental abundances for 20 red giant\nstars in the outer Galactic disk, located at Galactocentric distances between 9\nand 13 kpc. The outer disk sample is complemented with samples of red giants\nfrom the inner Galactic disk and the solar neighbourhood, analysed using\nidentical methods. For Galactocentric distances beyond 10 kpc, we only find\nchemical patterns associated with the local thin disk, even for stars far above\nthe Galactic plane. Our results show that the relative densities of the thick\nand thin disks are dramatically different from the solar neighbourhood, and we\ntherefore suggest that the radial scale length of the thick disk is much\nshorter than that of the thin disk. We make a first estimate of the thick disk\nscale-length of L_thick=2.0 kpc, assuming L_thin=3.8 kpc for the thin disk. We\nsuggest that radial migration may explain the lack of radial age, metallicity,\nand abundance gradients in the thick disk, possibly also explaining the link\nbetween the thick disk and the metal-poor bulge.\n",
        "  In this paper, we shall show a condition for that a chart is C-move\nequivalent to the product of two charts, the union of two charts $\\Gamma^*$ and\n$\\Gamma^{**}$ which are contained in disks $D^*$ and $D^{**}$ with $D^*\\cap\nD^{**}=\\emptyset$.\n",
        "  Magneto-optical imaging of a single crystal of CaFe1.94Co0.06As2, shows\nanomalous remnant magnetization within Meissner like regions of the\nsuperconductor. The unconventional shape of the local magnetization hysteresis\nloop suggests admixture of superconducting and magnetic fractions governing the\nresponse. Near the superconducting transition temperature, local magnetic field\nexceeds the applied field resulting in a diamagnetic to positive magnetization\ntransformation. The observed anomalies in the local magnetic field distribution\nare accompanied with enhanced bulk pinning in the CaFe1.94Co0.06As2 single\ncrystals. We propose our results suggest a coexistence of superconductivity and\nmagnetic correlations.\n",
        "  With the advent of Internet of Things (IoT) and Web 2.0 technologies, there\nhas been a tremendous growth in the amount of data generated. This chapter\nemphasizes on the need for big data, technological advancements, tools and\ntechniques being used to process big data are discussed. Technological\nimprovements and limitations of existing storage techniques are also presented.\nSince, the traditional technologies like Relational Database Management System\n(RDBMS) have their own limitations to handle big data, new technologies have\nbeen developed to handle them and to derive useful insights. This chapter\npresents an overview of big data analytics, its application, advantages, and\nlimitations. Few research issues and future directions are presented in this\nchapter.\n",
        "  The recent discovery of zero-determinant strategies for the iterated\nPrisoner's Dilemma sparked a surge of interest in the surprising fact that a\nplayer can exert unilateral control over iterated interactions. These\nremarkable strategies, however, are known to exist only in games in which\nplayers choose between two alternative actions such as \"cooperate\" and\n\"defect.\" Here we introduce a broader class of autocratic strategies by\nextending zero-determinant strategies to iterated games with more general\naction spaces. We use the continuous Donation Game as an example, which\nrepresents an instance of the Prisoner's Dilemma that intuitively extends to a\ncontinuous range of cooperation levels. Surprisingly, despite the fact that the\nopponent has infinitely many donation levels from which to choose, a player can\ndevise an autocratic strategy to enforce a linear relationship between his or\nher payoff and that of the opponent even when restricting his or her actions to\nmerely two discrete levels of cooperation. In particular, a player can use such\na strategy to extort an unfair share of the payoffs from the opponent.\nTherefore, although the action space of the continuous Donation Game dwarfs\nthat of the classical Prisoner's Dilemma, players can still devise relatively\nsimple autocratic and, in particular, extortionate strategies.\n",
        "  Person knowledge extraction is the foundation of the Tibetan knowledge graph\nconstruction, which provides support for Tibetan question answering system,\ninformation retrieval, information extraction and other researches, and\npromotes national unity and social stability. This paper proposes a SVM and\ntemplate based approach to Tibetan person knowledge extraction. Through\nconstructing the training corpus, we build the templates based the shallow\nparsing analysis of Tibetan syntactic, semantic features and verbs. Using the\ntraining corpus, we design a hierarchical SVM classifier to realize the entity\nknowledge extraction. Finally, experimental results prove the method has\ngreater improvement in Tibetan person knowledge extraction.\n",
        "  Context: Despite the low elemental deuterium abundance in the Galaxy,\nenhanced molecular D/H ratios have been found in the environments of low-mass\nstar-forming regions and, in particular, the Class 0 protostar IRAS 16293-2422.\nAims: The key program Chemical HErschel Surveys of Star forming regions (CHESS)\naims at studying the molecular complexity of the interstellar medium. The high\nsensitivity and spectral resolution of the Herschel/HIFI instrument provide a\nunique opportunity to observe the fundamental 1_{1,1}-0_{0,0} transition of\northo-D2O at 607 GHz and the higher energy 2_{1,2}-1_{0,1} transition of\npara-D2O at 898 GHz, both of which are inaccessible from the ground. Methods:\nThe ortho-D2O transition at 607 GHz was previously detected. We present in this\npaper the first tentative detection for the para-D2O transition at 898 GHz. The\nspherical Monte Carlo radiative transfer code RATRAN was used to reproduce the\nobserved line profiles of D2O with the same method that was used to reproduce\nthe HDO and H2-18O line profiles in IRAS 16293-2422. Results: As for HDO, the\nabsorption component seen on the D2O lines can only be reproduced by adding an\nexternal absorbing layer, possibly created by the photodesorption of the ices\nat the edges of the molecular cloud. The D2O column density is found to be\nabout 2.5e12 cm^{-2} in this added layer, leading to a D2O/H2O ratio of about\n0.5%. At a 3 sigma uncertainty, upper limits of 0.03% and 0.2% are obtained for\nthis ratio in the hot corino and the colder envelope of IRAS 16293-2422,\nrespectively. Conclusions: The deuterium fractionation derived in our study\nsuggests that the ices present in IRAS 16293-2422 formed on warm dust grains\n(~15-20 K) in dense (~1e4-5e4 cm^{-3}) translucent clouds. These results allow\nus to address the earliest phases of star formation and the conditions in which\nices form.\n",
        "  Distance-based approaches in phylogenetics such as Neighbor-Joining are a\nfast and popular approach for building trees. These methods take pairs of\nsequences from them construct a value that, in expectation, is additive under a\nstochastic model of site substitution. Most models assume a distribution of\nrates across sites, often based on a gamma distribution. Provided the (shape)\nparameter of this distribution is known, the method can correctly reconstruct\nthe tree. However, if the shape parameter is not known then we show that\ntopologically different trees, with different shape parameters and associated\npositive branch lengths, can lead to exactly matching distributions on pairwise\nsite patterns between all pairs of taxa. Thus, one could not distinguish\nbetween the two trees using pairs of sequences without some prior knowledge of\nthe shape parameter. More surprisingly, this can happen for {\\em any} choice of\ndistinct shape parameters on the two trees, and thus the result is not peculiar\nto a particular or contrived selection of the shape parameters. On a positive\nnote, we point out known conditions where identifiability can be restored\n(namely, when the branch lengths are clocklike, or if methods such as maximum\nlikelihood are used).\n",
        "  In this paper, we investigate protection strategies of sensitive body anatomy\nagainst the irradiation to the cancerous moving tumors in intensity modulated\nradiation therapy. Inspired by optimization techniques developed in statistical\nphysics and dynamical systems, we deploy a method based on variational\nprinciples and formulate an efficient genetic algorithm which enable us to\nsearch for global minima in a complex landscape of irradiation dose delivered\nto the radiosensitive organs at risk. We take advantage of the internal motion\nof body anatomy during radiation therapy to reduce the unintentional delivery\nof the radiation to sensitive organs. We show that the accurate optimization of\nthe control parameters, compare to the conventional IMRT and widely used\ndelivery based on static anatomy assumption, leads to a significant reduction\nof the dose delivered to the organs at risk.\n",
        "  In order to compare magnetic and non-magnetic pinning we have nanostructured\ntwo superconducting films with regular arrays of pinning centers: Cu\n(non-magnetic) dots in one case, and Py (magnetic) dots in the other. For low\napplied magnetic fields, when all the vortices are pinned in the artificial\ninclusions, magnetic dots prove to be better pinning centers, as has been\ngenerally accepted. Unexpectedly, when the magnetic field is increased and\ninterstitial vortices appear, the results are very different: we show how the\nstray field generated by the magnetic dots can produce an effective reduction\nof the penetration length. This results in strong consequences in the transport\nproperties, which, depending on the dot separation, can lead to an enhancement\nor worsening of the transport characteristics. Therefore, the election of the\nmagnetic or non-magnetic character of the pinning sites for an effective\nreduction of dissipation will depend on the range of the applied magnetic\nfield.\n",
        "  Computers play a crucial role in modern ancestry management, they are used to\ncollect, store, analyze, sort and display genealogical data. However, current\napplications do not take into account the kinship structure of a natural\nlanguage.\n  In this paper we propose a new domain-specific language KISP which is based\non a formalization of English kinship system, for accessing and querying\ntraditional genealogical trees. KISP is a dynamically typed LISP-like\nprogramming language with a rich set of features, such as kinship term\nreduction and temporal information expression.\n  Our solution provides a user with a coherent genealogical framework that\nallows for a natural navigation over any traditional family tree.\n",
        "  Traditionally the Costa Rican historians and genealogists have interpreted\nthat the Spanish ruling elite emerged after the conquest was exclusively of\nEuropean origin. On the other side, recent technological advances in Genetics\ngive us the opportunity to approach the study of pedigrees from a new\nperspective, examining alive people and simultaneously collating the historical\ninformation of their ancestors. In this paper, a complete matrilineal genealogy\nwas reconstructed from nowadays \"white\" Costa Ricans to their ancestors in the\nearly Colonial society (XVI century). It was compared the correlation between\nethnic affiliations deduced from historical records with the genetic\ninheritance from maternal lineages. The MtDNA lineage observed corresponds to a\nNative American ancestry. These results show that some Amerindian gene flow\ninto the Spanish group must have occurred since the first generation of\nColonial society, a finding that contrasts with the prevailing ideas that the\nSpanish elite avoided the intermarriage with other ethnic groups. Examples like\nthis one confirm that miscegenation began early in Costa Rica. So, those who\nconsidered themselves \"Spaniards\" in the late colonial era, were actually\nbiologically mestizos. It is widely accepted, that the general Costa Rican\npopulation is the result of an admixture process between Europeans, Amerindians\nand Africans.\n",
        "  The Milky Way bulge has a boxy/peanut morphology and an X-shaped structure.\nThis X-shape has been revealed by the `split in the red clump' from star counts\nalong the line of sight toward the bulge, measured from photometric surveys.\nThis boxy, X-shaped bulge morphology is not unique to the Milky Way and such\nbulges are observed in other barred spiral galaxies. N-body simulations show\nthat boxy and X-shaped bulges are formed from the disk via dynamical\ninstabilities. It has also been proposed that the Milky Way bulge is not\nX-shaped, but rather, the apparent split in the red clump stars is a\nconsequence of different stellar populations, in an old classical spheroidal\nbulge. We present a WISE image of the Milky Way bulge, produced by downsampling\nthe publicly available \"unWISE\" coadds. The WISE image of the Milky Way bulge\nshows that the X-shaped nature of the Milky Way bulge is self-evident and\nirrefutable. The X-shape morphology of the bulge in itself and the fraction of\nbulge stars that comprise orbits within this structure has important\nimplications for the formation history of the Milky Way, and, given the\nubiquity of boxy X-shaped bulges, spiral galaxies in general.\n",
        "  We introduce invariants of Hurwitz equivalence classes with respect to\narbitrary group $G$. The invariants are constructed from any right $G$-modules\n$M$ and any $G$-invariant bilinear function on $M$, and are of bilinear forms.\nFor instance, when $G$ is the mapping class group of the closed surface,\n$\\mathcal{M}_g$, we get an invariant of 4-dimensional Lefschetz fibrations over\nthe 2-sphere. Moreover, the construction is applicable for the quantum\nrepresentations of $\\mathcal{M}_g $ derived from Chern-Simons field theory. We\ncompute the associated invariants in some cases, and find infinitely many\nLefschetz fibrations which have the same Seiberg-Witten invariant and are\nhomeomorphic but not mutually isomorphic as fibrations.\n",
        "  Most modern data stores tend to be distributed, to enable the scaling of the\ndata across multiple instances of commodity hardware. Although this ensures a\nnear unlimited potential for storage, the data itself is not always ideally\npartitioned, and the cost of a network round-trip may cause a degradation of\nend-user experience with respect to response latency. The problem being solved\nis bringing the data objects closer to the frequent sources of requests using a\ndynamic repartitioning algorithm. This is important if the objective is to\nmitigate the overhead of network latency, and especially so if the partitions\nare widely geo-distributed. The intention is to bring these features to an\nexisting distributed key-value store product, Redis.\n",
        "  Inverse dynamic analysis using musculoskeletal modeling is a powerful tool,\nwhich is utilized in a range of applications to estimate forces in ligaments,\nmuscles, and joints, non-invasively. To date, the conventional input used in\nthis analysis is derived from optical motion capture (OMC) and force plate (FP)\nsystems, which restrict the application of musculoskeletal models to gait\nlaboratories. To address this problem, we propose a musculoskeletal model,\ncapable of estimating the internal forces based solely on inertial motion\ncapture (IMC) input and a ground reaction force and moment (GRF&M) prediction\nmethod. We validated the joint angle and kinetic estimates of the lower limbs\nagainst an equally constructed musculoskeletal model driven by OMC and FP\nsystem. The sagittal plane joint angles of ankle, knee, and hip presented\nexcellent Pearson correlations (\\rho = 0.95, 0.99, and 0.99, respectively) and\nroot-mean-squared differences (RMSD) of 4.1 $\\pm$ 1.3$\\circ$, 4.4 $\\pm$\n2.0$\\circ$, and 5.7 $\\pm$ 2.1$\\circ$, respectively. The GRF&M predicted using\nIMC input were found to have excellent correlations for three components\n(vertical:\\rho = 0.97, RMSD=9.3 $\\pm$ 3.0 %BW, anteroposterior: \\rho = 0.91,\nRMSD=5.5 $\\pm$ 1.2 %BW, sagittal: \\rho = 0.91, RMSD=1.6 $\\pm$ 0.6 %BW*BH), and\nstrong correlations for mediolateral (\\rho = 0.80, RMSD=2.1 $\\pm$ 0.6%BW ) and\ntransverse (\\rho = 0.82, RMSD=0.2 $\\pm$ 0.1 %BW*BH). The proposed IMC-based\nmethod removes the complexity and space-restrictions of OMC and FP systems and\ncould enable applications of musculoskeletal models in either monitoring\npatients during their daily lives or in wider clinical practice.\n",
        "  The dynamic analysis of erythrocyte deformability is used as an important\nmeans for early diagnosis of blood diseases and blood rheology. Yet no\neffective method is available in terms of three-dimensional reconstruction of\nerythrocytes in a capillary. In this study, ultrathin serial sections of\nskeletal muscle tissue are obtained from the ultramicrotome, the tomographic\nimages of an erythrocyte in a capillary are captured by the transmission\nelectron microscope, and then a method to position and restore is devised to\ndemonstrate the physiological relationship between two adjacent tomographic\nimages of an erythrocyte. Both the modeling and the physical verification\nreveal that this method is effective, which means that it can be used to make\nthree-dimensional reconstruction of an erythrocyte in a capillary. An example\nof reconstructed deformation of erythrocyte based on the serial ultrathin\nsections is shown at the end of this paper.\n",
        "  Many theoretical and experimental studies suggest that range expansions can\nhave severe consequences for the gene pool of the expanding population. Due to\nstrongly enhanced genetic drift at the advancing frontier, neutral and weakly\ndeleterious mutations can reach large frequencies in the newly colonized\nregions, as if they were surfing the front of the range expansion. These\nfindings raise the question of how frequently beneficial mutations successfully\nsurf at shifting range margins, thereby promoting adaptation towards a\nrange-expansion phenotype. Here, we use individual-based simulations to study\nthe surfing statistics of recurrent beneficial mutations on wave-like range\nexpansions in linear habitats. We show that the rate of surfing depends on two\nstrongly antagonistic factors, the probability of surfing given the spatial\nlocation of a novel mutation and the rate of occurrence of mutations at that\nlocation. The surfing probability strongly increases towards the tip of the\nwave. Novel mutations are unlikely to surf unless they enjoy a spatial head\nstart compared to the bulk of the population. The needed head start is shown to\nbe proportional to the inverse fitness of the mutant type, and only weakly\ndependent on the carrying capacity. The second factor is the mutation\noccurrence which strongly decreases towards the tip of the wave. Thus, most\nsuccessful mutations arise at an intermediate position in the front of the\nwave. We present an analytic theory for the tradeoff between these factors that\nallows to predict how frequently substitutions by beneficial mutations occur at\ninvasion fronts. We find that small amounts of genetic drift increase the\nfixation rate of beneficial mutations at the advancing front, and thus could be\nimportant for adaptation during species invasions.\n",
        "  Most biological systems are formed by component parts that to some degree are\ninter-related. Groups of parts that are more associated among themselves and\nare relatively autonomous from others are called modules. One of the\nconsequences of modularity is that biological systems usually present an\nunequal distribution of the genetic variation among variables. Estimating the\ncovariance matrix that describes these systems is a difficult problem due to a\nnumber of factors such as poor sample sizes and measurement errors. We show\nthat this problem will be exacerbated whenever matrix inversion is required, as\nin directional selection reconstruction analysis. We explore the consequences\nof varying degrees of modularity and signal-to-noise ratio on selection\nreconstruction. We then present and test the efficiency of available methods\nfor controlling noise in matrix estimates. In our simulations, controlling\nmatrices for noise vastly improves the reconstruction of selection gradients.\nWe also perform an analysis of selection gradients reconstruction over a New\nWorld Monkeys skull database in order to illustrate the impact of noise on such\nanalyses. Noise- controlled estimates render far more plausible interpretations\nthat are in full agreement with previous results.\n",
        "  The Twisted Stacked-Tape Cable (TSTC) is one of the major high temperature\nsuperconductor cable concepts combining scalability, ease of fabrication and\nhigh current density making it a possible candidate as conductor for large\nscale magnets. To simulate the boundary conditions of such a magnets as well as\nthe temperature dependence of Twisted Stacked-Tape Cables a 1.16 m long sample\nconsisting of 40, 4 mm wide SuperPower REBCO tapes is characterized using the\n\"FBI\" (force - field - current) superconductor test facility of the Institute\nfor Technical Physics (ITEP) of the Karlsruhe Institute of Technology (KIT). In\na first step, the magnetic background field is cycled while measuring the\ncurrent carrying capabilities to determine the impact of Lorentz forces on the\nTSTC sample performance. In the first field cycle, the critical current of the\nTSTC sample is tested up to 12 T. A significant Lorentz force of up to 65.6\nkN/m at the maximal magnetic background field of 12 T result in a 11.8 %\nirreversible degradation of the current carrying capabilities. The degradation\nsaturates (critical cable current of 5.46 kA at 4.2 K and 12 T background\nfield) and does not increase in following field cycles. In a second step, the\nsample is characterized at different background fields (4-12 T) and surface\ntemperatures (4.2-37.8 K) utilizing the variable temperature insert of the\n\"FBI\" test facility. In a third step, the performance along the length of the\nsample is determined at 77 K, self-field. A 15 % degradation is obtained for\nthe central part of the sample which was within the high field region of the\nmagnet during the in-field measurements.\n",
        "  The purpose of this work is to test the ability of a new class of passive\nelectromagnetic device to increase the penetration depth of phased arrays of\nsurface coils for magnetic resonance (MR) imaging systems. This new device is\nbased on the emerging technology of metamaterials and behaves like a lens for\nthe radiofrequency magnetic fields. The presented device was tested in several\n1.5-T MR systems from different companies in combination with different phased\narrays. One of the authors was enrolled as volunteer for the experiments. In\nthese experiments his knees were imaged by using a dual phased array. The\ndevice was placed between the knees to check that the penetration depth of the\ncoils was improved by this passive device. In all the experiments the presented\ndevice was successfully tested and it was checked that the knees of the\nvolunteer can be imaged at deeper distances and that the signal-to-noise-ratio\n(SNR) in the obtained MR images was improved by the presence of the lens. The\npresented device has proven to increase the penetration depth of MR phased\narrays of surface coils. The lens was tested by means of the MR imaging of the\nknees but it can be used to image any pair of joints simultaneously by placing\nit between the joints. The positive results suggest the possibility of using\nthe lens to image the female breast. This would make it possible to increase\nthe SNR without higher fields, thus fulfilling the safety regulations governing\nthe standard absorption rate (SAR).\n",
        "  The binding constant of copper(II) ions to C-PC were determined at different\nionic strengths from binding isotherms by equilibrium dialysis and flame atomic\nabsorption spectroscopy. Fluorescence and absorbtion spectroscopy provides\ninsight of metal-C-phycocyanin interactions. Fluorescence measurements\ndemonstrate C-PC quenching of heavy metal ions emission intensities.\nStern-Volmer quenching constants were obtained from the linear quenching plots.\nBlue shifts in the fluorescence spectra were observed during metal binding to\nC-PC. It was shown, that between bound metal ions in C-PC there exists positive\ncooperativity.\n",
        "  Given a finite graph of relatively hyperbolic groups with its fundamental\ngroup relatively hyperbolic and edge groups quasi-isometrically embedded and\nrelatively quasiconvex in vertex groups, we prove that vertex groups are\nrelatively quasiconvex if and only if all the vertex groups have finite\nrelative height in the fundamental group.\n",
        "  Indian free ranging dogs live in a carbohydrate rich environment as\nscavengers in and around human settlements. They rarely hunt and consequently\ndo not encounter rich sources of protein. Instead they have adapted to a diet\nof primarily carbohydrates. As descendants of the exclusively carnivorous\nwolves, they are subjected to the evolutionary load of a physiological demand\nfor proteins. To meet their protein needs they resort to a thumb rule, if it\nsmells like meat, eat it. Pups face high competition from group and non group\nmembers and are in a phase of rapid growth with high protein demands. Following\nthe thumb rule, then they can acquire more protein at the cost of increased\ncompetition and reduced supplementary non protein nutrition. However, if the\nmother supplements their diet with protein rich regurgitates and milk, then the\npups can benefit by being generalists. Using a choice test in the field we show\nthat while adults have a clear preference for meat, pups have no such\npreference, and they even eat degraded protein eagerly. Thus the thumb rule\nused by adult dogs for efficient scavenging is not innate, and needs to be\nlearned. The thumb rule might be acquired by cultural transmission, through\nexposure to meat in the regurgitate of the mother, or while accompanying her on\nforaging trips.\n",
        "  Undoped cuprates have long been considered to be antiferromagnetic\ninsulators. In this article, however, we report that superconductivity is\nachieved in undoped T'-RE2CuO4 (RE = Pr, Nd, Sm, Eu, and Gd). Our discovery was\nperformed by using metal-organic decomposition (MOD), an inexpensive and\neasy-to-implement thin-film process. The keys to prepare the superconducting\nfilms are firing with low partial-pressure of oxygen and reduction at low\ntemperatures. The highest Tc of undoped T'-RE2CuO4 is over 30 K, substantially\nhigher than \"electron-doped\" analogs. Remarkably, Gd2CuO4, even the derivatives\nof which have not shown superconductivity so far, gets superconducting with\nTconset as high as ~ 20 K. The implication of our discovery is briefly\ndiscussed.\n",
        "  The populations of pitvipers from south western Costa Rica, have\ntraditionally been identified as Bothriechis schlegelii (Berthold). However, in\n1954 E. H. Taylor described one specimen from the area as a new subspecies, B.\nschlegelii supraciliaris. Werman returned supraciliaris to synonymy with\nschlegelii four decades later. However, morphometry and color pattern in a SW\nCosta Rica population (25 specimens) differ from those of specimens (N=57) from\nother parts of Costa Rica and from descriptions of South American specimens.\nHere the epithet Bothriechis schlegelii supraciliaris Taylor 1954. is\nreestablished as a valid taxon and elevated to specific rank as B.\nsupraciliaris stat.nov. It is closely related to B. schlegelii from which it\ndiffers by its color patterns based on a uniform ground color with polymorphic\ndorsal designs, and its lower counts of ventral and caudal scales.\n",
        "  In this paper we will study properties of twisted Alexander polynomials of\nknots corresponding to metabelian representations. In particular we answer a\nquestion of Wada about the twisted Alexander polynomial associated to the\ntensor product of two representations, and we settle several conjectures of\nHirasawa and Murasugi.\n",
        "  Data describing the growth of the world population in the past 12,000 years\nare analysed. It is shown that, if unchecked, population does not increase\nexponentially but hyperbolically. This analysis reveals three\napproximately-determined episodes of hyperbolic growth: 10,000-500 BC, AD\n500-1200 and AD 1400-1950, representing a total of about 89% of the past 12,000\nyears. It also reveals three demographic transitions: 500 BC-AD 500, AD\n1200-1400 and AD 1950-present, representing the remaining 11% of the past\n12,000 years. The first two transitions were between sustained hyperbolic\ntrajectories. The current transition is to an unknown trajectory. There was\nnever any form of dramatic transition from stagnation to growth, described\noften as a takeoff, because there was no stagnation in the growth of the world\npopulation. Correct understanding of the historical growth of human population\nis essential in the correct interpretation of the historical growth of income\nper capita.\n",
        "  We prove a version of Poincar\\'e's polyhedron theorem whose requirements are\nas local as possible. New techniques such as the use of discrete groupoids of\nisometries are introduced. The theorem may have a wide range of applications\nand can be generalized to the case of higher dimension and other geometric\nstructures. It is planned as a first step in a program of constructing compact\n$\\mathbb C$-surfaces of general type satisfying $c_1^2=3c_2$.\n",
        "  We consider two types of magnetic Josephson junctions~(JJ). They are formed\nby two singlet superconductors~S and magnetic layers between them so that the\nJJ is a heterostructure of the S$_{\\text{m}}$/n/S$_{\\text{m}}$ type,\nwhere~S$_{\\text{m}}$ includes two magnetic layers with non-collinear\nmagnetization vectors. One layer is represented by a weak ferromagnet and\nanother one---the spin filter---is either conducting strong ferromagnet\n(nematic or N\\nobreakdash-type JJ) or magnetic tunnel barrier with\nspin-dependent transparency (magnetic or M\\nobreakdash-type JJ). Due to spin\nfiltering only fully polarized triplet component penetrates the normal n~wire\nand provides the Josephson coupling between the superconductors~S. Although\nboth filters let to pass triplet Cooper pairs with total spin~$\\mathbf{S}$\nparallel to the filter axes, the behavior of nematic and magnetic JJs is\ncompletely different. Whereas in the nematic case the charge and spin\ncurrents,~$I_{\\text{Q}}$ and~$I_{\\text{sp}}$, do not depend on mutual\norientation of the filter axes, both currents vanish in magnetic~JJ in case of\nantiparallel filter axes, and change sign under reversing the filter direction.\nThe obtained expressions for~$I_{\\text{Q}}$ and~$I_{\\text{sp}}$ show clearly a\nduality between the superconducting phase~$\\varphi$ and the angle~$\\alpha$\nbetween the exchange fields in the weak magnetic layers.\n",
        "  We consider young distant stars from the Gaia TGAS catalog. These are 250\nclassical Cepheids and 244 OB stars located at distances up to 4 kpc from the\nSun. These stars are used to determine the Galactic rotation parameters using\nboth trigonometric parallaxes and proper motions of the TGAS stars. In this\ncase the considered stars have relative parallax errors less than 200%.\nFollowing the well-known statistical approach, we assume that the kinematic\nparameters found from the line-of-sight velocities $V_r$ are less dependent on\nerrors of distances than the found from the velocity components $V_l$. From\nvalues of the first derivative of the Galactic rotation angular velocity\n$\\Omega{'}_0$, found from the analysis of velocities $V_r$ and $V_l$\nseparately, the scale factor of distances is determined. We found that from the\nsample of Cepheids the scale of distances of the TGAS should be reduced by 3%,\nand from the sample of OB stars, on the contrary, the scale should be increased\nby 9%.\n",
        "  The ability to extract public opinion from web portals such as review sites,\nsocial networks and blogs will enable companies and individuals to form a view,\nan attitude and make decisions without having to do lengthy and costly\nresearches and surveys. In this paper machine learning techniques are used for\ndetermining the polarity of forum posts on kajgana which are written in\nMacedonian language. The posts are classified as being positive, negative or\nneutral. We test different feature metrics and classifiers and provide detailed\nevaluation of their participation in improving the overall performance on a\nmanually generated dataset. By achieving 92% accuracy, we show that the\nperformance of systems for automated opinion mining is comparable to a human\nevaluator, thus making it a viable option for text data analysis. Finally, we\npresent a few statistics derived from the forum posts using the developed\nsystem.\n",
        "  We conducted astrometric VLBI observations of water-vapor maser emission in\nthe massive star forming region IRAS 21379+5106 to measure the annual parallax\nand proper motion, using VERA. The annual parallax was measured to be $0.262\n\\pm 0.031$ mas corresponding to a trigonometric distance of\n$3.82^{+0.51}_{-0.41}$ kpc. The proper motion was $(\\mu_\\alpha\\cos{\\delta},\n\\mu_\\delta)=(-2.74 \\pm 0.08, -2.87 \\pm 0.18)$ mas yr$^{-1}$. Using this result,\nthe Galactic rotational velocity was estimated to be $V_\\theta=218\\pm 19$ km\ns$^{-1}$ at the Galactocentric distance $R=9.22\\pm0.43$ kpc, when we adopted\nthe Galactic constants $R_0=8.05\\pm 0.45$ kpc and $V_0=238\\pm 14$ km s$^{-1}$.\nWith newly determined distance, {the bolometric luminosity of the central young\nstellar object was re-evaluated to $(2.15\\pm 0.54)\\times 10^3 L_\\odot$, which\ncorresponds to spectral type of} B2--B3. Maser features were found to be\ndistributed along a straight line from south-west to north-east. In addition, a\nvector map of the internal motions constructed from the residual proper motions\nimplies that maser features trace a bipolar flow and that it cannot be\nexplained by simple ballistic motion.\n",
        "  Goal-Oriented (GO) Dialogue Systems, colloquially known as goal oriented\nchatbots, help users achieve a predefined goal (e.g. book a movie ticket)\nwithin a closed domain. A first step is to understand the user's goal by using\nnatural language understanding techniques. Once the goal is known, the bot must\nmanage a dialogue to achieve that goal, which is conducted with respect to a\nlearnt policy. The success of the dialogue system depends on the quality of the\npolicy, which is in turn reliant on the availability of high-quality training\ndata for the policy learning method, for instance Deep Reinforcement Learning.\n  Due to the domain specificity, the amount of available data is typically too\nlow to allow the training of good dialogue policies. In this master thesis we\nintroduce a transfer learning method to mitigate the effects of the low\nin-domain data availability. Our transfer learning based approach improves the\nbot's success rate by $20\\%$ in relative terms for distant domains and we more\nthan double it for close domains, compared to the model without transfer\nlearning. Moreover, the transfer learning chatbots learn the policy up to 5 to\n10 times faster. Finally, as the transfer learning approach is complementary to\nadditional processing such as warm-starting, we show that their joint\napplication gives the best outcomes.\n",
        "  Using the growing volumes of vehicle trajectory data, it becomes increasingly\npossible to capture time-varying and uncertain travel costs in a road network,\nincluding travel time and fuel consumption. The current paradigm represents a\nroad network as a graph, assigns weights to the graph's edges by fragmenting\ntrajectories into small pieces that fit the underlying edges, and then applies\na routing algorithm to the resulting graph. We propose a new paradigm that\ntargets more accurate and more efficient estimation of the costs of paths by\nassociating weights with sub-paths in the road network. The paper provides a\nsolution to a foundational problem in this paradigm, namely that of computing\nthe time-varying cost distribution of a path.\n  The solution consists of several steps. We first learn a set of random\nvariables that capture the joint distributions of sub-paths that are covered by\nsufficient trajectories. Then, given a departure time and a path, we select an\noptimal subset of learned random variables such that the random variables'\ncorresponding paths together cover the path. This enables accurate joint\ndistribution estimation of the path, and by transferring the joint distribution\ninto a marginal distribution, the travel cost distribution of the path is\nobtained. The use of multiple learned random variables contends with data\nsparseness, the use of multi-dimensional histograms enables compact\nrepresentation of arbitrary joint distributions that fully capture the travel\ncost dependencies among the edges in paths. Empirical studies with substantial\ntrajectory data from two different cities offer insight into the design\nproperties of the proposed solution and suggest that the solution is effective\nin real-world settings.\n",
        "  Community ecology has traditionally relied on the competitive exclusion\nprinciple, a piece of common wisdom in conceptual frameworks developed to\ndescribe species assemblages. Key concepts in community ecology, such as\nlimiting similarity and niche partitioning, are based on competitive exclusion.\nHowever, this classical paradigm in ecology relies on implications derived from\nsimple, deterministic models. Here we show how the predictions of a symmetric,\ndeterministic model about the way extinctions proceed can be utterly different\nfrom the results derived from the same model when ecological drift (demographic\nstochasticity) is explicitly considered. Using analytical approximations to the\nsteady-state conditional probabilities for assemblages with two and three\nspecies, we demonstrate that stochastic competitive exclusion leads to a\ncascade of extinctions, whereas the symmetric, deterministic model predicts a\nmultiple collapse of species. To test the robustness of our results, we have\nstudied the effect of environmental stochasticity and relaxed the species\nsymmetry assumption. Our conclusions highlight the crucial role of\nstochasticity when deriving reliable theoretical predictions for species\ncommunity assembly.\n",
        "  Analysis of probability distributions conditional on species trees has\ndemonstrated the existence of anomalous ranked gene trees (ARGTs), ranked gene\ntrees that are more probable than the ranked gene tree that accords with the\nranked species tree. Here, to improve the characterization of ARGTs, we study\nenumerative and probabilistic properties of two classes of ranked labeled\nspecies trees, focusing on the presence or avoidance of certain subtree\npatterns associated with the production of ARGTs. We provide exact enumerations\nand asymptotic estimates for cardinalities of these sets of trees, showing that\nas the number of species increases without bound, the fraction of all ranked\nlabeled species trees that are ARGT-producing approaches 1. This result extends\nbeyond earlier existence results to provide a probabilistic claim about the\nfrequency of ARGTs.\n",
        "  A recent experiment on Nb-doped Bi2Se3 showed that zero field magnetization\nappears below the superconducting transition temperature. This gives evidence\nthat the superconducting state breaks time-reversal symmetry spontaneously and\npossibly be in the chiral topological phase. This is in sharp contrast to the\nCu-doped case which is possibly in the nematic phase and breaks rotational\nsymmetry spontaneously. By deriving the free energy of the system from a\nmicroscopic model, we show that the magnetic moments of the Nb atoms can be\npolarized by the chiral Cooper pairs and enlarge the phase space of the chiral\ntopological phase compared to the nematic phase. We further show that the\nchiral topological phase is a Weyl superconducting phase with bulk nodal points\nwhich are connected by surface Majorana arcs.\n",
        "  Inter-and intra-observer variation in delineating regions of interest (ROIs)\noccurs because of differences in expertise level and preferences of the\nradiation oncologists. We evaluated the accuracy of a segmentation model using\nthe U-Net structure to delineate the prostate, bladder, and rectum in male\npelvic CT images. The dataset used for training and testing the model consisted\nof raw CT scan images of 85 prostate cancer patients. We designed a 2D U-Net\nmodel to directly learn a mapping function that converts a 2D CT grayscale\nimage to its corresponding 2D OAR segmented image. Our network contains blocks\nof convolution 2D layers with variable kernel sizes, channel number, and\nactivation functions. On the left side of the U-Net model, we used three 3x3\nconvolutions, each followed by a rectified linear unit (ReLu) (activation\nfunction), and one max pooling operation. On the right side of the U-Net model,\nwe used a 2x2 transposed convolution and two 3x3 convolution networks followed\nby a ReLu activation function. The automatic segmentation using the U-Net\ngenerated an average dice similarity coefficient (DC) and standard deviation\n(SD) of the following: DC +- SD (0.88 +- 0.12), (0.95 +- 0.04), and (0.92 +-\n0.06) for the prostate, bladder, and rectum, respectively. Furthermore, the\nmean of average surface Hausdorff distance (ASHD) and SD were 1.2 +- 0.9 mm,\n1.08 +- 0.8 mm, and 0.8 +- 0.6 mm for the prostate, bladder, and rectum,\nrespectively. Our proposed method, which employs the U-Net structure, is highly\naccurate and reproducible for automated ROI segmentation. This provides a\nfoundation to improve automatic delineation of the boundaries between the\ntarget and surrounding normal soft tissues on a standard radiation therapy\nplanning CT scan.\n",
        "  Recently, the relation between the masses of the black hole ($M_{BH}$) and\nthe host galaxy ($M_{host}$) in quasars has been probed down to the parameter\nspace of $M_{BH}\\sim10^8 M_\\odot$ and $M_{host}\\sim10^{11} M_\\odot$ at z $<$\n0.5. In this study, we have investigated the $M_{BH}$ - $M_{host}$ log-linear\nrelation for a sample of 37 quasars with low black hole masses ($10^7 M_\\odot <\nM_{BH} < 10^{8.3} M_\\odot$) at 0.5 $<$ z $<$ 1.0. The black hole masses were\nderived using virial mass estimates from SDSS optical spectra. For 25 quasars,\nwe detected the presence of the host galaxy from deep near-infrared H-band\nimaging, whereas upper limits for the host galaxy luminosity (mass) were\nestimated for the 12 unresolved quasars. We combined our previous studies with\nthe results from this work to create a sample of 89 quasars at z $<$ 1.0 having\na large range of black hole masses ($10^7 M_\\odot < M_{BH} < 10^{10} M_\\odot$)\nand host galaxy masses ($10^{10} M_\\odot < M_{host} < 10^{13} M_\\odot$). Most\nof the quasars at the low mass end lie below the extrapolation of the local\nrelation. This apparent break in the linearity of the entire sample is due to\nincreasing fraction of disc-dominated host galaxies in the low-mass quasars.\nAfter correcting for the disc component, and considering only the bulge\ncomponent, the bilinear regression for the entire quasar sample holds over 3.5\ndex in both the black hole mass and the bulge mass, and is in very good\nagreement with the local relation. We advocate secular evolution of discs of\ngalaxies being responsible for the relatively strong disc domination.\n",
        "  Knee orthotic devices are widely proposed by physicians and medical\npractitioners for preventive or therapeutic objectives in relation with their\neffects, usually known as to stabilize joint or restrict ranges of motion. This\nstudy focuses on the understanding of force transfer mechanisms from the brace\nto the joint thanks to a Finite Element Model. A Design Of Experiments approach\nwas used to characterize the stiffness and comfort of various braces in order\nto identify their mechanically influent characteristics. Results show\nconflicting behavior: influent parameters such as the brace size or textile\nstiffness improve performance in detriment of comfort. Thanks to this\ncomputational tool, novel brace designs can be tested and evaluated for an\noptimal mechanical efficiency of the devices and a better compliance of the\npatient to the treatment.\n",
        "  In this paper we study the typical speed of a generic earthquake trajectory\nleaving compact sets in the moduli space of the once-punctured torus.\nMirzakhani showed that the earthquake flow is measurably equivalent to the\nhorocyclic flow, which has been studied extensively. Our main result shows that\nthe earthquake flow and the horocyclic flow behave very differently in cusp\nexcursions. In particular, we prove a relation between the systole function and\ncontinued fractions and discuss the cusp excursions of earthquake trajectories.\n",
        "  We show that any subgroup of a (virtually) nilpotent-by-polycyclic group\nsatisfies the bounded packing property of Hruska-Wise. In particular, the same\nis true about metabelian groups and linear solvable groups. However, we find an\nexample of a finitely generated solvable group of derived length 3 which admits\na finitely generated subgroup without the bounded packing property. In this\nexample the subgroup is a metabelian retract also. Thus we obtain a negative\nanswer to Problem 2.27 of Hruska-Wise. On the other hand, we show that\npolycyclic subgroups of solvable groups satisfy the bounded packing property.\n",
        "  A tagger is a mandatory segment of most text scrutiny systems, as it\nconsigned a s yntax class (e.g., noun, verb, adjective, and adverb) to every\nword in a sentence. In this paper, we present a simple part of speech tagger\nfor homoeopathy clinical language. This paper reports about the anticipated\npart of speech tagger for homoeopathy clinical language. It exploit standard\npattern for evaluating sentences, untagged clinical corpus of 20085 words is\nused, from which we had selected 125 sentences (2322 tokens). The problem of\ntagging in natural language processing is to find a way to tag every word in a\ntext as a meticulous part of speech. The basic idea is to apply a set of rules\non clinical sentences and on each word, Accuracy is the leading factor in\nevaluating any POS tagger so the accuracy of proposed tagger is also conversed.\n",
        "  We model the $z \\geq 6.6$ Ly$\\alpha$ luminosity function to estimate the\nnumber of lensed high$-z$ Ly$\\alpha$ emitters that may be detected by the\nEuclid Deep Survey. To span the whole range of possible predictions we exploit\ntwo Ly$\\alpha$ luminosity function models and two strong gravitational lensing\nmodels from the literature. We show that the planned Euclid Deep Survey\nobserving 40 deg$^2$ over the 920-1850 nm wavelength range down to a flux limit\nof $F_{lim}=5\\times10^{-17}\\,$erg s$^{-1}\\,$cm$^{-2}$ will enable us to find\nbetween $\\sim 0.85$ and $\\sim 1.82$ deg$^{-2}$ lensed Ly$\\alpha$ emitters at $z\n\\geq 6.6$ depending on the adopted Ly$\\alpha$ luminosity function and strong\ngravitational lensing model. The obvious [OII], [OIII] and H$\\beta$\ncontaminants of the Ly$\\alpha$ lensed population will be identified with the\nhelp of Euclid's spectral resolving power, while the SKA will enable the\nidentification of the interloper population of H$\\alpha$ emitters. By combining\nEuclid and the SKA, we will thus be able to identify, for the first time, a\nsample of $\\sim 34$ to $\\sim 73$ lensed Ly$\\alpha$ emitters at $z \\geq 6.6$.\n",
        "  We investigate the dynamics of a resistively shunted Josephson junction. We\ncompute the Josephson frequency and the generalized impedances for a variety of\nthe parameters, particularly with relevance to predicting the measurable\neffects of zero-temperature current noise in the resistor.\n",
        "  In named entity recognition, we often don't have a large in-domain training\ncorpus or a knowledge base with adequate coverage to train a model directly. In\nthis paper, we propose a method where, given training data in a related domain\nwith similar (but not identical) named entity (NE) types and a small amount of\nin-domain training data, we use transfer learning to learn a domain-specific NE\nmodel. That is, the novelty in the task setup is that we assume not just domain\nmismatch, but also label mismatch.\n",
        "  A new delivery option for cancer centers equipped with linear accelerators\nfitted with multi-leaf collimators (MLC) -- i.e. centers which can perform\nintensity modulated radiation therapy (IMRT) -- is rotational delivery. In\nrotational delivery, the beam is on while the gantry is rotating and the MLC\nleaves are moving, thus treating the patient more efficiently (regarding time)\nthan in IMRT. A consideration that should be examined when evaluating this new\ntype of delivery method is the tradeoff between treatment plan quality and\ndelivery time: what do we sacrifice in terms of plan quality by guaranteeing a\n2 minute delivery, for example? In this paper we examine this question by\nstudying a simplified 2D phantom where leaf and gantry motion are directly\nincluded in the optimization model. We formulate the model as a linear mixed\ninteger program. Because of the difficulty in solving to optimality, we employ\nadditional models which allow us to trap the true optimal solution between\nupper and lower bounds. The lower bound solutions reveal which beam directions\nare most useful for treatment (i.e. where the gantry should slow down), and\nthis information is used to produce deliverable solutions close to the lower\nbound solutions. For the phantom cases studied, we show that when time is not\nan issue, IMRT solutions are optimal, but when allowable treatment time is\nconstrained sufficiently, rotational delivery is the preferred choice.\n",
        "  We introduce the notion of a Khovanov-Floer theory. Roughly, such a theory\nassigns a filtered chain complex over Z/2 to a link diagram such that (1) the\nE_2 page of the resulting spectral sequence is naturally isomorphic to the\nKhovanov homology of the link; (2) this filtered complex behaves nicely under\nplanar isotopy, disjoint union, and 1-handle addition; and (3) the spectral\nsequence collapses at the E_2 page for any diagram of the unlink. We prove that\na Khovanov-Floer theory naturally yields a functor from the link cobordism\ncategory to the category of spectral sequences. In particular, every page\n(after E_1) of the spectral sequence accompanying a Khovanov-Floer theory is a\nlink invariant, and an oriented cobordism in R^3 \\times [0,1] between links in\nR^3 induces a map between each page of their spectral sequences, invariant up\nto smooth isotopy of the cobordism rel boundary.\n  We then show that the spectral sequences relating Khovanov homology to\nHeegaard Floer homology and singular instanton knot homology are induced by\nKhovanov-Floer theories and are therefore functorial in the manner described\nabove, as has been conjectured for some time. We further show that Szabo's\ngeometric spectral sequence comes from a Khovanov-Floer theory, and is thus\nfunctorial as well. In addition, we illustrate how our framework can be used to\ngive another proof that Lee's spectral sequence is functorial and that\nRasmussen's invariant is a knot invariant. Finally, we use this machinery to\ndefine some potentially new knot invariants.\n",
        "  We present further results of a search for extragalactic submillimeter H2O\nmasers using the Atacama Large Millimeter/Submillimeter Array (ALMA). The\ndetection of a 321 GHz H2O maser in the nearby Type 2 Seyfert galaxy, the\nCircinus galaxy, has previously been reported, and here the spectral analysis\nof four other galaxies is described. We have discovered H2O maser emission at\n321 GHz toward the center of NGC 4945, a nearby Type 2 Seyfert. The maser\nemission shows Doppler-shifted velocity features with velocity ranges similar\nto those of the previously reported 22 GHz H2O masers, however the\nnon-contemporaneous observations also show differences in velocity offsets. The\nsub-parsec-scale distribution of the 22 GHz H2O masers revealed by earlier VLBI\n(Very Long Baseline Interferometry) observations suggests that the\nsubmillimeter masers could arise in an edge-on rotating disk. The maser\nfeatures remain unresolved by the synthesized beam of ~ 0.54 (~30 pc) and are\nlocated toward the 321 GHz continuum peak within errors. A marginally detected\n(3 sigma) high-velocity feature is redshifted by 579 km/s with respect to the\nsystemic velocity of the galaxy. Assuming that this feature is real and arises\nfrom a Keplerian rotating disk in this galaxy, it is located at a radius of\n~0.020 pc (~1.5 x 10^5 Schwarzschild radii), which would enable molecular\nmaterial closer to the central engine to be probed than the 22 GHz H2O masers.\nThis detection confirms that submillimeter H2O masers are a potential tracer of\nthe circumnuclear regions of active galaxies, which will benefit from higher\nangular resolution studies with ALMA.\n",
        "  This paper proposes a novel framework for detecting redundancy in supervised\nsentence categorisation. Unlike traditional singleton neural network, our model\nincorporates character-aware convolutional neural network (Char-CNN) with\ncharacter-aware recurrent neural network (Char-RNN) to form a convolutional\nrecurrent neural network (CRNN). Our model benefits from Char-CNN in that only\nsalient features are selected and fed into the integrated Char-RNN. Char-RNN\neffectively learns long sequence semantics via sophisticated update mechanism.\nWe compare our framework against the state-of-the-art text classification\nalgorithms on four popular benchmarking corpus. For instance, our model\nachieves competing precision rate, recall ratio, and F1 score on the\nGoogle-news data-set. For twenty-news-groups data stream, our algorithm obtains\nthe optimum on precision rate, recall ratio, and F1 score. For Brown Corpus,\nour framework obtains the best F1 score and almost equivalent precision rate\nand recall ratio over the top competitor. For the question classification\ncollection, CRNN produces the optimal recall rate and F1 score and comparable\nprecision rate. We also analyse three different RNN hidden recurrent cells'\nimpact on performance and their runtime efficiency. We observe that MGU\nachieves the optimal runtime and comparable performance against GRU and LSTM.\nFor TFIDF based algorithms, we experiment with word2vec, GloVe, and sent2vec\nembeddings and report their performance differences.\n",
        "  Effect of atomic disordering induced by irradiation with fast neutrons on the\nproperties of the normal and superconducting states of polycrystalline samples\nFeSe has been studied. The irradiation with fast neutrons of fluencies up to\n1.25\\cdot10^20 cm^-2 at the irradiation temperature Tirr ~ 50 \\degree C results\nin relatively small changes in the temperature of the superconducting\ntransition T_c and electrical resistivity Rho_25. Such a behavior is considered\nto be traceable to rather low, with respect to that possible at a given\nirradiation temperature, concentration of radiation defects, which is caused by\na simpler crystal structure, considered to other layered compounds.\n",
        "  We study the left-orderability of the fundamental groups of cyclic branched\ncovers of links which admit co-oriented taut foliations. In particular we do\nthis for cyclic branched covers of fibred knots in integer homology $3$-spheres\nand cyclic branched covers of closed braids. The latter allows us to complete\nthe proof of the L-space conjecture for closed, connected, orientable,\nirreducible $3$-manifolds containing a genus $1$ fibred knot. We also prove\nthat the universal abelian cover of a manifold obtained by generic Dehn surgery\non a hyperbolic fibred knot in an integer homology $3$-sphere admits a\nco-oriented taut foliation and has left-orderable fundamental group, even if\nthe surgered manifold does not, and that the same holds for many branched\ncovers of satellite knots with braided patterns. A key fact used in our proofs\nis that the Euler class of a universal circle representation associated to a\nco-oriented taut foliation coincides with the Euler class of the foliation's\ntangent bundle. Though known to experts, no proof of this important result has\nappeared in the literature. We provide such a proof in the paper.\n",
        "  Thermally activated flux flow (TAFF) and flux flow Hall effect (FFHE) of\nFe(Te,S) single crystal in the mixed state are studied in magnetic fields up to\n35 T. Thermally activated energy (TAE) is analyzed using conventional Arrhenius\nrelation and modified TAFF theory which is closer to experimental results. The\nresults indicate that there is a crossover from single-vortex pinning region to\ncollective creep pinning region with increasing magnetic field. The temperature\ndependence of TAE is different for H//ab and H//c. On the other hand, the\nanalysis of FFHE in the mixed state indicates that there is no Hall sign\nreversal. We also observe scaling behavior |Rhoxy(H)| = A*Rhoxx(H)^beta.\n",
        "  One of the first successes of neutral ecology was to predict\nrealistically-broad distributions of rare and abundant species. However, it has\nremained an outstanding theoretical challenge to describe how this distribution\nof abundances changes with spatial scale, and this gap has hampered attempts to\nuse observed species abundances as a way to quantify what non-neutral processes\nare needed to fully explain observed patterns. To address this, we introduce a\nnew formulation of spatial neutral biodiversity theory and derive analytical\npredictions for the way abundance distributions change with scale. For tropical\nforest data where neutrality has been extensively tested before now, we apply\nthis approach and identify an incompatibility between neutral fits at regional\nand local scales. We use this approach derive a sharp quantification of what\nremains to be explained by non-neutral processes at the local scale, setting a\nquantitative target for more general models for the maintenance of\nbiodiversity.\n",
        "  The family Nymphalidae is the largest family within the true butterflies and\nhas been used to develop hypotheses explaining evolutionary interactions\nbetween plants and insects. Theories of insect and hostplant dynamics predict\naccelerated diversification in some scenarios. We investigated whether\nphylogenetic uncertainty affects a commonly used method (MEDUSA, modelling\nevolutionary diversity using stepwise AIC) for estimating shifts in\ndiversification rates in lineages of the family Nymphalidae, by extending the\nmethod to run across a random sample of phylogenetic trees from the posterior\ndistribution of a Bayesian run. We found that phylogenetic uncertainty greatly\naffects diversification rate estimates. Different trees from the posterior\ndistribution can give diversification rates ranging from high values to almost\nzero for the same clade, and for some clades both significant rate increase and\ndecrease were estimated. Only three out of 13 significant shifts found on the\nmaximum credibility tree were consistent across more than 95% of the trees from\nthe posterior: (i) accelerated diversification for Solanaceae feeders in the\ntribe Ithomiini; (ii) accelerated diversification in the genus Charaxes, and\n(iii) deceleration in the Danaina. By using the binary speciation and\nextinction model (BISSE), we found that a hostplant shift to Solanaceae or a\ncodistributed character is responsible for the increase in diversification rate\nin Ithomiini, and the result is congruent with the diffuse cospeciation\nhypothesis. A shift to Apocynaceae is not responsible for the slowdown of\ndiversification in Danaina. Our results show that taking phylogenetic\nuncertainty into account when estimating diversification rate shifts is of\ngreat importance, and relying on the maximum credibility tree alone potentially\ncan give erroneous results.\n",
        "  This paper investigates the robustness of NLP against perturbed word forms.\nWhile neural approaches can achieve (almost) human-like accuracy for certain\ntasks and conditions, they often are sensitive to small changes in the input\nsuch as non-canonical input (e.g., typos). Yet both stability and robustness\nare desired properties in applications involving user-generated content, and\nthe more as humans easily cope with such noisy or adversary conditions. In this\npaper, we study the impact of noisy input. We consider different noise\ndistributions (one type of noise, combination of noise types) and mismatched\nnoise distributions for training and testing. Moreover, we empirically evaluate\nthe robustness of different models (convolutional neural networks, recurrent\nneural networks, non-neural models), different basic units (characters, byte\npair encoding units), and different NLP tasks (morphological tagging, machine\ntranslation).\n",
        "  We introduce a controlled form of recursion in XQuery, inflationary fixed\npoints, familiar in the context of relational databases. This imposes\nrestrictions on the expressible types of recursion, but we show that\ninflationary fixed points nevertheless are sufficiently versatile to capture a\nwide range of interesting use cases, including the semantics of Regular XPath\nand its core transitive closure construct.\n  While the optimization of general user-defined recursive functions in XQuery\nappears elusive, we will describe how inflationary fixed points can be\nefficiently evaluated, provided that the recursive XQuery expressions exhibit a\ndistributivity property. We show how distributivity can be assessed both,\nsyntactically and algebraically, and provide experimental evidence that XQuery\nprocessors can substantially benefit during inflationary fixed point\nevaluation.\n",
        "  In the last decade C-arm-based cone-beam CT became a widely used modality for\nintraoperative imaging. Typically a C-arm scan is performed using a circle-like\ntrajectory around a region of interest. Therefor an angular range of at least\n180{\\deg} plus fan-angle must be covered to ensure a completely sampled data\nset. This fact defines some constraints on the geometry and technical\nspecifications of a C-arm system, for example a larger C radius or a smaller C\nopening respectively. These technical modifications are usually not benificial\nin terms of handling and usability of the C-arm during classical 2D\napplications like fluoroscopy. The method proposed in this paper relaxes the\nconstraint of 180{\\deg} plus fan-angle rotation to acquire a complete data set.\nThe proposed C-arm trajectory requires a motorization of the orbital axis of\nthe C and of ideally two orthogonal axis in the C plane. The trajectory\nconsists of three parts: A rotation of the C around a defined iso-center and\ntwo translational movements parallel to the detector plane at the begin and at\nthe end of the rotation. Combining these three parts to one trajectory enables\nfor the acquisition of a completely sampled dataset using only 180{\\deg} minus\nfan-angle of rotation. To evaluate the method we show animal scans acquired\nwith a mobile C-arm prototype. We expect that the transition of this method\ninto clinical routine will lead to a much broader use of intraoperative 3D\nimaging in a wide field of clinical applications.\n",
        "  A quantitative interpretation of the observed relation between the\ninterstellar linear polarization curve parameters $K$ and $\\lambda_{\\max}$\ncharacterizing the width and the wavelength of a polarization maximum,\nrespectively, is given. The observational data available for 57 stars located\nin the dark clouds in Taurus, Chamaeleon, around the stars $\\rho$ Oph and R CrA\nare considered. The spheroidal particle model of interstellar dust grains\nearlier applied to simultaneously interpret the interstellar extinction and\npolarization curves in a wide spectral range is utilized. The observed trend $K\n\\approx 1.7 \\lambda_{\\max}$ is shown to be most likely related to a growth of\ndust grains due to coagulation rather than mantle accretion. The relation of\nthe parameters $K$ and $\\lambda_{\\max}$ with an average size of silicate dust\ngrains is discussed.\n",
        "  Single-source and top-$k$ SimRank queries are two important types of\nsimilarity search in graphs with numerous applications in web mining, social\nnetwork analysis, spam detection, etc. A plethora of techniques have been\nproposed for these two types of queries, but very few can efficiently support\nsimilarity search over large dynamic graphs, due to either significant\npreprocessing time or large space overheads.\n  This paper presents ProbeSim, an index-free algorithm for single-source and\ntop-$k$ SimRank queries that provides a non-trivial theoretical guarantee in\nthe absolute error of query results. ProbeSim estimates SimRank similarities\nwithout precomputing any indexing structures, and thus can naturally support\nreal-time SimRank queries on dynamic graphs. Besides the theoretical guarantee,\nProbeSim also offers satisfying practical efficiency and effectiveness due to\nseveral non-trivial optimizations. We conduct extensive experiments on a number\nof benchmark datasets, which demonstrate that our solutions significantly\noutperform the existing methods in terms of efficiency and effectiveness.\nNotably, our experiments include the first empirical study that evaluates the\neffectiveness of SimRank algorithms on graphs with billion edges, using the\nidea of pooling.\n",
        "  Conductance-voltage characteristics (CVCs) for non-symmetric tunnel junctions\nbetween $d$-wave superconductors with charge-density waves (CDWs) and normal\nmetals were calculated. It was shown that they have a V-like form at small\nvoltages $V$ and are asymmetric at larger $V$ owing to the presence of CDW peak\nin one of the $V$-branches. The spatial scatter of the dielectric (CDW) order\nparameter smears the CDW peak into a hump and induces a peak-dip-hump structure\n(PDHS) typical of CVCs observed for such junctions. At temperatures larger than\nthe superconducting critical temperature, the PDHS evolves into a pseudogap\ndepression. The results agree well with the scanning tunneling microscopy data\nfor Bi$_{2}$Sr$_{2}$CaCu$_{2}$O$_{8+\\delta}$ and YBa$_{2}%\n$Cu$_{3}$O$_{7-\\delta}$.\n",
        "  In this paper, we have used the concepts of the fractional derivative of\nrough curves to characterize ECG of LVH patients and compared the results with\nnormal ECGs. In mathematical language, an ECG is a rough curve having Q, R, S\npoints as non-differentiable points where classical derivatives do not exist\nbut fractional derivatives exist. We have calculated both left and right\nmodified Riemann-Liouville fractional derivatives and their differences termed\nas phase transition at those non-differentiable points of V1, V2, V5, and V6\nleads.Investigation shows that phase transition is higher for LVH patients than\nnormal ones. This may be a method of determination of risk factor of LVH\npatients before doing Echocardiogram.\n",
        "  We construct an infinite family of slice disks with the same exterior, which\ngives an affirmative answer to an old question asked by Hitt and Sumners in\n1981. Furthermore, we prove that these slice disks are ribbon disks.\n",
        "  We examine the impact of interfacial phonons on the superconducting state of\nFeSe/SrTiO$_3$ developing a material's specific multiband, full bandwidth, and\nanisotropic Eliashberg theory for this system. Our self-consistent calculations\nhighlight the importance of the interfacial electron-phonon interaction, which\nis hidden behind the seemingly weak-coupling constant {\\lambda}$_m$ = 0.4, in\nmediating the high T$_c$, and explain other puzzling experimental observations,\nsuch as the s-wave symmetry and replica bands. We discover that the formation\nof replica bands has a T$_c$ decreasing effect that is nevertheless compensated\nby deep Fermi-sea Cooper pairing which has a T$_c$ enhancing effect. We predict\na strong-coupling dip-hump signature in the tunneling spectra due to the\ninterfacial coupling.\n",
        "  Using data from the WISE mission, we have measured near infra-red (NIR)\nphotometry of a diverse sample of dust-free stellar systems (globular clusters,\ndwarf and giant early-type galaxies) which have metallicities that span the\nrange -2.2 < [Fe/H] (dex) < 0.3. This dramatically increases the sample size\nand broadens the metallicity regime over which the 3.4 (W1) and 4.6 micron (W2)\nphotometry of stellar populations have been examined.\n  We find that the W1 - W2 colors of intermediate and old (> 2 Gyr) stellar\npopulations are insensitive to the age of the stellar population, but that the\nW1 - W2 colors become bluer with increasing metallicity, a trend not well\nreproduced by most stellar population synthesis (SPS) models. In common with\nprevious studies, we attribute this behavior to the increasing strength of the\nCO absorption feature located in the 4.6 micron bandpass with metallicity.\n  Having used our sample to validate the efficacy of some of the SPS models, we\nuse these models to derive stellar mass-to-light ratios in the W1 and W2 bands.\nUtilizing observational data from the SAURON and ATLAS3D surveys, we\ndemonstrate that these bands provide extremely simple, yet robust stellar mass\ntracers for dust free older stellar populations that are freed from many of the\nuncertainties common among optical estimators.\n",
        "  Our goal is to combine the rich multistep inference of symbolic logical\nreasoning with the generalization capabilities of neural networks. We are\nparticularly interested in complex reasoning about entities and relations in\ntext and large-scale knowledge bases (KBs). Neelakantan et al. (2015) use RNNs\nto compose the distributed semantics of multi-hop paths in KBs; however for\nmultiple reasons, the approach lacks accuracy and practicality. This paper\nproposes three significant modeling advances: (1) we learn to jointly reason\nabout relations, entities, and entity-types; (2) we use neural attention\nmodeling to incorporate multiple paths; (3) we learn to share strength in a\nsingle RNN that represents logical composition across all relations. On a\nlargescale Freebase+ClueWeb prediction task, we achieve 25% error reduction,\nand a 53% error reduction on sparse relations due to shared strength. On chains\nof reasoning in WordNet we reduce error in mean quantile by 84% versus previous\nstate-of-the-art. The code and data are available at\nhttps://rajarshd.github.io/ChainsofReasoning\n",
        "  We consider the problem of duplicate detection in noisy and incomplete data:\ngiven a large data set in which each record has multiple entries (attributes),\ndetect which distinct records refer to the same real world entity. This task is\ncomplicated by noise (such as misspellings) and missing data, which can lead to\nrecords being different, despite referring to the same entity. Our method\nconsists of three main steps: creating a similarity score between records,\ngrouping records together into \"unique entities\", and refining the groups. We\ncompare various methods for creating similarity scores between noisy records,\nconsidering different combinations of string matching, term frequency-inverse\ndocument frequency methods, and n-gram techniques. In particular, we introduce\na vectorized soft term frequency-inverse document frequency method, with an\noptional refinement step. We also discuss two methods to deal with missing data\nin computing similarity scores.\n  We test our method on the Los Angeles Police Department Field Interview Card\ndata set, the Cora Citation Matching data set, and two sets of restaurant\nreview data. The results show that the methods that use words as the basic\nunits are preferable to those that use 3-grams. Moreover, in some (but\ncertainly not all) parameter ranges soft term frequency-inverse document\nfrequency methods can outperform the standard term frequency-inverse document\nfrequency method. The results also confirm that our method for automatically\ndetermining the number of groups typically works well in many cases and allows\nfor accurate results in the absence of a priori knowledge of the number of\nunique entities in the data set.\n",
        "  Creating tetrahedral meshes with anatomically accurate surfaces is critically\nimportant for a wide range of model-based neuroimaging modalities. However,\ncomputationally efficient brain meshing algorithms and software are greatly\nlacking. Here, we report a fully automated open-source software to rapidly\ncreate high-quality tetrahedral meshes from brain segmentations. Built upon\nvarious open-source meshing utilities, the proposed meshing workflow allows\nrobust generation of complex head and brain mesh models from multi-label\nvolumes, tissue probability maps, surface meshes and their combinations. The\nquality of the complex tissue boundaries is preserved through a surface-based\napproach, allowing fine-grained control over the sizes and quality of the mesh\nelements through explicit user-defined meshing criteria. The proposed meshing\npipeline is highly versatile and compatible with many commonly used brain\nanalysis tools, including SPM, FSL, FreeSurfer, and BrainSuite. With this\nmesh-generation pipeline, we demonstrate that one can generate 3D full-head\nmeshes that combine scalp, skull, cerebrospinal fluid, gray matter, white\nmatter, and air cavities with a typical processing time of less than 40\nseconds. This approach can also incorporate highly detailed cortical and white\nmatter surface meshes derived from FSL and FreeSurfer with tissue segmentation\ndata. Finally, a high-quality brain atlas mesh library for different age\ngroups, ranging from infants to elderlies, was built to demonstrate the\nrobustness of the proposed workflow, as well as to serve as a common platform\nfor simulation-based brain studies. Our open-source meshing software\n\"brain2mesh\" and the human brain atlas mesh library can be downloaded at\nhttp://mcx.space/brain2mesh.\n",
        "  Recent neutron scattering and transport data obtained on underdoped YBCO,\nwith strong signatures of rotation symmetry breaking at low temperatures, point\ntoward electron-nematic order in the charge sector. Such order may originate\nfrom a uniform distortion with d-wave symmetry or as a precursor of a\nuni-directional stripe phase. Here, we discuss whether the neutron scattering\ndata can be linked to incipient charge stripes. We employ and extend a\nphenomenological model for collective spin and charge fluctuations and analyze\nthe resulting spin excitation spectrum under the influence of lattice\nanisotropies. Our results show that the experimentally observed\ntemperature-dependent magnetic incommensurability is compatible with a scenario\nof incipient stripes, the temperature dependence being due to the temperature\nvariation of both strength and correlation length of the charge stripes.\nFinally, we propose further experiments to distinguish the possible theoretical\nscenarios.\n",
        "  This document reports an Open 2D Electrical Impedance Tomography (EIT) data\nset. The EIT measurements were collected from a circular body (a flat tank\nfilled with saline) with various choices of conductive and resistive\ninclusions. Data are available at http://fips.fi/ EIT_dataset.php and can be\nfreely used for scientific purposes with appropriate references to them, and to\nthis document at https://arxiv.org. The data set consists of (1) current\npatterns and voltage measurements of a circular tank containing different\ntargets, (2) photos of the tank and targets and (3) a MATLAB-code for reading\nthe data. A video report of the data collection session is available at\nhttps://www.youtube.com/watch?v=65Zca_qd1Y8.\n",
        "  In clinical CT, the x-ray source emits polychromatic x-rays, which are\ndetected in the current-integrating mode. This physical process is accurately\ndescribed by an energy-dependent non-linear integral model on the basis of the\nBeer-Lambert law. However, the non-linear model is too complicated to be\ndirectly solved for the image reconstruction, and is often approximated to a\nlinear integral model in the form of the Radon transform, basically ignoring\nenergy-dependent information. This model approximation would generate\ninaccurate quantification of attenuation image and significant beam-hardening\nartifacts. In this paper, we develop a deep-learning-based CT image\nreconstruction method to address the mismatch of computing model to physical\nmodel. Our method learns a nonlinear transformation from big data to correct\nmeasured projection data to accurately match the linear integral model, realize\nmonochromatic imaging and overcome beam hardening effectively. The\ndeep-learning network is trained and tested using clinical dual-energy dataset\nto demonstrate the feasibility of the proposed methodology. Results show that\nthe proposed method can achieve a high accuracy of the projection correction\nwith a relative error of less than 0.2%.\n",
        "  In computing, spell checking is the process of detecting and sometimes\nproviding spelling suggestions for incorrectly spelled words in a text.\nBasically, a spell checker is a computer program that uses a dictionary of\nwords to perform spell checking. The bigger the dictionary is, the higher is\nthe error detection rate. The fact that spell checkers are based on regular\ndictionaries, they suffer from data sparseness problem as they cannot capture\nlarge vocabulary of words including proper names, domain-specific terms,\ntechnical jargons, special acronyms, and terminologies. As a result, they\nexhibit low error detection rate and often fail to catch major errors in the\ntext. This paper proposes a new context-sensitive spelling correction method\nfor detecting and correcting non-word and real-word errors in digital text\ndocuments. The approach hinges around data statistics from Google Web 1T 5-gram\ndata set which consists of a big volume of n-gram word sequences, extracted\nfrom the World Wide Web. Fundamentally, the proposed method comprises an error\ndetector that detects misspellings, a candidate spellings generator based on a\ncharacter 2-gram model that generates correction suggestions, and an error\ncorrector that performs contextual error correction. Experiments conducted on a\nset of text documents from different domains and containing misspellings,\nshowed an outstanding spelling error correction rate and a drastic reduction of\nboth non-word and real-word errors. In a further study, the proposed algorithm\nis to be parallelized so as to lower the computational cost of the error\ndetection and correction processes.\n",
        "  The compact radio source Sgr\\,A*, associated with the super massive black\nhole at the center of the Galaxy, has been studied with VLBA observations at 3\nfrequencies (22, 43, 86\\,GHz) performed on 10 consecutive days in May 2007. The\ntotal VLBI flux density of Sgr\\,A* varies from day to day. The variability is\ncorrelated at the 3 observing frequencies with higher variability amplitudes\nappearing at the higher frequencies. For the modulation indices, we find 8.4\\,%\nat 22\\,GHz, 9.3\\,% at 43\\,GHz, and 15.5\\,% at 86\\,GHz. The radio spectrum is\ninverted between 22 and 86\\,GHz, suggesting inhomogeneous synchrotron\nself-absorption with a turnover frequency at or above 86\\,GHz. The radio\nspectral index correlates with the flux density, which is harder (more inverted\nspectrum) when the source is brighter. The average source size does not appear\nto be variable over the 10-day observing interval. However, we see a tendency\nfor the sizes of the minor axis to increase with increasing total flux, whereas\nthe major axis remains constant. Towards higher frequencies, the position angle\nof the elliptical Gaussian increases, indicative of intrinsic structure, which\nbegins to dominate the scatter broadening. At cm-wavelength, the source size\nvaries with wavelength as $\\lambda^{2.12\\pm0.12}$, which is interpreted as the\nresult of interstellar scatter broadening. After removal of this scatter\nbroadening, the intrinsic source size varies as $\\lambda^{1.4 ... 1.5}$. The\nVLBI closure phases at 22, 43, and 86\\,GHz are zero within a few degrees,\nindicating a symmetric or point-like source structure. In the context of an\nexpanding plasmon model, we obtain an upper limit of the expansion velocity of\nabout 0.1\\,c from the non-variable VLBI structure. This agrees with the\nvelocity range derived from the radiation transport modeling of the flares from\nthe radio to NIR wavelengths.}\n",
        "  We investigated the complex conductivity spectrum of a Co-doped\nBaFe$_2$As$_2$ epitaxial thin film in the THz region. In the normal state, the\ncomplex conductivity shows a Drude-type frequency dependence, while in the\nsuperconducting state, the frequency dependence of the complex conductivity\nchanges to that of a typical superconducting materials. We estimated the\nmagnetic penetration depth at absolute zero to be 710 nm and the\nsuperconducting gap energy to be 2.8 meV, which is considered to be the\nsuperconducting gap opened at the electron-type Fermi surface near the M point.\nWe succeeded in obtaining the low-energy elementary excitation of a Fe-based\nsuperconductor using the electromagnetic method without invoking the\nKramers-Kronig transformation.\n",
        "  The reconstruction of phylogenetic trees from molecular sequence data relies\non modelling site substitutions by a Markov process, or a mixture of such\nprocesses. In general, allowing mixed processes can result in different tree\ntopologies becoming indistinguishable from the data, even for infinitely long\nsequences. However, when the underlying Markov process supports linear\nphylogenetic invariants, then provided these are sufficiently informative, the\nidentifiability of the tree topology can be restored. In this paper, we\ninvestigate a class of processes that support linear invariants once the\nstationary distribution is fixed, the `equal input model'. This model\ngeneralizes the `Felsenstein 1981' model (and thereby the Jukes--Cantor model)\nfrom four states to an arbitrary number of states (finite or infinite), and it\ncan also be described by a `random cluster' process. We describe the structure\nand dimension of the vector spaces of phylogenetic mixtures and of linear\ninvariants for any fixed phylogenetic tree (and for all trees -- the so called\n`model invariants'), on any number $n$ of leaves. We also provide a precise\ndescription of the space of mixtures and linear invariants for the special case\nof $n=4$ leaves. By combining techniques from discrete random processes and\n(multi-) linear algebra, our results build on a classic result that was first\nestablished by James Lake in 1987.\n",
        "  Virtual knot theory is a generalization of knot theory which is based on\nGauss chord diagrams and link diagrams on closed oriented surfaces. A twisted\nknot is a generalization of a virtual knot, which corresponds to a link diagram\non a possibly non-orientable surface. In this paper, we discuss an invariant of\ntwisted links which is obtained from the JKSS invariant of virtual links by use\nof double coverings. We also discuss some properties of double covering\ndiagrams.\n",
        "  In projection-based magnetic particle imaging (MPI) with a field-free-line\n(FFL) encoding scheme, projection data are usually acquired by moving the FFL\nin a zigzag and a difference in the projection data occurs depending on the\nscanning direction of FFL, resulting in blurring in the reconstructed images.\nIn this study, we developed a method for correcting the blur by deconvolution\nusing a signal-delay constant ({\\xi}). The {\\xi} value for correction ({\\xi}c)\nwas determined by acquiring projection data in positive and negative directions\nand searching for the {\\xi} value which minimized the 2-norm between the\ndeconvolved projection data in the two directions. We validated our method\nusing a line and A-shaped phantoms for various velocities of FFL (vFFL). The\n{\\xi}c value correlated linearly with vFFL. The full width at half maximum of\nthe line phantom decreased significantly after correction of the blur. The\neffectiveness of our method was also confirmed by the MPI images of the\nA-shaped phantom. These results suggest that our method will be useful for\nenhancing the reliability of projection-based MPI.\n",
        "  In this paper we investigate some connections between Topological Dynamics,\nthe theory of G-Principal Bundles, and the theory of Locally Trivial Groupoids.\n",
        "  Pairwise ranking methods are the basis of many widely used discriminative\ntraining approaches for structure prediction problems in natural language\nprocessing(NLP). Decomposing the problem of ranking hypotheses into pairwise\ncomparisons enables simple and efficient solutions. However, neglecting the\nglobal ordering of the hypothesis list may hinder learning. We propose a\nlistwise learning framework for structure prediction problems such as machine\ntranslation. Our framework directly models the entire translation list's\nordering to learn parameters which may better fit the given listwise samples.\nFurthermore, we propose top-rank enhanced loss functions, which are more\nsensitive to ranking errors at higher positions. Experiments on a large-scale\nChinese-English translation task show that both our listwise learning framework\nand top-rank enhanced listwise losses lead to significant improvements in\ntranslation quality.\n",
        "  Surface properties are examined in a chiral d-wave superconductor with\nhexagonal symmetry, whose one-body Hamiltonian possesses the intrinsic\nspin-orbit coupling identical to the one characterizing the topological nature\nof the Kane-Mele honeycomb insulator. In the normal state spin-orbit coupling\ngives rise to spontaneous surface spin currents, whereas in the superconducting\nstate there exist besides the spin currents also charge surface currents, due\nto the chiral pairing symmetry. Interestingly, the combination of these two\ncurrents results in a surface spin polarization, whose spatial dependence is\nmarkedly different on the zigzag and armchair surfaces. We discuss various\npotential candidate materials, such as SrPtAs, which may exhibit these surface\nproperties.\n",
        "  The singular instanton Floer homology was defined by Kronheimer and Mrowka in\nconnection with their proof that the Khovanov homology is an unknot detector.\nWe study this theory for knots and two-component links using equivariant gauge\ntheory on their double branched covers. We show that the special generator in\nthe singular instanton Floer homology of a knot is graded by the knot signature\nmod 4, thereby providing a purely topological way of fixing the absolute\ngrading in the theory. Our approach also results in explicit computations of\nthe generators and gradings of the singular instanton Floer chain complex for\nseveral classes of knots with simple double branched covers, such as two-bridge\nknots, torus knots, and Montesinos knots, as well as for several families of\ntwo-components links.\n",
        "  The bone quality is asociated with changes in its dielectric properties\n(permittivity and conductivity). The feasibility of detecting changes in these\nproperties is evaluated using a tomographic array of 16 monopole antennas with\nz-polarized microwaves at 1.3GHz. The direct problem was evaluated\ncomputationally with the Finite-Difference-Time-Domain (FDTD) method. Local and\nglobal sensitivity analysis were considered for identifiyng the parameters that\nmost affect the detection. We observed that the direct problem is highly\nsensitive to the conductivity of the tissues that surround the calcaneus and\nthe one of the calcaneus itself. Global and local sensitivity methods have\nshown evidences for feasible detection of variation in dielectric properties of\nbone.\n",
        "  We prove that for any distance at least 3 Heegaard splitting and a boundary\ncomponent $F$, there is a diameter finite ball in the curve complex $\\mathcal\n{C}(F)$ so that it contains all distance degenerate curves or slopes in $F$.\n",
        "  Using a series of computer simulations we have demonstrated a scenario of the\nearly evolution of the bird-type primitive language. We do not assume wise\nagents who can use a grammar and manage an evolution without a grammar.\nDuplication and mutation of phrases is our strategy. Such a strategy is seen in\nwide classes of living phenomena.\n",
        "  A BCS (Bardeen-Cooper-Schrieffer) superconductor, which is placed out of\nequilibrium, can develop quantum instabilities, which manifest themselves in\noscillations of the superconductor's order parameter (pairing amplitude\n$\\Delta$). These instabilities are a manifestations of the Cooper instability.\nInelastic collisions are essential in resolving those instabilities.\nIncorporating the quantum instabilities and collisions in a unified approach\nbased on Richardson's exact solution of the pairing Hamiltonian, we find that a\nBCS superconductor may end up in a state in which the spectrum has more than\none gap.\n",
        "  We present a multi-band model for superconductivity at the metallic interface\nbetween insulating oxides LaAlO$_3$ and SrTiO$_3$ (001). Using a\nself-consistent Bogoliubov-de Gennes theory, formulated with the realistic\nbands at the interface, we investigate the spin-singlet and spin-triplet\npairings in intra-band and inter-band channels. We find that the Rashba and\natomic spin-orbit interactions at the interface induce singlet pairing in the\ninter-band channel and triplet pairing in both the intra-band and inter-band\nchannels when the pairing amplitude in the singlet intra-band channel is\nfinite. The gate-voltage variation of superconductivity is resolved in\ndifferent pairing channels, compared with experimental results and found to\nmatch quite well. Interestingly, an enhancement of the superconducting\ntransition temperature by external in-plane magnetic field is found revealing\nthe existence of a hidden superconducting state above the observed one. As the\ninterface is known to possess high level of inhomogeneity, we explore the role\nof non-magnetic disorder incorporating thermal phase fluctuations by using a\nMonte-Carlo method. We show that even after the transition to the\nnon-superconducting phase, driven by temperature or magnetic field, the\ninterface possesses localized Cooper pairs whose signature was observed in\nprevious experiments.\n",
        "  The amounts of data available to decision makers are increasingly important,\ngiven the network availability, low cost storage and diversity of applications.\nTo maximize the potential of these data within the National Social Security\nFund (NSSF) in Tunisia, we have built a data warehouse as a multidimensional\ndatabase, cleaned, homogenized, historicized and consolidated. We used Oracle\nWarehouse Builder to extract, transform and load the source data into the Data\nWarehouse, by applying the KDD process. We have implemented the Data Warehouse\nas an Oracle OLAP. The knowledge extraction has been performed using the Oracle\nDiscoverer tool. This allowed users to take maximum advantage of knowledge as a\nregular report or as ad hoc queries. We started by implementing the main topic\nfor this public institution, accounting for the movements of insured persons.\nThe great success that has followed the completion of this work has encouraged\nthe NSSF to complete the achievement of other topics of interest within the\nNSSF. We suggest in the near future to use Multidimensional Data Mining to\nextract hidden knowledge and that are not predictable by the OLAP.\n",
        "  Following the analogies between 3-dimensional topology and number theory, we\nstudy an id\\`elic form of class field theory for 3-manifolds. For a certain set\n$\\mathcal{K}$ of knots in a 3-manifold $M$, we first present a local theory for\neach knot in $\\mathcal{K}$, which is analogous to local class field theory, and\nthen, getteing together over all knots in $\\mathcal{K}$, we give an analogue of\nid\\`elic global class field theory for an integral homology sphere $M$.\n",
        "  We describe a new sample of 226 GPS (GHz-Peaked Spectrum) source candidates\nselected using simultaneous 1-22 GHz multi-frequency observations with the\nRATAN-600 radio telescope. Sixty objects in our sample are identified as GPS\nsource candidates for the first time. The candidates were selected on the basis\nof their broad-band radio spectra only. We discuss the spectral and variability\nproperties of selected objects of different optical classes.\n",
        "  Many natural language processing tasks solely rely on sparse dependencies\nbetween a few tokens in a sentence. Soft attention mechanisms show promising\nperformance in modeling local/global dependencies by soft probabilities between\nevery two tokens, but they are not effective and efficient when applied to long\nsentences. By contrast, hard attention mechanisms directly select a subset of\ntokens but are difficult and inefficient to train due to their combinatorial\nnature. In this paper, we integrate both soft and hard attention into one\ncontext fusion model, \"reinforced self-attention (ReSA)\", for the mutual\nbenefit of each other. In ReSA, a hard attention trims a sequence for a soft\nself-attention to process, while the soft attention feeds reward signals back\nto facilitate the training of the hard one. For this purpose, we develop a\nnovel hard attention called \"reinforced sequence sampling (RSS)\", selecting\ntokens in parallel and trained via policy gradient. Using two RSS modules, ReSA\nefficiently extracts the sparse dependencies between each pair of selected\ntokens. We finally propose an RNN/CNN-free sentence-encoding model, \"reinforced\nself-attention network (ReSAN)\", solely based on ReSA. It achieves\nstate-of-the-art performance on both Stanford Natural Language Inference (SNLI)\nand Sentences Involving Compositional Knowledge (SICK) datasets.\n",
        "  With the rapid development of online social media, online shopping sites and\ncyber-physical systems, heterogeneous information networks have become\nincreasingly popular and content-rich over time. In many cases, such networks\ncontain multiple types of objects and links, as well as different kinds of\nattributes. The clustering of these objects can provide useful insights in many\napplications. However, the clustering of such networks can be challenging since\n(a) the attribute values of objects are often incomplete, which implies that an\nobject may carry only partial attributes or even no attributes to correctly\nlabel itself; and (b) the links of different types may carry different kinds of\nsemantic meanings, and it is a difficult task to determine the nature of their\nrelative importance in helping the clustering for a given purpose. In this\npaper, we address these challenges by proposing a model-based clustering\nalgorithm. We design a probabilistic model which clusters the objects of\ndifferent types into a common hidden space, by using a user-specified set of\nattributes, as well as the links from different relations. The strengths of\ndifferent types of links are automatically learned, and are determined by the\ngiven purpose of clustering. An iterative algorithm is designed for solving the\nclustering problem, in which the strengths of different types of links and the\nquality of clustering results mutually enhance each other. Our experimental\nresults on real and synthetic data sets demonstrate the effectiveness and\nefficiency of the algorithm.\n",
        "  We present a machine learning approach that ranked on the first place in the\nArabic Dialect Identification (ADI) Closed Shared Tasks of the 2018 VarDial\nEvaluation Campaign. The proposed approach combines several kernels using\nmultiple kernel learning. While most of our kernels are based on character\np-grams (also known as n-grams) extracted from speech or phonetic transcripts,\nwe also use a kernel based on dialectal embeddings generated from audio\nrecordings by the organizers. In the learning stage, we independently employ\nKernel Discriminant Analysis (KDA) and Kernel Ridge Regression (KRR).\nPreliminary experiments indicate that KRR provides better classification\nresults. Our approach is shallow and simple, but the empirical results obtained\nin the 2018 ADI Closed Shared Task prove that it achieves the best performance.\nFurthermore, our top macro-F1 score (58.92%) is significantly better than the\nsecond best score (57.59%) in the 2018 ADI Shared Task, according to the\nstatistical significance test performed by the organizers. Nevertheless, we\nobtain even better post-competition results (a macro-F1 score of 62.28%) using\nthe audio embeddings released by the organizers after the competition. With a\nvery similar approach (that did not include phonetic features), we also ranked\nfirst in the ADI Closed Shared Tasks of the 2017 VarDial Evaluation Campaign,\nsurpassing the second best method by 4.62%. We therefore conclude that our\nmultiple kernel learning method is the best approach to date for Arabic dialect\nidentification.\n",
        "  A confidence measure is able to estimate the reliability of an hypothesis\nprovided by a machine translation system. The problem of confidence measure can\nbe seen as a process of testing : we want to decide whether the most probable\nsequence of words provided by the machine translation system is correct or not.\nIn the following we describe several original word-level confidence measures\nfor machine translation, based on mutual information, n-gram language model and\nlexical features language model. We evaluate how well they perform individually\nor together, and show that using a combination of confidence measures based on\nmutual information yields a classification error rate as low as 25.1% with an\nF-measure of 0.708.\n",
        "  SentiWordNet is an important lexical resource supporting sentiment analysis\nin opinion mining applications. In this paper, we propose a novel approach to\nconstruct a Vietnamese SentiWordNet (VSWN). SentiWordNet is typically generated\nfrom WordNet in which each synset has numerical scores to indicate its opinion\npolarities. Many previous studies obtained these scores by applying a machine\nlearning method to WordNet. However, Vietnamese WordNet is not available\nunfortunately by the time of this paper. Therefore, we propose a method to\nconstruct VSWN from a Vietnamese dictionary, not from WordNet. We show the\neffectiveness of the proposed method by generating a VSWN with 39,561 synsets\nautomatically. The method is experimentally tested with 266 synsets with aspect\nof positivity and negativity. It attains a competitive result compared with\nEnglish SentiWordNet that is 0.066 and 0.052 differences for positivity and\nnegativity sets respectively.\n",
        "  We have developed a superconducting phase gradiometer consisting of two\nparallel DNA-templated nanowires connecting two thin-film leads. We have ramped\nthe cross current flowing perpendicular to the nanowires, and observed\noscillations in the lead-to-lead resistance due to cross-current-induced phase\ndifferences. By using this gradiometer we have measured the temperature and\nmagnetic field dependence of the superfluid density and observed an\namplification of phase gradients caused by elastic vortex displacements. We\nexamine our data in light of Miller-Bardeen theory of dirty superconductors and\na microscale version of Campbell's model of field penetration.\n",
        "  In this article we establish the relation between the spines of 3-manifolds\nand the polyhedra with identified faces. We do this by showing that the spines\nof the closed, connected, orientable 3-manifolds can be presented through\npolyhedra with identified faces in a very natural way. We also prove the\nequivalence between the special spines and a certain type of polyhedra, and\nother related results.\n",
        "  The present paper pertains to corrections which are due to the presence of\nbeam-limiting and beam-shaping devices in proton planning. Two types of\ncorrections are considered: those which are due to the nonzero thickness of\nsuch devices (geometrical effects) and those relating to the scattering of beam\nparticles off their material. The application of these two types of corrections\nis greatly facilitated by decomposing the physical effects (i.e., the\ncontribution to the fluence) of two-dimensional objects (i.e., of the apertures\nof the devices) into one-dimensional, easily-calculable contributions. To\nminimise the time requirements in the derivation of the scattering corrections,\na two-step process is introduced. The first step occurs at beam-configuration\nphase and comprises the analysis of half-block fluence measurements and the\nextraction of the one parameter of the model which is used in the description\nof the beamline characteristics; subsequently, a number of Monte-Carlo runs\nlead to the determination of the parameters of a convenient parameterisation of\nthe relevant fluence contributions. The second step involves (at planning time)\nthe reconstruction of the parameters (which are used in the description of the\nscattering contributions) via simple interpolations, performed on the results\nobtained during the beam-configuration phase. It is shown that the inclusion of\nthe scattering effects leads to substantial improvement in the reproduction of\nthe experimental data. The contributions from the block-thickness and\nblock-scattering effects have been presented separately in the case of a simple\nwater phantom. In this example, the maximal contribution of the block-relating\neffects amounts to a few percent of the prescribed dose.\n",
        "  We propose an SEIR model for the populations and an SEI model for the vector\nto describe the transmission dynamics of a four-strain model with both primary\nand secondary dengue infections. In order to accomplish this, we propose and\nobtain an analytic solution of a system of 47 coupled differential equations.\nThis would be the most complete epidemic model proposed to describe the dengue\nepidemic.\n",
        "  Electrons confined to two dimensions display an unexpected diversity of\nbehaviors as they are cooled to absolute zero. Noninteracting electrons are\npredicted to eventually \"localize\" into an insulating ground state, and it has\nlong been supposed that electron correlations stabilize only one other phase:\nsuperconductivity. However, many two-dimensional (2D) superconducting materials\nhave shown surprising evidence for metallic behavior, where the electrical\nresistivity saturates in the zero-temperature limit, the nature of this\nunexpected metallic state remains under intense scrutiny. We report electrical\ntransport properties for two disordered 2D superconductors, indium oxide and\ntantalum nitride, and observe a magnetic field-tuned transition from a true\nsuperconductor to a metallic phase with saturated resistivity. This metallic\nphase is characterized by a vanishing Hall resistivity, suggesting that it\nretains particle-hole symmetry from the disrupted superconducting state.\n",
        "  We prove an analogue of Farb-Masur's theorem that the length-spectra metric\non moduli space is \"almost isometric\" to a simple model $\\mathcal {V}(S)$ which\nis induced by the cone metric over the complex of curves. As an application, we\nknow that the Teichm\\\"{u}ller metric and the length-spectra metric are \"almost\nisometric\" on moduli space, while they are not even quasi-isometric on\nTeichm\\\"{u}ller space.\n",
        "  Knowledge of regional net primary productivity (NPP) is important for the\nsystematic understanding of the global carbon cycle. In this study,\nmulti-source data were employed to conduct a 33-year regional NPP study in\nsouthwest China, at a 1-km scale. A multi-sensor fusion framework was applied\nto obtain a new normalized difference vegetation index (NDVI) time series from\n1982 to 2014, combining the respective advantages of the different remote\nsensing datasets. As another key parameter for NPP modeling, the total solar\nradiation was calculated by the improved Yang hybrid model (YHM), using\nmeteorological station data. The verification described in this paper proved\nthe feasibility of all the applied data processes, and a greatly improved\naccuracy was obtained for the NPP calculated with the final processed NDVI. The\nspatio-temporal analysis results indicated that 68.07% of the study area showed\nan increasing NPP trend over the past three decades. Significant heterogeneity\nwas found in the correlation between NPP and precipitation at a monthly scale,\nspecifically, the negative correlation in the growing season and the positive\ncorrelation in the dry season. The lagged positive correlation in the growing\nseason and no lag in the dry season indicated the important impact of\nprecipitation on NPP.\n",
        "  The discoveries of superconductivity in heavily boron-doped diamond, silicon\nand silicon carbide renewed the interest in the ground states of charge-carrier\ndoped wide-gap semiconductors. Recently, aluminium doping in silicon carbide\nsuccessfully yielded a metallic phase from which at high aluminium\nconcentrations superconductivity emerges. Here, we present a specific-heat\nstudy on superconducting aluminium-doped silicon carbide. We observe a clear\njump anomaly at the superconducting transition temperature 1.5 K indicating\nthat aluminium-doped silicon carbide is a bulk superconductor. An analysis of\nthe jump anomaly suggests BCS-like phonon-mediated superconductivity in this\nsystem.\n",
        "  We give a refined upper bound for the hyperbolic volume of an alternating\nlink in terms of the first three and the last three coefficients of its colored\nJones polynomial.\n",
        "  We consider a buckled quantum spin Hall insulator (QSHI), such as silicene,\nproximity-coupled to a conventional spin-singlet, s-wave superconductor. Even\nlimiting the discussion to the disorder-robust s-wave pairing symmetry, we find\nboth odd-frequency ($\\omega$), spin-singlet and spin-triplet pair amplitudes\nand where both preserve time-reversal symmetry. Our results show that there are\ntwo unrelated mechanisms generating these different odd-$\\omega$ pair\namplitudes. The spin-singlet state is due to the strong inter-orbital processes\npresent in the QSHI. It is exists generically at the edges of the QSHI, but\nalso in the bulk in heavily doped regime if an electric field is applied. The\nspin-triplet state requires a finite gradient in the proximity-induced\nsuperconducting order along the edge, which we find is automatically generated\nat the atomic scale for armchair edges but not at zigzag edges. In combination\nthese results make superconducting QSHIs a very exciting venue for\ninvestigating not only the existence of odd-$\\omega$ superconductivity, but\nalso the interplay between different odd-$\\omega$ states.\n",
        "  We study the $r$-near neighbors reporting problem ($r$-NN), i.e., reporting\n\\emph{all} points in a high-dimensional point set $S$ that lie within a radius\n$r$ of a given query point $q$. Our approach builds upon on the\nlocality-sensitive hashing (LSH) framework due to its appealing asymptotic\nsublinear query time for near neighbor search problems in high-dimensional\nspace. A bottleneck of the traditional LSH scheme for solving $r$-NN is that\nits performance is sensitive to data and query-dependent parameters. On\ndatasets whose data distributions have diverse local density patterns, LSH with\ninappropriate tuning parameters can sometimes be outperformed by a simple\nlinear search.\n  In this paper, we introduce a hybrid search strategy between LSH-based search\nand linear search for $r$-NN in high-dimensional space. By integrating an\nauxiliary data structure into LSH hash tables, we can efficiently estimate the\ncomputational cost of LSH-based search for a given query regardless of the data\ndistribution. This means that we are able to choose the appropriate search\nstrategy between LSH-based search and linear search to achieve better\nperformance. Moreover, the integrated data structure is time efficient and fits\nwell with many recent state-of-the-art LSH-based approaches. Our experiments on\nreal-world datasets show that the hybrid search approach outperforms (or is\ncomparable to) both LSH-based search and linear search for a wide range of\nsearch radii and data distributions in high-dimensional space.\n",
        "  Most conspicuous organisms are multicellular and most multicellular organisms\ndevelop somatic cells to perform specific, non-reproductive tasks. The ubiquity\nof this division of labor suggests that it is highly advantageous. In this\npaper, I present a model to study the evolution of specialized cells. The model\nallows for unicellular and multicellular organisms that may contain somatic\n(terminally differentiated) cells. Cells contribute additively to a\nquantitative trait. The fitness of the organism depends on this quantitative\ntrait (via a benefit function), the size of the organism, and the number of\nsomatic cells. This model allows one to determine when somatic cells are\nadvantageous and to calculate the optimum number (or fraction) of reproductive\ncells. I show that the fraction of reproductive cells is always surprisingly\nhigh. If somatic cells are very small they can outnumber reproductive cells but\ntheir biomass is still less than the biomass of reproductive cells. Only for\nconvex benefit functions can the biomass of somatic cell exceed the biomass of\nreproductive cells. I discuss the biology of primitive multicellular organisms\nwith respect to the model predictions. I find good agreement and outline how\nthis work can be used to guide further quantitative studies of\nmulticellularity.\n",
        "  The problem of privately releasing data is to provide a version of a dataset\nwithout revealing sensitive information about the individuals who contribute to\nthe data. The model of differential privacy allows such private release while\nproviding strong guarantees on the output. A basic mechanism achieves\ndifferential privacy by adding noise to the frequency counts in the contingency\ntables (or, a subset of the count data cube) derived from the dataset. However,\nwhen the dataset is sparse in its underlying space, as is the case for most\nmulti-attribute relations, then the effect of adding noise is to vastly\nincrease the size of the published data: it implicitly creates a huge number of\ndummy data points to mask the true data, making it almost impossible to work\nwith.\n  We present techniques to overcome this roadblock and allow efficient private\nrelease of sparse data, while maintaining the guarantees of differential\nprivacy. Our approach is to release a compact summary of the noisy data.\nGenerating the noisy data and then summarizing it would still be very costly,\nso we show how to shortcut this step, and instead directly generate the summary\nfrom the input data, without materializing the vast intermediate noisy data. We\ninstantiate this outline for a variety of sampling and filtering methods, and\nshow how to use the resulting summary for approximate, private, query\nanswering. Our experimental study shows that this is an effective, practical\nsolution, with comparable and occasionally improved utility over the costly\nmaterialization approach.\n",
        "  Noise characterization in MRI has multiple applications, including quality\nassurance and protocol optimization. It is particularly important in the\npresence of parallel imaging acceleration, where the noise distribution can\ncontain severe spatial heterogeneities. If the parallel imaging reconstruction\nis a linear process, an exact noise analysis is possible by taking into account\nthe correlations between all the samples involved. However, for k-space based\ntechniques like GRAPPA, the exact analysis has been considered computationally\nprohibitive due to the very large size of the noise covariance matrices\nrequired to characterize the noise propagation from k-space to image-space.\nPrevious methods avoid this computational burden by approximating the GRAPPA\nreconstruction as a pixel-wise linear operation performed in the image-space.\nHowever, these methods are not exact in the presence of non-uniform k-space\nundersampling (e.g.: containing a calibration region). For this reason, in this\nwork we develop an exact characterization of the noise distribution for\nself-calibrated parallel imaging in the presence of arbitrary Cartesian\nundersampling patterns. By exploiting the symmetries and separability in the\nnoise propagation process, the proposed method is computationally efficient and\ndoes not require large matrices. In this manuscript, we present the proposed\nnoise characterization method and compare it to previous techniques using\nMonte-Carlo simulations as well as phantom acquisitions.\n",
        "  We present a microscopic theory of dc Josephson current, based on the\nconstruction of a coherent temperature Green function in the tight-binding\napproximation, in junctions with multiband superconductors. This theory is\napplied to the junctions with multiband Fe-based superconductors (FeBS)\ndescribed by anisotropic s-wave order parameter symmetries, which probably\nrealized in FeBS. We confirm microscopically the previously suggested crucial\nexperiment for determination of the type of the order parameter symmetry in\nFeBS.\n",
        "  The microwave thermometry method for the diagnosis of breast cancer is based\non an analysis of the internal temperature distribution.This paper is devoted\nto the construction of a mathematical model for increasing the accuracy of\nmeasuring the internal temperature of mammary glands, which are regarded as a\ncomplex combination of several components, such as fat tissue, muscle tissue,\nmilk lobules, skin, blood flows, tumor tissue. Each of these biocomponents is\ndetermined by its own set of physical parameters. Our numerical model is\ndesigned to calculate the spatial distributions of the electric microwave field\nand the temperature inside the biological tissue. We compare the numerical\nsimulations results to the real medical measurements of the internal\ntemperature.\n",
        "  Trapped low magnetic flux dynamics including local ones are investigated in\nYBCO single crystals in the strong thermal fluctuations domain near the\nsuperconducting phase transition temperatures. The essential difference from\nquasi-logarithmic isothermal relaxation of a magnetization behavior (observed\nearlier in high magnetic fields) was established. Within the framework of\nclassic model of a thermal activated vortices creep the estimation of an\neffective pinning potential was made at these temperatures.\n",
        "  OPTIONAL is a key feature in SPARQL for dealing with missing information.\nWhile this operator is used extensively, it is also known for its complexity,\nwhich can make efficient evaluation of queries with OPTIONAL challenging. We\ntackle this problem in the Ontology-Based Data Access (OBDA) setting, where the\ndata is stored in a SQL relational database and exposed as a virtual RDF graph\nby means of an R2RML mapping. We start with a succinct translation of a SPARQL\nfragment into SQL. It fully respects bag semantics and three-valued logic and\nrelies on the extensive use of the LEFT JOIN operator and COALESCE function. We\nthen propose optimisation techniques for reducing the size and improving the\nstructure of generated SQL queries. Our optimisations capture interactions\nbetween JOIN, LEFT JOIN, COALESCE and integrity constraints such as attribute\nnullability, uniqueness and foreign key constraints. Finally, we empirically\nverify effectiveness of our techniques on the BSBM OBDA benchmark.\n",
        "  With the ever-growing availability of so-called complex data, especially on\nthe Web, decision-support systems such as data warehouses must store and\nprocess data that are not only numerical or symbolic. Warehousing and analyzing\nsuch data requires the joint exploitation of metadata and domain-related\nknowledge, which must thereby be integrated. In this paper, we survey the types\nof knowledge and metadata that are needed for managing complex data, discuss\nthe issue of knowledge and metadata integration, and propose a CWM-compliant\nintegration solution that we incorporate into an XML complex data warehousing\nframework we previously designed.\n",
        "  The high field phase diagram and magnetic properties of CeCoIn_5 below\nH_c2(0) are examined from the picture regarding the high field and low\ntemperature (HFLT) phase as a possible Fulde-Ferrell-Larkin-Ovchinnikov (FFLO)\nvortex lattice. Crucial roles of antiferromagnetic (AFM) fluctuations enhanced\nclose to H_c2(0) are stressed. The FFLO vortex lattice with a longitudinal\nmodulation parallel to the field is stabilized compared with those with lateral\nmodulations as a consequence of the presence of AFM fluctuations. Further, an\nunusual field-induced enhancement of the flux distribution is argued to be a\nconsequence of interplay between the paramagnetic depairing and AFM\nfluctuation, both of which are enhanced with increasing field.\n",
        "  We consider what is the best way to extract science from large surveys of the\nMilky Way galaxy. The diversity of data gathered in these surveys, together\nwith our position within the Galaxy, imply that science must be extracted by\nfitting dynamical models to the data in the space of the observables. Models\nbased on orbital tori promise to be superior for this task than traditional\ntypes of models, such as N-body models and Schwarzschild models. A formalism\nthat allows such models to be fitted to data is developed and tested on\npseudodata of varying richness.\n",
        "  We aim to understand the accretion history of the Milky Way by exploring the\nvertical and radial properties of the Galactic thick disc.\n  We study the chemical and kinematic properties of roughly a thousand spectra\nof faint magnitude foreground Galactic stars observed serendipitously during\nextra-galactic surveys in four lines-of-sight: three in the southern Galactic\nhemisphere (surveys of the Carina, Fornax and Sculptor dwarf spheroidal\ngalaxies) and one in the northern Galactic hemisphere (a survey of the Sextans\ndwarf spheroidal galaxy). The foreground stars span distances up to 3kpc from\nthe Galactic plane and Galactocentric radii up to 11kpc.\n  Only three lines-of-sight have a sufficient number of foreground stars for a\nrobust analysis. Towards Sextans in the Northern Galactic hemisphere and\nSculptor in the South, we measure a consistent decrease in mean metallicity\nwith height from the Galactic plane, suggesting a chemically symmetric thick\ndisc. This decrease can either be due to an intrinsic thick disc metallicity\ngradient, or simply due to a change in the thin disc/thick disc population\nratio and no intrinsic metallicity gradients for the thick disc. We favour the\nlatter explanation. In contrast, we find evidence of an unpredicted metal-poor\npopulation in the direction of Carina. This population was earlier detected by\nWyse et al. (2006), but our more detailed analysis provides robust estimates of\nits location (|Z|<1kpc), metallicity (-2<[M/H]<-1 dex) and azimuthal orbital\nvelocity (V_phi ~120 km/s).\n  Given the chemo-dynamical properties of the over-density towards the Carina\nline-of-sight, we suggest that it represents the metal-poor tail of the\ncanonical thick disc. In spite of the small number of stars available, we\nsuggest that this metal-weak thick disc follows the often suggested thick disc\nvelocity-metallicity correlation of dV_phi/d[M/H]~40-50km/s/dex.\n",
        "  Starting from common assumptions, we build a rate equation model for\nmulti-strain disease dynamics in terms of immune repertoire classes. We then\nmove to a strain-level description where a low-order closure reminiscent of a\npair approximation can be applied. We characterize the endemic equilibrium of\nthe ensuing model in the absence of mutation and discuss the presence of\ndegeneracy regarding the prevalence of the different strains. Finally we study\nthe behavior of the system under the injection of mutant strains.\n",
        "  Chenopodium album seedling emergence studies were conducted at nine European\nand two North American locations comparing local populations with a common\npopulation from Denmark. It is hypothesized that C. album seedling recruitment\ntiming and magnitude have adapted to environmental and cropping system\npractices of a locality. Limitations in the habitat (filter 1) were reflected\nin local C. album population recruitment season length. Generally, the duration\nof seedling recruitment of both populations (local; DEN-COM) increased with\ndecreasing latitude, north-to-south. In general, compared to the local\npopulation, DEN-COM recruitment at locations north of Denmark was longer and\nsouth of Denmark was shorter, and ended sooner. Generally, the local cropping\nsystem disturbances (CSD) period increased with decreasing latitude. The total\nduration of the CSD period was over twice as long in the south as that in the\nnorth. Recruitment at each locality possessed seasonal structure (time, number)\nconsisting of 2-4 discrete seasonal cohorts. This may be an adaptive means by\nwhich C. album searches for, and exploits, recruitment opportunity just prior\nto, and after, predictable disturbances. The control of C. album seedling\nemergence is contained in the heteroblastic traits of its locally adapted\nseeds, and is stimulated by a complex interaction of light, heat, water,\nnitrate and oxygen signals inherent in the local environment. Our observations\nof complex recruitment patterns occurring at critical cropping times is strong\nevidence that C. album possesses a flexible and sensitive germination\nregulation system adaptable to opportunity in many different Eurasian and North\nAmerican agricultural habitats.\n",
        "  We review the major progress in the rigorous analysis of the classical\nquasispecies model that usually comes in two related but different forms: the\nEigen model and the Crow--Kimura model. The model itself was formulated almost\n50 years ago, and in its stationary form represents an easy to formulate\neigenvalue problem. Notwithstanding the simplicity of the problem statement, we\nstill lack full understanding of the behavior of the mean population fitness\nand the quasispecies distribution for an arbitrary fitness landscape. Our main\ngoal in this review is two-fold: First, to highlight a number of impressive\nmathematical results, including some of the recent ones, which pertain to the\nmathematical development of the quasispecies theory. Second, to emphasize that,\ndespite these 50 years of vigorous research, there are still very natural both\nbiological and mathematical questions that remain to be addressed within the\nquasispecies framework. Our hope is that at least some of the approaches we\nreview in this text can be of help for anyone embarking on further analysis of\nthe quasispecies model.\n",
        "  The inheritance of characteristics induced by the environment has often been\nopposed to the theory of evolution by natural selection. Yet, while evolution\nby natural selection requires new heritable traits to be produced and\ntransmitted, it does not prescribe, per se, the mechanisms by which this is\noperated. The mechanisms of inheritance are not, however, unconstrained, since\nthey are themselves subject to natural selection. We introduce a general,\nanalytically solvable mathematical model to compare the adaptive value of\ndifferent schemes of inheritance. Our model allows for variations to be\ninherited, randomly produced, or environmentally induced, and, irrespectively,\nto be either transmitted or not during reproduction. The adaptation of the\ndifferent schemes for processing variations is quantified for a range of\nfluctuating environments, following an approach that links quantitative\ngenetics with stochastic control theory.\n",
        "  Access limitations are restrictions in the way in which the tuples of a\nrelation can be accessed. Under access limitations, query answering becomes\nmore complex than in the traditional case, with no guarantee that the answer\ntuples that can be extracted (aka maximal answer) are all those that would be\nfound without access limitations (aka complete answer). The field of query\nanswering under access limitations has been broadly investigated in the past.\nAttention has been devoted to the problem of determining relations that are\nrelevant for a query, i.e., those (possibly off-query) relations that might\nneed to be accessed in order to find all tuples in the maximal answer. In this\nshort paper, we show that relevance is undecidable for Datalog queries.\n",
        "  We consider the problem of estimating the $2D$ vector displacement field in a\nheterogeneous elastic solid deforming under plane stress conditions. The\nproblem is motivated by applications in quasistatic elastography. From precise\nand accurate measurements of one component of the $2D$ vector displacement\nfield and very limited information of the second component, the method\nreconstructs the second component quite accurately. No a priori knowledge of\nthe heterogeneous distribution of material properties is required. This method\nrelies on using a special form of the momentum equations to filter ultrasound\ndisplacement measurements to produce more precise estimates. We verify the\nmethod with applications to simulated displacement data. We validate the method\nwith applications to displacement data measured from a tissue mimicking\nphantom, and in-vivo data; significant improvements are noticed in the filtered\ndisplacements recovered from all the tests. In verification studies, error in\nlateral displacement estimates decreased from about $50\\%$ to about $2\\%$, and\nstrain error decreased from more than $250\\%$ to below $2\\%$.\n",
        "  The sequence to sequence architecture is widely used in the response\ngeneration and neural machine translation to model the potential relationship\nbetween two sentences. It typically consists of two parts: an encoder that\nreads from the source sentence and a decoder that generates the target sentence\nword by word according to the encoder's output and the last generated word.\nHowever, it faces to the cold start problem when generating the first word as\nthere is no previous word to refer. Existing work mainly use a special start\nsymbol </s>to generate the first word. An obvious drawback of these work is\nthat there is not a learnable relationship between words and the start symbol.\nFurthermore, it may lead to the error accumulation for decoding when the first\nword is incorrectly generated. In this paper, we proposed a novel approach to\nlearning to generate the first word in the sequence to sequence architecture\nrather than using the start symbol. Experimental results on the task of\nresponse generation of short text conversation show that the proposed approach\noutperforms the state-of-the-art approach in both of the automatic and manual\nevaluations.\n",
        "  We compute, from first principles, the absolute dose or fluence distribution\nper incident proton charge in a known heterogeneous terrain exposed to known\nproton beams. The algorithm is equally amenable to scattered or scanned beams.\nAll objects in the terrain (including collimators) are sliced into slabs, of\nany convenient thickness, perpendicular to the nominal beam direction.\n  Transport is by standard Fermi-Eyges theory. Transverse heterogeneities are\nhandled by breaking up pencil beams (PBs) either by conventional redefinition\nor a new form of 2D recursive dynamic splitting: the mother PB is replaced,\nconserving emittance and charge, by seven daughters of equal transverse size.\nOne has 1/4 the charge and travels in the mother's direction and six have 1/8\nthe charge, are arranged hexagonally and radiate from the mother's virtual\npoint source.\n  The longitudinal (energy-like) variable is pv (proton momentum times speed).\nEach material encountered is treated on its own merits, not referenced to\nwater. Slowing down is handled by $R=a(pv)^b$ fitted to standard range-energy\ntables. Multiple Coulomb scattering is handled by a scattering power\n$T_\\mathrm{dM}$ acting as a proxy for Moliere theory. We present examples from\na proof-of-principle Fortran program and speculate about putting the algorithm\ninto practice.\n",
        "  Indices and materialized views are physical structures that accelerate data\naccess in data warehouses. However, these data structures generate some\nmaintenance overhead. They also share the same storage space. The existing\nstudies about index and materialized view selection consider these structures\nseparately. In this paper, we adopt the opposite stance and couple index and\nmaterialized view selection to take into account the interactions between them\nand achieve an efficient storage space sharing. We develop cost models that\nevaluate the respective benefit of indexing and view materialization. These\ncost models are then exploited by a greedy algorithm to select a relevant\nconfiguration of indices and materialized views. Experimental results show that\nour strategy performs better than the independent selection of indices and\nmaterialized views.\n",
        "  Let N be a closed irreducible 3-manifold and assume N is not a graph\nmanifold. We improve for all but finitely many S^1-bundles M over N the\nadjunction inequality for the minimal complexity of embedded surfaces. This\nallows us to completely determine the minimal complexity of embedded surfaces\nin all but finitely many S^1-bundles over a large class of 3-manifolds.\n",
        "  We present a model for semantic proto-role labeling (SPRL) using an adapted\nbidirectional LSTM encoding strategy that we call \"Neural-Davidsonian\":\npredicate-argument structure is represented as pairs of hidden states\ncorresponding to predicate and argument head tokens of the input sequence. We\ndemonstrate: (1) state-of-the-art results in SPRL, and (2) that our network\nnaturally shares parameters between attributes, allowing for learning new\nattribute types with limited added supervision.\n",
        "  Despite a substantial progress made in developing new sentiment lexicon\ngeneration (SLG) methods for English, the task of transferring these approaches\nto other languages and domains in a sound way still remains open. In this\npaper, we contribute to the solution of this problem by systematically\ncomparing semi-automatic translations of common English polarity lists with the\nresults of the original automatic SLG algorithms, which were applied directly\nto German data. We evaluate these lexicons on a corpus of 7,992 manually\nannotated tweets. In addition to that, we also collate the results of\ndictionary- and corpus-based SLG methods in order to find out which of these\nparadigms is better suited for the inherently noisy domain of social media. Our\nexperiments show that semi-automatic translations notably outperform automatic\nsystems (reaching a macro-averaged F1-score of 0.589), and that\ndictionary-based techniques produce much better polarity lists as compared to\ncorpus-based approaches (whose best F1-scores run up to 0.479 and 0.419\nrespectively) even for the non-standard Twitter genre.\n",
        "  In this paper, we report a knowledge-based method for Word Sense\nDisambiguation in the domains of biomedical and clinical text. We combine word\nrepresentations created on large corpora with a small number of definitions\nfrom the UMLS to create concept representations, which we then compare to\nrepresentations of the context of ambiguous terms. Using no relational\ninformation, we obtain comparable performance to previous approaches on the\nMSH-WSD dataset, which is a well-known dataset in the biomedical domain.\nAdditionally, our method is fast and easy to set up and extend to other\ndomains. Supplementary materials, including source code, can be found at https:\n//github.com/clips/yarn\n",
        "  Ac susceptibility and static magnetization measurements were performed in the\noptimally doped SmFeAsO$_{0.8}$F$_{0.2}$ superconductor. The field -\ntemperature phase diagram of the superconducting state was drawn and, in\nparticular, the features of the flux line lattice derived. The dependence of\nthe intra-grain depinning energy on the magnetic field intensity was derived in\nthe thermally-activated flux creep framework, enlightening a typical 1/H\ndependence in the high-field regime. The intra-grain critical current density\nwas extrapolated in the zero temperature and zero magnetic field limit, showing\na remarkably high value $J_{c0}(0) \\sim 2 \\cdot 10^{7}$ A/cm$^{2}$, which\ndemonstrates that this material is rather interesting for the potential future\ntechnological applications.\n",
        "  Performance evaluation is a key issue for designers and users of Database\nManagement Systems (DBMSs). Performance is generally assessed with software\nbenchmarks that help, e.g., test architectural choices, compare different\ntechnologies or tune a system. In the particular context of data warehousing\nand On-Line Analytical Processing (OLAP), although the Transaction Processing\nPerformance Council (TPC) aims at issuing standard decision-support benchmarks,\nfew benchmarks do actually exist. We present in this chapter the Data Warehouse\nEngineering Benchmark (DWEB), which allows generating various ad-hoc synthetic\ndata warehouses and workloads. DWEB is fully parameterized to fulfill various\ndata warehouse design needs. However, two levels of parameterization keep it\nrelatively easy to tune. We also expand on our previous work on DWEB by\npresenting its new Extract, Transform, and Load (ETL) feature as well as its\nnew execution protocol. A Java implementation of DWEB is freely available\non-line, which can be interfaced with most existing relational DMBSs. To the\nbest of our knowledge, DWEB is the only easily available, up-to-date benchmark\nfor data warehouses.\n",
        "  Dose distribution (depthwise and laterally) to organs outside the\nradiotherapy treatment field can be significant and therefore is of clinical\ninterest from the radiation protection point of view. In the present work,\nmeasurements were performed in a locally fabricated polystyrene phantom using\nTLD chips (LiF-100) for different teletherapy units (Cobalt-60 gamma ray, 120\nkVp X-ray and 250 kVp X-ray) to estimate the dose distribution at distances up\nto 40 cm from the field edge along the central axes of the field size. Finally,\nthe dose distribution for Cobalt-60 beam energy is parameterized as a function\nof depth, distance from field edge, and field size and shape.\n",
        "  We propose MVCNN, a convolution neural network (CNN) architecture for\nsentence classification. It (i) combines diverse versions of pretrained word\nembeddings and (ii) extracts features of multigranular phrases with\nvariable-size convolution filters. We also show that pretraining MVCNN is\ncritical for good performance. MVCNN achieves state-of-the-art performance on\nfour tasks: on small-scale binary, small-scale multi-class and largescale\nTwitter sentiment prediction and on subjectivity classification.\n",
        "  We describe a modular system for generating sentences from formal definitions\nof underlying linguistic structures using domain-specific languages. The system\nuses Java in general, Prolog for lexical entries and custom domain-specific\nlanguages based on Functional Grammar and Functional Discourse Grammar\nnotation, implemented using the ANTLR parser generator. We show how linguistic\nand technological parts can be brought together in a natural language\nprocessing system and how domain-specific languages can be used as a tool for\nconsistent formal notation in linguistic description.\n",
        "  In this paper we study the common distance between points and the behavior of\na constant length step discrete random walk on finite area hyperbolic surfaces.\nWe show that if the second smallest eigenvalue of the Laplacian is at least\n1/4, then the distances on the surface are highly concentrated around the\nminimal possible value, and that the discrete random walk exhibits cutoff. This\nextends the results of Lubetzky and Peres ([20]) from the setting of Ramanujan\ngraphs to the setting of hyperbolic surfaces. By utilizing density theorems of\nexceptional eigenvalues from [27], we are able to show that the results apply\nto congruence subgroups of $SL_{2}\\left(\\mathbb{Z}\\right)$ and other arithmetic\nlattices, without relying on the well known conjecture of Selberg ([28]).\n  Conceptually, we show the close relation between the cutoff phenomenon and\ntemperedness of representations of algebraic groups over local fields, partly\nanswering a question of Diaconis ([7]), who asked under what general phenomena\ncutoff exists.\n",
        "  We show that there are only finitely many homogeneous links whose Conway\npolynomial has any given degree. Using this we give an example of an\ninhomogeneous, fibred knot. Secondly, we show how to compute the monodromy of a\nhomogeneous link complement from a homogeneous braid word representative.\n",
        "  Nonreciprocal charge transport phenomena are studied theoretically for\ntwo-dimensional noncentrosymmetric superconductors under an external magnetic\nfield $B$. Rashba superconductors, surface superconductivity on the surface of\nthree-dimensional topological insulators, and transition metal dichalcogenides\n(TMD) are representative systems, and the current-voltage $I$-$V$\ncharacteristics, i.e., $V=V(I)$, for each of them is analyzed. $V(I)$ can be\nexpanded with respect to the current $I$ as $V(I)= \\sum_{j=1,\\infty} a_j(B,T)\nI^j$, and the $(B,T)$-dependence of $a_j$ depends on the mechanism of the\ncharge transport. Above the mean field transition temperature $T_0$, the\nfluctuation of the superconducting order parameter gives the additional\nconductivity, i.e., paraconductivity. Extending the analysis to the nonlinear\nresponse, we obtain the nonreciprocal charge transport expressed by $a_2(B,T) =\na_1(T) \\gamma(T) B$, where $\\gamma$ converges to a finite value at $T=T_0$.\nBelow $T_0$, the vortex motion is relevant to the voltage drop, and the\ndependence of $a_j$ on $B,T$ is different depending on the system and\nmechanisms. For the superconductors under the in-plane magnetic field, the\nKosterlitz-Thouless (KT) transition occurs at $T_{\\rm KT}$. In this case\n$\\gamma$ has the characteristic temperature dependences such as $\\gamma \\sim\n(T-T_{\\rm KT})^{-3/2}$ near $T_{\\rm KT}$. On the other hand, for TMD with\nout-plane magnetic field, the KT transition is gone, and there are two possible\nmechanisms for the nonreciprocal response. One is the anisotropy of the damping\nconstant for the motion of the vortex. In this case, $a_1(B) \\sim B$ and\n$a_2(B) \\sim B^2$. The other one is the ratchet potential acting on the vortex\nmotion, which gives $a_1(B) \\sim B$ and $a_2(B) \\sim B$. Based on these\nresults, we propose the experiments to identify the mechanism of the\nnonreciprocal charge transport.\n",
        "  This note has an experimental nature and contains no new theorems.\n  We introduce certain moves for classical knot diagrams that for all the very\nmany examples we have tested them on give a monotonic complete simplification.\nA complete simplification of a knot diagram D is a sequence of moves that\ntransform D into a diagram D' with the minimal possible number of crossings for\nthe isotopy class of the knot represented by D. The simplification is monotonic\nif the number of crossings never increases along the sequence. Our moves are\ncertain Z1, Z2, Z3 generalizing the classical Reidemeister moves R1, R2, R3,\nand another one C (together with a variant) aimed at detecting whether a knot\ndiagram can be viewed as a connected sum of two easier ones.\n  We present an accurate description of the moves and several results of our\nimplementation of the simplification procedure based on them, publicly\navailable on the web.\n",
        "  Complex event processing (CEP) is a prominent technology used in many modern\napplications for monitoring and tracking events of interest in massive data\nstreams. CEP engines inspect real-time information flows and attempt to detect\ncombinations of occurrences matching predefined patterns. This is done by\ncombining basic data items, also called primitive events, according to a\npattern detection plan, in a manner similar to the execution of multi-join\nqueries in traditional data management systems. Despite this similarity, little\nwork has been done on utilizing existing join optimization methods to improve\nthe performance of CEP-based systems. In this paper, we provide the first\ntheoretical and experimental study of the relationship between these two\nresearch areas. We formally prove that the CEP Plan Generation problem is\nequivalent to the Join Query Plan Generation problem for a restricted class of\npatterns and can be reduced to it for a considerably wider range of classes.\nThis result implies the NP-completeness of the CEP Plan Generation problem. We\nfurther show how join query optimization techniques developed over the last\ndecades can be adapted and utilized to provide practically efficient solutions\nfor complex event detection. Our experiments demonstrate the superiority of\nthese techniques over existing strategies for CEP optimization in terms of\nthroughput, latency, and memory consumption.\n",
        "  Diffuse pollution in rural areas due to agricultural run-off is a widespread\nand difficult problem to address due to the vast areas affected. Drainage\nchannels do receive these polluted waters, but its introduction in the\nconventional treatment network is unfeasible. Within this context,\nmicroalgae-based treatment systems could be used as alternative treatment\nplants. In this study, a new design of semi-closed (hybrid) tubular horizontal\nphotobioreactor (HTH-PBR) with low energy requirements has been evaluated for\nmicroalgae cultivation at full-scale (8.5 m3), using agricultural runoff as\nfeedstock. This novel system was tested in batch and continuous mode during a\ntotal of 4 and 135 days. Considering a full-scale application in an\nagricultural context, a batch test was carried out to evaluate the performance\nof the system. An increase of 22% in the biomass concentration in 4 days was\nregistered, and all nutrients were consumed during the first two days. In the\ncontinuous experiment (December- April), a productivity between 2-14 g TSS/m3d\nwas reached in winter, whereas values up to 76.4 g TSS/m3d were reached at the\nend of the study in spring, despite the low nutrients concentration in the\nfeedstock. The elimination of emerging contaminants was also evaluated,\nobtaining the highest removals for the fragrances tonalide and galaxolide (73%\nand 68%), and the antiinflamatory diclofenac (61%).\n",
        "  The methylation of DNA regulates gene expression. On cell division the\nmethylation state of the DNA is typically inherited from parent to daughter\ncells. While the chemical bond between the methyl group and the DNA is very\nstrong, changes to the methylation state do occur and are observed to occur\nrapidly in response to external stimulus. The loss of methylation can be active\nwhere enzyme physically breaks the bond, or passive where on cell division the\nnewly constructed strand of DNA is not properly inherited.\n  Here we present a mathematical model of single locus passive demethylation\nfor a dividing population of cells. The model describes the heterogenity in the\npopulation expected from passive mechanisms. We see that even when the site\nspecific probabilities of passive demethylation are independent, conservation\nof methylation on the inherited strand gives rise to site-site correlations of\nthe methylation state. We then extend the model to incorporate correlations\nbetween sites in the locus for demethylation rates. Biologically, correlations\nin demethylation rates might correspond to locus wide changes such as the\ninability of methyltransferase to access the locus. We also look at the effects\nof selection on the multicellular population.\n  The model of passive demethylation not only provides a tool for measurement\nof parameters in loci-specific cases where passive demethylation is the\ndominant mechanism, but also provides a baseline in the search for active\nmechanisms. The model tells us that there are states of methylation\ninaccessible by passive mechanisms. Observation of these states constitutes\nevidence of active mechanisms, either de novo methylation or enzymatic\ndemethylation. We also see that selection and passive demethylation combined\ncan give rise to a stable heterogeneous distribution of gene methylation states\nin a population.\n",
        "  Purpose: Siemens has developed several iterative reconstruction (IR)\nalgorithms on their CT scanners. SAFIRE is available on most of their CT\nscanners. The latest algorithm, ADMIRE, is available on their newest high-end\nCT scanners. The aim of our study was to compare the noise reduction properties\nof the two IR algorithms using objective methods. Methods and Materials: The\nhomogeneous module of the Catphan phantom was scanned on a Siemens AS+ and a\nSiemens Flash CT scanner using an axial abdomen protocol with fixed tube\ncurrent at two dose levels. The images were reconstructed with an abdomen\nfilter (B30) using filtered back projection (FBP) and a low, medium, and high\nlevel of SAFIRE or ADMIRE. Noise Power Spectrum (NPS) curves were calculated\nusing these images. Then, an anthropomorphic abdomen phantom (Kyoto Kagaku\nPH-5) was scanned using the same setup and exposure parameters. Fifty axial\nimages at the same slice location were used to calculate inter-image standard\ndeviation maps. Results: At full dose, the median values of the NPS curves were\nsimilar for both scanners at all IR levels. At low dose the median values of\nthe NPS curves were generally shifted towards lower spatial frequencies,\nusually resulting in a more blotchy image texture. This shift was more\nprominent for ADMIRE compared to SAFIRE for all IR levels. Based on the\ninter-image standard deviation maps of the anthropomorphic phantom, ADMIRE\nremoved noise near edges more efficiently than SAFIRE. Conclusion: No\nsignificant improvement in maintaining noise structure were found for the\nADMIRE algorithm. Based on the inter-image standard deviation maps, ADMIRE\nremoved noise near edges more efficiently than SAFIRE.\n",
        "  Here we present the first genome wide statistical test for recessive\nselection. This test uses explicitly non-equilibrium demographic differences\nbetween populations to infer the mode of selection. By analyzing the transient\nresponse to a population bottleneck and subsequent re-expansion, we\nqualitatively distinguish between alleles under additive and recessive\nselection. We analyze the response of the average number of deleterious\nmutations per haploid individual and describe time dependence of this quantity.\nWe introduce a statistic, $B_R$, to compare the number of mutations in\ndifferent populations and detail its functional dependence on the strength of\nselection and the intensity of the population bottleneck. This test can be used\nto detect the predominant mode of selection on the genome wide or regional\nlevel, as well as among a sufficiently large set of medically or functionally\nrelevant alleles.\n",
        "  Fisher developed his geometric model to support the micro-mutationalism\nhypothesis which claims that small mutations are more likely to be beneficial\nand therefore to contribute to evolution and adaptation. While others have\nprovided a general solution to the model using geometric approaches, we derive\nan equivalent general solution using a probabilistic approach. Our approach to\nFisher's geometric model provides alternative intuition and interpretation of\nthe solution in terms of the model's parameters: for mutation to improve a\nphenotype, its relative beneficial effect must be larger than the ratio of its\ntotal effect and twice the difference between the current phenotype and the\noptimal one. Our approach provides new insight into this classical model of\nadaptive evolution.\n",
        "  We describe the current plans for a spectroscopic survey of millions of stars\nin the Milky Way galaxy using the Guo Shou Jing Telescope (GSJT, formerly the\nLarge Area Multi-Object Spectroscopic Telescope - LAMOST). The survey will\nobtain spectra for 2.5 million stars brighter than $r<19$ during dark/grey\ntime, and 5 million stars brighter than $r<17$ or $J<16$ on nights that are\nmoonlit or have low transparency. The survey will begin in fall of 2012, and\nwill run for at least four years. The telescope design constrains the optimal\ndeclination range for observations to $10^\\circ<\\delta<50^\\circ$, and site\nconditions lead to an emphasis on stars in the direction of the Galactic\nanticenter. The survey is divided into three parts with different target\nselection strategies: disk, anticenter, and spheroid. The resulting dataset\nwill be used to study the merger history of the Milky Way, the substructure and\nevolution of the disks, the nature of the first generation of stars through\nidentification of the lowest metallicity stars, and star formation through\nstudy of open clusters and the OB associations. Detailed design of the LEGUE\nsurvey will be completed after a review of the results of the pilot survey in\nsummer 2012.\n",
        "  In 1938, H. Selye proposed the notion of adaptation energy and published\n\"Experimental evidence supporting the conception of adaptation energy\".\nAdaptation of an animal to different factors appears as the spending of one\nresource. Adaptation energy is a hypothetical extensive quantity spent for\nadaptation. This term causes much debate when one takes it literally, as a\nphysical quantity, i.e. a sort of energy. The controversial points of view\nimpede the systematic use of the notion of adaptation energy despite\nexperimental evidence. Nevertheless, the response to many harmful factors often\nhas general non-specific form and we suggest that the mechanisms of\nphysiological adaptation admit a very general and nonspecific description.\n  We aim to demonstrate that Selye's adaptation energy is the cornerstone of\nthe top-down approach to modelling of non-specific adaptation processes. We\nanalyse Selye's axioms of adaptation energy together with Goldstone's\nmodifications and propose a series of models for interpretation of these\naxioms. {\\em Adaptation energy is considered as an internal coordinate on the\n`dominant path' in the model of adaptation}. The phenomena of `oscillating\ndeath' and `oscillating remission' are predicted on the base of the dynamical\nmodels of adaptation. Natural selection plays a key role in the evolution of\nmechanisms of physiological adaptation. We use the fitness optimization\napproach to study of the distribution of resources for neutralization of\nharmful factors, during adaptation to a multifactor environment, and analyse\nthe optimal strategies for different systems of factors.\n",
        "  In this paper, we propose a 2D based partition method for solving the problem\nof Ranking under Team Context(RTC) on datasets without a priori. We first map\nthe data into 2D space using its minimum and maximum value among all\ndimensions. Then we construct window queries with consideration of current team\ncontext. Besides, during the query mapping procedure, we can pre-prune some\ntuples which are not top ranked ones. This pre-classified step will defer\nprocessing those tuples and can save cost while providing solutions for the\nproblem. Experiments show that our algorithm performs well especially on large\ndatasets with correctness.\n",
        "  In order to control the growth of human population it is helpful to\nunderstand correctly the mechanism of growth, and the first essential step is\nto investigate current interpretations and reject any unscientific\nexplanations. One of such popular but questionable interpretations is the\nconcept of the Epoch of Malthusian Stagnation. We discuss its origin, narrative\nand claims. We explain why this concept is scientifically unacceptable. This\ninvestigation questions also the closely-related Demographic Transition Theory,\nwhose essential component is the assumed mechanism of Malthusian stagnation for\nthe first stage of growth.\n",
        "  The potential to perform attenuation and scatter compensation (ASC) in\nsingle-photon emission computed tomography (SPECT) imaging using only the SPECT\nemission data is highly significant. In this context, attenuation in SPECT is\nprimarily due to Compton scattering, where the probability of Compton scatter\nis proportional to the attenuation coefficient of the tissue and the energy of\nthe scattered photon and the scattering angle are related. Given this premise,\nwe investigate whether the SPECT scattered-photon data acquired in list-mode\n(LM) format and including the energy information can be used to estimate the\nattenuation map. For this purpose, we propose a Fisher-information-based method\nthat yields the Cramer-Rao bound (CRB) for the task of jointly estimating the\nactivity/attenuation distribution using only the SPECT emission data. The\nproposed method is applied to analyze the information content of SPECT LM\nemission data in a 2D SPECT system using computational studies with digital\nphantoms for different photon-count levels. The results show that scattered\nphotons contain information to estimate the attenuation coefficients. An\nincrease in the number of detected photons leads to lower CRB for both the\nattenuation and activity coefficients. Also, the CRB obtained for the\nattenuation and activity coefficients is typically much lower than the true\nvalue of these coefficients. Further, processing the emission data in LM format\nyields a lower CRB in comparison to binning data. Finally, we observe that\nsystems with better energy resolution yield a lower CRB for the attenuation\ncoefficient. Overall, the results provide strong evidence that LM SPECT\nemission data, including the scattered photons, contains information to jointly\nestimate the activity and attenuation coefficients.\n",
        "  Single document summarization is the task of producing a shorter version of a\ndocument while preserving its principal information content. In this paper we\nconceptualize extractive summarization as a sentence ranking task and propose a\nnovel training algorithm which globally optimizes the ROUGE evaluation metric\nthrough a reinforcement learning objective. We use our algorithm to train a\nneural summarization model on the CNN and DailyMail datasets and demonstrate\nexperimentally that it outperforms state-of-the-art extractive and abstractive\nsystems when evaluated automatically and by humans.\n",
        "  Intrinsically stable magnetic levitation between superconductors and\npermanent magnets can be exploited in a variety of applications of great\ntechnical interest in the field of transportation (rail transportation), energy\n(flywheels) and industry. In this contribution, we present a new model for the\ncalculation of levitation forces between superconducting bulks and permanent\nmagnet, based on the $H$-formulation of Maxwell's equations coupled with an\nArbitrary Lagrangian-Eulerian formulation. The model uses a moving mesh that\nadapts at each time step based on the time-change of the distance between a\nsuperconductor bulk and a permanent magnet. The model is validated against a\nfixed mesh model (recently in turn validated against experiments) that uses an\nanalytical approach for calculating the magnetic field generated by the moving\npermanent magnet. Then, it is used to analyze the magnetic field dynamics both\nin field-cooled and zero-field-cooled conditions and successively used to test\ndifferent configurations of permanent magnets and to compare them in terms of\nlevitation forces. The easiness of implementation of this model and its\nflexibility in handling different geometries, material properties, and\napplication scenarios make the model an attractive tool for the analysis and\noptimization of magnetic levitation-based applications.\n",
        "  Organ motion induced by respiration may cause clinically significant\ntargeting errors and greatly degrade the effectiveness of conformal\nradiotherapy. It is therefore crucial to be able to model respiratory motion\naccurately. A recently proposed lung motion model based on principal component\nanalysis (PCA) has been shown to be promising on a few patients. However, there\nis still a need to understand the underlying reason why it works. In this\npaper, we present a much deeper and detailed analysis of the PCA-based lung\nmotion model. We provide the theoretical justification of the effectiveness of\nPCA in modeling lung motion. We also prove that under certain conditions, the\nPCA motion model is equivalent to 5D motion model, which is based on physiology\nand anatomy of the lung. The modeling power of PCA model was tested on clinical\ndata and the average 3D error was found to be below 1 mm.\n",
        "  Many works have focused, for over twenty five years, on the integration of\nthe time dimension in databases (DB). However, the standard SQL3 does not yet\nallow easy definition, manipulation and querying of temporal DBs. In this\npaper, we study how we can simplify querying and manipulating temporal facts in\nSQL3, using a model that integrates time in a native manner. To do this, we\npropose new keywords and syntax to define different temporal versions for many\nrelational operators and functions used in SQL. It then becomes possible to\nperform various queries and updates appropriate to temporal facts. We\nillustrate the use of these proposals on many examples from a real application.\n",
        "  Digital breast tomosynthesis (DBT) is an emerging modality for breast\nimaging. A typical tomosynthesis image is reconstructed from projection data\nacquired at a limited number of views over a limited angular range. In general,\nthe quantitative accuracy of the image can be significantly compromised by\nsevere artifacts and non-isotropic resolution resulting from the incomplete\ndata. Nevertheless, it has been demonstrated that DBT may yield useful\ninformation for detection/classification tasks and thus is considered a\npromising breast imaging modality currently undergoing pre-clinical evaluation\ntrials. The purpose of this work is to conduct a preliminary, but systematic,\ninvestigation and evaluation of the properties of reconstruction algorithms\nthat have been proposed for DBT. We use a breast phantom designed for DBT\nevaluation to generate analytic projection data for a typical DBT\nconfiguration, which is currently undergoing pre-clinical evaluation. The\nreconstruction algorithms under comparison include (i) filtered backprojection\n(FBP), (ii) expectation maximization (EM), and (iii) TV-minimization\nalgorithms. Results of our studies indicate that FBP reconstructed images are\ngenerally noisier and demonstrate lower in-depth resolution than those obtained\nthrough iterative reconstruction and that the TV-minimization reconstruction\nyield images with reduced artifacts as compared to that obtained with other\nalgorithms under study.\n",
        "  Many population genetic models have been developed for the purpose of\ninferring population size and growth rates from random samples of genetic data.\nWe examine two popular approaches to this problem, the coalescent and the\nbirth-death-sampling model, in the context of estimating population size and\nbirth rates in a population growing exponentially according to the birth-death\nbranching process. For sequences sampled at a single time, we found the\ncoalescent and the birth-death-sampling model gave virtually indistinguishable\nresults in terms of the growth rates and fraction of the population sampled,\neven when sampling from a small population. For sequences sampled at multiple\ntime points, we find that the birth-death model estimators are subject to large\nbias if the sampling process is misspecified. Since birth-death-sampling models\nincorporate a model of the sampling process, we show how much of the\nstatistical power of birth-death-sampling models arises from the sequence of\nsample times and not from the genealogical tree. This motivates the development\nof a new coalescent estimator, which is augmented with a model of the known\nsampling process and is potentially more precise than the coalescent that does\nnot use sample time information.\n",
        "  We present a simple neural network for word alignment that builds source and\ntarget word window representations to compute alignment scores for sentence\npairs. To enable unsupervised training, we use an aggregation operation that\nsummarizes the alignment scores for a given target word. A soft-margin\nobjective increases scores for true target words while decreasing scores for\ntarget words that are not present. Compared to the popular Fast Align model,\nour approach improves alignment accuracy by 7 AER on English-Czech, by 6 AER on\nRomanian-English and by 1.7 AER on English-French alignment.\n",
        "  We show that the exterior powers of the matrix valued random walk invariant\nof string links, introduced by Lin, Tian, and Wang, are isomorphic to the\ngraded components of the tangle functor associated to the Alexander Polynomial\nby Ohtsuki divided by the zero graded invariant of the functor. Several\nresulting properties of these representations of the string link monoids are\ndiscussed.\n",
        "  In the recent experiment by Cren \\textit{et al.} [Phys. Rev. Lett.\n\\textbf{102}, 127005 (2009)], no hysteresis for vortex penetration and\nexpulsion from the nano-island of Pb was observed. In the present paper, we\nargue that this effect can be associated with the thermoactivated surmounting\nof the surface barrier by a vortex. The typical entrance (exit) time is found\nanalytically from the Fokker-Planck equation, written in the form suitable for\nthe extreme vortex confinement. We show that this time is several orders of\nmagnitude smaller than 1 second under the conditions of the experiment\nconsidered. Our results thus demonstrate a possibility for the thermal\nsuppression of the surface barrier in nanosized low-$T_{c}$ superconductors. We\nalso briefly discuss other recent experiments on vortices in related\nstructures.\n",
        "  Prediction without justification has limited utility. Much of the success of\nneural models can be attributed to their ability to learn rich, dense and\nexpressive representations. While these representations capture the underlying\ncomplexity and latent trends in the data, they are far from being\ninterpretable. We propose a novel variant of denoising k-sparse autoencoders\nthat generates highly efficient and interpretable distributed word\nrepresentations (word embeddings), beginning with existing word representations\nfrom state-of-the-art methods like GloVe and word2vec. Through large scale\nhuman evaluation, we report that our resulting word embedddings are much more\ninterpretable than the original GloVe and word2vec embeddings. Moreover, our\nembeddings outperform existing popular word embeddings on a diverse suite of\nbenchmark downstream tasks.\n",
        "  In this first paper of a two-paper series, we present a method for optimizing\nthe dynamic delivery of fluence maps in radiation therapy. For a given fluence\nmap and a given delivery time, the optimization of the leaf trajectories of a\nmulti-leaf collimator to approximately form the given fluence map is a\nnon-convex optimization problem. Its general solution has not been addressed in\nthe literature, despite the fact that dynamic delivery of fluence maps has long\nbeen a common approach to intensity modulated radiation therapy. We model the\nleaf trajectory and dose rate optimization as a non-convex continuous\noptimization problem and solve it by an interior point method from randomly\ninitialized feasible starting solutions. We demonstrate the method on a fluence\nmap from a prostate case and a larger fluence map from a head-and-neck case.\nWhile useful for static beam IMRT delivery, our main motivation for this work\nis the extension to the case of sequential fluence map delivery, i.e. the case\nof VMAT, which is the topic of the second paper.\n",
        "  Current models for document summarization disregard user preferences such as\nthe desired length, style, the entities that the user might be interested in,\nor how much of the document the user has already read. We present a neural\nsummarization model with a simple but effective mechanism to enable users to\nspecify these high level attributes in order to control the shape of the final\nsummaries to better suit their needs. With user input, our system can produce\nhigh quality summaries that follow user preferences. Without user input, we set\nthe control variables automatically. On the full text CNN-Dailymail dataset, we\noutperform state of the art abstractive systems (both in terms of F1-ROUGE1\n40.38 vs. 39.53 and human evaluation).\n",
        "  Photoacoustic imaging of interphalangeal peripheral joints is of interest in\nthe context of using the synovial membrane as a surrogate marker of rheumatoid\narthritis. Previous work has shown that ultrasound produced by absorption of\nlight at the epidermis reflects on the bone surfaces within the finger. When\nthe reflected signals are backprojected in the region of interest, artifacts\nare produced, confounding interpretation of the images. In this work, we\npresent an approach where the photoacoustic signals known to originate from the\nepidermis, are treated as virtual ultrasound transmitters, and a separate\nreconstruction is performed as in ultrasound reflection imaging. This allows us\nto identify the bone surfaces. Further, the identification of the joint space\nis important as this provides a landmark to localize a region-of-interest in\nseeking the inflamed synovial membrane. The ability to delineate bone surfaces\nallows us not only to identify the artifacts, but also to identify the\ninterphalangeal joint space without recourse to new US hardware or a new\nmeasurement. We test the approach on phantoms and on a healthy human finger.\n",
        "  This article addresses the generation of the ETL\noperators(Extract-Transform-Load) for supplying a Data Warehouse from a\nrelational data source. As a first step, we add new rules to those proposed by\nthe authors of [1], these rules deal with the combination of ETL operators. In\na second step, we propose an automatic approach based on model transformations\nto generate the ETL operations needed for loading a data warehouse. This\napproach offers the possibility to set some designer requirements for loading.\n",
        "  We find bases for naturally defined lattices over certain rings of integers\nin the SU(2)-TQFT-theory modules of surfaces. We consider the TQFT where the\nKauffman's A variable is a root of unity of order four times an odd prime. As\nan application, we show that the Frohman Kania-Bartoszynska ideal invariant for\n3-manifolds with boundary using the SU(2)-TQFT-theory is equal to the product\nof the ideals using the 2^{'}-theory and the SO(3)-TQFT-theory under a certain\nchange of coefficients.\n",
        "  We present a novel analysis method for image reconstruction in emission\ntomography. The method, named Reconstructed Image from Simulations Ensemble\n(RISE), utilizes statistical physics concepts and Monte Carlo techniques to\nextract the parameters of a physical model representing the imaged object from\nits planar projections. Its capabilities are demonstrated and evaluated by\nreconstructing tomographic images from sets of simulated SPECT projections. The\nRISE results compare favourably to those derived from the well - known Maximum\nLikelihood Expectation Maximization (MLEM) method, the Algebraic Reconstruction\nTechnique (ART) and the Filtered Back Projection (FBP).\n",
        "  Wikipedia is a useful knowledge source that benefits many applications in\nlanguage processing and knowledge representation. An important feature of\nWikipedia is that of categories. Wikipedia pages are assigned different\ncategories according to their contents as human-annotated labels which can be\nused in information retrieval, ad hoc search improvements, entity ranking and\ntag recommendations. However, important pages are usually assigned too many\ncategories, which makes it difficult to recognize the most important ones that\ngive the best descriptions.\n  In this paper, we propose an approach to recognize the most descriptive\nWikipedia categories. We observe that historical figures in a precise category\npresumably are mutually similar and such categorical coherence could be\nevaluated via texts or Wikipedia links of corresponding members in the\ncategory. We rank descriptive level of Wikipedia categories according to their\ncoherence and our ranking yield an overall agreement of 88.27% compared with\nhuman wisdom.\n",
        "  This article contains general formulas for Tutte and Jones polynomial for\nfamilies of knots and links given in Conway notation.\n",
        "  The fundamental group of $M = \\sharp_n (S^2\\times S^1)$ is $F_n$, the free\ngroup with $n$ generators. There is a 1-1 correspondence between the\nequivalence classes of $\\mathbb{Z}$-- splittings of $F_n$ and homotopy classes\nof embedded essential tori in $M$. We define and prove a local notion of\nminimal intersection of a torus with respect to a maximal sphere system in $M$,\nwhich generalizes Hatcher's work \\cite{H1} on 2-spheres in the same manifold.\n",
        "  We study the ablation problem for the hyperbolic heat equation in an\naxisymmetrical geometry which can be conveniently realized in the lab. We\ndetermine an analytic solution which shows the approach to steady state. The\nthermal relaxation time $\\tau$ is best obtained from the small time behavior.\nThe measurements give a surprisingly large $\\tau$ of about 7 minutes for 0.5 %\nNaCl in water. This shows that the hyperbolic equation must certainly be used\ninstead of the parabolic heat equation in the ablation problem of\nelectrocardiology.\n",
        "  We investigate the expected time to extinction in the\nsusceptible-infectious-susceptible (SIS) model of disease spreading. Rather\nthan using stochastic simulations, or asymptotic calculations in network\nmodels, we solve the extinction time exactly for all connected graphs with\nthree to eight vertices. This approach enables us to discover descriptive\nrelations that would be impossible with stochastic simulations. It also helps\nus discovering graphs and configurations of S and I with anomalous behaviors\nwith respect to disease spreading. We find that for large transmission rates\nthe extinction time is independent of the configurations, just dependent on the\ngraph. In this limit, the number of vertices and edges determine the extinction\ntime very accurately (deviations primarily coming from the fluctuations in\ndegrees). We find that the rankings of configurations with respect to\nextinction times at low and high transmission rates are correlated at low\nprevalences and negatively correlated for high prevalences. The most important\nstructural factor determining this ranking is the degrees of the infectious\nvertices.\n",
        "  (Abridged) Bright-rimmed clouds (BRCs) are isolated molecular clouds located\non the edges of evolved HII regions where star formation is thought may have\nbeen triggered. In this paper we investigate the current level of star\nformation within a sample of BRCs and evaluate to what extent star formation\nmay have been induced. We present the results of a programme of\nposition-switched CO observations towards 45 southern BRCs. The 12CO, 13CO and\nC18O (J=1-0) were simultaneously observed using the 22m Mopra telescope. We\ncomplement these observations with archival mid-IR submm and radio data.\nAnalysis of the CO, mid-IR and radio data result in the clouds being divided\ninto three distinct groups. We refer to these groups as spontaneous, triggered,\nand zapped clouds, respectively. Comparing the physical parameters of\nspontaneous and triggered samples we find striking differences in luminosity,\nsurface temperature and column density with all three quantities significantly\nenhanced for the clouds considered to have been triggered. Furthermore, we find\nstrong evidence for star formation within the triggered sample by way of\nmethanol and H_2O masers, embedded mid-IR point sources and CO wings, however,\nwe find evidence of ongoing star formation within only two of the spontaneous\nsample. We have used CO, mid-IR and radio data to identify 24 of the 45\nsouthern BRCs that are undergoing a strong interaction with their HII region.\nWe can therefore exclude ~50% from future studies. 14 of the 24 interacting\nBRCs are found to be associated with embedded mid-IR point sources and we find\nstrong evidence of that these clouds are forming stars. The absence of\nmid-infrared sources towards the remaining ten clouds leads us to conclude that\nthese represent an earlier evolutionary stage of star formation.\n",
        "  In the treatment plan optimization for intensity modulated radiation therapy\n(IMRT), dose-deposition coefficient (DDC) matrix is often pre-computed to\nparameterize the dose contribution to each voxel in the volume of interest from\neach beamlet of unit intensity. However, due to the limitation of computer\nmemory and the requirement on computational efficiency, in practice matrix\nelements of small values are usually truncated, which inevitably compromises\nthe quality of the resulting plan. A fixed-point iteration scheme has been\napplied in IMRT optimization to solve this problem, which has been reported to\nbe effective and efficient based on the observations of the numerical\nexperiments. In this paper, we aim to point out the mathematics behind this\nscheme and to answer the following three questions: 1) whether the fixed-point\niteration algorithm converges or not? 2) when it converges, whether the fixed\npoint solution is same as the original solution obtained with the complete DDC\nmatrix? 3) if not the same, whether the fixed point solution is more accurate\nthan the naive solution of the truncated problem obtained without the\nfixed-point iteration? To answer these questions, we first performed\nmathematical analysis and deductions using a simplified fluence map\noptimization (FMO) model. Then we conducted numerical experiments on a\nhead-and-neck patient case using both the simplified and the original FMO\nmodel. Both our mathematical analysis and numerical experiments demonstrate\nthat with proper DDC matrix truncation, the fixed-point iteration can converge.\nEven though the converged solution is not the one that we obtain with the\ncomplete DDC matrix, the fixed-point iteration scheme could significantly\nimprove the plan accuracy compared with the solution to the truncated problem\nobtained without the fixed-point iteration.\n",
        "  The mechanisms by which blast pressure waves cause mild to moderate traumatic\nbrain injury (mTBI) are an open question. Possibilities include acceleration of\nthe head, direct passage of the blast wave via the cranium, and propagation of\nthe blast wave to the brain via a thoracic mechanism. The hypothesis that the\nblast pressure wave reaches the brain via a thoracic mechanism is considered in\nlight of ballistic and blast pressure wave research. Ballistic pressure waves,\ncaused by penetrating ballistic projectiles or ballistic impacts to body armor,\ncan only reach the brain via an internal mechanism and have been shown to cause\ncerebral effects. Similar effects have been documented when a blast pressure\nwave has been applied to the whole body or focused on the thorax in animal\nmodels. While vagotomy reduces apnea and bradycardia due to ballistic or blast\npressure waves, it does not eliminate neural damage in the brain, suggesting\nthat the pressure wave directly affects the brain cells via a thoracic\nmechanism. An experiment is proposed which isolates the thoracic mechanism from\ncranial mechanisms of mTBI due to blast wave exposure. Results have\nimplications for evaluating risk of mTBI due to blast exposure and for\ndeveloping effective protection.\n",
        "  The radiotherapy of malignant diseases has reached much progress during the\npast decade. Thus, intensity modulated radiation therapy (IMRT) and VMAT\n(Rapidarc) now belong to the standard modalities of tumor treatment with high\nenergy radiation in clinical practice. In recent time, the particle therapy\n(protons and partially with heavy carbon ions) has reached an important\ncompletion of these modalities with regard to some suitable applications. In\nspite of this enrichment essential features need further research activities\nand publications in this field: Nuclear reactions and the role of the released\nneutrons, electron capture of positively charged nuclei at lower projectile\nenergies (e.g. in the environment of the Bragg peak and at the distal end of\nthe particle track), correct dose delivery in scanning methods by accounting\nfor the influence of the lateral scatter of beam-lets. Deconvolution methods\ncan help to overcome these problems, which already occur in radiotherapy of\nvery small photon beams [1 - 8].\n",
        "  The measured orbital period decay of compact-star binaries, with\ncharacteristic orbital periods $\\sim 0.1$~days, is explained with very high\nprecision by the gravitational wave (GW) emission of an inspiraling binary in\nvacuum. However, the binary gravitational binding energy is also affected by an\nusually neglected phenomenon, namely the dark matter dynamical friction (DMDF)\nproduced by the interaction of the binary components with their respective DM\ngravitational wakes. The entity of this effect depends on the orbital period\nand on the local value of the DM density, hence on the position of the binary\nin the Galaxy. We evaluate the DMDF produced by three different DM profiles:\nthe Navarro-Frenk-White (NFW), the non-singular-isothermal-sphere (NSIS) and\nthe Ruffini-Arg\\\"uelles-Rueda (RAR) profile based on self-gravitating keV\nfermions. We first show that indeed, due to their Galactic position, the GW\nemission dominates over the DMDF in the NS-NS, NS-WD and WD-WD binaries for\nwhich measurements of the orbital decay exist. Then, we evaluate the conditions\nunder which the effect of DMDF on the binary evolution becomes comparable to,\nor overcomes, the one of the GW emission. We find that, for instance for\n$1.3$--$0.2$ $M_\\odot$ NS-WD, $1.3$--$1.3$~$M_\\odot$ NS-NS, and\n$0.25$--$0.50$~$M_\\odot$ WD-WD, located at 0.1~kpc, this occurs at orbital\nperiods around 20--30 days in a NFW profile while, in a RAR profile, it occurs\nat about 100 days. For closer distances to the Galactic center, the DMDF effect\nincreases and the above critical orbital periods become interestingly shorter.\nFinally, we also analyze the system parameters for which DMDF leads to an\norbital widening instead of orbital decay. All the above imply that a\ndirect/indirect observational verification of this effect in compact-star\nbinaries might put strong constraints on the nature of DM and its Galactic\ndistribution.\n",
        "  This paper aims at understanding if the normalization of the stellar initial\nmass function (IMF) of massive early-type galaxies (ETGs) varies with cosmic\ntime and/or with mean stellar mass density Sigma (M*/2\\pi Re^2). For this\npurpose we collected a sample of 18 dense (Sigma>2500 M_sun/pc^2) ETGs at\n1.2<z<1.6 with available velocity dispersion sigma_e. We have constrained their\nmass-normalization by comparing their true stellar masses (M_true) derived\nthrough virial theorem, hence IMF independent, with those inferred through the\nfit of the photometry assuming a reference IMF (M_ref). Adopting the virial\nestimator as proxy of the true stellar mass, we have assumed for these ETGs\nzero dark matter (DM). However, dynamical models and numerical simulations of\ngalaxy evolution have shown that the DM fraction within Re in dense high-z ETGs\nis negligible. We have considered the possible bias of virial theorem in\nrecovering the total masses and have shown that for dense ETGs the virial\nmasses are in agreement with those derived through more sophisticated dynamical\nmodels. The variation of the parameter Gamma = M_true/M_ref with sigma_e shows\nthat, on average, dense ETGs at <z> = 1.4 follow the same IMF-sigma_e trend of\ntypical local ETGs, but with a lower mass-normalization. Nonetheless, once the\nIMF-sigma_e trend we have found for high-z dense ETGs is compared with that of\nlocal ETGs with similar Sigma and sigma_e, they turn out to be consistent. The\nsimilarity between the IMF-sigma_e trends of dense high-z and low-z ETGs over 9\nGyr of evolution and their lower mass-normalization with respect to the mean\nvalue of local ETGs suggest that, independently on formation redshift, the\nphysical conditions characterizing the formation of a dense spheroid lead to a\nmass spectrum of new formed stars with an higher ratio of high- to low-mass\nstars with respect to the IMF of normal local ETGs.\n",
        "  The twist angle {\\gamma} dependence of the Josephson critical current of\nd-wave superconductors in one junction and a granular system is considered. Our\nresults show that the twist angle {\\gamma} dependence of the d-wave Josephson\ncritical current is the same for one junction and a granular system. The\nmagnetic field dependences of the critical-current of a granular d-wave\nsuperconductor has also been determined by considering the rectangular and\ncircular junction model of an array of small superconducting particles which\ninteracting by Josephson coupling through insulating barriers. We will show\nthat in the case of circular model, the critical current of the Josephson\ncurrent is larger than that of rectangular one.\n",
        "  The relative ease of collaborative data science and analysis has led to a\nproliferation of many thousands or millions of $versions$ of the same datasets\nin many scientific and commercial domains, acquired or constructed at various\nstages of data analysis across many users, and often over long periods of time.\nManaging, storing, and recreating these dataset versions is a non-trivial task.\nThe fundamental challenge here is the $storage-recreation\\;trade-off$: the more\nstorage we use, the faster it is to recreate or retrieve versions, while the\nless storage we use, the slower it is to recreate or retrieve versions. Despite\nthe fundamental nature of this problem, there has been a surprisingly little\namount of work on it. In this paper, we study this trade-off in a principled\nmanner: we formulate six problems under various settings, trading off these\nquantities in various ways, demonstrate that most of the problems are\nintractable, and propose a suite of inexpensive heuristics drawing from\ntechniques in delay-constrained scheduling, and spanning tree literature, to\nsolve these problems. We have built a prototype version management system, that\naims to serve as a foundation to our DATAHUB system for facilitating\ncollaborative data science. We demonstrate, via extensive experiments, that our\nproposed heuristics provide efficient solutions in practical dataset versioning\nscenarios.\n",
        "  The encoder-decoder framework for neural machine translation (NMT) has been\nshown effective in large data scenarios, but is much less effective for\nlow-resource languages. We present a transfer learning method that\nsignificantly improves Bleu scores across a range of low-resource languages.\nOur key idea is to first train a high-resource language pair (the parent\nmodel), then transfer some of the learned parameters to the low-resource pair\n(the child model) to initialize and constrain training. Using our transfer\nlearning method we improve baseline NMT models by an average of 5.6 Bleu on\nfour low-resource language pairs. Ensembling and unknown word replacement add\nanother 2 Bleu which brings the NMT performance on low-resource machine\ntranslation close to a strong syntax based machine translation (SBMT) system,\nexceeding its performance on one language pair. Additionally, using the\ntransfer learning model for re-scoring, we can improve the SBMT system by an\naverage of 1.3 Bleu, improving the state-of-the-art on low-resource machine\ntranslation.\n",
        "  We study in this work the importance of depth in convolutional models for\ntext classification, either when character or word inputs are considered. We\nshow on 5 standard text classification and sentiment analysis tasks that deep\nmodels indeed give better performances than shallow networks when the text\ninput is represented as a sequence of characters. However, a simple\nshallow-and-wide network outperforms deep models such as DenseNet with word\ninputs. Our shallow word model further establishes new state-of-the-art\nperformances on two datasets: Yelp Binary (95.9\\%) and Yelp Full (64.9\\%).\n",
        "  Viral capsids are structurally constrained by interactions amongst the amino\nacids of the constituting proteins. Therefore, epistasis is expected to evolve\namongst sites engaged in physical interactions, and to influence their\nsubstitution rates. In order to study the distribution of structural epistasis,\nwe modeled \\emph{in silico} the capsid of 18 species of the \\phix \\, family,\nincluding the wild type. \\phix \\, is amongst the simplest organisms, making it\nsuitable for experimental evolution and \\emph{in silico} modeling. We found\nnearly 40 variable amino acid sites in the main capsid protein across the 18\nspecies. To study how epistasis evolved in this group, we reconstructed the\nancestral sequences using a Bayesian phylogenetic framework. The ancestral\nstates include 8 variable amino acids, for a total of 256 possible haplotypes.\nThe $dN/dS$ ratio is low, suggesting strong purifying selection, consistent\nwith the idea that the structure is constrained by some form of stabilizing\nselection. For each haplotype in the ancestral node and for the extant species\nwe estimated \\emph{in silico} the distribution of free energies and of\nepistasis. We found that free energy has not significantly increased but\nepistasis has. We decomposed epistasis up to fifth order and found that\nhigh-order epistasis can sometimes compensate pairwise interactions, often\nmaking the free energy seem additive. By synthesizing some of the ancestral\nhaplotypes of the capsid gene, we measured their fitness experimentally, and\nfound that the predicted deviations in the coat protein free energy do not\nsignificantly affect fitness, which is consistent with the stabilizing\nselection hypothesis.\n",
        "  In a recent work of Ayaka Shimizu$^{[5]}$, she defined an operation named\nregion crossing change on link diagrams, and showed that region crossing change\nis an unknotting operation for knot diagrams. In this paper, we prove that\nregion crossing change on a 2-component link diagram is an unknotting operation\nif and only if the linking number of the diagram is even.\n  Besides, we define an incidence matrix of a link diagram via its signed\nplanar graph and its dual graph. By studying the relation between region\ncrossing change and incidence matrix, we prove that a signed planar graph\nrepresents an $n$-component link diagram if and only if the rank of the\nassociated incidence matrix equals to $c-n+1$, here $c$ denotes the size of the\ngraph.\n",
        "  Superconducting Quantum Interference Devices (SQUIDs) can have excellent spin\nsensitivity depending on their magnetic flux noise, pick-up loop diameter, and\ndistance from the sample. We report a family of scanning SQUID susceptometers\nwith terraced tips that position the pick-up loops 300 nm from the sample. The\n600 nm - 2 um pickup loops, defined by focused ion beam, are integrated into a\n12-layer optical lithography process allowing flux-locked feedback, in situ\nbackground subtraction and optimized flux noise. These features enable a\nsensitivity of ~70 electron spins per root Hertz at 4K.\n",
        "  We present critical fields, thermally-activated flux flow (TAFF) and critical\ncurrent density of tetragonal phase beta-FeSe single crystals. The upper\ncritical fields Hc2(T) for H||(101) and H\\bot(101) are nearly isotropic and are\nlikely governed by Pauli limiting process. The obtained large Ginzburg-Landau\nparameter k \\sim 72.3(2) indicates that beta-FeSe is a type-II superconductor\nwith smaller penetration depth than in Fe(Te,Se). The resistivity below Tc\nfollows Arrhenius TAFF behavior. For both field directions below 30 kOe single\nvortex pinning is dominant whereas collective creep becomes important above 30\nkOe. The critical current density Jc from M-H loops for H||(101) is about five\ntimes larger than for H\\bot(101), yet much smaller than in other iron-based\nsuperconductors.\n",
        "  \"Clustering\" the significance and application of this technique is spread\nover various fields. Clustering is an unsupervised process in data mining, that\nis why the proper evaluation of the results and measuring the compactness and\nseparability of the clusters are important issues.The procedure of evaluating\nthe results of a clustering algorithm is known as cluster validity measure.\nDifferent types of indexes are used to solve different types of problems and\nindices selection depends on the kind of available data.This paper first\nproposes Canonical PSO based K-means clustering algorithm and also analyses\nsome important clustering indices (intercluster, intracluster) and then\nevaluates the effects of those indices on real-time air pollution\ndatabase,wholesale customer, wine, and vehicle datasets using typical K-means,\nCanonical PSO based K-means, simple PSO based K-means,DBSCAN, and Hierarchical\nclustering algorithms.This paper also describes the nature of the clusters and\nfinally compares the performances of these clustering algorithms according to\nthe validity assessment. It also defines which algorithm will be more desirable\namong all these algorithms to make proper compact clusters on this particular\nreal life datasets. It actually deals with the behaviour of these clustering\nalgorithms with respect to validation indexes and represents their results of\nevaluation in terms of mathematical and graphical forms.\n",
        "  We define the fundamental quandle of a spatial graph and several invariants\nderived from it. In the category of graph tangles, we define an invariant based\non the walks in the graph and cocycles from nonabelian quandle cohomology.\n",
        "  We present a non-equilibrium statistical mechanics description of rank\nabundance relations (RAR) in random community models of ecology. Specifically,\nwe study a multi-species replicator system with quenched random interaction\nmatrices. We here consider symmetric interactions as well as asymmetric and\nanti-symmetric cases. RARs are obtained analytically via a generating\nfunctional analysis, describing fixed-point states of the system in terms of a\nsmall set of order parameters, and in dependence on the symmetry or otherwise\nof interactions and on the productivity of the community. Our work is an\nextension of Tokita [Phys. Rev. Lett. {\\bf 93} 178102 (2004)], where the case\nof symmetric interactions was considered within an equilibrium setup. The\nspecies abundance distribution in our model come out as truncated normal\ndistributions or transformations thereof and, in some case, are similar to\nleft-skewed distributions observed in ecology. We also discuss the interaction\nstructure of the resulting food-web of stable species at stationarity, cases of\nheterogeneous co-operation pressures as well as effects of finite system size\nand of higher-order interactions.\n",
        "  A didactic introduction, dated by 1999, to the ideas of the papers\narXiv:q-bio/0701050 and arXiv:0704.0034\n",
        "  Recently it was revealed that the whole Fermi surface is fully gapped for\nseveral families of underdoped cuprates. The existence of the finite energy gap\nalong the $d$-wave nodal lines (\"nodal gap\") contrasts the common understanding\nof the $d$-wave pairing symmetry, which challenges the present theories for the\nhigh-$T_c$ superconductors. Here we propose that the incommensurate diagonal\nspin-density-wave order can account for the above experimental observation. The\nFermi surface and the local density of states are also studied. Our results are\nin good agreement with many important experiments in high-$T_c$\nsuperconductors.\n",
        "  The interface superconductivity in LaAlO$_{3}$-SrTiO$_{3}$ heterostructures\nreveals a non-monotonic behavior of the critical temperature as a function of\nthe two-dimensional density of charge carriers. We develop a theoretical\ndescription of interface superconductivity in strongly polar heterostructures,\nbased on the dielectric function formalism. The density dependence of the\ncritical temperature is calculated accounting for all phonon branches including\ndifferent types of optical (interface and half-space) and acoustic phonons. The\nLO- and acoustic-phonon-mediated electron-electron interaction is shown to be\nthe dominating mechanism governing the superconducting phase transition in the\nheterostructure.\n",
        "  One of the most intriguing questions in evolution is how organisms exhibit\nsuitable phenotypic variation to rapidly adapt in novel selective environments\nwhich is crucial for evolvability. Recent work showed that when selective\nenvironments vary in a systematic manner, it is possible that development can\nconstrain the phenotypic space in regions that are evolutionarily more\nadvantageous. Yet, the underlying mechanism that enables the spontaneous\nemergence of such adaptive developmental constraints is poorly understood. How\ncan natural selection, given its myopic and conservative nature, favour\ndevelopmental organisations that facilitate adaptive evolution in future\npreviously unseen environments? Such capacity suggests a form of\n\\textit{foresight} facilitated by the ability of evolution to accumulate and\nexploit information not only about the particular phenotypes selected in the\npast, but regularities in the environment that are also relevant to future\nenvironments. Here we argue that the ability of evolution to discover such\nregularities is analogous to the ability of learning systems to generalise from\npast experience. Conversely, the canalisation of evolved developmental\nprocesses to past selective environments and failure of natural selection to\nenhance evolvability in future selective environments is directly analogous to\nthe problem of over-fitting and failure to generalise in machine learning. We\nshow that this analogy arises from an underlying mechanistic equivalence by\nshowing that conditions corresponding to those that alleviate over-fitting in\nmachine learning enhance the evolution of generalised developmental\norganisations under natural selection. This equivalence provides access to a\nwell-developed theoretical framework that enables us to characterise the\nconditions where natural selection will find general rather than particular\nsolutions to environmental conditions.\n",
        "  In order to account for the chemical composition of a stellar second\ngeneration (SG), Globular Clusters (GCs) evolution models based on the\nasymptotic giant branch (AGB) scenario so far included only the yields\navailable for the massive AGB stars, while the possible role of super-AGB\nejecta was either extrapolated or not considered. In this work, we explore the\nrole of super-AGB ejecta using yields recently calculated by Ventura and\nD'Antona. Models of clusters showing extended Na-O anticorrelations, like NGC\n2808, indicate that a SG formation history similar to that outlined in our\nprevious work is required: formation of an Extreme population with very large\nhelium content from the pure ejecta of super-AGB stars, followed by formation\nof an Intermediate population by dilution of stellar ejecta with pristine gas.\nThe very O-poor Na-rich Extreme stars can be accounted for once deep-mixing is\nassumed in SG giants forming in a gas with helium abundance Y> 0.34, which\nsignificantly reduces the atmospheric oxygen content, while preserving the\nsodium abundance. On the other hand, for clusters showing a mild O-Na\nanticorrelation, like M 4, the use of the new yields broadens the range of SG\nformation routes leading to abundance patterns consistent with observations. It\nis shown that models in which SG stars form only from super-AGB ejecta promptly\ndiluted with pristine gas can reproduce the observations. We discuss the\nvariety of small helium variations occurring in this model and its relevance\nfor the horizontal branch morphology. In some of these models the duration of\nthe SG formation episode can be as short as \\sim10 Myr; the formation time of\nthe SG is thus compatible with the survival of a cooling flow in the GC,\nprevious to the explosion of the SG core collapse supernovae. We also explore\nmodels with formation of multiple populations in individual bursts, each\nlasting no longer than \\sim10 Myr.\n",
        "  Purpose: Many planning methods for high dose rate (HDR) brachytherapy\ntreatment planning require an iterative approach. A set of computational\nparameters are hypothesized that will give a dose plan that meets dosimetric\ncriteria. A dose plan is computed using these parameters, and if any dosimetric\ncriteria are not met, the process is iterated until a suitable dose plan is\nfound. In this way, the dose distribution is controlled by abstract parameters.\nThe purpose of this study is to improve HDR brachytherapy planning by\ndeveloping a new approach that directly optimizes the dose distribution based\non dosimetric criteria.\n  Method: We develop Inverse Planning by Integer Program (IPIP), an\noptimization model for computing HDR brachytherapy dose plans and a fast\nheuristic for it. We used our heuristic to compute dose plans for 20 anonymized\nprostate cancer patient image data sets from our clinic database. Dosimetry was\nevaluated and compared to dosimetric criteria.\n  Results: Dose plans computed from IPIP satisfied all given dosimetric\ncriteria for the target and healthy tissue after a single iteration. The\naverage target coverage was 95%. The average computation time for IPIP was 30.1\nseconds on a Intel(R) CoreTM2 Duo CPU 1.67 GHz processor with 3 Gib RAM.\n  Conclusion: IPIP is an HDR brachytherapy planning system that directly\nincorporates dosimetric criteria. We have demonstrated that IPIP has clinically\nacceptable performance for the prostate cases and dosimetric criteria used in\nthis study, both in terms of dosimetry and runtime. Further study is required\nto determine if IPIP performs well for a more general group of patients and\ndosimetric criteria, including other cancer sites such as GYN.\n",
        "  Using intrinsic multiple Andreev reflection effect (IMARE) spectroscopy we\nstudied superconducting properties of nearly optimal oxygen-deficient\nGdFeAsO$_{0.88}$ polycrystalline samples (bulk critical temperatures\n$T_C^{bulk} = 49 \\div 52$\\,K). Temperature dependences for two superconducting\ngaps $\\Delta_{L,S}(T)$ ($T_C^{local} = 48 \\div 50$\\,K) have been measured in\nthe range from 4.2 to 50\\,K. The $\\Delta_{L,S}(T)$ dependences were found to\ndeviate from the BCS-like function; this suggests an importance of the\n$k$-space (internal) proximity effect between the two condensates.\n",
        "  In this work we analyze the post monsoon Dengue outbreaks by analyzing the\ntransient and long term dynamics of Dengue incidences and its environmental\ncorrelates in Ahmedabad city in western India from 2005-2012. We calculate the\nreproduction number $R_p$ using the growth rate of post monsoon Dengue\noutbreaks and biological parameters like host and vector incubation periods and\nvector mortality rate, and its uncertainties are estimated through Monte-Carlo\nsimulations by sampling parameters from their respective probability\ndistributions. Reduction in Female Aedes mosquito density required for an\neffective prevention of Dengue outbreaks is also calculated. The non stationary\npattern of Dengue incidences and its climatic correlates like rainfall\ntemperature is analyzed through Wavelet based methods. We find that the mean\ntime lag between peak of monsoon and Dengue is 9 weeks. Monsoon and Dengue\ncases are phase locked from 2008-2012 in the 16-32 weeks band. The duration of\npost monsoon outbreak has been increasing every year, especially post 2008,\neven though the intensity and duration of monsoon has been decreasing.\nTemperature and Dengue incidences show correlations in the same band, but phase\nlock is not stationary.\n",
        "  Dependencies have played a significant role in database design for many\nyears. They have also been shown to be useful in query optimization. In this\npaper, we discuss dependencies between lexicographically ordered sets of\ntuples. We introduce formally the concept of order dependency and present a set\nof axioms (inference rules) for them. We show how query rewrites based on these\naxioms can be used for query optimization. We present several interesting\ntheorems that can be derived using the inference rules. We prove that\nfunctional dependencies are subsumed by order dependencies and that our set of\naxioms for order dependencies is sound and complete.\n",
        "  Search engine logs store detailed information on Web users interactions.\nThus, as more and more people use search engines on a daily basis, important\ntrails of users common knowledge are being recorded in those files. Previous\nresearch has shown that it is possible to extract concept taxonomies from full\ntext documents, while other scholars have proposed methods to obtain similar\nqueries from query logs. We propose a mixture of both lines of research, that\nis, mining query logs not to find related queries nor query hierarchies, but\nactual term taxonomies that could be used to improve search engine\neffectiveness and efficiency. As a result, in this study we have developed a\nmethod that combines lexical heuristics with a supervised classification model\nto successfully extract hyponymy relations from specialization search patterns\nrevealed from log missions, with no additional sources of information, and in a\nlanguage independent way.\n",
        "  While bright, blue, compact galaxies are common at $\\rm z \\sim 1$, they are\nrelatively rare in the local universe, and their evolutionary paths are\nuncertain. We have obtained resolved H I observations of nine $\\rm z \\sim 0$\nluminous compact blue galaxies (LCBGs) using the Giant Metrewave Radio\nTelescope and Very Large Array in order to measure their kinematic and\ndynamical properties and better constrain their evolutionary possibilities. We\nfind that the LCBGs in our sample are rotating galaxies that tend to have\nnearby companions, relatively high central velocity dispersions, and can have\ndisturbed velocity fields. We calculate rotation velocities for each galaxy by\nmeasuring half of the velocity gradient along their major axes and correcting\nfor inclination using axis ratios derived from SDSS images of each galaxy. We\ncompare our measurements to those previously made with single dishes and find\nthat single dish measurements tend to overestimate LCBGs' rotation velocities\nand H I masses. We also compare the ratio of LCBGs' rotation velocities and\nvelocity dispersions to those of other types of galaxies and find that LCBGs\nare strongly rotationally supported at large radii, similar to other disk\ngalaxies, though within their half-light radii the $\\rm V_{rot}/ \\sigma$ values\nof their H I are comparable to stellar $\\rm V_{rot}/ \\sigma$ values of dwarf\nelliptical galaxies. We find that LCBGs' disks on average are gravitationally\nstable, though conditions may be conducive to local gravitational instabilities\nat the largest radii. Such instabilities could lead to the formation of\nstar-forming gas clumps in the disk, resulting eventually in a small central\nbulge or bar.\n",
        "  The forcing of interstellar turbulence, driven mainly by supernova\nexplosions, is irrotational in nature, but the development of significant\namounts of vorticity and helicity, accompanied by large-scale dynamo action,\nhas been reported. Several earlier investigations examined vorticity production\nin simpler systems; here all the relevant processes can be considered\nsimultaneously. We also investigate the mechanisms for the generation of net\nhelicity and large-scale flow in the system. We use a three-dimensional,\nstratified, rotating and shearing local simulation domain of the size 1x1x2\nkpc$^3$, forced with SN explosions occurring at the rate typical of the solar\nneighbourhood in the Milky Way. In addition to the nominal simulation run with\nrealistic Milky Way parameters, we vary the rotation and shear rates, but keep\nthe absolute value of their ratio fixed. Reversing the sign of shear vs.\nrotation allows us to separate the rotation- and shear-generated contributions.\nAs in earlier studies, we find the generation of significant amounts of\nvorticity, with on average 65% of the kinetic energy being in the rotational\nmodes. The vorticity production can be related to the baroclinicity of the\nflow, especially in the regions of hot, dilute clustered supernova bubbles. In\nthese regions, the vortex stretching acts as a sink of vorticity. The net\nhelicities produced by rotation and shear are of opposite signs for physically\nmotivated rotation laws, with the solar neighbourhood parameters resulting in\nthe near cancellation of the total net helicity. We also find the excitation of\noscillatory mean flows, the strength and oscillation period of which depend on\nthe Coriolis and shear parameters; we interpret these as signatures of the\nanisotropic kinetic (AKA) effect. We use the method of moments to fit for the\nturbulent transport coeffcients, and find $\\alpha_{\\rm AKA}$ values of the\norder 3-5 km/s.\n",
        "  Classical knot theory can be generalized to virtual knot theory and spatial\ngraph theory. In 2007, Fleming and Mellor combined virtual knot theory and\nspatial graph theory to form, combinatorially, virtual spatial graph theory. In\nthis paper, we introduce a topological definition of virtual spatial graphs\nthat is similar to the topological definition of a virtual link. Our main goal\nis to generalize the classical Yamada polynomial that is defined for a spatial\ngraph. We define a generalized Yamada polynomial for a virtual spatial graph\nand prove that it can be normalized to a rigid vertex isotopic invariant and to\na pliable vertex isotopic invariant for graphs with maximum degree at most 3.\nWe consider the connection and difference between the generalized Yamada\npolynomial and the Dubrovnik polynomial of a classical link. The generalized\nYamada polynomial specializes to a version of the Dubrovnik polynomial for\nvirtual links such that it can be used to detect the non-classicality of some\nvirtual links. We obtain a specialization for the generalized Yamada polynomial\n(via the Jones-Wenzl projector $P_2$ acting on a virtual spatial graph\ndiagram), that can be used to write a program for calculating it based on\nMathematica Code.\n",
        "  For any given integer $r \\geq 1$ and a quasitoric braid\n$\\beta_r=(\\sigma_r^{-\\epsilon} \\sigma_{r-1}^{\\epsilon}...$ $\n\\sigma_{1}^{(-1)^{r}\\epsilon})^3$ with $\\epsilon=\\pm 1$, we prove that the\nmaximum degree in $z$ of the HOMFLYPT polynomial $P_{W_2(\\hat\\beta_r)}(v,z)$ of\nthe doubled link $W_2(\\hat\\beta_r)$ of the closure $\\hat\\beta_r$ is equal to\n$6r-1$. As an application, we give a family $\\mathcal K^3$ of alternating\nknots, including $(2,n)$ torus knots, 2-bridge knots and alternating pretzel\nknots as its subfamilies, such that the minimal crossing number of any\nalternating knot in $\\mathcal K^3$ coincides with the canonical genus of its\nWhitehead double. Consequently, we give a new family $\\mathcal K^3$ of\nalternating knots for which Tripp's conjecture holds.\n",
        "  The first part of this article is a general introduction to the the theory of\nrepresentation spaces of discrete groups into SL(n,C). Special attention is\npaid to knot groups. In Section 2 we discuss the difference between the tangent\nspace at the representation variety, and the representation scheme. We give an\nexample of Lubotzky and Magid of a non scheme reduced representation (see\nExample 2.18). In the second part recent results about the representation and\ncharacter varieties of knot groups into SL(n,C) with n $\\ge$ 3 are presented.\nThis second part concerns mostly joint work with L. Ben Abdelghani, O.\nMedjerab, V. Mu{\\~n}os and J. Porti.\n",
        "  We argue that immune system is an adaptive complex system. It is shown that\nit has emergent properties. Its network structure is of the small world network\ntype. The network is of the threshold type, which helps in avoiding\nautoimmunity. It has the property that every antigen (e.g.virus or bacteria) is\ntypically attacked by more than one effector. This stabilizes the equilibrium\nstate. Modelling complex systems is discussed. Cellular automata (CA) type\nmodels are successful but there are much less analytic results about CA than\nabout other less successful models e.g. partial differential equations (PDE). A\ncompromise is proposed\n",
        "  In this contribution I shall focus on the structure of the Galactic thin\ndisk. The evolution of the thin disk and its chemical properties have been\ndiscussed in detail by T. Bensby's contribution in conjunction with the\nproperties of the Galactic thick disk, and by L.Olivia in conjunction with the\nproperties of the Galactic bulge. I will review and discuss the status of our\nunderstanding of three major topics, which have been the subject of intense\nresearch nowadays, after long years of silence: (1) the spiral structure of the\nMilky Way, (2) the size of the Galactic disk, and (3) the nature of the Local\narm (Orion spur), where the Sun is immersed. The provisional conclusions of\nthis discussion are that : (1) we still have quite a poor knowledge of the\nMilky Way spiral structure, and the main dis-agreements among various tracers\nare still to be settled; (2) the Galactic disk does clearly \\textit{not} have\nan obvious luminous cut-off at about 14 kpc from the Galactic center, and next\ngeneration Galactic models need to be updated in this respect, and (3) the\nLocal arm is most probably an inter-arm structure, similar to what we see in\nseveral external spirals, like M~74. Finally, the impact of GAIA and LAMOST in\nthis field will be briefly discussed as well.\n",
        "  In this paper we present a novel Neural Network algorithm for conducting\nsemi-supervised learning for sequence labeling tasks arranged in a\nlinguistically motivated hierarchy. This relationship is exploited to\nregularise the representations of supervised tasks by backpropagating the error\nof the unsupervised task through the supervised tasks. We introduce a neural\nnetwork where lower layers are supervised by junior downstream tasks and the\nfinal layer task is an auxiliary unsupervised task. The architecture shows\nimprovements of up to two percentage points F1 for Chunking compared to a\nplausible baseline.\n",
        "  In this work, two-dimensional simulations of the microwave dielectric\nproperties of models with ellipses and realistic models of trabecular bone\ntissue are performed. In these simulations, finite difference time domain\nmethodology has been applied to simulate two-phase structures containing\ninclusions. The results presented here show that the micro-structure is an\nimportant factor in the effective dielectric properties of trabecular bone. We\nconsider the feasibility of using the dielectric behaviour of bone tissue to be\nan indicator of bone health. The frequency used was 950 MHz. It was found that\nthe dielectric properties can be used as an estimate of the degree of\nanisotropy of the micro-structure of the trabecular tissue. Conductivity\nappears to be the most sensitive parameter in this respect. Models with ellipse\nshaped-inclusions are also tested to study their application to modelling bone\ntissue. Models with ellipses that had an aspect ratio of a/b = 1.5 showed\nrelatively good agreement when compared with realistic models of bone tissue.\nAccording to the results presented here, the anisotropy of trabecular bone must\nbe accounted for when measuring its dielectric properties using microwave\nimaging.\n",
        "  The present paper are the notes of a mini-course addressed mainly to\nnon-experts. It purpose it to provide a first approach to the theory of mapping\nclass groups of non-orientable surfaces.\n",
        "  Broad absorption line quasars (BAL QSOs) are objects showing absorption from\nrelativistic outflows, with velocities up to 0.2c. These manifest, in about 15%\nof quasars, as absorption troughs on the blue side of UV emission lines, such\nas C iv and Mg ii. In this work, we complement the information collected in the\ncm band for our previously presented sample of radio loud BAL QSOs with new\nobservations at m and mm bands. Our aim is to verify the presence of old,\nextended radio components in the MHz range, and probe the emission of dust\n(linked to star formation) in the mm domain. We observed 5 sources from our\nsample, already presenting hints of low-frequency emission, with the GMRT at\n235 and 610 MHz. Other 17 sources (more than half the sample) were observed\nwith bolometer cameras at IRAM-30m and APEX. All sources observed with the GMRT\npresent extended emission at a scale of tens of kpc. In some cases these\nmeasurements allow us to identify a second component in the SED, at frequencies\nbelow 1.4 GHz, beyond the one already studied in the GHz domain. In the\nmm-band, only one source shows emission clearly ascribable to dust. Upper\nlimits were obtained for the remaining targets. These findings confirm that BAL\nQSOs can also be present in old radio sources, or even in restarting ones,\nwhere favourable conditions for the outflow launching/acceleration are present.\nA suggestion that these outflows could be precursors of the jet comes from the\nfact that ~70% of our sample can be considered in a GigaHertz Peaked Spectrum\n(GPS) or Compact Steep Spectrum (CSS)+GPS phase. This would confirm the idea\nproposed by other authors that these outflows could be recollimated to form the\njet. Comparing with previous works in the literature, dust emission seems to be\nweaker than the what expected in 'normal' QSOs, suggesting that a feedback\nmechanism could inhibit star formation in radio-loud BAL QSOs.\n",
        "  In superconductors with strong coupling between superconductivity and\nelasticity manifested in a strong dependence of transition temperature on\npressure, there is an additional contribution to inter-vortex interactions due\nto the strain field generated by vortices. When vortex lines are along the $c$\naxis of a tetragonal crystal, a square vortex lattice (VL) is favored at low\nvortex densities, because the vortex-induced strains contribution to the\ninter-vortex interactions is long range. At intermediate magnetic fields, the\ntriangular lattice is stabilized. The triangular lattice evolves to the square\nlattice upon increasing magnetic field, and eventually the system locks to the\nsquare structure. We argue, however, that as $B$ approaches $H_{c2}$ the\nelastic inter-vortex interactions disappear faster than the standard London\ninteractions, so that VL should return to the triangular structure. Our results\nare compared to VLs observed in the heavy fermion superconductor\n$\\mathrm{CeCoIn_5}$.\n",
        "  We show that the bar version of the $\\mathrm{Pin}(2)$-monopole Floer homology\nof a three-manifold $Y$ equipped with a self-conjugate spin$^c$ structure\n$\\mathfrak{s}$ is determined by the triple cup product of $Y$ together with the\nRokhlin invariants of the spin structures inducing $\\mathfrak{s}$. This is a\nmanifestation of mod $2$ index theory, and can be interpreted as a\nthree-dimensional counterpart of Atiyah's classic results regarding spin\nstructures on Riemann surfaces.\n",
        "  Similarity group-by (SGB, for short) has been proposed as a relational\ndatabase operator to match the needs of emerging database applications. Many\nSGB operators that extend SQL have been proposed in the literature, e.g.,\nsimilarity operators in the one-dimensional space. These operators have various\nsemantics. Depending on how these operators are implemented, some of the\nimplementations may lead to different groupings of the data. Hence, if SQL code\nis ported from one database system to another, it is not guaranteed that the\ncode will produce the same results. In this paper, we investigate the various\nsemantics for the relational similarity group-by operators in the\nmulti-dimensional space. We define the class of order-independent SGB operators\nthat produce the same results regardless of the order in which the input data\nis presented to them. Using the notion of interval graphs borrowed from graph\ntheory, we prove that, for certain SGB operators, there exist order-independent\nimplementations. For each of these operators, we provide a sample algorithm\nthat is order-independent. Also, we prove that for other SGB operators, there\ndoes not exist an order-independent implementation for them, and hence these\nSGB operators are ill-defined and should not be adopted in extensions to SQL to\nrealize similarity group-by. In this paper, we introduce an SGB operator,\nnamely SGB-All, for grouping multi-dimensional data using similarity. SGB-All\nforms groups such that a data item, say O, belongs to a group, say G, if and\nonly if O is within a user-defined threshold from all other data items in G. In\nother words, each group in SGB-All forms a clique of nearby data items in the\nmulti-dimensional space. We prove that SGB-All are order-independent, i.e.,\nthere is at least one algorithm for each option that is independent of the\npresentation order of the input data.\n",
        "  The ability to accurately represent sentences is central to language\nunderstanding. We describe a convolutional architecture dubbed the Dynamic\nConvolutional Neural Network (DCNN) that we adopt for the semantic modelling of\nsentences. The network uses Dynamic k-Max Pooling, a global pooling operation\nover linear sequences. The network handles input sentences of varying length\nand induces a feature graph over the sentence that is capable of explicitly\ncapturing short and long-range relations. The network does not rely on a parse\ntree and is easily applicable to any language. We test the DCNN in four\nexperiments: small scale binary and multi-class sentiment prediction, six-way\nquestion classification and Twitter sentiment prediction by distant\nsupervision. The network achieves excellent performance in the first three\ntasks and a greater than 25% error reduction in the last task with respect to\nthe strongest baseline.\n",
        "  Bayesian inference is an important technique throughout statistics. The\nessence of Beyesian inference is to derive the posterior belief updated from\nprior belief by the learned information, which is a set of differentially\nprivate answers under differential privacy. Although Bayesian inference can be\nused in a variety of applications, it becomes theoretically hard to solve when\nthe number of differentially private answers is large. To facilitate Bayesian\ninference under differential privacy, this paper proposes a systematic\nmechanism. The key step of the mechanism is the implementation of Bayesian\nupdating with the best linear unbiased estimator derived by Gauss-Markov\ntheorem. In addition, we also apply the proposed inference mechanism into an\nonline queryanswering system, the novelty of which is that the utility for\nusers is guaranteed by Bayesian inference in the form of credible interval and\nconfidence level. Theoretical and experimental analysis are shown to\ndemonstrate the efficiency and effectiveness of both inference mechanism and\nonline query-answering system.\n",
        "  It is well-known that any pair of closed orientable 3-manifolds are related\nby a finite sequence of Dehn surgeries on knots. Furthermore Kawauchi showed\nthat such knots can be taken to be hyperbolic. In this article, we consider the\nminimal length of such sequences connecting a pair of 3-manifolds, in\nparticular, a pair of lens spaces.\n",
        "  The purpose of this study is to evaluate the clinical usefulness of modulated\narc (mARC) treatment techniques. The mARC treatment plans of the non-small cell\nlung cancer (NSCLC) patients were performed in order to verify the clinical\nusefulness of mARC. A pre study was conducted to find the most competent plan\ncondition of mARC treatment and the usefulness of mARC treatment plan was\nevaluated by comparing it with the other Arc treatment plans such as\nTomotherapy and RapidArc. In the case of mARC, the optimal condition for the\nmARC plan was determined by comparing the dosimetric performance of the mARC\nplans with the use of various parameters. The various parameters includes the\nphoton energies (6 MV, 10 MV), optimization point angle (6{\\deg}-10{\\deg}\nintervals), and total segment number (36-59 segment). The best dosimetric\nperformance of mARC was observed at 10 MV photon energy and the point angle 6\ndegree, and 59 segments. The each treatment plans of three different techniques\nwere compared with the following parameters: conformity index (CI), homogeneity\nindex (HI), target coverage, dose in the OARs, monitor units (MU), beam on time\nand the normal tissue complication probability (NTCP). As a result, all three\ndifferent treatment techniques show the similar target coverage. The mARC\nresults the lowest V20 and MU per fraction compared with both RapidArc and\nTomotherapy plan. The mARC plan reduces the beam on time as well. Therefore,\nthe results of this study provided a satisfactory result which mARC technique\nis considered as a useful clinical technique for radiation treatment.\n",
        "  Neural networks with attention have proven effective for many natural\nlanguage processing tasks. In this paper, we develop attention mechanisms for\nuncertainty detection. In particular, we generalize standardly used attention\nmechanisms by introducing external attention and sequence-preserving attention.\nThese novel architectures differ from standard approaches in that they use\nexternal resources to compute attention weights and preserve sequence\ninformation. We compare them to other configurations along different dimensions\nof attention. Our novel architectures set the new state of the art on a\nWikipedia benchmark dataset and perform similar to the state-of-the-art model\non a biomedical benchmark which uses a large set of linguistic features.\n",
        "  Future health systems require the means to assess and track the neural and\nphysiological function of a user over long periods of time and in the\ncommunity. Human body responses are manifested through multiple modalities,\nsuch as the mechanical, electrical and chemical; yet current physiological\nmonitors (actigraphy, heart rate) largely lack in both the desired cross-modal\nand non-stigmatizing aspects. We address these challenges through an\ninconspicuous and comfortable earpiece, equipped with miniature multimodal\nsensors, which benefits from the relatively stable position of the ear canal\nwith respect to vital organs to robustly measure the brain, cardiac and\nrespiratory functions. Comprehensive experiments validate each modality within\nthe proposed earpiece, while its potential in health monitoring is illustrated\nthrough case studies. We further demonstrate how combining data from multiple\nsensors within such an integrated wearable device improves both the accuracy of\nmeasurements and the ability to deal with artifacts in real-life scenarios.\n",
        "  We argue that the stochastic dynamics of interacting agents which replicate,\nmutate and die constitutes a non-equilibrium physical process akin to aging in\ncomplex materials. Specifically, our study uses extensive computer simulations\nof the Tangled Nature Model (TNM) of biological evolution to show that\npunctuated equilibria successively generated by the model's dynamics have\nincreasing entropy and are separated by increasing entropic barriers. We\nfurther show that these states are organized in a hierarchy and that limiting\nthe values of possible interactions to a finite interval leads to stationary\nfluctuations within a component of the latter. A coarse-grained description\nbased on the temporal statistics of quakes, the events leading from one\ncomponent of the hierarchy to the next, accounts for the logarithmic growth of\nthe population and the decaying rate of change of macroscopic variables.\nFinally, we question the role of fitness in large scale evolution models and\nspeculate on the possible evolutionary role of rejuvenation and memory effects.\n",
        "  Growing evidence supports an unusual elemental feature appearing in nearby\ndwarf galaxies, especially dwarf spheroidals (dSphs), indicating a key process\nof galaxy evolution that is different from that of the Galaxy. In addition to\nthe well-known deficiency of alpha-elements in dSphs, recent observations have\nclearly shown that s-process elements (Ba) are significantly enhanced relative\nto Fe, alpha-, and r-process elements. This enhancement occurs in some dSphs as\nwell as in the Large Magellanic Cloud, but is unseen in the Galaxy. Here we\nreport that this feature is evidence of the lack of very massive stars (> 25\nsolar mass) as predicted in the low star formation rate environment, and we\nconclude that the unique elemental feature of dwarf galaxies including a\nlow-alpha/Fe ratio in some low-metallicity stars is, at least in some part,\ncharacterized by a different form of the initial mass function. We present a\ndetailed model for the Fornax dSph galaxy and discuss its complex chemical\nenrichment history together with the nucleosynthesis site of the light\ns-process element Y.\n",
        "  We examine the spin and charge excitations in antiferromagnetic iron\npnictides by mean-field calculations with a random phase approximation in a\nfive-band itinerant model. The calculated excitation spectra reproduce well\nspin-wave dispersions observed in inelastic neutron scattering, with a\nrealistic magnetic moment for CaFe$_2$As$_2$. A particle-hole gap is found to\nbe crucial to obtain consistent results; we predict the spin wave in LaFeAsO\ndisappears at a lower energy than in CaFe$_2$As$_2$. We analyze that the charge\ndynamics to make predictions for resonant inelastic x-ray scattering spectra.\n",
        "  Dimensionality reduction is a critical step in scaling machine learning\npipelines. Principal component analysis (PCA) is a standard tool for\ndimensionality reduction, but performing PCA over a full dataset can be\nprohibitively expensive. As a result, theoretical work has studied the\neffectiveness of iterative, stochastic PCA methods that operate over data\nsamples. However, termination conditions for stochastic PCA either execute for\na predetermined number of iterations, or until convergence of the solution,\nfrequently sampling too many or too few datapoints for end-to-end runtime\nimprovements. We show how accounting for downstream analytics operations during\nDR via PCA allows stochastic methods to efficiently terminate after operating\nover small (e.g., 1%) subsamples of input data, reducing whole workload\nruntime. Leveraging this, we propose DROP, a DR optimizer that enables speedups\nof up to 5x over Singular-Value-Decomposition-based PCA techniques, and exceeds\nconventional approaches like FFT and PAA by up to 16x in end-to-end workloads.\n",
        "  Much of the work on modeling the spread of viral infections utilized partial\ndifferential equa- tions. Traveling-wave solutions to these PDEs are typically\nconcentrated on velocities and their dependence on the various parameters. Most\nof the investigations into the dynamical interaction of virus and defective\ninterfering particles (DIP), which are incomplete forms of the virus that\nreplicate through co-infection, have followed the same lines. In this work we\npresent an agent based model of viral infection with consideration of DIP and\nthe negative feedback loop introduced by interferon production as part of the\nhost innate immune response. The model is based high resolution microscopic\nimages of plaques of dead cells we took from mammalian cells infected with\nSendai virus with low and high DIP. In order to investigate the effects of the\ndiscrete stochastic microscopic mechanisms, which are responsible for virus\nspreading, have on the macroscopic growth of viral plaques, we generate an\nagent-based model of viral infection. The two main aims of this work are to (i)\ninvestigate the effects of discrete microscopic randomness on the macroscopic\ngrowth of viral plaques; and (ii) examine the dynamic interactions between the\nfull length virus, DIP and interferon, and interpret what may be the function\nof DIP. We find that we can explain the qualitative differences between our\nstochastic model and deterministic models in terms of the fractal geometry of\nthe resulting plaques, and that DIP have a delaying effect while the\ninteraction between interferon and DIP has a slowing effect on the growth of\nviral plaques, potentially contributing to viral latency.\n",
        "  The quasispecies model describes processes related to the origin of life and\nviral evolutionary dynamics. We discuss how the error catastrophe that reflects\nthe transition from localized to delocalized quasispecies population is\naffected by catalytic replication of different reaction orders. Specifically,\nwe find that 2nd order mechanisms lead to 1st order discontinuous phase\ntransitions in the viable population fraction, and conclude that the \"higher\"\nthe interaction the \"lower\" the transition. We discuss potential implications\nfor understanding the replication of highly mutating RNA viruses.\n",
        "  Multiple generations of stars are routinely encountered in globular clusters\nbut no convincing evidence has been found in Galactic open clusters to date. In\nthis paper we use new photometric and spectroscopic data to search for multiple\nstellar population signatures in the old, massive open cluster, Melotte~66. The\ncluster is known to have a red giant branch wide in color, which could be an\nindication of metallicity spread. Also the main sequence is wider than what is\nexpected from photometric errors only. This evidence might be associated with\neither differential reddening or binaries. Both hypothesis have, however, to be\nevaluated in detail before recurring to the presence of multiple stellar\npopulations. New, high-quality, CCD UBVI photometry have been acquired to this\naim with high-resolution spectroscopy of seven clump stars, that are\ncomplemented with literature data. Our photometric study confirms that the\nwidth of the main sequence close to the turn off point is entirely accounted\nfor by binary stars and differential reddening, with no need to advocate more\nsofisticated scenarios, such as metallicity spread or multiple main sequences.\nBy constructing synthetic color-magnitude diagrams, we infer that the binary\nfraction has to be as large as 30$%$ and their mass ratio in the range 0.6-1.0.\nAs a by-product of our simulations, we provide new estimates of the cluster\nfundamental parameters. We measure a reddening E(B-V)=0.15$\\pm$0.02, and\nconfirm the presence of a marginal differential reddening. The distance to the\ncluster is $4.7^{+0.2}_{-0.1} $kpc and the age is 3.4$\\pm$0.3 Gyr, which is\nsomewhat younger and better constrained than previous estimates. Our detailed\nabundance analysis reveals that, overall, Melotte~66 looks like a typical\nobject of the old thin disk population.\n",
        "  We present an observational study of the effect of bars on the gas component\nand on the star formation properties of their host galaxies in a statistically\nsignificant sample of resolved objects, the $Herschel$ Reference Sample. The\nanalysis of optical and far--infrared images allows us to identify a clear\nspatial correlation between stellar bars and the cold-gas distribution mapped\nby the warm dust emission. We find that the infrared counterparts of optically\nidentified bars are either bar--like structures or dead central regions in\nwhich star formation is strongly suppressed. Similar morphologies are found in\nthe distribution of star formation directly traced by H$\\alpha$ maps. The sizes\nof such optical and infrared structures correlate remarkably well, hinting at a\ncausal connection. In the light of previous observations and of theoretical\ninvestigations in the literature, we interpret our findings as further evidence\nof the scenario in which bars drive strong inflows toward their host nuclei:\nyoung bars are still in the process of perturbing the gas and star formation\nclearly delineates the shape of the bars; old bars on the contrary already\nremoved any gas within their extents, carving a dead region of negligible star\nformation.\n",
        "  Gordon and Litherland showed that all compact, unoriented, possibly\nnon-orientable surfaces in $S^3$ bounded by a link are realted by\nattaching/deleting tubes and half twisted bands. In this note we give an\nelementary proof for this result.\n",
        "  We study the geometric properties of the terms of the Goldman bracket between\ntwo free homotopy classes of oriented closed curves in a hyperbolic surface. We\nprovide an obstruction for the equality of two terms in the Goldman bracket,\nnamely if two terms in the Goldman bracket are equal to each other then for\nevery hyperbolic metric, the angles corresponding to the intersection points\nare equal to each other. As a consequence, we obtain an alternative proof of a\ntheorem of Chas, i.e. if one of the free homotopy classes contains a simple\nrepresentative then the geometric intersection number and the number of terms\n(counted with multiplicity) in the Goldman bracket are the same.\n",
        "  This paper addresses the problem of representing the set of repairs of a\npossibly inconsistent database by means of a disjunctive database.\nSpecifically, the class of denial constraints is considered. We show that,\ngiven a database and a set of denial constraints, there exists a (unique)\ndisjunctive database, called canonical, which represents the repairs of the\ndatabase w.r.t. the constraints and is contained in any other disjunctive\ndatabase with the same set of minimal models. We propose an algorithm for\ncomputing the canonical disjunctive database. Finally, we study the size of the\ncanonical disjunctive database in the presence of functional dependencies for\nboth repairs and cardinality-based repairs.\n",
        "  Alma Cycle 3 observations showed strong absorption from diffuse molecular gas\nin the bulge at -200 \\kms\\ $< {\\rm v} < -140$ \\kms\\ toward J1744-3116 (l,b)=\n(-2.13d,-1d) We aimed to test if bulge molecular gas could also be seen toward\nthe three other sufficiently strong mm-wave sources seen toward the bulgeat\n$|b| < 3$\\deg We took absorption profiles of \\hcop (1-0) and other species in\nALMA Cy 4 toward J1713-3418, J1717-3341, J1733-3722 and J1744-3116. Strong\nmolecular absorption from disk gas at $|\\rmv| \\la 30$ \\kms\\ was detected in all\ndirections, and absorption from the 3 kpc arm was newly detected toward J1717\nand J1744. However, only the sightline toward J1744 is dominated by molecular\ngas overall and no other sightlines showed molecular absorption from gas deep\ninside the bulge. No molecular absorption was detected toward J1717 where H I\nemission from the bulge was previously known. As observed in \\hcop, HCN, \\cch\\\nand CS, the bulge gas toward J1744 at $v < -135$ \\kms\\ has chemistry and\nkinematics like that seen near the Sun and in the Milky Way disk generally. We\nmeasured isotopologic ratios N(\\hcop)/N(H$^{13}$CO\\p) $> 51~(3\\sigma)$ for the\nbulge gas toward J1744 and $58\\pm9$ and $64\\pm4$ for the disk gas toward J1717\nand J1744, respectively, all well above the value of 20-25 typical of the\ncentral molecular zone.} %conclusions heading (optional), leave it empty if\nnecessary {The kinematics and chemistry of the bulge gas observed toward J1744\nmore nearly resemble that of gas in the Milky Way disk than in the central\nmolecular zone.}\n",
        "  Long pulse durations necessary in selective inversion recovery (SIR)\nexperiments along with radiation damping (RD) introduce difficulties in\nquantitative nuclear magnetic resonance measurements, such as those that allow\nfor the determination of a sample's characteristics, including the rates that\ngovern magnetization transfer. Because of these influences, the assumption of\nperfect inversion is invalid. In this work, we present data that demonstrates\nthat long pulse durations as well as RD cause difficulties in SIR experiments\nperformed on simple one-spin systems, indicating that they will be problematic\nfor multiple-spin systems as well. These results emphasize the importance of\nunderstanding the evolution of magnetization for all time points throughout an\nexperiment used in quantitative NMR measurements. Furthermore, experimental\nparameters must be chosen carefully and understood completely.\n",
        "  Here we present a calculation of the temperature-dependent London penetration\ndepth, $\\lambda(T)$, in Ba$_{1-x}$K$_{x}$Fe$_2$As$_2$ (BKFA) on the basis of\nthe electronic band structure [1,2] and momentum-dependent superconducting gap\n[3] extracted from angle-resolved photoemission spectroscopy (ARPES) data. The\nresults are compared to the direct measurements of $\\lambda(T)$ by muon spin\nrotation ($\\mu$SR) [4]. The value of $\\lambda(T=0)$, calculated with \\emph{no}\nadjustable parameters, equals 270 nm, while the directly measured one is 320\nnm; the temperature dependence $\\lambda(T)$ is also easily reproduced. Such\nagreement between the two completely different approaches allows us to conclude\nthat ARPES studies of BKFA are bulk-representative. Our review of the available\nexperimental studies of the superconducting gap in the new iron-based\nsuperconductors in general allows us to state that all hole-doped of them bear\ntwo nearly isotropic gaps with coupling constants $2\\Delta/k_{\\rm B}T_{\\rm\nc}=2.5\\pm1.5$ and $7\\pm2$.\n",
        "  The spectra of 413 star-forming (or HII) regions in M33 (NGC 598) were\nobserved by using the multifiber spectrograph of Hectospec at the 6.5-m\nMultiple Mirror Telescope (MMT). By using this homogeneous spectra sample, we\nmeasured the intensities of emission lines and some physical parameters, such\nas electron temperatures, electron densities, and metallicities. Oxygen\nabundances were derived via the direct method (when available) and two\nempirical strong-line methods, namely, O3N2 and N2. In the high-metallicity\nend, oxygen abundances derived from O3N2 calibration were higher than those\nderived from N2 index, indicating an inconsistency between O3N2 and N2\ncalibrations. We presented a detailed analysis of the spatial distribution of\ngas-phase oxygen abundances in M33 and confirmed the existence of the\naxisymmetric global metallicity distribution widely assumed in literature.\nLocal variations were also observed and subsequently associated with spiral\nstructures to provide evidence of radial migration driven by arms. Our O/H\ngradient fitted out to 1.1 $R_{25}$ resulted in slopes of $-0.17\\pm0.03$,\n$-0.19\\pm0.01$, and $-0.16\\pm0.17$ dex $R_{25}^{-1}$ utilizing abundances from\nO3N2, N2 diagnostics, and direct method, respectively.\n",
        "  At this centenary of the discovery of superconductivity, the design of new\nand more useful superconductors remains as enigmatic as ever. These materials\nplay crucial roles both for fundamental science and applications, and they hold\ngreat promise in addressing our global energy challenge. The recent discovery\nof a new class of high-temperature superconductors has made the community more\nenthusiastic than ever about finding new superconductors. Historically, these\ndiscoveries were almost completely guided by serendipity, and now, researchers\nin the field have grown into an enthusiastic global network to find a way,\ntogether, to predictively design new superconductors. After a short history of\ndiscoveries of superconducting materials, we share our own guidelines for\nsearching for high-temperature superconductors. Finally, we show how point\ncontact spectroscopy (PCS) is used to detect strong correlations in the normal\nstate with a focus on the strongly-correlated region in the normal state of the\nFe-based superconductors, defining a new region in the phase diagram of\nBa(Fe1-xCox)2As2.\n",
        "  Astrometric Very Long Baseline Interferometry (VLBI) observations of maser\nsources in the Milky Way are used to map the spiral structure of our Galaxy and\nto determine fundamental parameters such as the rotation velocity ($\\Theta_0$)\nand curve and the distance to the Galactic center (R$_0$). Here, we present an\nupdate on our first results, implementing a recent change in the knowledge\nabout the Solar motion. It seems unavoidable that the IAU recommended values\nfor R$_0$ and $\\Theta_0$ need a substantial revision. In particular the\ncombination of 8.5 kpc and 220 \\kms\\, can be ruled out with high confidence.\nCombining the maser data with the distance to the Galactic center from stellar\norbits and the proper motion of Sgr\\,A* gives best values of R$_0$ = 8.3 $\\pm$\n0.23 kpc and $\\Theta_0$ = 239 or 246 $\\pm$ 7 \\kms, for Solar motions of V$_\n\\odot$ = 12.23 and 5.25 \\kms, respectively. Finally, we give an outlook to\nfuture observations in the Bar and Spiral Structure Legacy (BeSSeL) Survey.\n",
        "  Knowledge bases of entities and relations (either constructed manually or\nautomatically) are behind many real world search engines, including those at\nYahoo!, Microsoft, and Google. Those knowledge bases can be viewed as graphs\nwith nodes representing entities and edges representing (primary)\nrelationships, and various studies have been conducted on how to leverage them\nto answer entity seeking queries. Meanwhile, in a complementary direction,\nanalyses over the query logs have enabled researchers to identify entity pairs\nthat are statistically correlated. Such entity relationships are then presented\nto search users through the \"related searches\" feature in modern search\nengines. However, entity relationships thus discovered can often be \"puzzling\"\nto the users because why the entities are connected is often indescribable. In\nthis paper, we propose a novel problem called \"entity relationship\nexplanation\", which seeks to explain why a pair of entities are connected, and\nsolve this challenging problem by integrating the above two complementary\napproaches, i.e., we leverage the knowledge base to \"explain\" the connections\ndiscovered between entity pairs. More specifically, we present REX, a system\nthat takes a pair of entities in a given knowledge base as input and\nefficiently identifies a ranked list of relationship explanations. We formally\ndefine relationship explanations and analyze their desirable properties.\nFurthermore, we design and implement algorithms to efficiently enumerate and\nrank all relationship explanations based on multiple measures of\n\"interestingness.\" We perform extensive experiments over real web-scale data\ngathered from DBpedia and a commercial search engine, demonstrating the\nefficiency and scalability of REX. We also perform user studies to corroborate\nthe effectiveness of explanations generated by REX.\n",
        "  Intrafraction motion can play a pivotal role in the success of abdominal and\nthoracic radiation therapy. Hybrid magnetic resonance-guided radiotherapy\nsystems have the potential to control for intrafraction motion. Recently, we\nintroduced an MRI sequence capable of acquiring real-time cine imaging in two\northogonal planes (SOPI). We extend SOPI here to permit dynamic updating of\nslice positions in one plane while keeping the other plane position fixed. In\nthis implementation, cine images from the static plane are used for motion\nmonitoring and as image navigators to sort stepped images in the other plane,\nproducing dynamic 4D image volumes for use in dose reconstruction. A custom\n3D-printed target filled with radiochromic FXG gel was interfaced to a dynamic\nmotion phantom. 4D-SOPI was acquired in a dynamic motion phantom driven by an\nactual patient respiratory waveform displaying amplitude/frequency variations\nand drifting and in a healthy volunteer. Unique 4D-MRI epochs were\nreconstructed from a time series of phantom motion. Dose from a static 4cmx15cm\nfield was calculated on each 4D respiratory phase bin and epoch image, scaled\nby the time spent in each bin, and then rigidly accumulated. The phantom was\nthen positioned on an Elekta MR Linac and irradiated while moving. Following\nirradiation, actual dose deposited to the FXG gel was determined by applying a\nR1 versus dose calibration curve to R1 maps of the phantom. The 4D-SOPI cine\nimages produced a respiratory motion navigator that was highly correlated with\nthe actual phantom motion (CC=0.9981). The mean difference between the\naccumulated and measured dose inside the target was 4.4% of the maximum\nprescribed dose. These results provide early validation that 4D-SOPI\nsimultaneously enables real-time motion monitoring and truth-in-delivery\nanalysis for integrated MR-guided radiation therapy (MR-gRT) systems.\n",
        "  This paper considers a two-dimensional logistic model to study populations\nwith two genders. The growth behavior of a population is guided by two coupled\nordinary differential equations given by a non-differentiable vector field\nwhose parameters are the secondary sex ratio (the ratio of males to females at\ntime of birth), inter-, intra- and outer-gender competitions, fertility and\nmortality rates and a mating function. For the case where there is no\ninter-gender competition and the mortality rates are negligible with respect to\nthe density-dependent mortality, using geometrical techniques, we analyze the\nsingularities and the basin of attraction of the system, determining the\nrelationships between the parameters for which the system presents an\nequilibrium point. In particular, we describe conditions on the secondary sex\nratio and discuss the role of the average number of female sexual partners of\neach male for the conservation of a two-sex species.\n",
        "  In this paper we re-analyse the amplification process of broadband continuum\nradiation by astronomical masers in one-dimensional case. The basic equations\nappropriate for the scalar maser and the random nature of the maser radiation\nfield are derived from basic physical principles. Comparision with the standard\nradiation transfer equation allows us to examine the underlying assumptions\ninvolved in the current theory of astronomical masers. Simulations are carried\nout to follow the amplification of different realisations of the broadband\nbackground radiation by the maser. The observable quantities such as intensity,\nspectral line profile are obtained by averaging over an ensemble of the\nemerging radiation corresponding to the amplified background radiation field.\nOur simulations show that the fluctuations of the radiation field inside the\nastronomical maser deviates significantly from Gaussian statistics even when\nthe maser is only partially saturated. Coupling between different frequency\nmodes and the population pulsing are shown to have increasing importance in the\ntransport of maser radiation as the maser approaches saturation. Our results\nsuggest that the standard formulation of radiation transfer provides a\nsatisfactory description of the intensity and the line narrowing effect in the\nunsaturated and partially saturated masers within the framework of\none-dimensional model. Howerver, the application of the same formulation to the\nstrong saturation regime should be considered with caution.\n",
        "  We used PdBI observations of SiO (2-1) to investigate the morphology and\nprofile of the SiO emission within several massive dense clumps (MDCs) in\nCygnus-X. We find that most molecular outflows are detected in both SiO and CO,\nalthough there are some cases of CO outflows with no SiO counterpart. We find a\nsignificant amount of narrow line SiO emission that appears to be unrelated to\noutflows. The fraction of the total SiO luminosity that is not associated with\noutflows is highly variable in the different MDCs (from 10% to 90%); this might\nbe a problem when extrapolating outflow properties from SiO luminosities\nwithout resolving individual outflows. The extent of the narrow SiO emission\nvaries from rather compact (~ 0.03 pc) to widespread (~0.2 pc), and its\nkinematics often differs from those found by other high-density tracers such as\nH13CO+. We find that the least centrally concentrated clumps with the least\nmassive protostellar cores have the most widespread narrow SiO emission. In\nline with previous evidence of SiO emission associated with low-velocity\nshocks, we propose an evolutionary picture to explain the existence and\ndistribution of narrow SiO line profiles. In this scenario, the least centrally\ncondensed MDCs are at an early stage where the SiO emission traces shocks from\nthe large-scale collapse of material onto the MDC (e.g. CygX-N40). As the MDC\ncollapses, the SiO emission becomes more confined to the close surroundings of\ncores, tracing the post-shock material from the infalling MDC against the dense\ncores (e.g. CygX-N3, N12, and N48). At later stages, when single massive\nprotostars are formed, the SiO luminosity is largely dominated by powerful\noutflows, and the weaker narrow component shows perhaps the last remnants of\nthe initial collapse (e.g. CygX-N53 and N63).\n",
        "  It is shown in this paper that given any closed oriented hyperbolic\n3-manifold, every closed oriented 3-manifold is mapped onto by a finite cover\nof that manifold via a map of degree 1, or in other words, virtually\n1-dominated by that manifold. This improves a known result of virtual\n2-domination. The proof invokes a recently developed enhanced version of the\nconnection principle in good pants constructions.\n",
        "  The growing volumes of XML data sources on the Web or produced by\nenterprises, organizations etc. raise many performance challenges for data\nmanagement applications. In this work, we are concerned with the distributed,\npeer-to-peer management of large corpora of XML documents, based on distributed\nhash table (or DHT, in short) overlay networks. We present ViP2P (standing for\nViews in Peer-to-Peer), a distributed platform for sharing XML documents based\non a structured P2P network infrastructure (DHT). At the core of ViP2P stand\ndistributed materialized XML views, defined by arbitrary XML queries, filled in\nwith data published anywhere in the network, and exploited to efficiently\nanswer queries issued by any network peer. ViP2P allows user queries to be\nevaluated over XML documents published by peers in two modes. First, a\nlong-running subscription mode, when a query can be registered in the system\nand receive answers incrementally when and if published data matches the query.\nSecond, queries can also be asked in an ad-hoc, snapshot mode, where results\nare required immediately and must be computed based on the results of other\nlong-running, subscription queries. ViP2P innovates over other similar\nDHT-based XML sharing platforms by using a very expressive structured XML query\nlanguage. This expressivity leads to a very flexible distribution of XML\ncontent in the ViP2P network, and to efficient snapshot query execution. ViP2P\nhas been tested in real deployments of hundreds of computers. We present the\nplatform architecture, its internal algorithms, and demonstrate its efficiency\nand scalability through a set of experiments. Our experimental results outgrow\nby orders of magnitude similar competitor systems in terms of data volumes,\nnetwork size and data dissemination throughput.\n",
        "  Quantiles are very important statistics information used to describe the\ndistribution of datasets. Given the quantiles of a dataset, we can easily know\nthe distribution of the dataset, which is a fundamental problem in data\nanalysis. However, quite often, computing quantiles directly is inappropriate\ndue to the memory limitations. Further, in many settings such as data streaming\nand sensor network model, even the data size is unpredictable. Although the\nquantiles computation has been widely studied, it was mostly in the sequential\nsetting. In this paper, we study several quantile computation algorithms in the\ndistributed setting and compare them in terms of space usage, running time, and\naccuracy. Moreover, we provide detailed experimental comparisons between\nseveral popular algorithms. Our work focuses on the approximate quantile\nalgorithms which provide error bounds. Approximate quantiles have received more\nattentions than exact ones since they are often faster, can be more easily\nadapted to the distributed setting while giving sufficiently good statistical\ninformation on the data sets.\n",
        "  It is conjectured that a hyperbolic knot admits at most three Dehn surgeries\nwhich yield closed three manifolds containing incompressible tori. We show that\nthere exist infinitely many hyperbolic knots which attain the conjectural\nmaximum number. Interestingly, those surgeries correspond to consecutive\nintegers.\n",
        "  Populations can evolve in order to adapt to external changes. The capacity to\nevolve and adapt makes successful treatment of infectious diseases and cancer\ndifficult. Indeed, therapy resistance has quickly become a key challenge for\nglobal health. Therefore, ideas of how to control evolving populations in order\nto overcome this threat are valuable. Here we use the mathematical concepts of\nstochastic optimal control to study what is needed to control evolving\npopulations. Following established routes to calculate control strategies, we\nfirst study how a polymorphism can be maintained in a finite population by\nadaptively tuning selection. We then introduce a minimal model of drug\nresistance in a stochastically evolving cancer cell population and compute\nadaptive therapies, where decisions are based on monitoring the response of the\ntumor, which can outperform established therapy paradigms. For both case\nstudies, we demonstrate the importance of high-resolution monitoring of the\ntarget population in order to achieve a given control objective: to control one\nmust monitor.\n",
        "  We propose a selective encoding model to extend the sequence-to-sequence\nframework for abstractive sentence summarization. It consists of a sentence\nencoder, a selective gate network, and an attention equipped decoder. The\nsentence encoder and decoder are built with recurrent neural networks. The\nselective gate network constructs a second level sentence representation by\ncontrolling the information flow from encoder to decoder. The second level\nrepresentation is tailored for sentence summarization task, which leads to\nbetter performance. We evaluate our model on the English Gigaword, DUC 2004 and\nMSR abstractive sentence summarization datasets. The experimental results show\nthat the proposed selective encoding model outperforms the state-of-the-art\nbaseline models.\n",
        "  We present an analysis of archival {\\it HST/ACS} imaging in the F475W\n($g_{475}$), F606W ($V_{606}$) and F814W ($I_{814}$) bands of the globular\ncluster (GC) system of a large (3.4 kpc effective radius) ultra-diffuse galaxy\n(DF17) believed located in the Coma Cluster of galaxies. We detect 11 GCs down\nto the 5$\\sigma$ completeness limit of the imaging ($I_{814}=$27 mag).\nCorrecting for background and our detection limits yields a total population of\nGCs in this galaxy of $27\\pm5$ and a $V$-band specific frequency, $S_N=28\\pm5$.\nBased on comparisons to the GC systems of Local galaxies, we show that both the\nabsolute number and the colors of the GC system of DF17 are consistent with the\nGC system of a dark-matter dominated dwarf galaxy with virial mass\n$\\sim0.9\\times10^{10}$~\\msun and a dark-to-stellar mass ratio, $M_{vir} / M_{\nstar}\\sim 1000$. Based on the stellar mass-growth of the Milky Way, we show\nthat DF17 cannot be understood as a failed Milky Way-like system, but is more\nsimilar to quenched Large Magellanic Cloud-like systems. We find that the mean\ncolor of GC population, $g_{475}-I_{814}$ = $0.91\\pm0.05$ mag, coincides with\nthe peak of the color distribution of intracluster GCs and are also similar to\nthose of the blue GCs in the outer regions of massive galaxies. We suggest that\nboth the intracluster GC population in Coma and the blue-peak in the GC\npopulations of massive galaxies may be fed - at least in part - by the\ndisrupted equivalents of systems such as DF17.\n",
        "  In this paper, the support genus of all Legendrian right handed trefoil knots\nand some other Legendrian knots is computed. We give examples of Legendrian\nknots in the three-sphere with the standard contact structure which have\npositive support genus with arbitrarily negative Thurston-Benniquin invariant.\nThis answers a question in Onaran.\n",
        "  The admittance of two types of Josephson weak links is calculated, i.e., of a\none-dimensional superconducting wire with a local suppression of the order\nparameter, and the second is a short S-c-S structure, where S denotes a\nsuperconductor and c---a constriction. The systems of the first type are\nanalyzed on the basis of time-dependent Ginzburg-Landau equations. We show that\nthe impedance $Z(\\Omega)$ has a maximum as a function of the frequency\n$\\Omega$, and the electric field $E_{\\Omega}$ is determined by two\ngauge-invariant quantities---the condensate momentum $Q_{\\Omega}$ and the\npotential $\\mu$ related to charge imbalance. The structures of the second type\nare studied on the basis of microscopic equations for quasiclassical Green's\nfunctions in the Keldysh technique. For short S-c-S contacts (the Thouless\nenergy ${E_{\\text{Th}} = D/L^{2} \\gg \\Delta}$) we present a formula for\nadmittance $Y$ valid at frequencies $\\Omega$ and temperatures $T$ less than the\nThouless energy but arbitrary with respect to the energy gap $\\Delta$. It is\nshown that, at low temperatures, the absorption is absent [${\\mathrm{Re}(Y) =\n0}$] if the frequency does not exceed the energy gap in the center of the\nconstriction (${\\Omega < \\Delta \\cos \\varphi_{0}}$, where $2 \\varphi_{0}$ is\nthe phase difference between the S reservoirs). The absorption gradually\nincreases with increasing the difference ${(\\Omega - \\Delta \\cos \\varphi_{0})}$\nif $2 \\varphi_{0}$ is less than the phase difference $2 \\varphi_{\\text{c}}$\ncorresponding to the critical Josephson current. In the interval ${2\n\\varphi_{\\text{c}} < 2 \\varphi_{0} < \\pi}$, the absorption has a maximum. This\ninterval of the phase difference is achievable in phase-biased Josephson\njunctions. Close to $T_{\\text{c}}$ the admittance has a maximum at low $\\Omega$\nwhich is described by an analytical formula.\n",
        "  Regular expressions with capture variables, also known as \"regex formulas,\"\nextract relations of spans (interval positions) from text. These relations can\nbe further manipulated via Relational Algebra as studied in the context of\ndocument spanners, Fagin et al.'s formal framework for information extraction.\nWe investigate the complexity of querying text by Conjunctive Queries (CQs) and\nUnions of CQs (UCQs) on top of regex formulas. We show that the lower bounds\n(NP-completeness and W[1]-hardness) from the relational world also hold in our\nsetting; in particular, hardness hits already single-character text! Yet, the\nupper bounds from the relational world do not carry over. Unlike the relational\nworld, acyclic CQs, and even gamma-acyclic CQs, are hard to compute. The source\nof hardness is that it may be intractable to instantiate the relation defined\nby a regex formula, simply because it has an exponential number of tuples. Yet,\nwe are able to establish general upper bounds. In particular, UCQs can be\nevaluated with polynomial delay, provided that every CQ has a bounded number of\natoms (while unions and projection can be arbitrary). Furthermore, UCQ\nevaluation is solvable with FPT (Fixed-Parameter Tractable) delay when the\nparameter is the size of the UCQ.\n",
        "  The propagation of unreliable information is on the rise in many places\naround the world. This expansion is facilitated by the rapid spread of\ninformation and anonymity granted by the Internet. The spread of unreliable\ninformation is a wellstudied issue and it is associated with negative social\nimpacts. In a previous work, we have identified significant differences in the\nstructure of news articles from reliable and unreliable sources in the US\nmedia. Our goal in this work was to explore such differences in the Brazilian\nmedia. We found significant features in two data sets: one with Brazilian news\nin Portuguese and another one with US news in English. Our results show that\nfeatures related to the writing style were prominent in both data sets and,\ndespite the language difference, some features have a universal behavior, being\nsignificant to both US and Brazilian news articles. Finally, we combined both\ndata sets and used the universal features to build a machine learning\nclassifier to predict the source type of a news article as reliable or\nunreliable.\n",
        "  James et al. (2012) presented simulations that apparently falsify the\nanalytical result by Bastolla et al. (2009), who showed that nested mutualistic\ninteractions decrease interspecific competition and increase biodiversity in\nmodel ecosystems. This contradiction, however, mainly stems from the incorrect\napplication of formulas derived for fully connected networks to empirical,\nsparse networks.\n",
        "  IBM models are very important word alignment models in Machine Translation.\nFollowing the Maximum Likelihood Estimation principle to estimate their\nparameters, the models will easily overfit the training data when the data are\nsparse. While smoothing is a very popular solution in Language Model, there\nstill lacks studies on smoothing for word alignment. In this paper, we propose\na framework which generalizes the notable work Moore [2004] of applying\nadditive smoothing to word alignment models. The framework allows developers to\ncustomize the smoothing amount for each pair of word. The added amount will be\nscaled appropriately by a common factor which reflects how much the framework\ntrusts the adding strategy according to the performance on data. We also\ncarefully examine various performance criteria and propose a smoothened version\nof the error count, which generally gives the best result.\n",
        "  Bitmap indexes must be compressed to reduce input/output costs and minimize\nCPU usage. To accelerate logical operations (AND, OR, XOR) over bitmaps, we use\ntechniques based on run-length encoding (RLE), such as Word-Aligned Hybrid\n(WAH) compression. These techniques are sensitive to the order of the rows: a\nsimple lexicographical sort can divide the index size by 9 and make indexes\nseveral times faster. We investigate row-reordering heuristics. Simply\npermuting the columns of the table can increase the sorting efficiency by 40%.\nSecondary contributions include efficient algorithms to construct and aggregate\nbitmaps. The effect of word length is also reviewed by constructing 16-bit,\n32-bit and 64-bit indexes. Using 64-bit CPUs, we find that 64-bit indexes are\nslightly faster than 32-bit indexes despite being nearly twice as large.\n",
        "  Most of the work on query evaluation in probabilistic databases has focused\non the simple tuple-independent data model, where tuples are independent random\nevents. Several efficient query evaluation techniques exists in this setting,\nsuch as safe plans, algorithms based on OBDDs, tree-decomposition and a variety\nof approximation algorithms. However, complex data analytics tasks often\nrequire complex correlations, and query evaluation then is significantly more\nexpensive, or more restrictive. In this paper, we propose MVDB as a framework\nboth for representing complex correlations and for efficient query evaluation.\nAn MVDB specifies correlations by views, called MarkoViews, on the\nprobabilistic relations and declaring the weights of the view's outputs. An\nMVDB is a (very large) Markov Logic Network. We make two sets of contributions.\nFirst, we show that query evaluation on an MVDB is equivalent to evaluating a\nUnion of Conjunctive Query(UCQ) over a tuple-independent database. The\ntranslation is exact (thus allowing the techniques developed for tuple\nindependent databases to be carried over to MVDB), yet it is novel and quite\nnon-obvious (some resulting probabilities may be negative!). This translation\nin itself though may not lead to much gain since the translated query gets\ncomplicated as we try to capture more correlations. Our second contribution is\nto propose a new query evaluation strategy that exploits offline compilation to\nspeed up online query evaluation. Here we utilize and extend our prior work on\ncompilation of UCQ. We validate experimentally our techniques on a large\nprobabilistic database with MarkoViews inferred from the DBLP data.\n",
        "  A common problem which is faced by the researchers when dealing with arterial\ncarotid imaging data is the registration of the geometrical structures between\ndifferent imaging modalities or different timesteps. The use of the \"Patient\nPosition\" DICOM field is not adequate to achieve accurate results due to the\nfact that the carotid artery is a relatively small structure and even\nimperceptible changes in patient position and/or direction make it difficult.\nWhile there is a wide range of simple/advanced registration techniques in the\nliterature, there is a considerable number of studies which address the\ngeometrical structure of the carotid artery without using any registration\ntechnique. On the other hand the existence of various registration techniques\nprohibits an objective comparison of the results using different registration\ntechniques. In this paper we present a method for estimating the statistical\nsignificance that the choice of the registration technique has on the carotid\ngeometry. One-Way Analysis of Variance(ANOVA) showed that the p-values were\n<0.0001 for the distances of the lumen from the centerline for both right and\nleft carotids of the patient case that was studied.\n",
        "  HDBSCAN*, a state-of-the-art density-based hierarchical clustering method,\nproduces a hierarchical organization of clusters in a dataset w.r.t. a\nparameter mpts. While the performance of HDBSCAN* is robust w.r.t. mpts in the\nsense that a small change in mpts typically leads to only a small or no change\nin the clustering structure, choosing a \"good\" mpts value can be challenging:\ndepending on the data distribution, a high or low value for mpts may be more\nappropriate, and certain data clusters may reveal themselves at different\nvalues of mpts. To explore results for a range of mpts values, however, one has\nto run HDBSCAN* for each value in the range independently, which is\ncomputationally inefficient. In this paper, we propose an efficient approach to\ncompute all HDBSCAN* hierarchies for a range of mpts values by replacing the\ngraph used by HDBSCAN* with a much smaller graph that is guaranteed to contain\nthe required information. An extensive experimental evaluation shows that with\nour approach one can obtain over one hundred hierarchies for the computational\ncost equivalent to running HDBSCAN* about 2 times.\n",
        "  We determine the anisotropy of the spin fluctuation induced pairing gap on\nthe Fermi surface of the FeAs based superconductors as function of the exchange\nand Hund's coupling $J_{H}$. We find that for sufficiently large $J_{H}$,\nnearly commensurate magnetic fluctuations yield a fully gapped\n$s^{\\pm}$-pairing state with small anisotropy of the gap amplitude on each\nFermi surface sheet, but significant variations of the gap amplitude for\ndifferent sheets of the Fermi surface. In particular, we obtain the large\nvariation of the gap amplitude on different Fermi surface sheets, as seen in\nARPES experiments. For smaller values of Hund's coupling incommensurate\nmagnetic fluctuations yield an $s^{\\pm}$-pairing state with line nodes. Such a\nstate is also possible once the anisotropy of the material is reduced and three\ndimensional effects come into play.\n",
        "  Core decomposition is a fundamental graph problem with a large number of\napplications. Most existing approaches for core decomposition assume that the\ngraph is kept in memory of a machine. Nevertheless, many real-world graphs are\nbig and may not reside in memory. In the literature, there is only one work for\nI/O efficient core decomposition that avoids loading the whole graph in memory.\nHowever, this approach is not scalable to handle big graphs because it cannot\nbound the memory size and may load most parts of the graph in memory. In\naddition, this approach can hardly handle graph updates. In this paper, we\nstudy I/O efficient core decomposition following a semi-external model, which\nonly allows node information to be loaded in memory. This model works well in\nmany web-scale graphs. We propose a semi-external algorithm and two optimized\nalgorithms for I/O efficient core decomposition using very simple structures\nand data access model. To handle dynamic graph updates, we show that our\nalgorithm can be naturally extended to handle edge deletion. We also propose an\nI/O efficient core maintenance algorithm to handle edge insertion, and an\nimproved algorithm to further reduce I/O and CPU cost by investigating some new\ngraph properties. We conduct extensive experiments on 12 real large graphs. Our\noptimal algorithm significantly outperform the existing I/O efficient algorithm\nin terms of both processing time and memory consumption. In many\nmemory-resident graphs, our algorithms for both core decomposition and\nmaintenance can even outperform the in-memory algorithm due to the simple\nstructures and data access model used. Our algorithms are very scalable to\nhandle web-scale graphs. As an example, we are the first to handle a web graph\nwith 978.5 million nodes and 42.6 billion edges using less than 4.2 GB memory.\n",
        "  We have performed N-body simulations of globular clusters (GCs) in order to\nestimate a detection rate of mergers of Binary stellar-mass Black Holes (BBHs)\nby means of gravitational wave (GW) observatories. For our estimate, we have\nonly considered mergers of BBHs which escape from GCs (BBH escapers). BBH\nescapers merge more quickly than BBHs inside GCs because of their small\nsemi-major axes. N-body simulation can not deal with a GC with the number of\nstars N ~ 10^6 due to its high calculation cost. We have simulated dynamical\nevolution of small-N clusters (10^4 <~ N <~ 10^5), and have extrapolated our\nsimulation results to large-N clusters. From our simulation results, we have\nfound the following dependence of BBH properties on N. BBHs escape from a\ncluster at each two-body relaxation time at a rate proportional to N.\nSemi-major axes of BBH escapers are inversely proportional to N, if initial\nmass densities of clusters are fixed. Eccentricities, primary masses, and mass\nratios of BBH escapers are independent of N. Using this dependence of BBH\nproperties, we have artificially generated a population of BBH escapers from a\nGC with N ~ 10^6, and have estimated a detection rate of mergers of BBH\nescapers by next-generation GW observatories. We have assumed that all the GCs\nare formed 10 or 12Gyrs ago with their initial numbers of stars N_i=5 x 10^5 --\n2 x 10^6 and their initial stellar mass densities inside their half-mass radii\n\\rho_h,i=6 x 10^3 -- 10^6M_sun pc^-3. Then, the detection rate of BBH escapers\nis 0.5 -- 20 yr^-1 for a BH retention fraction R_BH=0.5. A few BBH escapers are\ncomponents of hierarchical triple systems, although we do not consider secular\nperturbation on such BBH escapers for our estimate. Our simulations have shown\nthat BHs are still inside some of GCs at the present day. These BHs may\nmarginally contribute to BBH detection.\n",
        "  We present a sample of 40 AGN in dwarf galaxies at redshifts $z \\lesssim$\n2.4. The galaxies are drawn from the \\textit{Chandra} COSMOS-Legacy survey as\nhaving stellar masses $10^{7}\\leq M_{*}\\leq3 \\times 10^{9}$ M$_{\\odot}$. Most\nof the dwarf galaxies are star-forming. After removing the contribution from\nstar formation to the X-ray emission, the AGN luminosities of the 40 dwarf\ngalaxies are in the range $L_\\mathrm{0.5-10 keV} \\sim10^{39} - 10^{44}$ erg\ns$^{-1}$. With 12 sources at $z > 0.5$, our sample constitutes the\nhighest-redshift discovery of AGN in dwarf galaxies. The record-holder is\ncid\\_1192, at $z = 2.39$ and with $L_\\mathrm{0.5-10 keV} \\sim 10^{44}$ erg\ns$^{-1}$. One of the dwarf galaxies has $M_\\mathrm{*} = 6.6 \\times 10^{7}$\nM$_{\\odot}$ and is the least massive galaxy found so far to host an AGN. All\nthe AGN are of type 2 and consistent with hosting intermediate-mass black holes\n(BHs) with masses $\\sim 10^{4} - 10^{5}$ M$_{\\odot}$ and typical Eddington\nratios $> 1\\%$. We also study the evolution, corrected for completeness, of AGN\nfraction with stellar mass, X-ray luminosity, and redshift in dwarf galaxies\nout to $z$ = 0.7. We find that the AGN fraction for $10^{9}< M_{*}\\leq3 \\times\n10^{9}$ M$_{\\odot}$ and $L_\\mathrm{X} \\sim 10^{41}-10^{42}$ erg s$^{-1}$ is\n$\\sim$0.4\\% for $z \\leq$ 0.3 and that it decreases with X-ray luminosity and\ndecreasing stellar mass. Unlike massive galaxies, the AGN fraction seems to\ndecrease with redshift, suggesting that AGN in dwarf galaxies evolve\ndifferently than those in high-mass galaxies. Mindful of potential caveats, the\nresults seem to favor a direct collapse formation mechanism for the seed BHs in\nthe early Universe.\n",
        "  Let Ng be the connected closed nonorientable surface of genus g >= 5 and\nMod(Ng) denote the mapping class group of Ng. We prove that the outer\nautomorphism group of Mod(Ng) is either trivial or Z if g is odd, and injects\ninto the mapping class group of sphere with four holes if g is even.\n",
        "  In genetic drift of small population, it is well known that even when the\nratio of alleles is 0.5, specific genes are fixed in or disappear from the\npopulation. It seems the reason why inbreeding is avoided. On the other hand,\nthis phenomenon suggests an interesting possibility. The mutant gene does not\nincrease the number of genes at once in a large population. A gene is partially\nfixed by increasing the number within a small population because of inbreeding,\nand the gene increases in a large group by Darwin's natural selection. It would\nbe more reasonable to think in this way. We studied this mathematically based\non the concept of genetic drift. This suggested that inbreeding could be useful\nas a trigger for fixation of mutation.\n",
        "  We present Charagram embeddings, a simple approach for learning\ncharacter-based compositional models to embed textual sequences. A word or\nsentence is represented using a character n-gram count vector, followed by a\nsingle nonlinear transformation to yield a low-dimensional embedding. We use\nthree tasks for evaluation: word similarity, sentence similarity, and\npart-of-speech tagging. We demonstrate that Charagram embeddings outperform\nmore complex architectures based on character-level recurrent and convolutional\nneural networks, achieving new state-of-the-art performance on several\nsimilarity tasks.\n",
        "  Over the years numerous models of SIS (susceptible - infected - susceptible)\ndisease dynamics unfolding on networks have been proposed. Here, we discuss the\nlinks between many of these models and how they can be viewed as more general\nmotif-based models. We illustrate how the different models can be derived from\none another and, where this is not possible, discuss extensions to established\nmodels that enables this derivation. We also derive a general result for the\nexact differential equations for the expected number of an arbitrary motif\ndirectly from the Kolmogorov/master equations and conclude with a comparison of\nthe performance of the different closed systems of equations on networks of\nvarying structure.\n",
        "  We presented a study on the Galactic bubble N4 using the 13.7 m millimeter\ntelescope of Purple Mountain Observatory at the Qinghai Station. N4 is one of\nthe science demonstration regions for the Milky Way Imaging Scroll Painting\n(WMISP). Simultaneous observations of $^{12}$CO (J = 1$-$0), $^{13}$CO (J =\n1$-$0) and C$^{18}$O (J = 1$-$0) line emission towards N4 were carried out. We\nanalyzed the spectral profile and the distribution of the molecular gas.\nMorphologically, the CO emissions correlate well with Spitzer IRAC 8.0 $\\mu$m\nemission. The channel map and velocity-position diagram shows that N4 is more\nlikely an inclined expanding ring than a spherical bubble. We calculated the\nphysical parameters of N4 including the mass, size, column density and optical\ndepth. Some massive star candidates were discovered in the region of N4 using\n(J, J$-$H) color-magnitude diagram. We found an energy source candidate for the\nexpansion of N4, a massive star with a mass of ${\\sim} 15\\,M_{\\odot}$ and an\nage of $\\sim$ 1 Myr. There exists infall motion signature in N4, which can be a\ngood candidate of infall area. Combined mm and infrared data, we think there\nmay exists triggered star formation in N4.\n",
        "  For a genus two Heegaard splitting of a lens space, the primitive disk\ncomplex is defined to be the full subcomplex of the disk complex for one of the\nhandlebodies of the splitting spanned by all vertices of primitive disks. In\nthis work, we describe the complete combinatorial structure of the primitive\ndisk complex for the genus two Heegaard splitting of each lens space. In\nparticular, we find all lens spaces whose primitive disk complexes are\ncontractible.\n",
        "  A sparsity-exploiting algorithm intended for few-view Single Photon Emission\nComputed Tomography (SPECT) reconstruction is proposed and characterized. The\nalgorithm models the object as piecewise constant subject to a blurring\noperation. To validate that the algorithm closely approximates the true object\nin the noiseless case, projection data were generated from an object assuming\nthis model and using the system matrix. Monte Carlo simulations were performed\nto provide more realistic data of a phantom with varying smoothness across the\nfield of view. Reconstructions were performed across a sweep of two primary\ndesign parameters. The results demonstrate that the algorithm recovers the\nobject in a noiseless simulation case. While the algorithm assumes a specific\nblurring model, the results suggest that the algorithm may provide high\nreconstruction accuracy even when the object does not match the assumed\nblurring model. Generally, increased values of the blurring parameter and TV\nweighting parameters reduced noise and streaking artifacts, while decreasing\nspatial resolution. As the number of views decreased from 60 to 9 the accuracy\nof images reconstructed using the proposed algorithm varied by less than 3%.\nOverall, the results demonstrate preliminary feasibility of a\nsparsity-exploiting reconstruction algorithm which may be beneficial for\nfew-view SPECT.\n",
        "  Recent observations have revealed that at least 8 globular clusters (GCs) in\nthe Galaxy show internal abundance spreads in [Fe/H]. We investigate the origin\nof these `anomalous' GCs using numerical simulations of GCs in the dwarfs\norbiting around the Galaxy and chemical evolution model of dwarfs hosting the\nGCs. The principal results are as follows. GCs formed in a host dwarf galaxy\nwith a total mass of ~ 10^10 M_sun can merge to form a single nuclear GC before\nthe host is completely destroyed by the Galaxy, if they are massive (> 3*10^5\nM_sun) and if they are formed in the inner region (R<400 pc). The GC merger\nremnants can capture field stars during its spiral-in to nuclear regions. If\ntwo GCs are formed from star formation events separated by ~300 Myr in their\nhost dwarf, then the new GC formed from GC merging can have [Fe/H] spread of\n0.2 dex and [Ba/Fe] spread of 0.3 dex. GCs formed from GC merging can show\nvariety of internal abundance spreads depending on the details of their hosts'\nchemical evolution. We suggest that anomalous GCs were formed from GC merging\nthat occurred before the destruction of GC host dwarfs yet after\nself-enrichment processes responsible for the observed anti-correlations\nbetween chemical abundances of light elements. We also suggest that the\nobserved no/little dependence of [Eu/Fe] on [Fe/H] in the Galactic GC M22 is\nevidence of massive dwarf galaxies hosting these anomalous GCs.\n",
        "  Research in blast-induced lung injury resulted in exposure thresholds that\nare useful in understanding and protecting humans from such injury. Because\ntraumatic brain injury (TBI) due to blast exposure has become a prominent\nmedical and military problem, similar thresholds should be identified that can\nput available research results in context and guide future research toward\nprotecting warfighters as well as diagnosis and treatment. At least three\nmechanical mechanisms by which the blast wave may result in brain injury have\nbeen proposed - a thoracic mechanism, head acceleration and direct cranial\ntransmission. These mechanisms need not be mutually exclusive. In this study,\nlikely regions of interest for the first two mechanisms based on blast\ncharacteristics (positive pulse duration and peak effective overpressure) are\ndeveloped using available data from blast experiments and related studies,\nincluding behind-armor blunt trauma and ballistic pressure wave studies. These\nrelated studies are appropriate to include because blast-like pressure waves\nare produced that result in neurological effects like those caused by blast.\nResults suggest that injury thresholds for each mechanism are dependent on\nblast conditions, and that under some conditions, more than one mechanism may\ncontribute. There is a subset of blast conditions likely to result in TBI due\nto head acceleration and/or a thoracic mechanism without concomitant lung\ninjury. These results can be used to guide experimental designs and compare\nadditional data as they become available. Additional data are needed before\nactual probabilities or severity of TBI for a given exposure can be described.\n",
        "  The star formation properties of early-type galaxies (ETGs) are currently the\nsubject of considerable interest, particularly whether they differ from those\nof gas-rich spirals. We perform a systematic study of star formation in a large\nsample of local ETGs using polycyclic aromatic hydrocarbon (PAH) and dust\nemission, focusing on the galaxies' star formation rates (SFRs) and star\nformation efficiencies (SFEs). Our sample is composed of the 260 ETGs from the\nATLAS3D survey, from which we use the cold gas measurements (HI and CO). The\nSFRs are estimated from stellar, PAH and dust fits to spectral energy\ndistributions created from new AKARI measurements and literature data from WISE\nand 2MASS. The mid-infrared luminosities of non-CO-detected galaxies are well\ncorrelated with their stellar luminosities, showing that they trace\n(circum)stellar dust emission. CO-detected galaxies show an excess above these\ncorrelations, uncorrelated with their stellar luminosities, indicating that\nthey likely contain PAHs and dust of interstellar origin. PAH and dust\nluminosities of CO-detected galaxies show tight correlations with their\nmolecular gas masses, and the derived current SFRs are typically 0.01-1\nMsun/yr. These SFRs systematically decrease with stellar age at fixed stellar\nmass, while they correlate nearly linearly with stellar mass at fixed age. The\nmajority of local ETGs follow the same star-formation law as local star-forming\ngalaxies, and their current SFEs do not depend on either stellar mass or age.\nOur results clearly indicate that molecular gas is fueling current star\nformation in local ETGs, that appear to acquire this gas via mechanisms\nregulated primarily by stellar mass. The current SFEs of local ETGs are similar\nto those of local star-forming galaxies, indicating that their low SFRs are\nlikely due to smaller cold gas fractions rather than a suppression of star\nformation.\n",
        "  Recent studies have shown that human populations have experienced a complex\ndemographic history, including a recent epoch of rapid population growth that\nled to an excess in the proportion of rare genetic variants in humans today.\nThis excess can impact the burden of private mutations for each individual,\ndefined here as the proportion of heterozygous variants in each newly sequenced\nindividual that are novel compared to another large sample of sequenced\nindividuals. We calculated the burden of private mutations predicted by\ndifferent demographic models, and compared with empirical estimates based on\ndata from the NHLBI Exome Sequencing Project and data from the Neutral Regions\n(NR) dataset. We observed a significant excess in the proportion of private\nmutations in the empirical data compared with models of demographic history\nwithout a recent epoch of population growth. Incorporating recent growth into\nthe model provides a much improved fit to empirical observations. This\nphenomenon becomes more marked for larger sample sizes. The proportion of\nprivate mutations is additionally increased by purifying selection, which\ndifferentially affect mutations of different functional annotations. These\nresults have important implications to the design and analysis of\nsequencing-based association studies of complex human disease as they pertain\nto private and very rare variants.\n",
        "  We re-examine the evolutionary dynamics of RNA secondary structures under\ndirectional selection towards an optimum RNA structure. We find that the\npunctuated equilibria lead to a very slow approach to the optimum, following on\naverage an inverse power of the evolutionary time. In addition, our study of\nthe trajectories shows that the out-of-equilibrium effects due to the\nevolutionary process are very weak. In particular, the distribution of\ngenotypes is close to that arising during equilibrium stabilizing selection. As\na consequence, the evolutionary dynamics leave almost no measurable\nout-of-equilibrium trace, only the transition genotypes (close to the border\nbetween different periods of stasis) have atypical mutational properties.\n",
        "  This paper describes the AMU-UEDIN submissions to the WMT 2016 shared task on\nnews translation. We explore methods of decode-time integration of\nattention-based neural translation models with phrase-based statistical machine\ntranslation. Efficient batch-algorithms for GPU-querying are proposed and\nimplemented. For English-Russian, our system stays behind the state-of-the-art\npure neural models in terms of BLEU. Among restricted systems, manual\nevaluation places it in the first cluster tied with the pure neural model. For\nthe Russian-English task, our submission achieves the top BLEU result,\noutperforming the best pure neural system by 1.1 BLEU points and our own\nphrase-based baseline by 1.6 BLEU. After manual evaluation, this system is the\nbest restricted system in its own cluster. In follow-up experiments we improve\nresults by additional 0.8 BLEU.\n",
        "  Purpose: To introduce and evaluate the use of stable distributions as a means\nof describing the behavior of charged particle pencil beams in a medium, with\nspecific emphasis on proton beam scanning (PBS). Methods: The proton pencil\nbeams of a clinically commissioned proton treatment facility are replicated in\na Monte Carlo simulation system (FLUKA). For each available energy the beam\ndeposition in water medium is characterized by the dose deposition. Using an\nalpha--stable distribution methodology each beam with a nominal energy $E$ is\ncharacterized by the lateral spread at depth $z$: $S(z;\\alpha,\\gamma,E)$ and a\ntotal energy deposition $I_D(z)$. The beams are then described as a function of\nthe variation of the parameters at depth. Finally, an implementation in a\nfreely available open source dose calculation suite (matRad, DKFZ, Heidelberg,\nGermany) is proposed. Results: Quantitatively, the fit of the stable\ndistributions, compared to those implemented in standard treatment planning\nsystems, are equivalent. The efficiency of the representation is better (2\ncompared to 3 and more parameters needed). The meta--parametrization (i.e. the\ndescription of the dose deposition by only providing the fitted parameters)\nallows for interpolation of non--measured data. In the case of the clinical\ndata used in this paper, it was possible to only commission 1 out of 5 nominal\nenergies to obtain a viable data set. Conclusions: Alpha--stable distributions\nare intrinsically suited to describe charged particle pencil beams in a medium\nand can be easily implemented in existing treatment planning systems. The use\nof alpha-distributions can easily be extended to other particles.\n",
        "  We have systematically searched for the ground state structures of bismuth\nhydrides based on evolutionary algorithm method and particle swarm optimization\nalgorithm method. Given only rich-hydrogen region, except for BiH$_{3}$, other\nhydrides (BiH, BiH$_{2}$, BiH$_{4}$, BiH$_{5}$, BiH$_{6}$) have been predicted\nto be stable with pressurization. With the increase of hydrogen content,\nhydrogen exists in bismuth hydrides with the different forms and presents the\ncharacteristics of ionicity. Under high pressure, the remarkable structural\nfeature is the emergence of H$_{2}$ units in BiH$_{2}$, BiH$_{4}$ and\nBiH$_{6}$, and BiH$_{6}$ adopts a startling layered structure intercalated by\nH$_{2}$ and the linear H$_{3}$ units. Further calculations show these\nenergetically stable hydrides are good metal and their metallic pressures are\nlower than that of pure solid hydrogen because of the doping impurities. The\n$T_{c}$ in the range of 20-119 K has been calculated by the Allen-Dynes\nmodified McMillan equation, which indicates all these stable hydrides are\npotential high-temperature superconductors. Remarkably, it is the H-Bi-H and Bi\natoms vibrations rather than the high-frequency H$_{2}$ or H$_{3}$ units that\ndominate the superconductivity. In addition, hydrogen content has a great\ninfluence on the superconducting transition temperature.\n",
        "  HIV is a deadly virus transmitted either through having of unprotected sex,\nmother to child transmission, sharing of unsterilized objects that is capable\nof making cut or wounds on the body, through blood or bodily fluid\ntransmission. AIDS has no permanent cure but remedies that help in suppressing\nthe power effect of the virus are available. Previous studies has shown that\nthe epidemic claimed more lives via vertical transmission, mother-to-child\ntransmission, blood transfusion, sexual intercourse and injection drug users.\nThe associated models were derived using diagnosis and treatments records of\nconfirmed status of HIV/AIDS clients. A new model analysis were proposed that\ncould handle cases of HIV/AIDS routes of transmission and treatments via\nvertical, mother-to-child, blood transfusion, sexual intercourse, injection\ndrug users that were not considered in previous studies. The new model was\nsolved using Fractional Differential Transformation by Caputo sense algorithm.\nThe new model is a major contribution for optimal assessment, monitoring,\nevaluation, control and management of HIV/AIDS, with 75% accuracy.\n",
        "  We consider the interplay between superconducting (SC) and commensurate\nspin-density-wave (SDW) orders in iron-pnictides by analyzing a multiple order\nGinzburg-Landau free energy. We are particularly interested in whether the\ndoping-induced transition between the two states is first order, or the two\npure phases are separated by an intermediate phase with coexisting SC and SDW\norders. For perfect nesting, the two orders do not coexist, because SDW order,\nwhich comes first, gaps the full Fermi surface leaving no space for SC to\ndevelop. When nesting is not perfect due to either ellipticity of electron\nbands or doping-induced difference in chemical potentials for holes and\nelectrons, SDW order still leaves modified Fermi surfaces for not too strong\nSDW magnetism and the SC order may develop. We show that the two orders coexist\nonly when certain relations between ellipticity and doping are met. In\nparticular, in a compensated metal, ellipticity alone is not sufficient for\ncoexistence of the two orders.\n",
        "  We present a three epoch survey for transient and variables in the extended\nChandra Deep Field South at 5.5 GHz with the Australia Telescope Compact Array.\nA region covering $\\sim$0.3 deg$^{2}$ was observed on timescales of 2.5 months\nand 2.5 years and typical sensitivities 12.1 $-$ 17.1 $\\mu$Jy beam$^{-1}$\n(1$\\sigma$) were achieved. This survey represents the deepest search for\ntransient and variable radio sources at 5.5 GHz. In total 124 sources were\ndetected above the 5.5$\\sigma$ level. One highly variable radio source was\nfound with $\\Delta S > 50%$ implying a surface density of $\\sim$3 deg$^{-2}$. A\nfurther three radio sources were found with lower levels of variability\nequating to a surface density of $\\sim$13 deg$^{-2}$ above a detection\nthreshold of 82.3 $\\mu$Jy. All of the variable sources have inverted radio\nspectra (between 1.4 and 5.5 GHz) and are associated with active galactic\nnuclei. We conclude that these variables are young gigahertz peaked-spectrum\nsources with active and self-absorbed radio jets. We explore the variability\ncompleteness of this sample and conclude that the fairly low levels of\nvariability would only be detectable in 3$-$25% of all sources within the\nfield. No radio transients were detected in this survey and we place an upper\nlimit on the surface density of transient events $ < 7.5$ deg$^{-2}$ above a\ndetection threshold of 68.8~$\\mu$Jy.\n",
        "  The electron-boson spectral density function I^2ChiOmega responsible for\ncarrier scattering of the high temperature superconductor HgBa2CuO4 (Tc = 90 K)\nis calculated from new data on the optical scattering rate. A maximum entropy\ntechnique is used. Published data on HgBa2Ca2Cu3O8 (Tc = 130 K) are also\ninverted and these new results are put in the context of other known cases. All\nspectra (with two notable exceptions) show a peak at an energy (Omega_r)\nproportional to the superconducting transition temperature Omega_r ~= 6.3\nkB.Tc. This charge channel relationship follows closely the magnetic resonance\nseen by polarized neutron scattering, Omega_r^{neutron} ~= 5.4 kB.Tc. The\namplitudes of both peaks decrease strongly with increasing temperature. In some\ncases, the peak at Omega_r is weak and the spectrum can have additional maxima\nand a background extending up to several hundred meV.\n",
        "  Shark is a new data analysis system that marries query processing with\ncomplex analytics on large clusters. It leverages a novel distributed memory\nabstraction to provide a unified engine that can run SQL queries and\nsophisticated analytics functions (e.g., iterative machine learning) at scale,\nand efficiently recovers from failures mid-query. This allows Shark to run SQL\nqueries up to 100x faster than Apache Hive, and machine learning programs up to\n100x faster than Hadoop. Unlike previous systems, Shark shows that it is\npossible to achieve these speedups while retaining a MapReduce-like execution\nengine, and the fine-grained fault tolerance properties that such engines\nprovide. It extends such an engine in several ways, including column-oriented\nin-memory storage and dynamic mid-query replanning, to effectively execute SQL.\nThe result is a system that matches the speedups reported for MPP analytic\ndatabases over MapReduce, while offering fault tolerance properties and complex\nanalytics capabilities that they lack.\n",
        "  We present deep optical images of the Large and Small Magellanic Clouds (LMC\nand SMC) using a low cost telephoto lens with a wide field of view to explore\nstellar substructure in the outskirts of the stellar disk of the LMC (r < 10\ndegrees from the center). These data have higher resolution than existing star\ncount maps, and highlight the existence of stellar arcs and multiple spiral\narms in the northern periphery, with no comparable counterparts in the South.\nWe compare these data to detailed simulations of the LMC disk outskirts,\nfollowing interactions with its low mass companion, the SMC. We consider\ninteraction in isolation and with the inclusion of the Milky Way tidal field.\nThe simulations are used to assess the origin of the northern structures,\nincluding also the low density stellar arc recently identified in the DES data\nby Mackey et al. 2015 at ~ 15 degrees. We conclude that repeated close\ninteractions with the SMC are primarily responsible for the asymmetric stellar\nstructures seen in the periphery of the LMC. The orientation and density of\nthese arcs can be used to constrain the LMC's interaction history with and\nimpact parameter of the SMC. More generally, we find that such asymmetric\nstructures should be ubiquitous about pairs of dwarfs and can persist for 1-2\nGyr even after the secondary merges entirely with the primary. As such, the\nlack of a companion around a Magellanic Irregular does not disprove the\nhypothesis that their asymmetric structures are driven by dwarf-dwarf\ninteractions.\n",
        "  We used the Atacama Large (sub-)Millimeter Array (ALMA) and the IRAM Plateau\nde Bure Interferometer (PdBI) to image, with an angular resolution of 0.5$''$\n(120 au) and 1$''$ (235 au), respectively, the emission from 11 different\norganic molecules in the protostellar binary NGC1333 IRAS 4A. We clearly\ndisentangled A1 and A2, the two protostellar cores present. For the first time,\nwe were able to derive the column densities and fractional abundances\nsimultaneously for the two objects, allowing us to analyse the chemical\ndifferences between them. Molecular emission from organic molecules is\nconcentrated exclusively in A2 even though A1 is the strongest continuum\nemitter. The protostellar core A2 displays typical hot corino abundances and\nits deconvolved size is 70 au. In contrast, the upper limits we placed on\nmolecular abundances for A1 are extremely low, lying about one order of\nmagnitude below prestellar values. The difference in the amount of organic\nmolecules present in A1 and A2 ranges between one and two orders of magnitude.\nOur results suggest that the optical depth of dust emission at these\nwavelengths is unlikely to be sufficiently high to completely hide a hot corino\nin A1 similar in size to that in A2. Thus, the significant contrast in\nmolecular richness found between the two sources is most probably real. We\nestimate that the size of a hypothetical hot corino in A1 should be less than\n12 au. Our results favour a scenario in which the protostar in A2 is either\nmore massive and/or subject to a higher accretion rate than A1, as a result of\ninhomogeneous fragmentation of the parental molecular clump. This naturally\nexplains the smaller current envelope mass in A2 with respect to A1 along with\nits molecular richness.\n",
        "  General aspects about the thermodynamics of astrophysical systems are\ndiscussed, overall, those concerning to astrophysical systems in mutual\ninteraction (or the called \\emph{open astrophysical systems}). A special\ninterest is devoted along the paper to clarify several misconceptions that are\nstill common in the recent literature, such as the direct application to the\nastrophysical scenario of notions and theoretical frameworks that were\noriginally conceived to deal with extensive systems of the everyday practice\n(large systems with short-range interactions).\n",
        "  We consider unordered XML, where the relative order among siblings is\nignored, and we investigate the problem of learning schemas from examples given\nby the user. We focus on the schema formalisms proposed in [10]: disjunctive\nmultiplicity schemas (DMS) and its restriction, disjunction-free multiplicity\nschemas (MS). A learning algorithm takes as input a set of XML documents which\nmust satisfy the schema (i.e., positive examples) and a set of XML documents\nwhich must not satisfy the schema (i.e., negative examples), and returns a\nschema consistent with the examples. We investigate a learning framework\ninspired by Gold [18], where a learning algorithm should be sound i.e., always\nreturn a schema consistent with the examples given by the user, and complete\ni.e., able to produce every schema with a sufficiently rich set of examples.\nAdditionally, the algorithm should be efficient i.e., polynomial in the size of\nthe input. We prove that the DMS are learnable from positive examples only, but\nthey are not learnable when we also allow negative examples. Moreover, we show\nthat the MS are learnable in the presence of positive examples only, and also\nin the presence of both positive and negative examples. Furthermore, for the\nlearnable cases, the proposed learning algorithms return minimal schemas\nconsistent with the examples.\n",
        "  This paper is a short empirical study of the performance of centrality and\nclassification based iterative term set expansion methods for distributional\nsemantic models. Iterative term set expansion is an interactive process using\ndistributional semantics models where a user labels terms as belonging to some\nsought after term set, and a system uses this labeling to supply the user with\nnew, candidate, terms to label, trying to maximize the number of positive\nexamples found. While centrality based methods have a long history in term set\nexpansion, we compare them to classification methods based on the the Simple\nMargin method, an Active Learning approach to classification using Support\nVector Machines. Examining the performance of various centrality and\nclassification based methods for a variety of distributional models over five\ndifferent term sets, we can show that active learning based methods\nconsistently outperform centrality based methods.\n",
        "  Many cell populations, exemplified by certain tumors, grow approximately\naccording to a Gompertzian growth model which has a slower approach to an upper\nlimit than that of logistic growth. Certain populations of animals and other\norganisms have also recently been analyzed with the Gompertz model. This\narticle addresses the question of how long it takes to reduce the population\nfrom one level to a lower one under a schedule of sudden decrements, each of\nwhich removes a given fraction of the cell mass or population. A deterministic\nperiodic schedule is first examined and yields exact results for the\neradication or extinction time which is defined as that required to reduce the\nnumber of cells to less than unity. The decrements in cell mass at each hit\ncould correspond to an approximation to reduction of a tumor by external beam\nradiation therapy. The effects of variations in magnitude of successive\ndecrements, the time interval between them, the initial population size and the\nintrinsic growth rate are calculated and results presented graphically.\n  With a schedule governed by a Poisson process, the number of organisms or\ncells satisfies a stochastic differential equation whose solution sample paths\nhave downward jumps as random times. The moments of the exit time then satisfy\na system of recurrent differential-difference equations. A simple\ntransformation results in a simpler system which has been studied both\nanalytically and numerically in the context of interspike intervals of a model\nneuron. Results are presented for the mean eradication time for various\nfrequencies and magnitudes of hits and for various eventual and initial\npopulation sizes. The standard deviation of the eradication time is also\ninvestigated.\n",
        "  We study the electronic structure of iron-based superconductors\nFeSe$_{1-x}$Te$_x$ within the density functional theory. We pay particular\nattention to the pressure effects on the Fermi surface (FS) topology, which\nseem to be correlated with a critical superconducting temperature TC of iron\nchalcogenides and pnictides. A reduction of the FS nesting between hole and\nelectron cylinders with increasing pressure is observed, which can lead to\nhigher values of TC . The tellurium substitution into selenium sites yields FS\nchanges similar to the pressure effect.\n",
        "  The shape of the Galactic dark halo can, in principle, be inferred through\nmodelling of stellar tidal streams in the Milky Way halo. The brightest and the\nlongest of these, the Sagittarius stream, reaches out to large Galactocentric\ndistances and hence can deliver the tightest constraints on the Galaxy's\npotential. In this contribution, we revisit the idea that the Sagittarius\nStream was formed from a rotating progenitor. We demonstrate that the angle\nbetween the disk's angular momentum and the progenitor's orbital angular\nmomentum does not remain constant throughout the disruption. Instead, it\nundergoes a dramatic evolution caused, in part, by the changes in the\nprogenitor's moment of inertia tensor. We show that, even in a spherical\npotential, the streams produced as a result of a disky dwarf disruption appear\nto be \"precessing\". Yet, this debris plane evolution is illusory as it is\nsolely caused by the swaying and wobbling of the progenitor's disk. Stream\nplane precession is therefore not an unambiguous indicator of asphericity of\nthe dark halo.\n",
        "  We present a new non-Archimedean model of evolutionary dynamics, in which the\ngenomes are represented by p-adic numbers. In this model the genomes have a\nvariable length, not necessarily bounded, in contrast with the classical models\nwhere the length is fixed. The time evolution of the concentration of a given\ngenome is controlled by a p-adic evolution equation. This equation depends on a\nfitness function f and on mutation measure Q. By choosing a mutation measure of\nGibbs type, and by using a p-adic version of the Maynard Smith Ansatz, we show\nthe existence of threshold function M_{c}(f,Q), such that the long term\nsurvival of a genome requires that its length grows faster than M_{c}(f,Q).\nThis implies that Eigen's paradox does not occur if the complexity of genomes\ngrows at the right pace. About twenty years ago, Scheuring and Poole, Jeffares,\nPenny proposed a hypothesis to explain Eigen's paradox. Our mathematical model\nshows that this biological hypothesis is feasible, but it requires p-adic\nanalysis instead of real analysis. More exactly, the Darwin-Eigen cycle\nproposed by Poole et al. takes place if the length of the genomes exceeds\nM_{c}(f,Q).\n",
        "  Starting with Darwin, biologists have asked how populations evolve from a low\nfitness state that is evolutionarily stable to a high fitness state that is\nnot. Specifically of interest is the emergence of cooperation and\nmulticellularity where the fitness of individuals often appears in conflict\nwith that of the population. Theories of social evolution and evolutionary game\ntheory have produced a number of fruitful results employing two-state two-body\nframeworks. In this study we depart from this tradition and instead consider a\nmulti-player, multi-state evolutionary game, in which the fitness of an agent\nis determined by its relationship to an arbitrary number of other agents. We\nshow that populations organize themselves in one of four distinct phases of\ninterdependence depending on one parameter, selection strength. Some of these\nphases involve the formation of specialized large-scale structures. We then\ndescribe how the evolution of independence can be manipulated through various\nexternal perturbations.\n",
        "  It is widely acknowledged that a good object clustering is critical to the\nperformance of OODBs. Clustering means storing related objects close together\non secondary storage so that when one object is accessed from disk, all its\nrelated objects are also brought into memory. Then access to these related\nobjects is a main memory access that is much faster than a disk access. The aim\nof this paper is to compare the performance of three clustering algorithms:\nCactis, CK and ORION. Simulation experiments we performed showed that the\nCactis algorithm is better than the ORION algorithm and that the CK algorithm\ntotally out-performs both other algorithms in terms of response time and\nclustering overhead.\n",
        "  The outbreak of enterohemorrhagic Escherichia coli (EHEC) in May 2011 warns\nthe potential threats of the world vegetables trade network (VTN) in spreading\nfatal infectious diseases. The heterogeneous weight distribution and\nmulti-scale activity of intermediary networks affects the diffusion,\nproliferation and extinction of epidemics. Here, we constructed a dual-weighted\nVTN with 118 major countries and territories from FAO 2008 statistic data about\nglobal vegetation production and trade, and develop a reaction-diffusion model\nto simulate the epidemic behaviors in through VTN. We found an emerged\nasymmetric threshold of epidemic on VTN, in which local proliferation within\nnodes plays a more critical role than global diffusion in spreading of\nEHEC-like diseases, i.e. sufficient local proliferation is the precondition for\nglobal diffusion. We also found that a strong modularity on VTN structure,\nwhich restricts the spreading of EHEC-like diseases; however, within the\ncommunities, the diffusion is quick and easy. There is, moreover, a critical\n\"epidemic stem\", in which a serial of positive feedback loop for amplifying the\nproliferation and diffusion pathogens has been identified from entire VTN.\nSurprisingly, statistical analysis shows a well consistency between theoretical\ncomposition of stem and actual pattern of EHEC. The results provide a chance to\ndesign gradient control strategies for controlling disease global diffusion.\nOur analysis provided the first inspect of global epidemics mediated by trade\nnetworks for improved control and immunity strategies in the future.\n",
        "  Story comprehension requires a deep semantic understanding of the narrative,\nmaking it a challenging task. Inspired by previous studies on ROC Story Cloze\nTest, we propose a novel method, tracking various semantic aspects with\nexternal neural memory chains while encouraging each to focus on a particular\nsemantic aspect. Evaluated on the task of story ending prediction, our model\ndemonstrates superior performance to a collection of competitive baselines,\nsetting a new state of the art.\n",
        "  Research on Galactic Center star formation is making great advances, in\nparticular due to new data from interferometers spatially resolving molecular\nclouds in this environment. These new results are discussed in the context of\nestablished knowledge about the Galactic Center. Particular attention is paid\nto suppressed star formation in the Galactic Center and how it might result\nfrom shallow density gradients in molecular clouds.\n",
        "  Distant supervised relation extraction is an efficient approach to scale\nrelation extraction to very large corpora, and has been widely used to find\nnovel relational facts from plain text. Recent studies on neural relation\nextraction have shown great progress on this task via modeling the sentences in\nlow-dimensional spaces, but seldom considered syntax information to model the\nentities. In this paper, we propose to learn syntax-aware entity embedding for\nneural relation extraction. First, we encode the context of entities on a\ndependency tree as sentence-level entity embedding based on tree-GRU. Then, we\nutilize both intra-sentence and inter-sentence attentions to obtain sentence\nset-level entity embedding over all sentences containing the focus entity pair.\nFinally, we combine both sentence embedding and entity embedding for relation\nclassification. We conduct experiments on a widely used real-world dataset and\nthe experimental results show that our model can make full use of all\ninformative instances and achieve state-of-the-art performance of relation\nextraction.\n",
        "  We describe a new approach to data modeling, called the concept-oriented\nmodel (COM), and a novel concept-oriented query language (COQL). The model is\nbased on three principles: duality principle postulates that any element is a\ncouple consisting of one identity and one entity, inclusion principle\npostulates that any element has a super-element, and order principle assumes\nthat any element has a number of greater elements within a partially ordered\nset. Concept-oriented query language is based on a new data modeling construct,\ncalled concept, inclusion relation between concepts, and concept partial\nordering in which greater concepts are represented by their field types. It is\ndemonstrated how COM and COQL can be used to solve three general data modeling\ntasks: logical navigation, multidimensional analysis and inference. Logical\nnavigation is based on two operations of projection and de-projection.\nMultidimensional analysis uses product operation for producing a cube from\nlevel concepts chosen along the chosen dimension paths. Inference is defined as\na two-step procedure where input constraints are first propagated downwards\nusing de-projection and then the constrained result is propagated upwards using\nprojection.\n",
        "  We predict parametrically strong enhancement of the thermoelectric effect in\nmetallic bilayers consisting of two superconductors separated by a spin-active\ninterface. The physical mechanism for such an enhancement is directly related\nto electron-hole imbalance generated by spin-sensitive quasiparticle scattering\nat the interface between superconducting layers. We explicitly evaluate the\nthermoelectric currents flowing in the system and demonstrate that they can\nreach maximum values comparable to the critical ones for superconductors under\nconsideration.\n",
        "  We report a high-pressure study on the heavily electron doped Lix(NH3)yFe2Se2\nsingle crystal by using the cubic anvil cell apparatus. The superconducting\ntransition temperature Tc = 44 K at ambient pressure is first suppressed to\nbelow 20 K upon increasing pressure to Pc = 2 GPa, above which the pressure\ndependence of Tc(P) reverses and Tc increases steadily to ca. 55 K at 11 GPa.\nThese results thus evidenced a pressure-induced second high-Tc superconducting\n(SC-II) phase in Lix(NH3)yFe2Se2 with the highest Tcmax = 55K among the\nFeSe-based bulk materials. Hall data confirm that in the emergent SC-II phase\nthe dominant electron-type carrier density undergoes a fourfold enhancement and\ntracks the same trend as Tc(P). Interesting, we find a nearly parallel scaling\nbehavior between Tc and the inverse Hall coefficient for the SC-II phases of\nboth Lix(NH3)yFe2Se2 and (Li,Fe)OHFeSe. The present work demonstrates that high\npressure offers a distinctive means to further raising the maximum Tc of\nheavily electron doped FeSe-based materials by increasing the effective charge\ncarrier concentration via a plausible Fermi surface reconstruction at Pc.\n",
        "  In cemented Total Hip Arthroplasty (THA), material chosen for femoral stem\nand cross section of stem itself, proved to be critical parameters for, stress\ndistribution in the femoral components, interfacial stresses and micro\nmovements. Titanium alloy (Ti6Al4V), when used as a material for femoral stem,\nrecorded large displacement as compared to Chromium alloy (CoCrMo) stems. This\nlarge displacement in case of Ti6Al4V caused the stem to bend inside the cement\nmantle, thus destroying it. Thus, CoCrMo proved to be a better in cemented THA.\nFailure in THA may occur at cement-stem or cement-bone interface, thus\ninterfacial stresses and micro movements were analysed in the present study.\nComparison between trapezium and circular cross section showed that, femoral\nstem with trapezium cross section underwent lesser amount of sliding and\ndebonding, at both interfaces, as compared to circular cross section. Moreover,\ntrapezium cross section also generated lower peak stresses in femoral stem and\ncortical femur. The present study also took into account, effect of diameter of\nfemur head. A 36mm diameter femur head generated lower peak stress in\nacetabulum liner and arrested dislocation. Metallic femur head was coupled with\na liner made of cross linked polyethylene (XLPE). This material experiences\nalmost negligible wear when compared to typical metallic and polyethylene\nliners, and unlike metallic liner, it is non-carcinogenic.\n",
        "  Antianemic medicament Ascofer and ferrous gluconate, its basic iron bearing\ningredient, were studied with the use of Mossbauer spectroscopy. Room\ntemperature spectra gave a clear evidence that two phases of iron were present\nviz. ferrous (Fe2+) as a major one with a contribution of 85+-5%, and ferric\n(Fe3+) whose contribution was found to be 15+-5%. However, the actual values of\nthe contributions of the two kind of the iron ions in Ascofer depend on\nsample's age: the abundance of Fe2+ ions increases with time by 10% after 51\nmonths, while that of Fe3+ decreases by the same amount. This means that an\ninternal reduction of Fe3+ ions takes place. Ferrous ions were shown to occupy\nat least two different sites. In Ascofer, the relative abundance of the two\nsites does not depend on the age of sample, while in the gluconate the\npopulation of site 1 increases and that of site 2 decreases with the age of the\nsample.\n",
        "  We offer a new proof that two closed oriented 4-manifolds are cobordant if\ntheir signatures agree, in the spirit of Lickorish's proof that all closed\noriented 3-manifolds bound 4-manifolds. Where Lickorish uses Heegaard\nsplittings we use trisections. In fact we begin with a subtle recasting of\nLickorish's argument: Instead of factoring the gluing map for a Heegaard\nsplitting as a product of Dehn twists, we encode each handlebody in a Heegaard\nsplitting in terms of a Morse function on the surface and build the 4-manifold\nfrom a generic homotopy between the two functions. This extends up a dimension\nby encoding a trisection of a closed 4-manifold as a triangle (circle) of\nfunctions and constructing an associated 5-manifold from an extension to a\n2-simplex (disk) of functions. This borrows ideas from Hatcher and Thurston's\nproof that the mapping class group of a surface is finitely presented.\n",
        "  Location Based Services (LBS) have become extremely popular and used by\nmillions of users. Popular LBS run the entire gamut from mapping services (such\nas Google Maps) to restaurants (such as Yelp) and real-estate (such as Redfin).\nThe public query interfaces of LBS can be abstractly modeled as a kNN interface\nover a database of two dimensional points: given an arbitrary query point, the\nsystem returns the k points in the database that are nearest to the query\npoint. Often, k is set to a small value such as 20 or 50. In this paper, we\nconsider the novel problem of enabling density based clustering over an LBS\nwith only a limited, kNN query interface. Due to the query rate limits imposed\nby LBS, even retrieving every tuple once is infeasible. Hence, we seek to\nconstruct a cluster assignment function f(.) by issuing a small number of kNN\nqueries, such that for any given tuple t in the database which may or may not\nhave been accessed, f(.) outputs the cluster assignment of t with high\naccuracy. We conduct a comprehensive set of experiments over benchmark datasets\nand popular real-world LBS such as Yahoo! Flickr, Zillow, Redfin and Google\nMaps.\n",
        "  Humans, like all organisms, are subject to fundamental biophysical laws. Van\nValen predicted that, because of zero-sum dynamics, all populations of all\nspecies in a given environment flux the same amount of energy on average.\nDamuth's 'energetic equivalence rule' supported Van Valen's conjecture by\nshowing a trade off between few big animals per area with high individual\nmetabolic rates compared to abundant small species with low energy\nrequirements. We use established metabolic scaling theory to compare variation\nin densities and individual energy use in human societies to other land\nmammals. We show that hunter-gatherers occurred at lower densities than a\nmammal of our size. Most modern humans, in contrast, concentrate in large\ncities at densities that are up to four orders of magnitude greater than\nhunter-gatherers yet cities consume up to two orders of magnitude greater\nenergy per capita. Today, cities across the globe flux greater energy than net\nprimary productivity on a per area basis. This is possible through enormous\nfluxes of energy and materials across urban boundaries to sustain hyper-dense,\nmodern humans. The metabolic rift with nature created by hyper-dense cities\nsupported by fossil fuel energy poses formidable challenges for establishing a\nsustainable relationship on a rapidly urbanizing, yet finite planet.\n",
        "  A simple model for image formation in linear shift-invariant systems is\nconsidered, in which both the detected signal and the noise variance are\nvarying slowly compared to the point-spread function of the system. It is shown\nthat within the constraints of this model, the square of the signal-to-noise\nratio is always proportional to the \"volume\" of the spatial resolution unit. In\nthe case of Poisson statistics, the ratio of these two quantities divided by\nthe incident density of the imaging particles (e.g. photons) represents a\ndimensionless invariant of the imaging system, which was previously termed the\nintrinsic imaging quality. The relationship of this invariant to the notion of\ninformation capacity of communication and imaging systems, which was previously\nconsidered by Shannon, Gabor and others, is investigated. The results are then\napplied to a simple generic model of quantitative imaging of weakly scattering\nobjects, leading to an estimate of the upper limit for the amount of\ninformation about the sample that can be obtained in such experiments. It is\nshown that this limit depends only on the total number of imaging particles\nincident on the sample, the average scattering coefficient, the size of the\nsample and the number of spatial resolution units.\n",
        "  This research is about an online forum designed and developed to improve the\ncommunication process between alumni, new, old and upcoming students. In this\nresearch paper we present targeted problems, designed architecture, used\ntechnologies in development and final end product in detail.\n",
        "  Chart descriptions are a graphic method to describe monodromy representations\nof various topological objects. Here we introduce a chart description for\nhyperelliptic Lefschetz fibrations, and show that any hyperelliptic Lefschetz\nfibration can be stabilized by fiber-sum with certain basic Lefschetz\nfibrations.\n",
        "  We focus on named entity recognition (NER) for Chinese social media. With\nmassive unlabeled text and quite limited labelled corpus, we propose a\nsemi-supervised learning model based on B-LSTM neural network. To take\nadvantage of traditional methods in NER such as CRF, we combine transition\nprobability with deep learning in our model. To bridge the gap between label\naccuracy and F-score of NER, we construct a model which can be directly trained\non F-score. When considering the instability of F-score driven method and\nmeaningful information provided by label accuracy, we propose an integrated\nmethod to train on both F-score and label accuracy. Our integrated model yields\n7.44\\% improvement over previous state-of-the-art result.\n",
        "  Detecting variation in the evolutionary process along chromosomes is\nincreasingly important as whole-genome data becomes more widely available. For\nexample, factors such as incomplete lineage sorting, horizontal gene transfer,\nand chromosomal inversion are expected to result in changes in the underlying\ngene trees along a chromosome, while changes in selective pressure and\nmutational rates for different genomic regions may lead to shifts in the\nunderlying mutational process. We propose the split score as a general method\nfor quantifying support for a particular phylogenetic relationship within a\ngenomic data set. Because the split score is based on algebraic properties of a\nmatrix of site pattern frequencies, it can be rapidly computed, even for data\nsets that are large in the number of taxa and/or in the length of the\nalignment, providing an advantage over other methods (e.g., maximum likelihood)\nthat are often used to assess such support. Using simulation we explore the\nproperties of the split score, including its dependence on sequence length,\nbranch length, size of a split and its ability to detect true splits in the\nunderlying tree. Using a sliding window analysis, we show that split scores can\nbe used to detect changes in the underlying evolutionary process for\ngenome-scale data from primates, mosquitoes, and viruses in a computationally\nefficient manner. Computation of the split score has been implemented in the\nsoftware package SplitSup.\n",
        "  In a classic paper Zeeman introduced the k-twist spin of a knot K and showed\nthat the exterior of a twist spin fibers over S^1. In particular this result\nshows that the knot K # -K is doubly slice. In this paper we give a quick proof\nof Zeeman's result. The k-twist spin of K also gives rise to two metabolizers\nfor K # -K and we determine these two metabolizers precisely.\n",
        "  Motivation: Word-based or `alignment-free' methods for phylogeny\nreconstruction are much faster than traditional approaches, but they are\ngenerally less accurate. Most of these methods calculate pairwise distances for\na set of input sequences, for example from word frequencies, from so-called\nspaced-word matches or from the average length of common substrings.\n  Results: In this paper, we propose the first word-based approach to tree\nreconstruction that is based on multiple sequence comparison and Maximum\nLikelihood. Our algorithm first samples small, gap-free alignments involving\nfour taxa each. For each of these alignments, it then calculates a quartet tree\nand, finally, the program Quartet MaxCut is used to infer a super tree topology\nfor the full set of input taxa from the calculated quartet trees. Experimental\nresults show that trees calculated with our approach are of high quality.\n  Availability: The source code of the program is available at\nhttps://github.com/tdencker/multi-SpaM\n  Contact: thomas.dencker@stud.uni-goettingen.de\n",
        "  Document similarity is the problem of estimating the degree to which a given\npair of documents has similar semantic content. An accurate document similarity\nmeasure can improve several enterprise relevant tasks such as document\nclustering, text mining, and question-answering. In this paper, we show that a\ndocument's thematic flow, which is often disregarded by bag-of-word techniques,\nis pivotal in estimating their similarity. To this end, we propose a novel\nsemantic document similarity framework, called SimDoc. We model documents as\ntopic-sequences, where topics represent latent generative clusters of related\nwords. Then, we use a sequence alignment algorithm to estimate their semantic\nsimilarity. We further conceptualize a novel mechanism to compute topic-topic\nsimilarity to fine tune our system. In our experiments, we show that SimDoc\noutperforms many contemporary bag-of-words techniques in accurately computing\ndocument similarity, and on practical applications such as document clustering.\n",
        "  The discovery of the new class of pnictide superconductors has engendered a\ncontroversy about their pairing symmetry, with proposals ranging from an\nextended s-wave or \"s$_{\\pm}$\" symmetry to nodal or nodeless d-wave symmetry to\nstill more exotic order parameters such as p-wave. In this paper, building on\nthe earlier, similar work performed for the cuprates, we propose several\nphase-sensitive Josephson interferometry experiments, each of which may allow\nresolution of the issue.\n",
        "  The discovery of the pseudogap in the cuprates created significant excitement\namongst physicists as it was believed to be a signature of pairing, in some\ncases well above the room temperature. In this \"pre-formed pairs\" scenario, the\nformation of pairs without quantum phase rigidity occurs below T*. These pairs\ncondense and develop phase coherence only below Tc. In contrast, several recent\nexperiments reported that the pseudogap and superconducting states are\ncharacterized by two different energy scales, pointing to a scenario, where the\ntwo compete. However a number of transport, magnetic, thermodynamic and\ntunneling spectroscopy experiments consistently detect a signature of\nphase-fluctuating superconductivity above leaving open the question of whether\nthe pseudogap is caused by pair formation or not. Here we report the discovery\nof a spectroscopic signature of pair formation and demonstrate that in a region\nof the phase diagram commonly referred to as the \"pseudogap\", two distinct\nstates coexist: one that persists to an intermediate temperature Tpair and a\nsecond that extends up to T*. The first state is characterized by a doping\nindependent scaling behavior and is due to pairing above Tc, but significantly\nbelow T*. The second state is the \"proper\" pseudogap - characterized by a\n\"checker board\" pattern in STM images, the absence of pair formation, and is\nlikely linked to Mott physics of pristine CuO2 planes. Tpair has a universal\nvalue around 130-150K even for materials with very different Tc, likely setting\nlimit on highest, attainable Tc in cuprates. The observed universal scaling\nbehavior with respect to Tpair indicates a breakdown of the classical picture\nof phase fluctuations in the cuprates.\n",
        "  In geographic information science and semantics, the computation of semantic\nsimilarity is widely recognised as key to supporting a vast number of tasks in\ninformation integration and retrieval. By contrast, the role of geo-semantic\nrelatedness has been largely ignored. In natural language processing, semantic\nrelatedness is often confused with the more specific semantic similarity. In\nthis article, we discuss a notion of geo-semantic relatedness based on Lehrer's\nsemantic fields, and we compare it with geo-semantic similarity. We then\ndescribe and validate the Geo Relatedness and Similarity Dataset (GeReSiD), a\nnew open dataset designed to evaluate computational measures of geo-semantic\nrelatedness and similarity. This dataset is larger than existing datasets of\nthis kind, and includes 97 geographic terms combined into 50 term pairs rated\nby 203 human subjects. GeReSiD is available online and can be used as an\nevaluation baseline to determine empirically to what degree a given\ncomputational model approximates geo-semantic relatedness and similarity.\n",
        "  The proliferation of ontologies and taxonomies in many domains increasingly\ndemands the integration of multiple such ontologies. The goal of ontology\nintegration is to merge two or more given ontologies in order to provide a\nunified view on the input ontologies while maintaining all information coming\nfrom them. We propose a new taxonomy merging algorithm that, given as input two\ntaxonomies and an equivalence matching between them, can generate an integrated\ntaxonomy in a fully automatic manner. The approach is target-driven, i.e. we\nmerge a source taxonomy into the target taxonomy and preserve the structure of\nthe target ontology as much as possible. We also discuss how to extend the\nmerge algorithm providing auxiliary information, like additional relationships\nbetween source and target concepts, in order to semantically improve the final\nresult. The algorithm was implemented in a working prototype and evaluated\nusing synthetic and real-world scenarios.\n",
        "  Population genetics struggles to model extinction; standard models track the\nrelative rather than absolute fitness of genotypes, while the exceptions\ndescribe only the short-term transition from imminent doom to evolutionary\nrescue. But extinction can result from failure to adapt not only to\ncatastrophes, but also to a backlog of environmental challenges. We model\nlong-term evolution to long series of small challenges, where fitter\npopulations reach higher population sizes. The population's long-term fitness\ndynamic is well approximated by a simple stochastic Markov chain model.\nLong-term persistence occurs when the rate of adaptation exceeds the rate of\nenvironmental deterioration for some genotypes. Long-term persistence times are\nconsistent with typical fossil species persistence times of several million\nyears. Immediately preceding extinction, fitness declines rapidly, appearing as\nthough a catastrophe disrupted a stably established population, even though\ngradual evolutionary processes are responsible. New populations go through an\nestablishment phase where, despite being demographically viable, their\nextinction risk is elevated. Should the population survive long enough,\nextinction risk later becomes constant over time.\n",
        "  We investigate the error threshold for the emergence of quasispecies in the\nEigen model. By mapping to to an effective Hamiltonian ruled by the\n\"imaginary-time\" Schr\\\"odinger equation, a variational ansatz is proposed and\napplied to calculate various quantities associated with the quasispecies. The\nvariational ansatz gives correct predictions for the survival population of the\nwild-type sequence and also reveals an unexpected universal scaling behaviors\nnear the error threshold. We check the validity of the variational ansatz by\nnumerical methods and find excellent agreement. Though the emergence of the\nscaling behaviors is not yet fully understood, it is remarkable that the\nuniversal scaling function reigns even for relatively short genome length such\nas L=16. Further investigations may reveal the mechanism of the universal\nscaling and extract the essential ingredients for the emergence of the\nquasispecies in molecular evolution.\n",
        "  Spectral molecular imaging is a new imaging technique able to discriminate\nand quantify different components of tissue simultaneously at high spatial and\nhigh energy resolution. Our MARS scanner is an x-ray based small animal CT\nsystem designed to be used in the diagnostic energy range (20 to 140 keV). In\nthis paper, we demonstrate the use of the MARS scanner, equipped with the\nMedipix3RX spectroscopic photon-processing detector, to discriminate fat,\ncalcium, and water in tissue. We present data collected from a sample of lamb\nmeat including bone as an illustrative example of human tissue imaging. The\ndata is analyzed using our 3D Algebraic Reconstruction Algorithm (MARS-ART) and\nby material decomposition based on a constrained linear least squares\nalgorithm. The results presented here clearly show the quantification of\nlipid-like, water-like and bone-like components of tissue. However, it is also\nclear to us that better algorithms could extract more information of clinical\ninterest from our data. Because we are one of the first to present data from\nmulti-energy photon-processing small animal CT systems, we make the raw,\npartial and fully processed data available with the intention that others can\nanalyze it using their familiar routines. The raw, partially processed and\nfully processed data of lamb tissue along with the phantom calibration data can\nbe found at [http://hdl.handle.net/10092/8531].\n",
        "  Movement is a fundamental behaviour of organisms that brings about beneficial\nencounters with resources and mates, but at the same time exposes the organism\nto dangerous encounters with predators. The movement patterns adopted by\norganisms should reflect a balance between these contrasting processes. This\ntrade-off can be hypothesized as being evident in the behaviour of plankton,\nwhich inhabit a dilute 3D environment with few refuges or orienting landmarks.\nWe present an analysis of the swimming path geometries based on a volumetric\nMonte Carlo sampling approach, which is particularly adept at revealing such\ntrade-offs by measuring the self-overlap of the trajectories. Application of\nthis method to experimentally measured trajectories reveals that swimming\npatterns in copepods are shaped to efficiently explore volumes at small scales,\nwhile achieving a large overlap at larger scales. Regularities in the observed\ntrajectories make the transition between these two regimes always sharper than\nin randomized trajectories or as predicted by random walk theory. Thus real\ntrajectories present a stronger separation between exploration for food and\nexposure to predators. The specific scale and features of this transition\ndepend on species, gender, and local environmental conditions, pointing at\nadaptation to state and stage dependent evolutionary trade-offs.\n",
        "  We explore recently introduced definition modeling technique that provided\nthe tool for evaluation of different distributed vector representations of\nwords through modeling dictionary definitions of words. In this work, we study\nthe problem of word ambiguities in definition modeling and propose a possible\nsolution by employing latent variable modeling and soft attention mechanisms.\nOur quantitative and qualitative evaluation and analysis of the model shows\nthat taking into account words ambiguity and polysemy leads to performance\nimprovement.\n",
        "  We study the algebraic structures of the virtual singular braid monoid,\n$VSB_n$, and the virtual singular pure braid monoid, $VSP_n$. The monoid\n$VSB_n$ is the splittable extension of $VSP_n$ by the symmetric group $S_n$. We\nalso construct a representation of $VSB_n$.\n",
        "  A MapReduce algorithm can be described by a mapping schema, which assigns\ninputs to a set of reducers, such that for each required output there exists a\nreducer that receives all the inputs that participate in the computation of\nthis output. Reducers have a capacity, which limits the sets of inputs that\nthey can be assigned. However, individual inputs may vary in terms of size. We\nconsider, for the first time, mapping schemas where input sizes are part of the\nconsiderations and restrictions. One of the significant parameters to optimize\nin any MapReduce job is communication cost between the map and reduce phases.\nThe communication cost can be optimized by minimizing the number of copies of\ninputs sent to the reducers. The communication cost is closely related to the\nnumber of reducers of constrained capacity that are used to accommodate\nappropriately the inputs, so that the requirement of how the inputs must meet\nin a reducer is satisfied. In this work, we consider a family of problems where\nit is required that each input meets with each other input in at least one\nreducer. We also consider a slightly different family of problems in which,\neach input of a set, X, is required to meet each input of another set, Y, in at\nleast one reducer. We prove that finding an optimal mapping schema for these\nfamilies of problem is NP-hard, and present several approximation algorithms\nfor finding a near optimal mapping schema.\n",
        "  The HeII transverse proximity effect -- enhanced HeII Ly$\\alpha$~transmission\nin a background sightline caused by the ionizing radiation of a foreground\nquasar -- offers a unique opportunity to probe the morphology of quasar-driven\nHeII reionization. We conduct a comprehensive spectroscopic survey to find\n$z\\sim3$ quasars in the foreground of 22 background quasar sightlines with\nHST/COS HeII Ly$\\alpha$~transmission spectra. With our two-tiered survey\nstrategy, consisting of a deep pencil-beam survey and a shallow wide-field\nsurvey, we discover 131 new quasars, which we complement with known SDSS/BOSS\nquasars in our fields. Using a restricted sample of 66 foreground quasars with\ninferred HeII photoionization rates greater than the expected UV background at\nthese redshifts ($\\Gamma_\\mathrm{QSO}^\\mathrm{HeII} > 5 \\times\n10^{-16}\\,\\mathrm{s}^{-1}$) we perform the first statistical analysis of the\nHeII transverse proximity effect. Our results show qualitative evidence for a\nlarge object-to-object variance: among the four foreground quasars with the\nhighest $\\Gamma_\\mathrm{QSO}^\\mathrm{HeII}$ only one (previously known) quasar\nis associated with a significant HeII transmission spike. We perform a stacking\nanalysis to average down these fluctuations, and detect an excess in the\naverage HeII transmission near the foreground quasars at $3\\sigma$\nsignificance. This statistical evidence for the transverse proximity effect is\ncorroborated by a clear dependence of the signal strength on\n$\\Gamma_\\mathrm{QSO}^\\mathrm{HeII}$. Our detection places a purely geometrical\nlower limit on the quasar lifetime of $t_\\mathrm{Q} > 25\\,\\mathrm{Myr}$.\nImproved modeling would additionally constrain quasar obscuration and the mean\nfree path of HeII-ionizing photons.\n",
        "  The significant progress in angle-resolved photoemission spectroscopy (ARPES)\nin last three decades has elevated it from a traditional band mapping tool to a\nprecise probe of many-body interactions and dynamics of quasiparticles in\ncomplex quantum systems. The recent developments of deep ultraviolet (DUV,\nincluding ultraviolet and vacuum ultraviolet) laser-based ARPES have further\npushed this technique to a new level. In this paper, we review some latest\ndevelopments in DUV laser-based photoemission systems, including the super-high\nenergy and momentum resolution ARPES, the spin-resolved ARPES, the\ntime-of-flight ARPES, and the time-resolved ARPES. We also highlight some\nscientific applications in the study of electronic structure in unconventional\nsuperconductors and topological materials using these state-of-the-art DUV\nlaser-based ARPES. Finally we provide our perspectives on the future directions\nin the development of laser-based photoemission systems.\n",
        "  We extend the Heegaard Floer homological definition of spectral order for\nclosed contact 3-manifolds due to Kutluhan, Mati\\'c, Van Horn-Morris, and Wand\nto contact 3-manifolds with convex boundary. We show that the order of a\ncodimension zero contact submanifold bounds the order of the ambient manifold\nfrom above. As the neighborhood of an overtwisted disk has order zero, we\nobtain that overtwisted contact structures have order zero. We also prove that\nthe order of a small perturbation of a $2\\pi$ Giroux torsion domain has order\nat most two, hence any contact structure with positive Giroux torsion has order\nat most two (and, in particular, a vanishing contact invariant).\n",
        "  We prove that Nakhleh's latest dissimilarity measure for phylogenetic\nnetworks is a metric on the classes of tree-child phylogenetic networks, of\nsemi-binary time consistent tree-sibling phylogenetic networks, and of\nmulti-labeled phylogenetic trees. We also prove that it distinguishes\nphylogenetic networks with different reduced versions. In this way, it becomes\nthe dissimilarity measure for phylogenetic networks with the strongest\nseparation power available so far.\n",
        "  We study microscopic scenario of vortex escape from a columnar defect under\nthe influence of a transport current. For defect radii smaller than the\nsuperconducting coherence length the depinning process is shown to be a\nconsequence of two subsequent topological electronic transitions in a trapped\nvortex core. The first transition at a critical current $j_L$ is associated\nwith the opening of Fermi surface segments corresponding to the creation of a\nvortex--antivortex pair bound to the defect. The second transition at a certain\ncurrent $j_d > j_L$ is caused by merging of different Fermi surface segments,\nwhich accompanies the formation of a freely moving vortex.\n",
        "  In this paper, attention is drawn to the importance of accounting for osmotic\npressure when analyzing physiological effects on cellular structures in plasma\nmedicine. Interaction of a weakly ionized plasma jet with a saline solution\nleads to a detectable changes in the saline's ion-molecular composition and\nhence changes in the osmotic pressure. This, in turn, leads to a stretching or\ncompression of the membrane, depending on the difference of total external and\ninternal pressures. The selective effect of plasma on cells, observed in\nexperiments, is associated with the change in the mechanical properties of\nmembranes (and thereby, a weakening of their protective properties).\nCorresponding estimates are given in the article.\n",
        "  In this work we revisit the vortex matter phase diagram in layered\nsuperconductors solving still open questions by means of AC and DC local\nmagnetic measurements in the paradigmatic Bi$_{2}$Sr$_{2}$CaCu$_{2}$O$_{8}$\ncompound. We show that measuring with AC magnetic techniques is mandatory in\norder to probe the bulk response of vortex matter, particularly at\nhigh-temperatures where surface barriers for vortex entrance dominate. From the\n$T_{\\rm FOT}$-evolution of the enthalpy and latent-heat at the transition we\nfind that, contrary to previous reports, the nature of the dominant interlayer\ncoupling is electromagnetic in the whole temperature range. By studying the\ndynamic properties of the phase located at $T \\gtrsim T_{\\rm FOT}$, we reveal\nthe spanning in a considerable fraction of the phase diagram of a non-linear\nvortex phase suggesting bulk pinning might play a role even in the liquid\nvortex phase.\n",
        "  The purpose of this study is to determine whether organ sparing and target\ncoverage can be simultaneously maintained for pencil beam scanning (PBS) proton\ntherapy treatment of thoracic tumors in the presence of motion, stopping power\nuncertainties and patient setup variations. Ten consecutive patients that were\npreviously treated with proton therapy to 66.6/1.8 Gy (RBE) using double\nscattering (DS) were replanned with PBS. Minimum and maximum intensity images\nfrom 4DCT were used to introduce flexible smearing in the determination of the\nbeam specific PTV (BSPTV). Datasets from eight 4DCT phases, using +-3%\nuncertainty in stopping power, and +-3 mm uncertainty in patient setup in each\ndirection were used to create 8X12X10=960 PBS plans for the evaluation of ten\npatients. Plans were normalized to provide identical coverage between DS and\nPBS. The average lung V20, V5, and mean doses were reduced from 29.0%, 35.0%,\nand 16.4 Gy with DS to 24.6%, 30.6%, and 14.1 Gy with PBS, respectively. The\naverage heart V30 and V45 were reduced from 10.4% and 7.5% in DS to 8.1% and\n5.4% for PBS, respectively. Furthermore, the maximum spinal cord, esophagus and\nheart dose were decreased from 37.1 Gy, 71.7 Gy and 69.2 Gy with DS to 31.3 Gy,\n67.9 Gy and 64.6 Gy with PBS. The conformity index (CI), homogeneity index\n(HI), and global maximal dose were improved from 3.2, 0.08, 77.4 Gy with DS to\n2.8, 0.04 and 72.1 Gy with PBS. All differences are statistically significant,\nwith p values <0.05, with the exception of the heart V45 (p= 0.146). PBS with\nBSPTV achieves better organ sparing and improves target coverage using a\nrepainting method for the treatment of thoracic tumors. Incorporating\nmotion-related uncertainties is essential in maintaining marginal coverage and\nhomogenous dose of treatment targets.\n",
        "  In the search for superconductivity in BaAu2Sb2-type monoclinic structure, we\nhave successfully synthesized a new compound BaPt2Bi2, which crystallizes in\nthe space group P21/m (S.G. 11; Pearson symbol mP10) according to a combination\nof powder and single crystal X-ray diffraction and scanning electron\nmicroscopy. Sharp electrical resistivity drop and large diamagnetic\nmagnetization below 2.0 K indicates it owns the superconducting ground state.\nThis makes BaPt2Bi2 the first reported superconductor in mono-clinic\nBaAu2Sb2-type structure, a previously unappreciated structure for\nsuperconductivity. First-principles calculations considering the spin-orbit\ncoupling indicate that Pt-Bi anti-bonding interaction plays a critical role in\ninducing superconductivity.\n",
        "  We show that any closed manifold with a metric of nonpositive curvature that\nadmits either a single point rank condition or a single point curvature\ncondition has positive simplicial volume. We use this to provide a differential\ngeometric proof of a conjecture of Gromov in dimension three.\n",
        "  This paper presents preliminary results of Croatian syllable networks\nanalysis. Syllable network is a network in which nodes are syllables and links\nbetween them are constructed according to their connections within words. In\nthis paper we analyze networks of syllables generated from texts collected from\nthe Croatian Wikipedia and Blogs. As a main tool we use complex network\nanalysis methods which provide mechanisms that can reveal new patterns in a\nlanguage structure. We aim to show that syllable networks have much higher\nclustering coefficient in comparison to Erd\\\"os-Renyi random networks. The\nresults indicate that Croatian syllable networks exhibit certain properties of\na small world networks. Furthermore, we compared Croatian syllable networks\nwith Portuguese and Chinese syllable networks and we showed that they have\nsimilar properties.\n",
        "  Glioblastoma are known to infiltrate the brain parenchyma instead of forming\na solid tumor mass with a defined boundary. Only the part of the tumor with\nhigh tumor cell density can be localized through imaging directly. In contrast,\nbrain tissue infiltrated by tumor cells at low density appears normal on\ncurrent imaging modalities. In clinical practice, a uniform margin is applied\nto account for microscopic spread of disease.\n  The current treatment planning procedure can potentially be improved by\naccounting for the anisotropy of tumor growth: Anatomical barriers such as the\nfalx cerebri represent boundaries for migrating tumor cells. In addition, tumor\ncells primarily spread in white matter and infiltrate gray matter at lower\nrate. We investigate the use of a phenomenological tumor growth model for\ntreatment planning. The model is based on the Fisher-Kolmogorov equation, which\nformalizes these growth characteristics and estimates the spatial distribution\nof tumor cells in normal appearing regions of the brain. The target volume for\nradiotherapy planning can be defined as an isoline of the simulated tumor cell\ndensity.\n  A retrospective study involving 10 glioblastoma patients has been performed.\nTo illustrate the main findings of the study, a detailed case study is\npresented for a glioblastoma located close to the falx. In this situation, the\nfalx represents a boundary for migrating tumor cells, whereas the corpus\ncallosum provides a route for the tumor to spread to the contralateral\nhemisphere. We further discuss the sensitivity of the model with respect to the\ninput parameters. Correct segmentation of the brain appears to be the most\ncrucial model input.\n  We conclude that the tumor growth model provides a method to account for\nanisotropic growth patterns of glioblastoma, and may therefore provide a tool\nto make target delineation more objective and automated.\n",
        "  In this paper I discuss analytic and numerical calculations of the\nmagnetic-field and sheet-current distributions in superconducting strips of\nwidth 2a and arbitrary thickness 2b at the center when the cross section is an\nellipse, a rectangle, and a shape intermediate between these limits. Using\ncritical-state theory, I use several methods to determine the functional\ndependence of the ac transport-current losses upon F = I/Ic, where I is the\npeak alternating current and Ic is the critical current, and I discuss how this\ndependence can be affected by the cross-sectional shape, aspect ratio, and a\nflux-density-dependent critical current density Jc(B).\n",
        "  We introduce the first analytical model of asymmetric community dynamics to\nyield Hubbell's neutral theory in the limit of functional equivalence among all\nspecies. Our focus centers on an asymmetric extension of Hubbell's local\ncommunity dynamics, while an analogous extension of Hubbell's metacommunity\ndynamics is deferred to an appendix. We find that mass-effects may facilitate\ncoexistence in asymmetric local communities and generate unimodal species\nabundance distributions indistinguishable from those of symmetric communities.\nMultiple modes, however, only arise from asymmetric processes and provide a\nstrong indication of non-neutral dynamics. Although the exact stationary\ndistributions of fully asymmetric communities must be calculated numerically,\nwe derive approximate sampling distributions for the general case and for\nnearly neutral communities where symmetry is broken by a single species\ndistinct from all others in ecological fitness and dispersal ability. In the\nlatter case, our approximate distributions are fully normalized, and novel\nasymptotic expansions of the required hypergeometric functions are provided to\nmake evaluations tractable for large communities. Employing these results in a\nBayesian analysis may provide a novel statistical test to assess the\nconsistency of species abundance data with the neutral hypothesis.\n",
        "  Using pre-trained word embeddings as input layer is a common practice in many\nnatural language processing (NLP) tasks, but it is largely neglected for neural\nmachine translation (NMT). In this paper, we conducted a systematic analysis on\nthe effect of using pre-trained source-side monolingual word embedding in NMT.\nWe compared several strategies, such as fixing or updating the embeddings\nduring NMT training on varying amounts of data, and we also proposed a novel\nstrategy called dual-embedding that blends the fixing and updating strategies.\nOur results suggest that pre-trained embeddings can be helpful if properly\nincorporated into NMT, especially when parallel data is limited or additional\nin-domain monolingual data is readily available.\n",
        "  We show that if a positive integral surgery on a knot K inside a homology\nsphere X with Seifert genus g(K) results in an induced knot K_n in X_n(K)=Y\nwhich has simple Floer homology, we should have n>=2g(K). Moreover, if X is the\nstandard sphere, the three-manifold Y is a L-space and the Heegaard Floer\nhomology groups of K are determined by its Alexander polynomial.\n",
        "  Algebraic knots are known to be iterated torus knots and to admit L-space\nsurgeries. However, Hedden proved that there are iterated torus knots that\nadmit L-space surgeries but are not algebraic. We present an infinite family of\nsuch examples, with the additional property that no nontrivial linear\ncombination of knots in this family is concordant to a linear combination of\nalgebraic knots. The proof uses the Ozsvath-Stipsicz-Szabo Upsilon function,\nand also introduces a new invariant of L-space knots, the formal semigroup.\n",
        "  - Modeling Human cardiovascular system is always an important issue. One of\nthe most effective methods is using lumped model to reach to a complete model\nof human cardiovascular system. Such modeling with advanced considerations is\nused in this paper. Some of these considerations are as follow: Exact\nsimulating of ventricles as pressure suppliers, peristaltic motion of\ndescending arteries as additional suppliers, and dividing each vessel into more\nthan one compartment to reach more accurate answers. Finally a circuit with\nmore than 150 RLC segments and different elements is made. Then the\nverification of our complex circuit is done and at the end, obstruction as an\nimportant abnormality is investigated. For this aim different percents of\nobstruction in vital arteries are considered and the results are brought as\ndifferent graphs at the end. According to physiological texts the citation of\nour simulation and its results are obvious. To earn productive information\nabout arteries characteristics a 36-vessels model was chosen from biological\nsources.\n",
        "  Predators often consume multiple prey and by mutually subsidizing a shared\npredator, the prey may reciprocally harm each other. When predation levels are\nhigh, this apparent competition can culminate in a prey species being\ndisplaced. Coupling quantitative genetics and Lotka-Volterra models, we study\nhow predator evolution alters this and other ecological outcomes. These models\naccount for a trade-off between the predator's attack rates on two prey\nspecies. We provide a mathematical characterization of a strong form of\npersistence--permanence--for which there is a global attractor bounded away\nfrom extinction. When the evolutionary dynamics occur at a sufficiently slower\ntime scale than the ecological dynamics, we also characterize attractors and\ntheir basins' of attraction using singular perturbation theory and a graphical\napproach to the eco-evolutionary dynamics. Our results show that\neco-evolutionary feedbacks can mediate permanence at intermediate trade-offs in\nthe attack rates. However, at strong trade-offs, permanence is lost. Despite\nthis loss of permanence, there can be attractors supporting coexistence. These\nattractors, however, may coincide with attractors at which the predator is\nexcluded. Our results highlight that evo-evolutionary feedbacks can alter\ncommunity structure by mediating coexistence or leading to trait-dependent\nalternative stable states.\n",
        "  We present a transition-based dependency parser that uses a convolutional\nneural network to compose word representations from characters. The character\ncomposition model shows great improvement over the word-lookup model,\nespecially for parsing agglutinative languages. These improvements are even\nbetter than using pre-trained word embeddings from extra data. On the SPMRL\ndata sets, our system outperforms the previous best greedy parser (Ballesteros\net al., 2015) by a margin of 3% on average.\n",
        "  Titanium and its alloy are most widely used implant materials in dental and\northopaedic fields. However, infections occurring during implantation leads to\nimplant failure in most of the cases. Here, we have demonstrated antibacterial\nbehavior of Ti6Al4V alloy achieved when surface modified using femtosecond\nlaser beam. Post laser treatment conical microstructures were observed on the\nTi6Al4V alloy surface. Generation of different sub-oxide phases of titanium\ndioxide were detected on laser treated samples using X-ray diffraction and\nX-ray photoelectron spectroscopy. Wettability of Ti6Al4V alloy surface changed\nsignificantly after interaction with the laser. Adhesion and growth of two gram\npositive; Staphylococcus aureus and Streptococcus mutans and one gram negative\nPseudomonas aeruginosa bacteria have been explored on pristine, as well as, on\nlaser textured Ti6Al4V alloy surfaces. In-vitro investigation on agar plate\nshowed inhibition of bacterial growth on most of the laser treated surface.\nSuperior surface roughness and occurrence of magneli phases of titanium dioxide\non laser treated surface were probably responsible for the antibacterial\nbehavior exhibited by the laser treated samples. Therefore, femtosecond laser\nsurface treatment of Ti6Al4V alloy could find potential application in the\ndevelopment of infection free medical implants for dental and orthopedic\nusages.\n",
        "  In this paper, we formulate a top-k query that compares objects in a database\nto a user-provided query object on a novel scoring function. The proposed\nscoring function combines the idea of attractive and repulsive dimensions into\na general framework to overcome the weakness of traditional distance or\nsimilarity measures. We study the properties of the proposed class of scoring\nfunctions and develop efficient and scalable index structures that index the\nisolines of the function. We demonstrate various scenarios where the query\nfinds application. Empirical evaluation demonstrates a performance gain of one\nto two orders of magnitude on querying time over existing state-of-the-art\ntop-k techniques. Further, a qualitative analysis is performed on a real\ndataset to highlight the potential of the proposed query in discovering hidden\ndata characteristics.\n",
        "  Standard chemical evolution models based on long-term infall are affected by\na number of problems, evidenced by the analysis of the most recent data. Among\nthese: (1) models rely on the local metallicity distribution, assuming its\nshape is valid for the entire Galaxy, which it is not; (2) they assume that the\nsolar vicinity abundance patterns resulted from a unique chemical evolution,\nwhich it does not; (3) they assume the disk is a single structure with chemical\nproperties that are a smooth function of the distance to the galactic center,\nwhich it is not. Moreover, new results point to a thick disk being as massive\nas the thin disk, leading to a change of paradigm in the way we see the\nformation of these structures. I discuss these various issues, and, commenting\non Snaith et al. (2014), how a closed box model offers an interesting\napproximation to the galactic chemical evolution, by providing the conditions\nin which large amounts of gas are available in the disk at high redshift. The\nnovel way presented in Snaith et al. (2014) to derive SFH from stellar\nabundances is also discussed, providing a measurement of the SFH of old\npopulations that is valid for the entire Galaxy. The derived SFH shows that the\nformation of the thick disk has been the dominant epoch of star formation in\nour Galaxy.\n",
        "  Animals that scavenge in and around human localities need to utilize a broad\nrange of resources. Preference for any one kind of food, under such\ncircumstances, might be inefficient. Indian free-ranging dogs, Canis lupus\nfamiliaris are scavengers that are heavily dependent on humans for sustaining\ntheir omnivorous diet. The current study suggests that because of evolutionary\nload, these dogs, which are descendants of the decidedly carnivorous gray wolf,\nstill retain a preference for meat though they live on carbohydrate-rich\nresources. The plasticity in their diet probably fosters efficient scavenging\nin a competitive environment, while a thumb rule for preferentially acquiring\nspecific nutrients enables them to sequester proteins from the\ncarbohydrate-rich environment.\n",
        "  A simple empirical relation has been found to exist between optimum Tc and\nthe formal mean cation charge <qc> of perovskite-related superconductors,\ncovering both conventional superconductors and superconducting cuprates. Tc is\nshown to increase exponentially with decreasing <qc>. It is suggested that a\nBa-based cuprate with <qc> about 2 reach a Tc around 200 K. The strong\ncorrelation may be thought of as an indication for a common mechanism of\nsuperconductivity of the whole family of compounds. In addition, the structural\nassignment for the 'cubic' high-Tc phase reported by Volkov (Tc = 117 K) to the\northorhombic BaCuO2.5 prototype is proposed.\n",
        "  We present a magnetization study of low density YBCO ceramics carried out in\nmagnetic fields 0.5 Oe < H < 50 kOe. It was demonstrated that superconducting\nlinks between grains may be completely suppressed either by a magnetic field of\nthe order of 100 Oe (at low temperatures) or by an increase of temperature\nabove 70 K. This property of present samples allowed to evaluate the ratio\nbetween an average grain size and the magnetic field penetration depth lambda.\nFurthermore, at temperatures T > 85 K, using low-field magnetization\nmeasurements, we could evaluate the temperature dependence of lambda, which\nturned out to be very close to predictions of the conventional Ginzburg-Landau\ntheory. Although present samples consisted of randomly oriented grains,\nspecifics of magnetization measurements allowed for evaluation of lambda_ab(T).\nGood agreement between our estimation of the grain size with the real sample\nstructure provides evidence for the validity of this analysis of magnetization\ndata. Measurements of equilibrium magnetization in high magnetic fields were\nused for evaluation of Hc2(T). At temperatures close to T_c, the Hc2(T)\ndependence turned out to be linear in agreement with the Ginzburg-Landau\ntheory. The value of temperature, at which Hc2 vanishes, coincides with the\nsuperconducting critical temperature evaluated from low-field measurements.\n",
        "  We define a torsion invariant T for every balanced sutured manifold (M,g),\nand show that it agrees with the Euler characteristic of sutured Floer homology\nSFH. The invariant T is easily computed using Fox calculus. With the help of T,\nwe prove that if (M,g) is complementary to a Seifert surface of an alternating\nknot, then SFH(M,g) is either 0 or Z in every spin^c structure. T can also be\nused to show that a sutured manifold is not disk decomposable, and to\ndistinguish between Seifert surfaces.\n  The support of SFH gives rise to a norm z on H_2(M, \\partial M; R). Then T\ngives a lower bound on the norm z, which in turn is at most the sutured\nThurston norm x^s. For closed three-manifolds, it is well known that Floer\nhomology determines the Thurston norm, but we show that z < x^s can happen in\ngeneral. Finally, we compute T for several wide classes of sutured manifolds.\n",
        "  Word sense disambiguation (WSD) is a problem in the field of computational\nlinguistics given as finding the intended sense of a word (or a set of words)\nwhen it is activated within a certain context. WSD was recently addressed as a\ncombinatorial optimization problem in which the goal is to find a sequence of\nsenses that maximize the semantic relatedness among the target words. In this\narticle, a novel algorithm for solving the WSD problem called D-Bees is\nproposed which is inspired by bee colony optimization (BCO)where artificial bee\nagents collaborate to solve the problem. The D-Bees algorithm is evaluated on a\nstandard dataset (SemEval 2007 coarse-grained English all-words task corpus)and\nis compared to simulated annealing, genetic algorithms, and two ant colony\noptimization techniques (ACO). It will be observed that the BCO and ACO\napproaches are on par.\n",
        "  When polygenic traits are under stabilizing selection, many different\ncombinations of alleles allow close adaptation to the optimum. If alleles have\nequal effects, all combinations that result in the same deviation from the\noptimum are equivalent. Furthermore, the genetic variance that is maintained by\nmutation-selection balance is $2 \\mu/S$ per locus, where $\\mu$ is the mutation\nrate and $S$ the strength of stabilizing selection. In reality, alleles vary in\ntheir effects, making the fitness landscape asymmetric, and complicating\nanalysis of the equilibria. We show that that the resulting genetic variance\ndepends on the fraction of alleles near fixation, which contribute by $2\n\\mu/S$, and on the total mutational effects of alleles that are at intermediate\nfrequency. The interplay between stabilizing selection and mutation leads to a\nsharp transition: alleles with effects smaller than a threshold value of\n$2\\sqrt{\\mu / S}$ remain polymorphic, whereas those with larger effects are\nfixed. The genetic load in equilibrium is less than for traits of equal\neffects, and the fitness equilibria are more similar. We find that if the\noptimum is displaced, alleles with effects close to the threshold value sweep\nfirst, and their rate of increase is bounded by $\\sqrt{\\mu S}$. Long term\nresponse leads in general to well-adapted traits, unlike the case of equal\neffects that often end up at a sub-optimal fitness peak. However, the\nparticular peaks to which the populations converge are extremely sensitive to\nthe initial states, and to the speed of the shift of the optimum trait value.\n",
        "  We consider the existence of simple closed geodesics or \"geodesic knots\" in\nfinite volume orientable hyperbolic 3-manifolds. Previous results show that at\nleast one geodesic knot always exists [Bull. London Math. Soc. 31(1) (1999)\n81-86], and that certain arithmetic manifolds contain infinitely many geodesic\nknots [J. Diff. Geom. 38 (1993) 545-558], [Experimental Mathematics 10(3)\n(2001) 419-436]. In this paper we show that all cusped orientable finite volume\nhyperbolic 3-manifolds contain infinitely many geodesic knots. Our proof is\nconstructive, and the infinite family of geodesic knots produced approach a\nlimiting infinite simple geodesic in the manifold.\n",
        "  The relation of syntax and prosody (the syntax--prosody interface) has been\nan active area of research, mostly in linguistics and typically studied under\ncontrolled conditions. More recently, prosody has also been successfully used\nin the data-based training of syntax parsers. However, there is a gap between\nthe controlled and detailed study of the individual effects between syntax and\nprosody and the large-scale application of prosody in syntactic parsing with\nonly a shallow analysis of the respective influences. In this paper, we close\nthe gap by investigating the significance of correlations of prosodic\nrealization with specific syntactic functions using linear mixed effects models\nin a very large corpus of read-out German encyclopedic texts. Using this\ncorpus, we are able to analyze prosodic structuring performed by a diverse set\nof speakers while they try to optimize factual content delivery. After\nnormalization by speaker, we obtain significant effects, e.g. confirming that\nthe subject function, as compared to the object function, has a positive effect\non pitch and duration of a word, but a negative effect on loudness.\n",
        "  We present results from the largest CaII triplet line metallicity study of\nSmall Magellanic Cloud (SMC) field red giant stars to date, involving 3037\nobjects spread across approximately 37.5 sq. deg., centred on this galaxy. We\nfind a median metallicity of [Fe/H]=-0.99+/-0.01, with clear evidence for an\nabundance gradient of -0.075+/-0.011 dex / deg. over the inner 5 deg. We\ninterpret the abundance gradient to be the result of an increasing fraction of\nyoung stars with decreasing galacto-centric radius, coupled with a uniform\nglobal age-metallicity relation. We also demonstrate that the age-metallicity\nrelation for an intermediate age population located 10kpc in front of the NE of\nthe Cloud is indistinguishable from that of the main body of the galaxy,\nsupporting a prior conjecture that this is a stellar analogue of the Magellanic\nBridge. The metal poor and metal rich quartiles of our RGB star sample (with\ncomplementary optical photometry from the Magellanic Clouds Photometric Survey)\nare predominantly older and younger than approximately 6Gyr, respectively.\nConsequently, we draw a link between a kinematical signature, tentatively\nassociated by us with a disk-like structure, and the upsurges in stellar\ngenesis imprinted on the star formation history of the central regions of the\nSMC. We conclude that the increase in the star formation rate around 5-6Gyr ago\nwas most likely triggered by an interaction between the SMC and LMC.\n",
        "  We consider the task of learning a context-dependent mapping from utterances\nto denotations. With only denotations at training time, we must search over a\ncombinatorially large space of logical forms, which is even larger with\ncontext-dependent utterances. To cope with this challenge, we perform\nsuccessive projections of the full model onto simpler models that operate over\nequivalence classes of logical forms. Though less expressive, we find that\nthese simpler models are much faster and can be surprisingly effective.\nMoreover, they can be used to bootstrap the full model. Finally, we collected\nthree new context-dependent semantic parsing datasets, and develop a new\nleft-to-right parser.\n",
        "  Navigational graph queries are an important class of queries that canextract\nimplicit binary relations over the nodes of input graphs. Most of the\nnavigational query languages used in the RDF community, e.g. property paths in\nW3C SPARQL 1.1 and nested regular expressions in nSPARQL, are based on the\nregular expressions. It is known that regular expressions have limited\nexpressivity; for instance, some natural queries, like same generation-queries,\nare not expressible with regular expressions. To overcome this limitation, in\nthis paper, we present cfSPARQL, an extension of SPARQL query language equipped\nwith context-free grammars. The cfSPARQL language is strictly more expressive\nthan property paths and nested expressions. The additional expressivity can be\nused for modelling graph similarities, graph summarization and ontology\nalignment. Despite the increasing expressivity, we show that cfSPARQL still\nenjoys a low computational complexity and can be evaluated efficiently.\n",
        "  We survey recent work on the specification of an access control mechanism in\na collaborative environment. The work is presented in the context of the\nWebdamLog language, an extension of datalog to a distributed context. We\ndiscuss a fine-grained access control mechanism for intentional data based on\nprovenance as well as a control mechanism for delegation, i.e., for deploying\nrules at remote peers.\n",
        "  For a plane symmetric object we can find two views - mirrored at the plane of\nsymmetry - that will yield the exact same image of that object. In consequence,\nhaving one image of a plane symmetric object and a calibrated camera, we can\nautomatically have a second, virtual image of that object if the 3-D location\nof the symmetry plane is known. In this work, we show for the first time that\nthe above concept naturally extends to transmission imaging and present an\nalgorithm to estimate the 3-D symmetry plane from a set of projection domain\nimages based on Grangeat's theorem. We then exploit symmetry to generate a\nvirtual trajectory by mirroring views at the plane of symmetry. If the plane is\nnot perpendicular to the acquired trajectory plane, the virtual and real\ntrajectory will be oblique. The resulting X-shaped trajectory will be\ndata-complete, allowing for the compensation of in-plane motion using epipolar\nconsistency. We evaluate the proposed method on a synthetic symmetric phantom\nand, in a proof-of-concept study, apply it to a real scan of an anthropomorphic\nhuman head phantom.\n",
        "  The lack of large realistic datasets presents a bottleneck in online\ndeception detection studies. In this paper, we apply a data collection method\nbased on social network analysis to quickly identify high-quality deceptive and\ntruthful online reviews from Amazon. The dataset contains more than 10,000\ndeceptive reviews and is diverse in product domains and reviewers. Using this\ndataset, we explore effective general features for online deception detection\nthat perform well across domains. We demonstrate that with generalized features\n- advertising speak and writing complexity scores - deception detection\nperformance can be further improved by adding additional deceptive reviews from\nassorted domains in training. Finally, reviewer level evaluation gives an\ninteresting insight into different deceptive reviewers' writing styles.\n",
        "  Recent studies suggest that 4DCT is unable to accurately measure\nrespiratory-induced pancreatic tumor motion. In this work, we assessed the\ndaily motion of pancreatic tumors treated with SBRT, and developed adaptive\nstrategies to predict and account for this motion. The daily motion trajectory\nof pancreatic tumors during CBCT acquisition was calculated using a model which\nreconstructs the instantaneous 3D position in each 2D CBCT projection image. We\ndeveloped a metric (termed \"Spectral Coherence,\" SC) based on the Fourier\nfrequency spectrum of motion in the SI direction, and analyzed the ability of\nSC to predict motion-based errors and classify patients according to motion\ncharacteristics. The amplitude of daily motion exceeded the predictions of\npre-treatment 4DCT imaging by an average of 3.0 mm, 2.3 mm, and 3.5 mm in the\nAP, LR, and SI directions. SC was correlated with daily motion differences and\ntumor dose coverage. In a simulated adaptive protocol, target margins were\nadjusted based on SC, resulting in significant increases in mean target D95,\nD99, and minimum dose. Our Fourier-based approach differentiates between\nconsistent and inconsistent motion characteristics of respiration and\ncorrelates with daily motion deviations from pre-treatment 4DCT. The\nfeasibility of an SC-based adaptive protocol was demonstrated, and this\npatient-specific respiratory information was used to improve target dosimetry\nby expanding coverage in inconsistent breathers while shrinking treatment\nvolumes in consistent breathers.\n",
        "  We describe a variant of Child-Sum Tree-LSTM deep neural network (Tai et al,\n2015) fine-tuned for working with dependency trees and morphologically rich\nlanguages using the example of Polish. Fine-tuning included applying a custom\nregularization technique (zoneout, described by (Krueger et al., 2016), and\nfurther adapted for Tree-LSTMs) as well as using pre-trained word embeddings\nenhanced with sub-word information (Bojanowski et al., 2016). The system was\nimplemented in PyTorch and evaluated on phrase-level sentiment labeling task as\npart of the PolEval competition.\n",
        "  It is well known that superconductivity in thin films is generally suppressed\nwith decreasing thickness. This suppression is normally governed by either\ndisorder-induced localization of Cooper pairs, weakening of Coulomb screening,\nor generation and unbinding of vortex-antivortex pairs as described by the\nBerezinskii-Kosterlitz-Thouless (BKT) theory. Defying general expectations,\nfew-layer NbSe2 - an archetypal example of ultrathin superconductors - has been\nfound to remain superconducting down to monolayer thickness. Here we report\nmeasurements of both the superconducting energy gap and critical temperature in\nhigh-quality monocrystals of few-layer NbSe2, using planar-junction tunneling\nspectroscopy and lateral transport. We observe a fully developed gap that\nrapidly reduces for devices with the number of layers N < 5, as does their\nctitical temperature. We show that the observed reduction cannot be explained\nby disorder, and the BKT mechanism is also excluded by measuring its transition\ntemperature that for all N remains very close to Tc. We attribute the observed\nbehavior to changes in the electronic band structure predicted for mono- and\nbi- layer NbSe2 combined with inevitable suppression of the Cooper pair density\nat the superconductor-vacuum interface. Our experimental results for N > 2 are\nin good agreement with the dependences of the gap and Tc expected in the latter\ncase while the effect of band-structure reconstruction is evidenced by a\nstronger suppression of the gap and the disappearance of its anisotropy for N =\n2. The spatial scale involved in the surface suppression of the density of\nstates is only a few angstroms but cannot be ignored for atomically thin\nsuperconductors.\n",
        "  Computational modeling can provide critical insight into existing and\npotential new surgical procedures, medical or minimally-invasive treatments for\nheart failure, one of the leading causes of deaths in the world that has\nreached epidemic proportions. In this paper, we present our\nAbaqus/Standard-based pipeline to create subject-specific left ventricular\nmodels. We first review our generic left ventricular model, and then the\npersonalization process based on magnetic resonance images. Identification of\nsubject-specific cardiac material properties is done by coupling\nAbaqus/Standard to the python optimization library NL-Opt. Compared to previous\nstudies from our group, the emphasis is here on the fully implicit solving of\nthe model, and the two-parameter optimization of the passive cardiac material\nproperties.\n",
        "  Assessing the degree of the child's body building optimal for obtaining\nperformance in the practiced sport can be done by reporting the individual\ndeveloping model to the typological models considered optimal for the\nrequirements of each sportive branch in part.\n",
        "  A rooted acyclic digraph N with labelled leaves displays a tree T when there\nexists a way to select a unique parent of each hybrid vertex resulting in the\ntree T. Let Tr(N) denote the set of all trees displayed by the network N. In\ngeneral, there may be many other networks M such that Tr(M) = Tr(N). A network\nis regular if it is isomorphic with its cover digraph. This paper shows that if\nN is regular, there is a procedure to reconstruct N given Tr(N). Hence if N and\nM are regular networks and Tr(N) = Tr(M), it follows that N = M, proving that a\nregular network is uniquely determined by its displayed trees.\n",
        "  We monitored twelve acoustically-tagged small pelagic fish (Selar\ncrumenophthalmus) around a floating object in shallow water, playing the role\nof a coastal fish aggregating device (FAD). We characterized the response of\nthe tagged-fish aggregation to varying current strengths and daylight. We found\nthat the current induced a displacement of the aggregation upstream of the FAD,\nat distances that were increasing with the current strength. We gave evidence,\nof an expansion and a higher coordination in the aggregation at dusk, with\nincreasing swimming speed, distances among congeners and alignment. We\ndiscussed possible scenarios where fish polarization increases at dusk and\nproposed complementary measurements in future experiments that could confirm\nour findings.\n",
        "  We consider a model for the evolution of cooperation in a population where\nindividuals may have one of a number of different heritable and distinguishable\nmarkers or tags. Individuals interact with each of their neighbours on a square\nlattice by either cooperating by donating some benefit at a cost to themselves\nor defecting by doing nothing. The decision to cooperate or defect is\ncontingent on each individual's perception of its interacting partner's tag.\nUnlike in other tag-based models individuals do not compare their own tag to\nthat of their interaction partner. That is, there is no {\\em self-matching}.\nWhen perception is perfect the cooperation rate is substantially higher than in\nthe usual spatial prisoner's dilemma game when the cost of cooperation is high.\nThe enhancement in cooperation is positively correlated with the number of\ndifferent tags. The more diverse a population is the more cooperative it\nbecomes. When individuals start with an inability to perceive tags the\npopulation evolves to a state where individuals gain at least partial\nperception. With some reproduction mechanisms perfect perception evolves, but\nwith others the ability to perceive tags is imperfect. We find that perception\nof tags evolves to lower levels when the cost of cooperation is higher.\n",
        "  In this work we present a complete (no misses, no duplicates) census for\nclosed, connected, orientable and prime 3-manifolds induced by plane graphs\nwith a bipartition of its edge set (blinks) up to $k=9$ edges. Blinks form a\nuniversal encoding for such manifolds. In fact, each such a manifold is a\nsubtle class of blinks, \\cite{lins2013B}. Blinks are in 1-1 correpondence with\n{\\em blackboard framed links}, \\cite {kauffman1991knots, kauffman1994tlr} We\nhope that this census becomes as useful for the study of concrete examples of\n3-manifolds as the tables of knots are in the study of knots and links.\n",
        "  Sarkar and Wang have given a combinatorial algorithm for computing Heegaard\nFloer homology and Plamenevskaya has improved their method to compute\nOzsvath-Szabo invariant. In this paper, applying the combinatorial method to\nstabilizations of an open book, we prove basic properties of Ozsvath-Szabo\ninvariant.\n",
        "  In this paper we prove the validity of a formula for computing the Alexander\ninvariant which was originally conjectured by Bar-Natan and Dancso in [BND].\n",
        "  We are living in an ever more connected world, where data recording the\ninteractions between people, software systems, and the physical world is\nbecoming increasingly prevalent. This data often takes the form of a temporally\nevolving graph, where entities are the vertices and the interactions between\nthem are the edges. We call such graphs interaction graphs. Various application\ndomains, including telecommunications, transportation, and social media, depend\non analytics performed on interaction graphs. The ability to efficiently\nsupport historical analysis over interaction graphs require effective solutions\nfor the problem of data layout on disk. This paper presents an adaptive disk\nlayout called the railway layout for optimizing disk block storage for\ninteraction graphs. The key idea is to divide blocks into one or more\nsub-blocks, where each sub-block contains a subset of the attributes, but the\nentire graph structure is replicated within each sub-block. This improves query\nI/O, at the cost of increased storage overhead. We introduce optimal ILP\nformulations for partitioning disk blocks into sub-blocks with overlapping and\nnon-overlapping attributes. Additionally, we present greedy heuristic\napproaches that can scale better compared to the ILP alternatives, yet achieve\nclose to optimal query I/O. To demonstrate the benefits of the railway layout,\nwe provide an extensive experimental study comparing our approach to a few\nbaseline alternatives.\n",
        "  Massive early-type galaxies have higher metallicities and higher ratios of\n$\\alpha$ elements to iron than their less massive counterparts. Reproducing\nthese correlations has long been a problem for hierarchical galaxy formation\ntheory, both in semi-analytic models and cosmological hydrodynamic simulations.\nWe show that a simulation in which gas cooling in massive dark haloes is\nquenched by radio-mode active galactic nuclei (AGNs) feedback naturally\nreproduces the observed trend between $\\alpha$/Fe and the velocity dispersion\nof galaxies, $\\sigma$. The quenching occurs earlier for more massive galaxies.\nConsequently, these galaxies complete their star formation before $\\alpha$/Fe\nis diluted by the contribution from type Ia supernovae. For galaxies more\nmassive than $\\sim 10^{11}~M_\\odot$ whose $\\alpha$/Fe correlates positively\nwith stellar mass, we find an inversely correlated mass-metallicity relation.\nThis is a common problem in simulations in which star formation in massive\ngalaxies is quenched either by quasar- or radio-mode AGN feedback. The early\nsuppression of gas cooling in progenitors of massive galaxies prevents them\nfrom recapturing enriched gas ejected as winds. Simultaneously reproducing the\n[$\\alpha$/Fe]-$\\sigma$ relation and the mass-metallicity relation is, thus,\ndifficult in the current framework of galaxy formation.\n",
        "  White matter tract integrity (WMTI) can characterize brain microstructure in\nareas with highly aligned fiber bundles. Several WMTI biomarkers have now been\nvalidated against microscopy and provided promising results in studies of brain\ndevelopment and aging, and in a number of brain disorders. Currently, WMTI is\nmostly used in dedicated animal studies and clinical studies of slowly\nprogressing diseases but has not yet emerged as a routine clinical tool. To\nthis end, a less data intensive experimental method would be beneficial by\nenabling high resolution validation studies, and ease clinical applications by\nspeeding up data acquisition compared to typical diffusion kurtosis imaging\n(DKI) protocols utilized as part of WMTI imaging. Here, we evaluate WMTI based\non recently introduced axially symmetric DKI which has lower data demand than\nconventional DKI. We compare WMTI parameters derived from conventional DKI to\nthose calculated analytically from axially symmetric DKI. We employ numerical\nsimulations, as well as data from fixed rat spinal cord (1) and in vivo human\n(3) and rat brain (4). Our analysis shows that analytical WMTI based on axially\nsymmetric DKI with sparse data sets (19 images) produces WMTI metrics that\ncorrelate strongly with estimates based on traditional DKI data sets (60 images\nor more). We demonstrate the preclinical potential of the proposed WMTI\ntechnique in in vivo rat brain (300 {\\mu}m isotropic resolution with whole\nbrain coverage). WMTI parameter estimates are subject to a duality leading to\ntwo solution branches dependent on a sign choice which is currently debated.\nResults from both of these branches are presented and discussed throughout our\nanalysis. The proposed fast WMTI approach may be useful for preclinical\nresearch and e.g. clinical evaluation of patients with traumatic white matter\ninjuries or symptoms of neurovascular or neuroinflammatory disorders.\n",
        "  Exceptional growth in the availability of large-scale clinical imaging\ndatasets has led to the development of computational infrastructures offering\nscientists access to image repositories and associated clinical variables data.\nThe EU FP7 neuGRID and its follow on neuGRID4You (N4U) project is a leading\ne-Infrastructure where neuroscientists can find core services and resources for\nbrain image analysis. The core component of this e-Infrastructure is the N4U\nVirtual Laboratory, which offers an easy access for neuroscientists to a wide\nrange of datasets and algorithms, pipelines, computational resources, services,\nand associated support services. The foundation of this virtual laboratory is a\nmassive data store plus information services called the Data Atlas that stores\ndatasets, clinical study data, data dictionaries, algorithm/pipeline\ndefinitions, and provides interfaces for parameterised querying so that\nneuroscientists can perform analyses on required datasets. This paper presents\nthe overall design and development of the Data Atlas, its associated datasets\nand indexing and a set of retrieval services that originated from the\ndevelopment of the N4U Virtual Laboratory in the EU FP7 N4U project in the\nlight of user requirements.\n",
        "  Terraces are potentially large sets of trees with precisely the same\nlikelihood or parsimony score, which can be induced by missing sequences in\npartitioned multi-locus phylogenetic data matrices. The set of trees on a\nterrace can be characterized by enumeration algorithms or consensus methods\nthat exploit the pattern of partial taxon coverage in the data, independent of\nthe sequence data themselves. Terraces add ambiguity and complexity to\nphylogenetic inference particularly in settings where inference is already\nchallenging: data sets with many taxa and relatively few loci. In this paper we\npresent five new findings about terraces and their impacts on phylogenetic\ninference. First we clarify assumptions about model parameters that are\nnecessary for the existence of terraces. Second, we explore the dependence of\nterrace size on partitioning scheme and indicate how to find the partitioning\nscheme associated with the largest terrace containing a given tree. Third, we\nhighlight the impact of terraces on bootstrap estimates of confidence limits in\nclades, and characterize the surprising result that the bootstrap proportion\nfor a clade can be entirely determined by the frequency of bipartitions on a\nterrace, with some bipartitions receiving high support even when incorrect.\nFourth, we dissect some effects of prior distributions of edge lengths on the\ncomputed posterior probabilities of clades on terraces, to understand an\nexample in which long edges \"attract\" each other in Bayesian inference. Fifth,\nwe show that even if data are not partitioned, patterns of missing data studied\nin the terrace problem can lead to instances of apparent statistical\ninconsistency when even a small element of heterotachy is introduced to the\nmodel generating the sequence data. Finally, we discuss strategies for\nremediation of some of these problems.\n",
        "  Sentence simplification aims to simplify the content and structure of complex\nsentences, and thus make them easier to interpret for human readers, and easier\nto process for downstream NLP applications. Recent advances in neural machine\ntranslation have paved the way for novel approaches to the task. In this paper,\nwe adapt an architecture with augmented memory capacities called Neural\nSemantic Encoders (Munkhdalai and Yu, 2017) for sentence simplification. Our\nexperiments demonstrate the effectiveness of our approach on different\nsimplification datasets, both in terms of automatic evaluation measures and\nhuman judgments.\n",
        "  It has been noticed that the critical current density Jc of some of the\nsuperconducting samples, calculated on the basis of Bean model, increases with\nincreasing magnetic field H up to a significant range above H=0. This is an\ninconsistent behavior of Jc since the theory of Kim and the theories based on\nvortex dynamics, all, lead to decreasing Jc with increasing H for H > 0. It has\nbeen argued that a realistic variation of Jc for low H may be obtained within\nBean framework by redefining the width of the hysteresis loop. The new\ndefinition of the loop width is guided by the requirement that Jc stays as\nclose to the Jc of the theory of Kim as possible. Illustrative calculations of\nJc show its considerable enhancement over the Bean values.\n",
        "  Purpose/Objective: Backscattered radiation (BSR) into linac monitor chamber\nhas to be accounted for in radiotherapy dose calculations. In Monte Carlo (MC)\ncalculations, the BSR can be modeled explicitly, but only when treatment head\ngeometry is available. In this study, monitor backscatter factors (MBSFs),\ndefined as the ratio of the charge collected in the monitor chamber for a\nreference field to that of a given field, have been evaluated experimentally\nand incorporated into MC modelling of linacs with either known or unknown\ntreatment head geometry.\n  Materials and methods: A telescopic technique similar to that by Kubo (1989)\nwas used. However, instead of lead slits, a 1.8 mm diameter collimator and a\nsmall (2 mm diameter) detector positioned at extended SDD were used. This setup\nprovided a field of view to the source of less than 3.1 mm and allowed for MBSF\nmeasurements of open fields from 1 x 1 to 40 x 40 cm2. MBSFs were also\nexplicitly modeled in MC calculations using BEAMnrc and DOSXYZnrc codes for 6MV\nand 18MV beams of a Varian 21EX linac. A method for deriving the Dchforward\nvalues that are used in MC absolute dose calculations was demonstrated. These\nvalues were derived from measured MBSFs for two 21EX and four TrueBeam\nenergies.\n  Results: MBSFs were measured for 6MV and 18MV beams from Varian 21EX, and for\n6MV, 10MV-FFF, 10MV, and 15MV beams from Varian TrueBeam linacs. For the open\nfield sizes modeled in this study for the 21EX, the measured MBSFs agreed with\nMC calculated values within combined statistical (0.4%) and experimental (0.2%)\nuncertainties. Variation of MBSFs across field sizes was about a factor of two\nsmaller for the TrueBeam compared to 21EX Varian linacs.\n  Conclusions: Measured MBSFs and the derived Dchforward factors allow for the\nincorporation of the BSR effect into accurate radiotherapy dose calculations\nwithout explicit backscatter modelling.\n",
        "  Recent experiments on superfluid $^3$He in globally anisotropic aerogels have\nshown realization of the polar superfluid phase and of the half-quantum\nvortices (HQVs) in this phase upon rotation. To clarify why the HQVs, which had\nnot been detected clearly in the A phase of the bulk liquid, have been realized\nin the polar phase, we theoretically examine the relative stability of a\nHQV-pair against a single phase vortex in both the bulk A-phase and the polar\nphase in an aerogel. By taking care of important roles of a higher order\ngradient term, which assists the stability of HQVs but has never been\nincorporated so far in the Ginzburg-Landau (GL) approach, we find that several\nconsequences, including the extension of the polar phase at lower pressures in\nthe phase diagram, facilitate realization of the HQVs there in contrast to the\ncase of the A phase in a slab geometry.\n",
        "  A hundred years after discovery of superconductivity, one fundamental\nprediction of the theory, the coherent quantum phase slip (CQPS), has not been\nobserved. CQPS is a phenomenon exactly dual to the Josephson effect: whilst the\nlatter is a coherent transfer of charges between superconducting contacts, the\nformer is a coherent transfer of vortices or fluxes across a superconducting\nwire. In contrast to previously reported observations of incoherent phase slip,\nthe CQPS has been only a subject of theoretical study. Its experimental\ndemonstration is made difficult by quasiparticle dissipation due to gapless\nexcitations in nanowires or in vortex cores. This difficulty might be overcome\nby using certain strongly disordered superconductors in the vicinity of the\nsuperconductor-insulator transition (SIT). Here we report the first direct\nobservation of the CQPS in a strongly disordered indium-oxide (InOx)\nsuperconducting wire inserted in a loop, which is manifested by the\nsuperposition of the quantum states with different number of fluxes. Similarly\nto the Josephson effect, our observation is expected to lead to novel\napplications in superconducting electronics and quantum metrology.\n",
        "  Ly-alpha blobs (LABs) offer insight into the complex interface between\ngalaxies and their circumgalactic medium. Whilst some LABs have been found to\ncontain luminous star-forming galaxies and active galactic nuclei that could\npotentially power the Ly-alpha emission, others appear not to be associated\nwith obvious luminous galaxy counterparts. It has been speculated that LABs may\nbe powered by cold gas streaming on to a central galaxy, providing an\nopportunity to directly observe the `cold accretion' mode of galaxy growth.\nStar-forming galaxies in LABs could be dust obscured and therefore detectable\nonly at longer wavelengths. We stack deep SCUBA-2 observations of the SSA22\nfield to determine the average 850um flux density of 34 LABs. We measure S_850\n= 0.6 +/- 0.2mJy for all LABs, but stacking the LABs by size indicates that\nonly the largest third (area > 1794 kpc^2) have a mean detection, at 4.5 sigma,\nwith S_850 = 1.4 +/- 0.3mJy. Only two LABs (1 and 18) have individual SCUBA-2 >\n3.5 sigma detections at a depth of 1.1mJy/beam. We consider two possible\nmechanisms for powering the LABs and find that central star formation is likely\nto dominate the emission of Ly-alpha, with cold accretion playing a secondary\nrole.\n",
        "  Maintaining a legacy database is a difficult task especially when system\ndocumentation is poor written or even missing. Database reverse engineering is\nan attempt to recover high-level conceptual design from the existing database\ninstances. In this paper, we propose a technique to discover conceptual schema\nusing the association mining technique. The discovered schema corresponds to\nthe normalization at the third normal form, which is a common practice in many\nbusiness organizations. Our algorithm also includes the rule filtering\nheuristic to solve the problem of exponential growth of discovered rules\ninherited with the association mining technique.\n",
        "  Given a genus two Heegaard splitting for a non-prime 3-manifold, we define a\nspecial subcomplex of the disk complex for one of the handlebodies of the\nsplitting, and then show that it is contractible. As applications, first we\nshow that the complex of Haken spheres for the splitting is contractible, which\nrefines the results of Lei and Lei-Zhang. Secondly, we classify all the genus\ntwo Heegaard splittings for non-prime 3-manifolds, which is a generalization of\nthe result of Montesinos-Safont. Finally, we show that the mapping class group\nof the splitting, called the Goeritz group, is finitely presented by giving its\nexplicit presentation.\n",
        "  In-memory columnar databases have become mainstream over the last decade and\nhave vastly improved the fast processing of large volumes of data through\nmulti-core parallelism and in-memory compression thereby eliminating the usual\nbottlenecks associated with disk-based databases. For scenarios, where the data\nvolume grows into terabytes and petabytes, keeping all the data in memory is\nexorbitantly expensive. Hence, the data is compressed efficiently using\ndifferent algorithms to exploit the multi-core parallelization technologies for\nquery processing. Several compression methods are studied for compressing the\ncolumn array, post Dictionary Encoding. In this paper, we will present two\nnovel optimizations in compression techniques - Block Size Optimized Cluster\nEncoding and Block Size Optimized Indirect Encoding - which perform better than\ntheir predecessors. In the end, we also propose heuristics to choose the best\nencoding amongst common compression schemes.\n",
        "  Contact round surgery of contact 3-manifolds is introduced in this paper. By\nusing this method, an alternative proof of the existence of a contact structure\non any closed orientable 3-manifold is given. It is also proved that any\ncontact structure on any closed orientable 3-manifold is constructed from the\nstandard contact structure on the 3-dimensional sphere by contact round\nsurgeries. For the proof, important operations in contact topology, the Lutz\ntwist and the Giroux torsion, are described in terms of contact round\nsurgeries.\n",
        "  We propose a unified Implicit Dialog framework for goal-oriented, information\nseeking tasks of Conversational Search applications. It aims to enable dialog\ninteractions with domain data without replying on explicitly encoded the rules\nbut utilizing the underlying data representation to build the components\nrequired for dialog interaction, which we refer as Implicit Dialog in this\nwork. The proposed framework consists of a pipeline of End-to-End trainable\nmodules. A centralized knowledge representation is used to semantically ground\nmultiple dialog modules. An associated set of tools are integrated with the\nframework to gather end users' input for continuous improvement of the system.\nThe goal is to facilitate development of conversational systems by identifying\nthe components and the data that can be adapted and reused across many end-user\napplications. We demonstrate our approach by creating conversational agents for\nseveral independent domains.\n",
        "  Coevolution of two species is typically thought to favour the evolution of\nfaster evolutionary rates helping a species keep ahead in the Red Queen race,\nwhere `it takes all the running you can do to stay where you are'. In contrast,\nif species are in a mutualistic relationship, it was proposed that the Red King\neffect may act, where it can be beneficial to evolve slower than the\nmutualistic species. The Red King hypothesis proposes that the species which\nevolves slower can gain a larger share of the benefits. However, the\ninteractions between the two species may involve multiple individuals. To\nanalyse such a situation, we resort to evolutionary multiplayer games. Even in\nsituations where evolving slower is beneficial in a two-player setting, faster\nevolution may be favoured in a multiplayer setting. The underlying features of\nmultiplayer games can be crucial for the distribution of benefits. They also\nsuggest a link between the evolution of the rate of evolution and group size.\n",
        "  The need for performance measurement tools appeared soon after the emergence\nof the first Object-Oriented Database Management Systems (OODBMSs), and proved\nimportant for both designers and users (Atkinson \\& Maier, 1990). Performance\nevaluation is useful to designers to determine elements of architecture and\nmore generally to validate or refute hypotheses regarding the actual behavior\nof an OODBMS. Thus, performance evaluation is an essential component in the\ndevelopment process of well-designed and efficient systems. Users may also\nemploy performance evaluation, either to compare the efficiency of different\ntechnologies before selecting an OODBMS or to tune a system.Performance\nevaluation by experimentation on a real system is generally referred to as\nbenchmarking. It consists in performing a series of tests on a given OODBMS to\nestimate its performance in a given setting. Benchmarks are generally used to\ncompare the global performance of OODBMSs, but they can also be exploited to\nillustrate the advantages of one system or another in a given situation, or to\ndetermine an optimal hardware configuration. Typically, a benchmark is\nconstituted of two main elements: a workload model constituted of a database\nand a set of read and write operations to apply on this database, and a set of\nperformance metrics.\n",
        "  Phase contrast X-ray imaging (PCXI) is an emerging imaging modality that has\nthe potential to greatly improve radiography for medical imaging and materials\nanalysis. PCXI makes it possible to visualise soft-tissue structures that are\notherwise unresolved with conventional CT by rendering phase gradients in the\nX-ray wavefield visible. This can improve the contrast resolution of soft\ntissues structures, like the lungs and brain, by orders of magnitude. Phase\nretrieval suppresses noise, revealing weakly-attenuating soft tissue\nstructures, however it does not remove the artefacts from the highly\nattenuating bone of the skull and from imperfections in the imaging system that\ncan obscure those structures. The primary causes of these artefacts are\ninvestigated and a simple method to visualise the features they obstruct is\nproposed, which can easily be implemented for preclinical animal studies. We\nshow that phase contrast X-ray CT (PCXI-CT) can resolve the soft tissues of the\nbrain in situ without a need for contrast agents at a dose $\\sim$400 times\nlower than would be required by standard absorption contrast CT. We generalise\na well-known phase retrieval algorithm for multiple-material samples\nspecifically for CT, validate its use for brain CT, and demonstrate its high\nstability in the presence of noise.\n",
        "  We simulate mergers between galaxies containing collisionally-relaxed nuclei\naround massive black holes (MBHs). Our galaxies contain four mass groups,\nrepresentative of old stellar populations; a primary goal is to understand the\ndistribution of stellar-mass black holes (BHs) after the merger. Mergers are\nfollowed using direct-summation N-body simulations, assuming a mass ratio of\n1:3 and two different orbits. Evolution of the binary MBH is followed until its\nseparation has shrunk by a factor of 20 below the hard-binary separation.\nDuring the galaxy merger, large cores are carved out in the stellar\ndistribution, with radii several times the influence radius of the massive\nbinary. Much of the pre-existing mass segregation is erased during this phase.\nWe follow the evolution of the merged galaxies for approximately three, central\nrelaxation times after coalescence of the massive binary; both standard, and\ntop-heavy, mass functions are considered. The cores that were formed in the\nstellar distribution persist, and the distribution of the stellar-mass black\nholes evolves against this essentially fixed background. Even after one central\nrelaxation time, these models look very different from the relaxed, multi-mass\nmodels that are often assumed to describe the distribution of stars and stellar\nremnants near a massive BH. While the stellar BHs do form a cusp on roughly a\nrelaxation time-scale, the BH density can be much smaller than in those models.\nWe discuss the implications of our results for the EMRI problem and for the\nexistence of Bahcall-Wolf cusps.\n",
        "  We investigated the time-dependent radiative and dynamical properties of\nlight supersonic jets launched into an external medium, using hydrodynamic\nsimulations and numerical radiative transfer calculations. These involved\nvarious structural models for the ambient media, with density profiles\nappropriate for galactic and extragalactic systems. The radiative transfer\nformulation took full account of emission, absorption, re-emission, Faraday\nrotation and Faraday conversion explicitly. High time-resolution intensity maps\nwere generated, frame-by-frame, to track the spatial hydrodynamical and\nradiative properties of the evolving jets. Intensity light curves were computed\nvia integrating spatially over the emission maps. We apply the models to jets\nin active galactic nuclei (AGN). From the jet simulations and the\ntime-dependent emission calculations we derived empirical relations for the\nemission intensity and size for jets at various evolutionary stages. The\ntemporal properties of jet emission are not solely consequences of intrinsic\nvariations in the hydrodynamics and thermal properties of the jet. They also\ndepend on the interaction between the jet and the ambient medium. The\ninterpretation of radio jet morphology therefore needs to take account of\nenvironmental factors. Our calculations have also shown that the environmental\ninteractions can affect specific emitting features, such as internal shocks and\nhotspots. Quantification of the temporal evolution and spatial distribution of\nthese bright features, together with the derived relations between jet size and\nemission, would enable us to set constraints on the hydrodynamics of AGN and\nthe structure of the ambient medium.\n",
        "  A conflict exists between field biologists and physiologists (\"functional\nbiologists\" or \"evolutionary ecologists\") on the one hand and those working in\nmolecular evolution (\"evolutionary biologists\" or \"population geneticists\") on\nthe other concerns the relative importance of natural selection and genetic\ndrift. This paper is concerned with this issue in the case of vertebrates such\nas birds, fishes, mammals, and specifically humans, and views the issue in that\ncontext from a multilevel perspective. It proposes that the resolution is that\nadaptive selection outcomes occurring at the organism level chain down to\ndetermine outcomes at the genome level. The multiple realizability of higher\nlevel processes at lower levels then causes the adaptive nature of such\nprocesses at the organism level to be largely hidden at the genomic level. The\ndiscussion is further related to the \"negative view\" of selection, the Evo-Devo\nand Extended Evolutionary Synthesis views, and the levels of selection debate,\nwhere processes at the population level can also chain down in a similar way\n",
        "  This document is another installment in a series of near real-time weekly\ninfluenza forecasts made during the 2012-2013 influenza season. Here we present\nsome of the results of forecasts initiated following assimilation of\nobservations for Week 52 (i.e. the forecast begins December 30, 2012) for\nmunicipalities in the United States. The forecasts were made on January 4,\n2013. Results from forecasts initiated the five previous weeks (Weeks 47-51)\nare also presented.\n",
        "  We study the static and dynamic behavior of charge ordering within a d-wave\npair pseudogap (pg) scenario. This is addressed using a density-density\ncorrelation function derived from the standard pg self energy, $\\Sigma$ and\ncompatible with the longitudinal and transverse sum rules. The broadening\nfactor $\\gamma$ in $\\Sigma$ reflects the breaking of pairs into constituent\nfermions. We apply this form for $\\Sigma$ (derived elsewhere for high fields)\nto demonstrate the existence of quantum oscillations in a non-Fermi liquid pg\nstate. Our conclusion is that the pseudogap-induced pairbreaking, via $\\gamma$,\nallows the underlying fermiology to be revealed; in YBCO, finite $\\omega$ and\n$\\gamma$ enable antinodal fluctuations, despite the competition with a d-wave\ngap in the static and superconducting limits.\n",
        "  RNA viruses exist as genetically diverse populations displaying different\nphenotypes, including diverse degrees of virulence. The evolution of virulence\nin viral populations is, however, poorly understood. Based on the experimental\nobservation of an RNA virus clone in cell culture diversifying into two\nsubpopulations of different virulence, we study the dynamics of heterogeneous\nvirus populations and the evolution of virulence. We introduce a\ncompetition-colonization trade-off into standard mathematical models of\nintra-host viral infection. Colonizers are fast spreading, virulent strains,\nwhereas competitors are less virulent variants that are more successful within\ncoinfected cells. We observe biphasic dynamics of the population: Early in the\ninfection the population is dominated by colonizers, which later will be\noutcompeted by competitors. The simulations suggest the existence of a steady\nstate where few low-virulence variants coexist. This equilibrium implies\ncollective virulence attenuation in the population, in contrast to previous\nmodels predicting development of the population towards increased virulence.\nNevertheless, the attenuation effect disappears if we include a highly\nsimplified immune response in our models. Thus, the competition-colonization\ntrade-off indicates a role for virulence in the modulation of the viral\npopulation diversity. The evolution of virulence is a dynamic feature of the\npopulation shaped by interactions between individuals and by the structure of\nthe patchy habitat.\n",
        "  A series of tests were performed on a single post-mortem human subject at\nvarious length scales. First, tabletop tests were performed. Next, the ribs and\nintercostal muscles were tested with the view to characterize the load transfer\nbetween the ribs. Finally, the costal cartilage was tested under shear loading,\nas it plays an important in the transfer of the load between the ribs and the\nsternum. This paper reports the results of dynamic shear loading tests\nperformed on three samples of costal cartilage harvested from a single\npost-mortem human subject, as well as the quantification of the effective\nYoung's modulus estimated from the amount of cartilage calcification.\n",
        "  Integrity constraints (ICs) provide a valuable tool for expressing and\nenforcing application semantics. However, formulating constraints manually\nrequires domain expertise, is prone to human errors, and may be excessively\ntime consuming, especially on large datasets. Hence, proposals for automatic\ndiscovery have been made for some classes of ICs, such as functional\ndependencies (FDs), and recently, order dependencies (ODs). ODs properly\nsubsume FDs, as they can additionally express business rules involving order;\ne.g., an employee never has a higher salary while paying lower taxes compared\nwith another employee.\n  We address the limitations of prior work on OD discovery which has factorial\ncomplexity in the number of attributes, is incomplete (i.e., it does not\ndiscover valid ODs that cannot be inferred from the ones found) and is not\nconcise (i.e., it can result in \"redundant\" discovery and overly large\ndiscovery sets). We improve significantly on complexity, offer completeness,\nand define a compact canonical form. This is based on a novel polynomial\nmapping to a canonical form for ODs, and a sound and complete set of axioms\n(inference rules) for canonical ODs. This allows us to develop an efficient\nset-containment, lattice-driven OD discovery algorithm that uses the inference\nrules to prune the search space. Our algorithm has exponential worst-case time\ncomplexity in the number of attributes and linear complexity in the number of\ntuples. We prove that it produces a complete, minimal set of ODs (i.e., minimal\nwith regards to the canonical representation). Finally, using real and\nsynthetic datasets, we experimentally show orders-of-magnitude performance\nimprovements over the current state-of-the-art algorithm and demonstrate\neffectiveness of our techniques.\n",
        "  Star formation in our Galaxy occurs in molecular clouds that are\nself-gravitating, highly turbulent, and magnetized. We study the conditions\nunder which cloud cores inherit large-scale magnetic field morphologies and how\nthe field is governed by cloud turbulence. We present four moving-mesh\nsimulations of supersonic, turbulent, isothermal, self-gravitating gas with a\nrange of magnetic mean-field strengths characterized by the Alfv\\'enic Mach\nnumber $\\mathcal{M}_{{\\rm A}, 0}$, resolving pre-stellar core formation from\nparsec to a few AU scales. In our simulations with the turbulent kinetic energy\ndensity dominating over magnetic pressure ($\\mathcal{M}_{{\\rm A}, 0}>1$), we\nfind that the collapse is approximately isotropic with $B\\propto\\rho^{2/3}$,\ncore properties are similar regardless of initial mean-field strength, and the\nfield direction on $100$ AU scales is uncorrelated with the mean field.\nHowever, in the case of a dominant large-scale magnetic field\n($\\mathcal{M}_{{\\rm A}, 0}=0.35$), the collapse is anisotropic with\n$B\\propto\\rho^{1/2}$. This transition at $\\mathcal{M}_{{\\rm A}, 0}\\sim1$ is not\nexpected to be sharp, but clearly signifies two different paths for magnetic\nfield evolution in star formation. Based on observations of different star\nforming regions, we conclude that star formation in the interstellar medium may\noccur in both regimes. Magnetic field correlation with the mean-field extends\nto smaller scales as $\\mathcal{M}_{{\\rm A}, 0}$ decreases, making future ALMA\nobservations useful for constraining $\\mathcal{M}_{{\\rm A}, 0}$ of the\ninterstellar medium.\n",
        "  Multi-source translation is an approach to exploit multiple inputs (e.g. in\ntwo different languages) to increase translation accuracy. In this paper, we\nexamine approaches for multi-source neural machine translation (NMT) using an\nincomplete multilingual corpus in which some translations are missing. In\npractice, many multilingual corpora are not complete due to the difficulty to\nprovide translations in all of the relevant languages (for example, in TED\ntalks, most English talks only have subtitles for a small portion of the\nlanguages that TED supports). Existing studies on multi-source translation did\nnot explicitly handle such situations. This study focuses on the use of\nincomplete multilingual corpora in multi-encoder NMT and mixture of NMT experts\nand examines a very simple implementation where missing source translations are\nreplaced by a special symbol <NULL>. These methods allow us to use incomplete\ncorpora both at training time and test time. In experiments with real\nincomplete multilingual corpora of TED Talks, the multi-source NMT with the\n<NULL> tokens achieved higher translation accuracies measured by BLEU than\nthose by any one-to-one NMT systems.\n",
        "  Experiment and analytical calculations show that the demagnetizing field of a\nsuperconductor is a sensitive probe of quantities otherwise difficult to\nmeasure, such as the sample-probe distance in flux-density imaging experiments,\nand the field of first flux penetration Hp. In particular, the ratio of the\nmaximum field measured above the superconductor edge and the applied field can\nbe determined unambiguously so as to define a linear \"geometric\"\nsusceptibility. The evolution of this susceptibility with field depends on the\nregime of flux penetration, and can be used as a means to determine Hp and the\neffect of a parallel field component in magneto-optical imaging experiments.\n",
        "  We show that, under certain assumptions, the fitness of almost all\nquasi-species becomes independent of mutational probabilities and the initial\nfrequency distributions of the sequences in high dimensional sequence spaces.\nThis result is the consequence of the concentration of measure on a high\ndimensional hypersphere and its extension to Lipschitz functions knows as the\nLevy's Lemma. Therefore, evolutionary dynamics almost always yields the same\nvalue for fitness of the quasi-species, independent of the mutational process\nand initial conditions, and is quite robust to mutational changes and\nfluctuations in initial conditions. Our results naturally extend to any\nLipschitz function whose input parameters are the frequencies of individual\nconstituents of the quasi-species. This suggests that the functional\ncapabilities of high dimensional quasi-species are robust to fluctuations in\nthe mutational probabilities and initial conditions.\n",
        "  An automorphism $f$ of a closed orientable surface $\\Sigma$ is said to be\nextendable over the 3-sphere $S^3$ if $f$ extends to an automorphism of the\npair $(S^3, \\Sigma)$ with respect to some embedding $\\Sigma \\hookrightarrow\nS^3$. We prove that if an automorphism of a genus-2 surface $\\Sigma$ is\nextendable over $S^3$, then $f$ extends to an automorphism of the pair $(S^3,\n\\Sigma)$ with respect to an embedding $\\Sigma \\hookrightarrow S^3$ such that\n$\\Sigma$ bounds genus-2 handlebodies on both sides. The classification of\nessential annuli in the exterior of genus-2 handlebodies embedded in $S^3$ due\nto Ozawa and the second author plays a key role.\n",
        "  We quantify if the chemical abundance gradients given by a dynamical model of\ncore collapse including time-dependent changes in density and temperature\ndiffer greatly from abundances derived from static models, where the density\nand temperature structures of the core are kept fixed as the chemistry evolves.\nFor this study we developed a new one-dimensional spherically symmetric\nhydrodynamics code that couples the hydrodynamics equations with a\ncomprehensive time-dependent gas-grain chemical model, including deuterium and\nspin-state chemistry, and radiative transfer calculations to derive\nself-consistent time-dependent chemical abundance gradients. We applied the\ncode to model the collapse of a starless core up to the point when the infall\nflow becomes supersonic.\n  The abundances predicted by the dynamical and static models are almost\nidentical during the quiescent phase of core evolution, but the results start\nto diverge after the onset of core collapse, where the static model\nunderestimates abundances at high medium density (inner core) and\nunderestimates them at low density (outer core), and this is clearly reflected\nin simulated lines. The static model generally overestimates deuteration, which\nis increasingly evident the more D atoms are substituted in the molecule. We\nalso find that using a limited chemical network, or a limited set of cooling\nmolecules, may lead to an overestimate of the collapse timescale, and in some\ncases may prevent the collapse altogether. In our model, most of the line\ncooling near the center of the core is due to HCN, CO, and NO. In conclusion,\nthe use of a static physical model is not a reliable method of simulating\nchemical abundances in starless cores after the onset of gravitational\ncollapse. The adoption of complex chemistry and a comprehensive set of cooling\nmolecules is necessary to model the collapse adequately.\n",
        "  The dynamics of ecosystem collapse are fundamental to determining how and why\nbiological communities change through time, as well as the potential effects of\nextinctions on ecosystems. Here we integrate depictions of mammals from\nEgyptian antiquity with direct lines of paleontological and archeological\nevidence to infer local extinctions and community dynamics over a 6000-year\nspan. The unprecedented temporal resolution of this data set enables\nexamination of how the tandem effects of human population growth and climate\nchange can disrupt mammalian communities. We show that the extinctions of\nmammals in Egypt were nonrandom, and that destabilizing changes in community\ncomposition coincided with abrupt aridification events and the attendant\ncollapses of some complex societies. We also show that the roles of species in\na community can change over time, and that persistence is predicted by measures\nof species sensitivity, a function of local dynamic stability. Our study is the\nfirst high-resolution analysis of the ecological impacts of environmental\nchange on predator-prey networks over millennial timescales, and sheds light on\nthe historical events that have shaped modern animal communities.\n",
        "  Purpose: Implanted fiducial markers are often used in radiotherapy to\nfacilitate accurate visualization and localization of tumors. Typically, such\nmarkers are used to aid daily patient positioning and to verify the target's\nposition during treatment. This work introduces a novel, automated method for\nidentifying fiducial markers in planar x-ray imaging.\n  Methods: In brief, the method consists of automated filtration and\nreconstruction steps that generate 3D templates of marker positions. The\nnormalized cross-correlation was the used to identify fiducial markers in\nprojection images. To quantify the accuracy of the technique, a phantom study\nwas performed. 75 pre-treatment CBCT scans of 15 pancreatic cancer patients\nwere analyzed to test the automated technique under real life conditions,\nincluding several challenging scenarios for tracking fiducial markers.\n  Results: In phantom and patient studies, the method automatically tracked\nvisible marker clusters in 100% of projection images. For scans in which a\nphantom exhibited 0D, 1D, and 3D motion, the automated technique showed median\nerrors of 39 $\\mu$m, 53 $\\mu$m, and 93 $\\mu$m, respectively. Human precision\nwas worse in comparison. Automated tracking was performed accurately despite\nthe presence of other metallic objects. Additionally, transient differences in\nthe cross-correlation score identified instances where markers disappeared from\nview.\n  Conclusions: A novel, automated method for producing dynamic templates of\nfiducial marker clusters has been developed. Production of these templates\nautomatically provides measurements of tumor motion that occurred during the\nCBCT scan that was used to produce them. Additionally, using these templates\nwith intra-fractional images could potentially allow for more robust real-time\ntarget tracking in radiotherapy.\n",
        "  Using high-resolution, multiple-passband Hubble Space Telescope images\nspanning the entire optical/near-infrared wavelength range, we obtained a\nstatistically complete sample, $U$-band selected sample of 846 extended star\nclusters across the disk of the nearby starburst galaxy M82. Based on careful\nanalysis of their spectral energy distributions, we determined their\ngalaxy-wide age and mass distributions. The M82 clusters exhibit three clear\npeaks in their age distribution, thus defining a relatively young, log(t/yr) <\n7.5, an intermediate-age, log(t/yr) $\\in$ [7.5, 8.5], and an old sample,\nlog(t/yr) > 8.5. Comparison of the completeness-corrected mass distributions\noffers a firm handle on the galaxy's star cluster disruption history. The most\nmassive star clusters in the young and old samples are (almost) all\nconcentrated in the most densely populated central region, while the\nintermediate-age sample's most massive clusters are more spatially dispersed,\nwhich may reflect the distribution of the highest-density gas throughout the\ngalaxy's evolutionary history, combined with the solid-body nature of the\ngalaxy's central region.\n",
        "  We construct, for $m\\geq 6$ and $2n\\leq m$, closed manifolds $M^{m}$ with\nfinite nonzero $\\varphi(M^{m},S^{n}$), where $\\varphi(M,N)$ denotes the minimum\nnumber of critical points of a smooth map $M\\to N$. We also give some explicit\nfamilies of examples for even $m\\geq 6, n=3$, taking advantage of the Lie group\nstructure on $S^3$. Moreover, there are infinitely many such examples with\n$\\varphi(M^{m},S^{n})=1$. Eventually we compute the signature of the manifolds\n$M^{2n}$ occurring for even $n$.\n",
        "  Data mining techniques must be developed and applied to analyse the large\npublic data bases containing hundreds to thousands of millions entries. The aim\nof this study is to develop methods for locating previously unknown stellar\nclusters from the UKIDSS Galactic Plane Survey catalogue data. The cluster\ncandidates are computationally searched from pre-filtered catalogue data using\na method that fits a mixture model of Gaussian densities and background noise\nusing the Expectation Maximization algorithm. The catalogue data contains a\nsignificant number of false sources clustered around bright stars. A large\nfraction of these artefacts were automatically filtered out before or during\nthe cluster search. The UKIDSS data reduction pipeline tends to classify\nmarginally resolved stellar pairs and objects seen against variable surface\nbrightness as extended objects (or \"galaxies\" in the archive parlance). 10% or\n66 x 10^6 of the sources in the UKIDSS GPS catalogue brighter than 17\nmagnitudes in the K band are classified as \"galaxies\". Young embedded clusters\ncreate variable NIR surface brightness because the gas/dust clouds in which\nthey were formed scatters the light from the cluster members. Such clusters\nappear therefore as clusters of \"galaxies\" in the catalogue and can be found\nusing only a subset of the catalogue data. The detected \"galaxy clusters\" were\nfinally screened visually to eliminate the remaining false detections due to\ndata artefacts. Besides the embedded clusters the search also located locations\nof non clustered embedded star formation. The search covered an area of 1302\nsquare degrees and 137 previously unknown cluster candidates and 30 previously\nunknown sites of star formation were found.\n",
        "  We present our submitted systems for Semantic Textual Similarity (STS) Track\n4 at SemEval-2017. Given a pair of Spanish-English sentences, each system must\nestimate their semantic similarity by a score between 0 and 5. In our\nsubmission, we use syntax-based, dictionary-based, context-based, and MT-based\nmethods. We also combine these methods in unsupervised and supervised way. Our\nbest run ranked 1st on track 4a with a correlation of 83.02% with human\nannotations.\n",
        "  We investigate the star-formation ocurring in the region towards\nIRAS07527-3446 in the molecular cloud [MAB97]250.63-3.63, in the far outer\nGalaxy. We report the discovery of a new young stellar cluster, and describe\nits properties and those of its parent molecular cloud. Near-infrared JHKS\nimages were obtained with VLT/ISAAC, and millimetre line CO spectra were\nobtained with the SEST telescope. VLA archive date were also used. The cloud\nand cluster are located at a distance of 10.3 kpc and a Galactocentric distance\nof 15.4 kpc, in the far outer Galaxy. Morphologically, IRAS 07527-3446 appears\nas a young embedded cluster of a few hundred stars seen towards the position of\nthe IRAS source, extending for about 2-4 pc and exhibiting sub-clustering. The\ncluster contains low and intermediate-mass young reddened stars, a large\nfraction having cleared the inner regions of their circumstellar discs\nresponsible for (H-Ks) colour excess. The observations are compatible with a <\n5 Myr cluster with variable spatial extinction of between Av = 5 and Av = 11.\nDecomposition of CO emission in clumps, reveals a clump clearly associated with\nthe cluster position, of mass 3.3 x 10^3 M(solar). Estimates of the slopes of\nthe Ks-band luminosity function and of the star-formation efficiency yield\nvalues similar to those seen in nearby star-formation sites. These findings\nreinforce previous results that the distant outer Galaxy continues to be active\nin the production of new and rich stellar clusters, with the physical\nconditions required for the formation of rich clusters continuing to be met in\nthe very distant environment of the outer Galactic disc.\n",
        "  Magnetization and muon spin relaxation or rotation (muSR) measurements have\nbeen performed to study the superconducting and magnetic properties of\nSr3Ir4Sn13. From magnetization measurements the lower and upper critical fields\nof Sr3Ir4Sn13 are found to be 81(1) Oe and 14.4(2) kOe, respectively.\nZero-field muSR data show no sign of any magnetic ordering or weak magnetism in\nSr3Ir4Sn13. Transverse-field muSR measurements in the vortex state provided the\ntemperature dependence of the magnetic penetration depth. The dependence of\npenetration depth with temperature is consistent with the existence of single\ns-wave energy gap in the superconducting state of Sr3Ir4Sn13 with a gap value\nof 0.82(2) meV at absolute zero temperature. The magnetic penetration depth at\nzero temperature is 291(3) nm. The gap to Tc ratio is 2.1(1), indicates that\nSr3Ir4Sn13 should be considered as a strong-coupling superconductor.\n",
        "  We present deep imaging of the most distant dwarf discovered by the Dark\nEnergy Survey, Eridanus II (Eri II). Our Magellan/Megacam stellar photometry\nreaches $\\sim$$3$ mag deeper than previous work, and allows us to confirm the\npresence of a stellar cluster whose position is consistent with Eri II's\ncenter. This makes Eri II, at $M_V=-7.1$, the least luminous galaxy known to\nhost a (possibly central) cluster. The cluster is partially resolved, and at\n$M_V=-3.5$ it accounts for $\\sim$$4\\%$ of Eri II's luminosity. We derive\nupdated structural parameters for Eri II, which has a half-light radius of\n$\\sim$$280$ pc and is elongated ($\\epsilon$$\\sim$$0.48$), at a measured\ndistance of $D$$\\sim$$370$ kpc. The color-magnitude diagram displays a blue,\nextended horizontal branch, as well as a less populated red horizontal branch.\nA central concentration of stars brighter than the old main sequence turnoff\nhints at a possible intermediate-age ($\\sim$$3$ Gyr) population; alternatively,\nthese sources could be blue straggler stars. A deep Green Bank Telescope\nobservation of Eri II reveals no associated atomic gas.\n",
        "  We present the fourth portion of a Galactic Plane survey of methanol masers\nat 6668 MHz, spanning the longitude range 186 degrees to 330 degrees. We report\n207 maser detections, 89 new to the survey. This completes the southern sky\npart of the Methanol Multibeam survey and includes a large proportion of new\nsources, 43%. We also include results from blind observations of the\nOrion-Monoceros star forming region, formally outside the latitude range of the\nMethanol Multibeam survey; only the four previously known methanol emitting\nsites were detected, of which we present new positions and spectra for masers\nat Orion-A (south) and Orion-B, obtained with the MERLIN array.\n",
        "  Phylogenetic trees are widely used to display estimates of how groups of\nspecies evolved. Each phylogenetic tree can be seen as a collection of\nclusters, subgroups of the species that evolved from a common ancestor. When\nphylogenetic trees are obtained for several data sets (e.g. for different\ngenes), then their clusters are often contradicting. Consequently, the set of\nall clusters of such a data set cannot be combined into a single phylogenetic\ntree. Phylogenetic networks are a generalization of phylogenetic trees that can\nbe used to display more complex evolutionary histories, including reticulate\nevents such as hybridizations, recombinations and horizontal gene transfers.\nHere we present the new CASS algorithm that can combine any set of clusters\ninto a phylogenetic network. We show that the networks constructed by CASS are\nusually simpler than networks constructed by other available methods. Moreover,\nwe show that CASS is guaranteed to produce a network with at most two\nreticulations per biconnected component, whenever such a network exists. We\nhave implemented CASS and integrated it in the freely available Dendroscope\nsoftware.\n",
        "  There are two major theoretical issues for the star formation law (the\nrelation between the surface densities of molecular gas and star formation rate\non a galaxy scale): (i) At low metallicity, it is not obvious that star-forming\nregions are rich in H$_2$ because the H$_2$ formation rate depends on the dust\nabundance; and (ii) whether or not CO really traces H$_2$ is uncertain,\nespecially at low metallicity. To clarify these issues, we use a hydrodynamic\nsimulation of an isolated disc galaxy with a spatial resolution of a few tens\nparsecs. The evolution of dust abundance and grain size distribution is treated\nconsistently with the metal enrichment and the physical state of the\ninterstellar medium. We compute the H$_2$ and CO abundances using a subgrid\npost-processing model based on the dust abundance and the dissociating\nradiation field calculated in the simulation. We find that when the metallicity\nis $\\lesssim 0.4$ Z$_\\odot$ ($t<1$ Gyr), H$_2$ is not a good tracer of star\nformation rate because H$_2$-rich regions are limited to dense compact regions.\nAt $Z\\gtrsim 0.8$ Z$_\\odot$, a tight star formation law is established for both\nH$_2$ and CO. At old ($t \\sim 10$ Gyr) ages, we also find that adopting the\nso-called MRN grain size distribution with an appropriate dust-to-metal ratio\nover the entire disc gives reasonable estimates for the H$_2$ and CO\nabundances. For CO, improving the spatial resolution of the simulation is\nimportant while the H$_2$ abundance is not sensitive to sub-resolution\nstructures at $Z\\gtrsim 0.4$ Z$_\\odot$.\n",
        "  In 1974, Thurston proved that, up to isotopy, every automorphism of closed\norientable surface is either periodic, reducible, or pseudo-Anosov. The latter\ncase has lead to a rich theory with applications ranging from dynamical systems\nto low dimensional topology. Associated with every pseudo-Anosov map is a real\nnumber $\\lambda > 1$ known as the stretch factor. Thurston showed that every\nstretch factor is an algebraic unit but it is unknown exactly which units can\nappear as stretch factors. In this paper we show that every Salem number has a\npower that is the stretch factor of a pseudo-Anosov map arising from a\nconstruction due to Thurston. We also show that every totally real number field\n$K$ is of the form $K = \\mathbb{Q}(\\lambda + \\lambda^{-1})$, where $\\lambda$ is\nthe stretch factor of a pseudo-Anosov map arising from Thurston's construction.\n",
        "  Suicide is among the leading causes of death in China. However, technical\napproaches toward preventing suicide are challenging and remaining under\ndevelopment. Recently, several actual suicidal cases were preceded by users who\nposted microblogs with suicidal ideation to Sina Weibo, a Chinese social media\nnetwork akin to Twitter. It would therefore be desirable to detect suicidal\nideations from microblogs in real-time, and immediately alert appropriate\nsupport groups, which may lead to successful prevention. In this paper, we\npropose a real-time suicidal ideation detection system deployed over Weibo,\nusing machine learning and known psychological techniques. Currently, we have\nidentified 53 known suicidal cases who posted suicide notes on Weibo prior to\ntheir deaths.We explore linguistic features of these known cases using a\npsychological lexicon dictionary, and train an effective suicidal Weibo post\ndetection model. 6714 tagged posts and several classifiers are used to verify\nthe model. By combining both machine learning and psychological knowledge, SVM\nclassifier has the best performance of different classifiers, yielding an\nF-measure of 68:3%, a Precision of 78:9%, and a Recall of 60:3%.\n",
        "  Many massive star forming disc galaxies in the redshift range 3 to 0.5 are\nobserved to have a clumpy morphology showing giant clumps of size $\\sim$1 kpc\nand masses of about $10^7M_{\\odot}$ to $10^{10} M_{\\odot}$. The nature and fate\nof these giant clumps is still under debate. In this work we use 19\nhigh-resolution simulations of disc galaxies from the NIHAO sample to study the\nformation and the evolution of clumps in the discs of high redshift galaxies.\nWe use mock HST - CANDELS observations created with the radiative transfer code\nGRASIL-3D to carry out, for the first time, a quantitative comparison of the\nobserved fraction of clumpy galaxies and its evolution with redshift with\nsimulations. We find a good agreement between the observed clumpy fraction and\nthe one of the NIHAO galaxies. We find that dust attenuation can suppress\nintrinsically bright clumps and enhance less luminous ones. In our galaxy\nsample we only find clumps in light (u-band) from young stars but not in\nstellar mass surface density maps. This means that the NIHAO sample does not\nshow clumpy stellar discs but rather a clumpy light distribution originating\nfrom clumpy star formation events. The clumps found in the NIHAO sample match\nobserved age/color gradients as a function of distance from the galaxy center\nbut they show no sign of inward migration. Clumps in our simulations disperse\non timescales of a about a hundred Myr and their contribution to bulge growth\nis negligible.\n",
        "  Accurate systems for extracting Protein-Protein Interactions (PPIs)\nautomatically from biomedical articles can help accelerate biomedical research.\nBiomedical Informatics researchers are collaborating to provide metaservices\nand advance the state-of-art in PPI extraction. One problem often neglected by\ncurrent Natural Language Processing systems is the characteristic complexity of\nthe sentences in biomedical literature. In this paper, we report on the impact\nthat automatic simplification of sentences has on the performance of a\nstate-of-art PPI extraction system, showing a substantial improvement in recall\n(8%) when the sentence simplification method is applied, without significant\nimpact to precision.\n",
        "  Learning distributed sentence representations is one of the key challenges in\nnatural language processing. Previous work demonstrated that a recurrent neural\nnetwork (RNNs) based sentence encoder trained on a large collection of\nannotated natural language inference data, is efficient in the transfer\nlearning to facilitate other related tasks. In this paper, we show that joint\nlearning of multiple tasks results in better generalizable sentence\nrepresentations by conducting extensive experiments and analysis comparing the\nmulti-task and single-task learned sentence encoders. The quantitative analysis\nusing auxiliary tasks show that multi-task learning helps to embed better\nsemantic information in the sentence representations compared to single-task\nlearning. In addition, we compare multi-task sentence encoders with\ncontextualized word representations and show that combining both of them can\nfurther boost the performance of transfer learning.\n",
        "  High engineering critical current density JE of >500 A/mm2 at 20 T and 4.2 K\ncan be regularly achieved in Ag-sheathed multifilamentary Bi2Sr2CaCu2Ox\n(Bi-2212) round wire when the sample length is several centimeters. However,\nJE(20 T) in Bi-2212 wires of several meters length, as well as longer pieces\nwound in coils, rarely exceeds 200 A/mm2. Moreover, long-length wires often\nexhibit signs of Bi-2212 leakage after melt processing that are rarely found in\nshort, open-end samples. We studied the length dependence of JE of\nstate-of-the-art powder-in-tube (PIT) Bi-2212 wires and gases released by them\nduring melt processing using mass spectroscopy, confirming that JE degradation\nwith length is due to wire swelling produced by high internal gas pressures at\nelevated temperatures [1,2]. We further modeled the gas transport in Bi-2212\nwires and examined the wire expansion at critical stages of the melt processing\nof as-drawn PIT wires and the wires that received a degassing treatment or a\ncold-densification treatment before melt processing. These investigations\nshowed that internal gas pressure in long-length wires drives creep of the Ag\nsheath during the heat treatment, causing wire to expand, lowering the density\nof Bi-2212 filaments, and therefore degrading the wire JE; the creep rupture of\nsilver sheath naturally leads to the leakage of Bi-2212 liquid. Our work shows\nthat proper control of such creep is the key to preventing Bi-2212 leakage and\nachieving high JE in long-length Bi-2212 conductors and coils.\n",
        "  Data quality on categorical attribute is a difficult problem that has not\nreceived as much attention as numerical counterpart. Our basic idea is to\nemploy association rule for the purpose of data quality measurement. Strong\nrule generation is an important area of data mining. Association rule mining\nproblems can be considered as a multi objective problem rather than as a single\nobjective one. The main area of concentration was the rules generated by\nassociation rule mining using genetic algorithm. The advantage of using genetic\nalgorithm is to discover high level prediction rules is that they perform a\nglobal search and cope better with attribute interaction than the greedy rule\ninduction algorithm often used in data mining. Genetic algorithm based approach\nutilizes the linkage between association rule and feature selection. In this\npaper, we put forward a Multi objective genetic algorithm approach for data\nquality on categorical attributes. The result shows that our approach is\noutperformed by the objectives like accuracy, completeness, comprehensibility\nand interestingness.\n",
        "  We study cosmetic crossings in knots of genus one and obtain obstructions to\nsuch crossings in terms of knot invariants determined by Seifert matrices. In\nparticular, we prove that for genus one knots the Alexander polynomial and the\nhomology of the double cover branching over the knot provide obstructions to\ncosmetic crossings. As an application we prove the nugatory crossing conjecture\nfor twisted Whitehead doubles of non-cable knots. We also verify the conjecture\nfor several families of pretzel knots and all genus one knots with up to 12\ncrossings.\n",
        "  In this paper, we study the communication complexity for the problem of\ncomputing a conjunctive query on a large database in a parallel setting with\n$p$ servers. In contrast to previous work, where upper and lower bounds on the\ncommunication were specified for particular structures of data (either data\nwithout skew, or data with specific types of skew), in this work we focus on\nworst-case analysis of the communication cost. The goal is to find worst-case\noptimal parallel algorithms, similar to the work of [18] for sequential\nalgorithms.\n  We first show that for a single round we can obtain an optimal worst-case\nalgorithm. The optimal load for a conjunctive query $q$ when all relations have\nsize equal to $M$ is $O(M/p^{1/\\psi^*})$, where $\\psi^*$ is a new query-related\nquantity called the edge quasi-packing number, which is different from both the\nedge packing number and edge cover number of the query hypergraph. For multiple\nrounds, we present algorithms that are optimal for several classes of queries.\nFinally, we show a surprising connection to the external memory model, which\nallows us to translate parallel algorithms to external memory algorithms. This\ntechnique allows us to recover (within a polylogarithmic factor) several recent\nresults on the I/O complexity for computing join queries, and also obtain\noptimal algorithms for other classes of queries.\n",
        "  The need for discovering knowledge from XML documents according to both\nstructure and content features has become challenging, due to the increase in\napplication contexts for which handling both structure and content information\nin XML data is essential. So, the challenge is to find an hierarchical\nstructure which ensure a combination of data levels and their representative\nstructures. In this work, we will be based on the Formal Concept Analysis-based\nviews to index and query both content and structure. We evaluate given\nstructure in a querying process which allows the searching of user query\nanswers.\n",
        "  Natural language text corpora are often available as sets of syntactically\nparsed trees. A wide range of expressive tree queries are possible over such\nparsed trees that open a new avenue in searching over natural language text.\nThey not only allow for querying roles and relationships within sentences, but\nalso improve search effectiveness compared to flat keyword queries. One major\ndrawback of current systems supporting querying over parsed text is the\nperformance of evaluating queries over large data. In this paper we propose a\nnovel indexing scheme over unique subtrees as index keys. We also propose a\nnovel root-split coding scheme that stores subtree structural information only\npartially, thus reducing index size and improving querying performance. Our\nextensive set of experiments show that root-split coding reduces the index size\nof any interval coding which stores individual node numbers by a factor of 50%\nto 80%, depending on the sizes of subtrees indexed. Moreover, We show that our\nindex using root-split coding, outperforms previous approaches by at least an\norder of magnitude in terms of the response time of queries.\n",
        "  Dedicated application-specific CT systems are popular solutions to\nhigh-resolution clinical needs. Some applications, such as mammography and\nextremities imaging, require spatial resolution beyond current capabilities.\nThorough understanding of system properties may help tailor system design,\nacquisition protocols, and reconstruction algorithms to improve image quality.\nUsing a high-fidelity measurement model, we analyze the effects of\nshift-variant focal spot blur due to depth-dependence and anode angulation on\nimage quality throughout the three-dimensional field of view of a simulated\nextremities scanner. A model of the shift-variant blur associated with this\ndevice is then incorporated into a Model-Based Iterative Reconstruction (MBIR)\nalgorithm, which is then compared to FDK and MBIR with simpler blur models at\nselect locations throughout the field of view. We show that shift-variant focal\nspot blur leads to location-dependent imaging performance. Furthermore,\nchanging the orientation of the X-ray tube alters this spatial dependence. The\nanalysis suggests methods to improve imaging performance based on specific\nimage quality needs. The results also demonstrate that image quality can be\nimproved by combining accurate blur modeling with MBIR. Specifically, across\nthe entire field of view, MBIR with shift-variant blur modeling yielded the\nbest image quality, followed by MBIR with a shift-invariant blur model, MBIR\nwith an identity blur model, and FDK, respectively. These results suggest a\nnumber of opportunities for the optimization of imaging system performance in\nthe hardware setup, the imaging protocol, and the reconstruction approach.\nWhile the high-fidelity models used here are applied using the specifications\nof a dedicated extremities imaging system, the methods are general and may be\napplied to optimize imaging performance in any CT system.\n",
        "  We demonstrate the effectiveness of multilingual learning for unsupervised\npart-of-speech tagging. The central assumption of our work is that by combining\ncues from multiple languages, the structure of each becomes more apparent. We\nconsider two ways of applying this intuition to the problem of unsupervised\npart-of-speech tagging: a model that directly merges tag structures for a pair\nof languages into a single sequence and a second model which instead\nincorporates multilingual context using latent variables. Both approaches are\nformulated as hierarchical Bayesian models, using Markov Chain Monte Carlo\nsampling techniques for inference. Our results demonstrate that by\nincorporating multilingual evidence we can achieve impressive performance gains\nacross a range of scenarios. We also found that performance improves steadily\nas the number of available languages increases.\n",
        "  Background: Much has been written about introduced rainbow trout\n(Oncorhynchus mykiss) interbreeding and outcompeting cutthroat trout\n(Oncorhynchus clarkii). However, the specific mechanisms by which rainbow trout\nand their hybrids outcompete cutthroat trout have not been thoroughly explored,\nand the published data is limited to lotic ecosystems. Materials and Methods:\nSamples of rainbow trout and cutthroat trout were obtained from a lentic\necosystem by angling. The total length and weight of each fish was measured and\nthe relative weight of each fish was computed (Anderson R.O., Neumann R.M.\n1996. Length, Weight, and Associated Structural Indices, Pp. 447-481. In:\nMurphy B.E. and Willis D.W. (eds.) Fisheries Techniques, second edition.\nAmerican Fisheries Society.), along with the mean and uncertainty in the mean\nfor each species. Data from an independent source (K.D. Carlander, 1969.\nHandbook of Freshwater Fishery Biology, Volume One, Iowa University Press,\nAmes.) was also used to generate mean weight-length curves, as well as 25th and\n75th percentile curves for each species to allow further comparison. Results:\nThe mean relative weight of the rainbow trout was 72.5 (+/- 2.1); whereas, the\nmean relative weight of the cutthroat trout was 101.0 (+/- 4.9). The rainbow\ntrout were thin; 80% weighed below the 25th percentile. The cutthroat trout\nwere plump; 86% weighed above the 75th percentile, and 29% were above the\nheaviest recorded specimens at a given length in the Carlander (1969) data set.\nConclusion: This data casts doubt on the hypothesis that rainbow trout are\nstrong food competitors with cutthroat trout in lentic ecosystems. On the\ncontrary, in the lake under study, the cutthroat trout seem to be outcompeting\nrainbow trout for the available food.\n",
        "  We introduce a neural reading comprehension model that integrates external\ncommonsense knowledge, encoded as a key-value memory, in a cloze-style setting.\nInstead of relying only on document-to-question interaction or discrete\nfeatures as in prior work, our model attends to relevant external knowledge and\ncombines this knowledge with the context representation before inferring the\nanswer. This allows the model to attract and imply knowledge from an external\nknowledge source that is not explicitly stated in the text, but that is\nrelevant for inferring the answer. Our model improves results over a very\nstrong baseline on a hard Common Nouns dataset, making it a strong competitor\nof much more complex models. By including knowledge explicitly, our model can\nalso provide evidence about the background knowledge used in the RC process.\n",
        "  Inspired by the recent working effort towards a recommendation by the World\nWide Web Consortium (W3C) for tabular data and metadata on the Web, we present\nin this paper a concept for a schema language for tabular web data called\nSCULPT. The language consists of rules constraining and defining the structure\nof regions in the table. These regions are defined through the novel formalism\nof region selection expressions. We present a formal model for SCULPT and\nobtain a linear time combined complexity evaluation algorithm. In addition, we\nconsider weak and strong streaming evaluation for SCULPT and present a fragment\nfor each of these streaming variants. Finally, we discuss several extensions of\nSCULPT including alternative semantics, types, complex content, and explore\nregion selection expressions as a basis for a transformation language.\n",
        "  Based on BCS model with the external pair potential formulated in a work\nGrigorishin (2017) [1], analogous model with electron-phonon coupling and\nCoulomb coupling is proposed. The generalized Eliashberg equations in the\nregime of renormalization of the order parameter are obtained. High temperature\nasymptotics and effect of Coulomb pseudopotential on them are investigated: as\nin the BCS model the order parameter asymptotically tends to zero as\ntemperature rises, but the accounting of the Coulomb pseudopotential leads to\nexistence of critical temperature. The effective Ginzburg-Landau theory is\nformulated for such model.\n",
        "  Diffuse Ionized Gas (DIG) is prevalent in star-forming galaxies. Using a\nsample of 365 nearly face-on star-forming galaxies observed by MaNGA, we\ndemonstrate how DIG in star-forming galaxies impacts the measurements of\nemission line ratios, hence the interpretation of diagnostic diagrams and\ngas-phase metallicity measurements. At fixed metallicity, DIG-dominated low\nH\\alpha\\ surface brightness regions display enhanced [SII]/H\\alpha,\n[NII]/H\\alpha, [OII]/H\\beta, and [OI]/H\\alpha. The gradients in these line\nratios are determined by metallicity gradients and H\\alpha\\ surface brightness.\nIn line ratio diagnostic diagrams, contamination by DIG moves HII regions\ntowards composite or LI(N)ER-like regions. A harder ionizing spectrum is needed\nto explain DIG line ratios. Leaky HII region models can only shift line ratios\nslightly relative to HII region models, and thus fail to explain the\ncomposite/LI(N)ER line ratios displayed by DIG. Our result favors ionization by\nevolved stars as a major ionization source for DIG with LI(N)ER-like emission.\n  DIG can significantly bias the measurement of gas metallicity and metallicity\ngradients derived using strong-line methods. Metallicities derived using N2O2\nare optimal because they exhibit the smallest bias and error. Using O3N2, R23,\nN2=[NII]/H\\alpha, and N2S2H\\alpha\\ (Dopita et al. 2016) to derive metallicities\nintroduces bias in the derived metallicity gradients as large as the gradient\nitself. The strong-line method of Blanc et al. (2015; IZI hereafter) cannot be\napplied to DIG to get an accurate metallicity because it currently contains\nonly HII region models which fail to describe the DIG.\n",
        "  Carbon ion radiotherapy is known as a less invasive cancer treatment. The\nradiation quality is an important parameter to evaluate the biological effect\nand the clinical dose from the measured physical dose. The performance of\nSOPHIAS detector, which is the SOI image sensor having a wide dynamic range and\nlarge active area, was tested by using therapeutic carbon ion beam at Gunma\nUniversity Heavy Ion Medical Center (GHMC). It was shown that the primary\ncarbon and secondary particles can be distinguishable by SOPHIAS detector. On\nthe other hand, a LET dependence was observed especially at the high LET\nregion. This phenomenon will be studied by using the device simulator together\nwith Monte Carlo simulation.\n",
        "  Machine translation systems are very sensitive to the domains they were\ntrained on. Several domain adaptation techniques have been deeply studied. We\npropose a new technique for neural machine translation (NMT) that we call\ndomain control which is performed at runtime using a unique neural network\ncovering multiple domains. The presented approach shows quality improvements\nwhen compared to dedicated domains translating on any of the covered domains\nand even on out-of-domain data. In addition, model parameters do not need to be\nre-estimated for each domain, making this effective to real use cases.\nEvaluation is carried out on English-to-French translation for two different\ntesting scenarios. We first consider the case where an end-user performs\ntranslations on a known domain. Secondly, we consider the scenario where the\ndomain is not known and predicted at the sentence level before translating.\nResults show consistent accuracy improvements for both conditions.\n",
        "  We develop a phenomenological theory to predict the characteristic features\nof the momentum-dependent scattering amplitude in resonant inelastic x-ray\nscattering (RIXS) at the energy scale of the superconducting gap in iron-based\nsuperconductors. Taking into account all relevant orbital states as well as\ntheir specific content along the Fermi surface we evaluate the charge and spin\ndynamical structure factors for the compounds LaOFeAs and LiFeAs, based on\ntight-binding models which are fully consistent with recent angle-resolved\nphotoemission spectroscopy (ARPES) data. We find a characteristic intensity\nredistribution between charge and spin dynamical structure factors which\ndiscriminates between sign-reversing and sign-preserving quasiparticle\nexcitations. Consequently, our results show that RIXS spectra can distinguish\nbetween $s_\\pm$ and $s_{++}$ wave gap functions in the singlet pairing case. In\naddition, we find that an analogous intensity redistribution at small momenta\ncan reveal the presence of a chiral $p$-wave triplet pairing.\n",
        "  By using the equivariant localization formula of toric varieties. We prove\nthe vanishing of the Witten genus of some string complete intersections in\nsmooth toric varieties.\n",
        "  This paper presents a comparison of classification methods for linguistic\ntypology for the purpose of expanding an extensive, but sparse language\nresource: the World Atlas of Language Structures (WALS) (Dryer and Haspelmath,\n2013). We experimented with a variety of regression and nearest-neighbor\nmethods for use in classification over a set of 325 languages and six syntactic\nrules drawn from WALS. To classify each rule, we consider the typological\nfeatures of the other five rules; linguistic features extracted from a\nword-aligned Bible in each language; and genealogical features (genus and\nfamily) of each language. In general, we find that propagating the majority\nlabel among all languages of the same genus achieves the best accuracy in label\npre- diction. Following this, a logistic regression model that combines\ntypological and linguistic features offers the next best performance.\nInterestingly, this model actually outperforms the majority labels among all\nlanguages of the same family.\n",
        "  With the availability of more powerful computing processors, iterative\nreconstruction algorithms have recently been successfully implemented as an\napproach to achieving significant dose reduction in X-ray CT. In this report,\nwe describe our proposal of an adaptive iterative reconstruction algorithm for\nX-ray CT, that is shown to provide results comparable to those obtained by\nproprietary algorithms, both in terms of reconstruction accuracy and execution\ntime. Implementation code in the C language is provided, along with example of\nuser interface.\n",
        "  Trigonometric parallax measurements of nine water masers associated with the\nLocal arm of the Milky Way were carried out as part of the BeSSeL Survey using\nthe VLBA. When combined with 21 other parallax measurements from the\nliterature, the data allow us to study the distribution and 3-dimensional\nmotions of star forming regions in the spiral arm over the entire northern sky.\nOur results suggest that the Local arm does not have the large pitch angle\ncharacteristic of a short spur. Instead its active star formation, overall\nlength (>5 kpc), and shallow pitch angle (~10 degrees) suggest that it is more\nlike the adjacent Perseus and Sagittarius arms; perhaps it is a branch of one\nof these arms. Contrary to previous results, we find the Local arm to be closer\nto the Perseus than to the Sagittarius arm, suggesting that a branching from\nthe former may be more likely. An average peculiar motion of near-zero toward\nboth the Galactic center and north Galactic pole, and counter rotation of ~ 5\nkm/s were observed, indicating that the Local arm has similar kinematic\nproperties as found for other major spiral arms.\n",
        "  Implicit arguments are not syntactically connected to their predicates, and\nare therefore hard to extract. Previous work has used models with large numbers\nof features, evaluated on very small datasets. We propose to train models for\nimplicit argument prediction on a simple cloze task, for which data can be\ngenerated automatically at scale. This allows us to use a neural model, which\ndraws on narrative coherence and entity salience for predictions. We show that\nour model has superior performance on both synthetic and natural data.\n",
        "  This paper proposes a general framework for matching similar subsequences in\nboth time series and string databases. The matching results are pairs of query\nsubsequences and database subsequences. The framework finds all possible pairs\nof similar subsequences if the distance measure satisfies the \"consistency\"\nproperty, which is a property introduced in this paper. We show that most\npopular distance functions, such as the Euclidean distance, DTW, ERP, the\nFrechet distance for time series, and the Hamming distance and Levenshtein\ndistance for strings, are all \"consistent\". We also propose a generic index\nstructure for metric spaces named \"reference net\". The reference net occupies\nO(n) space, where n is the size of the dataset and is optimized to work well\nwith our framework. The experiments demonstrate the ability of our method to\nimprove retrieval performance when combined with diverse distance measures. The\nexperiments also illustrate that the reference net scales well in terms of\nspace overhead and query time.\n",
        "  Spatial crowdsourcing refers to a system that periodically assigns a number\nof location-based workers with spatial tasks nearby (e.g., taking photos or\nvideos at some spatial locations). Previous works on the spatial crowdsourcing\nusually designed task assignment strategies that maximize some assignment\nscores, which are however only based on available workers/tasks in the system\nat the time point of assigning workers/tasks. These strategies may achieve\nlocal optimality, due to the neglect of future workers/tasks that may join the\nsystem. In contrast, in this paper, we aim to achieve \"globally\" optimal task\nassignments, by considering not only those present, but also future (via\npredictions), workers/tasks. Specifically, we formalize an important problem,\nnamely prediction-based spatial crowdsourcing (PB-SC), which expects to obtain\na \"globally\" optimal strategy for worker-and-task assignments, over both\npresent and predicted task/worker locations, such that the total assignment\nquality score is maximized under the constraint of the traveling budget. In\nthis paper, we design an effective grid-based prediction method to estimate\nspatial distributions of workers/tasks in the future, and then utilize the\npredicted ones in our procedure of task assignments. We prove that the PB-SC\nproblem is NP-hard, and thus intractable. Therefore, we propose efficient\napproximate algorithms to tackle the PB-SC problem, including greedy and\ndivide-and-conquer (D&C) approaches, which can efficiently assign workers to\nspatial tasks with high quality scores and low budget consumptions, by\nconsidering both current and future task/worker distributions. Through\nextensive experiments, we demonstrate the efficiency and effectiveness of our\nPB-SC processing approaches on real/synthetic data.\n",
        "  Main Memory Map Reduce (M3R) is a new implementation of the Hadoop Map Reduce\n(HMR) API targeted at online analytics on high mean-time-to-failure clusters.\nIt does not support resilience, and supports only those workloads which can fit\ninto cluster memory. In return, it can run HMR jobs unchanged -- including jobs\nproduced by compilers for higher-level languages such as Pig, Jaql, and\nSystemML and interactive front-ends like IBM BigSheets -- while providing\nsignificantly better performance than the Hadoop engine on several workloads\n(e.g. 45x on some input sizes for sparse matrix vector multiply). M3R also\nsupports extensions to the HMR API which can enable Map Reduce jobs to run\nfaster on the M3R engine, while not affecting their performance under the\nHadoop engine.\n",
        "  We propose a combined reconstruction-classification method for simultaneously\nrecovering absorption and scattering in turbid media from images of absorbed\noptical energy. This method exploits knowledge that optical parameters are\ndetermined by a limited number of classes to iteratively improve their\nestimate. Numerical experiments show that the proposed approach allows for\naccurate recovery of absorption and scattering in 2 and 3 dimensions, and\ndelivers superior image quality with respect to traditional reconstruction-only\napproaches.\n",
        "  We introduce ParlAI (pronounced \"par-lay\"), an open-source software platform\nfor dialog research implemented in Python, available at http://parl.ai. Its\ngoal is to provide a unified framework for sharing, training and testing of\ndialog models, integration of Amazon Mechanical Turk for data collection, human\nevaluation, and online/reinforcement learning; and a repository of machine\nlearning models for comparing with others' models, and improving upon existing\narchitectures. Over 20 tasks are supported in the first release, including\npopular datasets such as SQuAD, bAbI tasks, MCTest, WikiQA, QACNN, QADailyMail,\nCBT, bAbI Dialog, Ubuntu, OpenSubtitles and VQA. Several models are integrated,\nincluding neural models such as memory networks, seq2seq and attentive LSTMs.\n",
        "  The prevalence of sexual reproduction (\"sex\") in eukaryotes is an enigma of\nevolutionary biology. Sex increases genetic variation only tells its long-term\nsuperiority in essence. The accumulation of harmful mutations causes an\nimmediate and ubiquitous pressure for organisms. Contrary to the common sense,\nour theoretical model suggests that reproductive rate can influence\ninitiatively the accumulation of harmful mutations. The interaction of\nreproductive rate and the integrated harm of mutations causes a critical\nreproductive rate R*. A population will become irreversibly extinct once the\nreproductive rate reduces to lower than R*. A sexual population has a R* lower\nthan 1 and an asexual population has a R* higher than 1. The mean reproductive\nrate of a population reached to the carrying capacity has to reduce to 1. That\nexplains the widespread sex as well as the persistence of facultative and\nasexual organisms. Computer simulations support significantly our conclusion.\n",
        "  Increasingly, keyword, natural language and NoSQL queries are being used for\ninformation retrieval from traditional as well as non-traditional databases\nsuch as web, document, image, GIS, legal, and health databases. While their\npopularity are undeniable for obvious reasons, their engineering is far from\nsimple. In most part, semantics and intent preserving mapping of a well\nunderstood natural language query expressed over a structured database schema\nto a structured query language is still a difficult task, and research to tame\nthe complexity is intense. In this paper, we propose a multi-level\nknowledge-based middleware to facilitate such mappings that separate the\nconceptual level from the physical level. We augment these multi-level\nabstractions with a concept reasoner and a query strategy engine to dynamically\nlink arbitrary natural language querying to well defined structured queries. We\ndemonstrate the feasibility of our approach by presenting a Datalog based\nprototype system, called BioSmart, that can compute responses to arbitrary\nnatural language queries over arbitrary databases once a syntactic\nclassification of the natural language query is made.\n",
        "  Measuring the similarities between objects in information networks has\nfundamental importance in recommendation systems, clustering and web search.\nThe existing metrics depend on the meta path or meta structure specified by\nusers. In this paper, we propose a stratified meta structure based similarity\n$SMSS$ in heterogeneous information networks. The stratified meta structure can\nbe constructed automatically and capture rich semantics. Then, we define the\ncommuting matrix of the stratified meta structure by virtue of the commuting\nmatrices of meta paths and meta structures. As a result, $SMSS$ is defined by\nvirtue of these commuting matrices. Experimental evaluations show that the\nproposed $SMSS$ on the whole outperforms the state-of-the-art metrics in terms\nof ranking and clustering.\n",
        "  In this paper we present extinction properties of interstellar dust in a\nprominent dust lane galaxy NGC 4370 based on the optical broad band (BVRI)\nimaging observations taken from the Himalaya Chandra Telescope (HCT), Hanle and\nthe near-IR (J,H,K$_s$) images taken from the 2MASS archive. NGC 4370 belongs\nto the Virgo cluster (VCC 0758) and form a non-interactive pair with NGC 4365\nat 10$\\arcmin$. NGC 4370 hosts a prominent dust lane running parallel to its\noptical major axis and is extended almost up to 1\\arcmin. The extinction curve\nderived for NGC 4370 is found to run parallel to Galactic extinction curve,\nimplying that the properties of dust in NGC 4370 are identical to those of the\ncanonical grains in the Milky Way. The $R_V$ value is found to be equal to\n2.85$\\pm$0.05 and is consitent with the values reported for the dust lane\ngalaxies. The total dust content of NGC 4370 estimated using optical extinction\nand IRAS flux densities are found to be equal to $4.4\\times 10^4$ \\msol and\n$2.0\\times 10^5$ \\msol, respectively. As regard to the origin of dust and ISM\nin this galaxy, the accumulated dust by this galaxy over its life-time is\ninsufficient to account for the detected mass by optical means, which in turn\nimply that the ISM might have been acquired by the NGC 4370 through a merger\nlike event. An attempt is also made to study the apparent spatial\ncorrespondence between the multiple phases of ISM, i.e., hot gas, warm gas and\ndust in this galaxy by obtaining optical emission maps from narrow band imaging\nand diffuse X-ray emission map obtained from the analysis of \\emph{Chandra}\narchival data. This analysis implies a physical connection between the dust and\nwarm gas in terms of their physical co-existence and common origin too.\n",
        "  Chronic disease is a long-lasting condition that can be controlled but not\ncured. Owing to the portability and continuous action, wearable devices are\nregarded as promising ways for management and rehabilitation of patients with\nchronic diseases. Although the monitoring functions of mobile health care are\nin full swing, the research direction along wearable therapy devices has not\nbeen paid with enough attention. The aim of this review paper is to summarize\nrecent developments in the field of wearable devices, with particular focus on\nspecific interpretations of therapy for chronic diseases including diabetes,\nheart disease, Parkinson's disease, chronic kidney disease and movement\ndisability after neurologic injuries. A brief summary on the key enabling\ntechnologies allowing for the implementation of wearable monitoring and therapy\ndevices is given, such as miniaturization of electronic circuits, data analysis\ntechniques, telecommunication strategy and physical therapy etc. For future\ndevelopments, research directions worth of pursuing are outlined.\n",
        "  Word vectors require significant amounts of memory and storage, posing issues\nto resource limited devices like mobile phones and GPUs. We show that high\nquality quantized word vectors using 1-2 bits per parameter can be learned by\nintroducing a quantization function into Word2Vec. We furthermore show that\ntraining with the quantization function acts as a regularizer. We train word\nvectors on English Wikipedia (2017) and evaluate them on standard word\nsimilarity and analogy tasks and on question answering (SQuAD). Our quantized\nword vectors not only take 8-16x less space than full precision (32 bit) word\nvectors but also outperform them on word similarity tasks and question\nanswering.\n",
        "  Recent studies have shown potential for using polarisation sensitive optical\ncoherence tomography (PS-OCT) to study cartilage morphology, and to be\npotentially used as an in-vivo, non-invasive tool for detecting osteoarthritic\nchanges. However, there has been relatively limited ability of this method to\nquantify the subtle changes that occur in the early stages of cartilage\ndegeneration. An established mechanical indenting technique that has previously\nbeen used to examine the microstructural response of articular cartilage was\nemployed to fix the bovine samples in an indented state. The samples were\nsubject to creep loading with a constant compressive stress of 4.5 MPa and,\nwhen imaged using PS-OCT, enabled birefringent banding patterns to be observed.\nThe magnitude of the birefringence was quantified using the birefringence\ncoefficient (BRC) and statistical analysis revealed that PS-OCT is able to\ndetect and quantify significant changes between healthy and early\nosteoarthritic cartilage (p<0.001). This presents a novel utilization of PS-OCT\nfor future development as an in-vivo assessment tool.\n",
        "  Peripheral nerve injuries are difficult to treat due to limited axon\nregeneration; brief electrical stimulation of injured nerves is an emerging\ntherapy that can relieve pain and enhance regeneration. We report an original\nwireless stimulator based on a metal loop (diameter ~1 mm) that is powered by a\ntranscranial magnetic stimulator (TMS). The loop can be integrated in a\nchitosan scaffold that functions as a graft when applied onto transected nerves\n(graft-antenna). The graft-antenna was bonded to rat sciatic nerves by a laser\nwithout sutures; it did not migrate after implantation and was able to trigger\nsteady compound muscle action potentials for 12 weeks (CMAP ~1.3 mV). Eight\nweeks post-operatively, axon regeneration was facilitated in transected nerves\nthat were repaired with the graft-antenna and stimulated by the TMS for 1\nhour/week. The graft-antenna is an innovative and minimally-invasive device\nthat functions concurrently as a wireless stimulator and adhesive scaffold for\nnerve repair.\n",
        "  The appearance of wireless communication is dramatically changing our life.\nMobile telecommunications emerged as a technological marvel allowing for access\nto personal and other services, devices, computation and communication, in any\nplace and at any time through effortless plug and play. Setting up wireless\nmobile networks often requires: Frequency Assignment, Communication Protocol\nselection, Routing schemes selection, and cells towers distributions. This\nresearch aims to optimize the cells towers distribution by using spatial mining\nwith Geographic Information System (GIS) as a tool. The distribution\noptimization could be done by applying the Digital Elevation Model (DEM) on the\nimage of the area which must be covered with two levels of hierarchy. The\nresearch will apply the spatial association rules technique on the second level\nto select the best square in the cell for placing the antenna. From that the\nproposal will try to minimize the number of installed towers, makes tower's\nlocation feasible, and provides full area coverage.\n",
        "  This article presents classifiers based on SVM and Convolutional Neural\nNetworks (CNN) for the TASS 2017 challenge on tweets sentiment analysis. The\nclassifier with the best performance in general uses a combination of SVM and\nCNN. The use of word embeddings was particularly useful for improving the\nclassifiers performance.\n",
        "  Multimodal models have been proven to outperform text-based approaches on\nlearning semantic representations. However, it still remains unclear what\nproperties are encoded in multimodal representations, in what aspects do they\noutperform the single-modality representations, and what happened in the\nprocess of semantic compositionality in different input modalities. Considering\nthat multimodal models are originally motivated by human concept\nrepresentations, we assume that correlating multimodal representations with\nbrain-based semantics would interpret their inner properties to answer the\nabove questions. To that end, we propose simple interpretation methods based on\nbrain-based componential semantics. First we investigate the inner properties\nof multimodal representations by correlating them with corresponding\nbrain-based property vectors. Then we map the distributed vector space to the\ninterpretable brain-based componential space to explore the inner properties of\nsemantic compositionality. Ultimately, the present paper sheds light on the\nfundamental questions of natural language understanding, such as how to\nrepresent the meaning of words and how to combine word meanings into larger\nunits.\n",
        "  This paper presents the QAMDiagnos, a model of Quantum Associative Memory\n(QAM) that can be a helpful tool for medical staff without experience or\nlaboratory facilities, for the diagnosis of four tropical diseases (malaria,\ntyphoid fever, yellow fever and dengue) which have several similar signs and\nsymptoms. The memory can distinguish a single infection from a polyinfection.\nOur model is a combination of the improved versions of the original linear\nquantum retrieving algorithm proposed by Ventura and the non-linear quantum\nsearch algorithm of Abrams and Lloyd. From the given simulation results, it\nappears that the efficiency of recognition is good when particular signs and\nsymptoms of a disease are inserted given that the linear algorithm is the main\nalgorithm. The non-linear algorithm helps confirm or correct the diagnosis or\ngive some advice to the medical staff for the treatment. So, our QAMDiagnos\nthat has a friendly graphical user interface for desktop and smart-phone is a\nsensitive and a low-cost diagnostic tool that enables rapid and accurate\ndiagnosis of four tropical diseases.\n",
        "  Assume that \\Gamma_{v_0} is a tree with vertex set Vert(\\Gamma_{v_0})={v_0,\nv_1,..., v_n}, and with an integral framing (weight) attached to each vertex\nexcept v_0. Assume furthermore that the intersection matrix of\nG=\\Gamma_{v_0}-{v_0} is negative definite. We define a filtration on the chain\ncomplex computing the lattice homology of G and show how to use this\ninformation in computing lattice homology groups of a negative definite graph\nwe get by attaching some framing to v_0. As a simple application we produce\nfamilies of graphs which have arbitrarily many bad vertices for which the\nlattice homology groups are shown to be isomorphic to the corresponding\nHeegaard Floer homology groups.\n",
        "  Discourse segmentation is a crucial step in building end-to-end discourse\nparsers. However, discourse segmenters only exist for a few languages and\ndomains. Typically they only detect intra-sentential segment boundaries,\nassuming gold standard sentence and token segmentation, and relying on\nhigh-quality syntactic parses and rich heuristics that are not generally\navailable across languages and domains. In this paper, we propose statistical\ndiscourse segmenters for five languages and three domains that do not rely on\ngold pre-annotations. We also consider the problem of learning discourse\nsegmenters when no labeled data is available for a language. Our fully\nsupervised system obtains 89.5% F1 for English newswire, with slight drops in\nperformance on other domains, and we report supervised and unsupervised\n(cross-lingual) results for five languages in total.\n",
        "  Latent tree learning models represent sentences by composing their words\naccording to an induced parse tree, all based on a downstream task. These\nmodels often outperform baselines which use (externally provided) syntax trees\nto drive the composition order. This work contributes (a) a new latent tree\nlearning model based on shift-reduce parsing, with competitive downstream\nperformance and non-trivial induced trees, and (b) an analysis of the trees\nlearned by our shift-reduce model and by a chart-based model.\n",
        "  The human lung and its functions are extremely sensitive to orientation and\nposture, and debate continues as to the role of gravity and the surrounding\nanatomy in determining lung function and heterogeneity of perfusion and\nventilation. However, study of these effects is difficult. The conventional\nhigh-field magnets used for most hyperpolarized 3He MRI of the human lung, and\nmost other common radiological imaging modalities including PET and CT,\nrestrict subjects to lying horizontally, minimizing most gravitational effects.\nIn this paper, we briefly review the motivation for posture-dependent studies\nof human lung function, and present initial imaging results of human lungs in\nthe supine and vertical body orientations using inhaled hyperpolarized 3He gas\nand an open-access MRI instrument. The open geometry of this MRI system\nfeatures a \"walk-in\" capability that permits subjects to be imaged in vertical\nand horizontal positions, and potentially allows for complete rotation of the\norientation of the imaging subject in a two-dimensional plane. Initial results\ninclude two-dimensional lung images acquired with ~ 4 mm in-plane resolution\nand three-dimensional images with ~ 1.5 cm slice thickness. Effects of posture\nvariation are observed.\n",
        "  Language, which allows complex ideas to be communicated through symbolic\nsequences, is a characteristic feature of our species and manifested in a\nmultitude of forms. Using large written corpora for many different languages\nand scripts, we show that the occurrence probability distributions of signs at\nthe left and right ends of words have a distinct heterogeneous nature.\nCharacterizing this asymmetry using quantitative inequality measures, viz.\ninformation entropy and the Gini index, we show that the beginning of a word is\nless restrictive in sign usage than the end. This property is not simply\nattributable to the use of common affixes as it is seen even when only word\nroots are considered. We use the existence of this asymmetry to infer the\ndirection of writing in undeciphered inscriptions that agrees with the\narchaeological evidence. Unlike traditional investigations of phonotactic\nconstraints which focus on language-specific patterns, our study reveals a\nproperty valid across languages and writing systems. As both language and\nwriting are unique aspects of our species, this universal signature may reflect\nan innate feature of the human cognitive phenomenon.\n",
        "  The existence of phenotypic heterogeneity in single-species bacterial\nbiofilms is well-established in the published literature. However, the modeling\nof population dynamics in biofilms from the viewpoint of social interactions,\ni.e. interplay between heterotypic strains, and the analysis of this kind using\ncontrol theory are not addressed significantly. Therefore, in this paper, we\ntheoretically analyze the population dynamics model in microbial biofilms with\nnon-participating strains (coexisting with public goods producers and\nnon-producers) in the context of evolutionary game theory and nonlinear\ndynamics. Our analysis of the replicator dynamics model is twofold: first\nwithout the inclusion of spatial pattern, and second with the consideration of\ndegree of assortment. In the first case, Lyapunov stability analysis of the\nstable equilibrium point of the proposed replicator system determines (1,0)\n('full dominance of cooperators') as a global asymptotic stable equilibrium\nwhenever the return exceeds the metabolic cost of cooperation. Hence, the\nglobal asymptotic stable nature of (1,0) in the context of non-consideration of\nspatial pattern helps to justify mathematically the adversity in the\neradication of \"cooperative enterprise\" that is an infectious biofilm. In the\nsecond case, we found non-existence of global asymptotic stability in the\nsystem, and it unveils two additional phenomena - bistability and coexistence.\nIn this context, two inequality conditions are derived for the 'full dominance\nof cooperators' and coexistence. Therefore, the inclusion of spatial pattern in\nbiofilms with non-competing strains intends conditional dominance of pathogenic\n(with respect to the hosts) public goods producers which can be an effective\nstrategy towards the control of an infectious biofilm with the drug-dependent\nregulation of degree of segregation.\n",
        "  Web sequential patterns are important for analyzing and understanding users\nbehaviour to improve the quality of service offered by the World Wide Web. Web\nPrefetching is one such technique that utilizes prefetching rules derived\nthrough Cyclic Model Analysis of the mined Web sequential patterns. The more\naccurate the prediction and more satisfying the results of prefetching if we\nuse a highly efficient and scalable mining technique such as the Bidirectional\nGrowth based Directed Acyclic Graph. In this paper, we propose a novel\nalgorithm called Bidirectional Growth based mining Cyclic behavior Analysis of\nweb sequential Patterns (BGCAP) that effectively combines these strategies to\ngenerate prefetching rules in the form of 2-sequence patterns with Periodicity\nand threshold of Cyclic Behaviour that can be utilized to effectively prefetch\nWeb pages, thus reducing the users perceived latency. As BGCAP is based on\nBidirectional pattern growth, it performs only (log n+1) levels of recursion\nfor mining n Web sequential patterns. Our experimental results show that\nprefetching rules generated using BGCAP is 5-10 percent faster for different\ndata sizes and 10-15% faster for a fixed data size than TD-Mine. In addition,\nBGCAP generates about 5-15 percent more prefetching rules than TD-Mine.\n",
        "  We present a novel approach for recognizing what we call targetable named\nentities; that is, named entities in a targeted set (e.g, movies, books, TV\nshows). Unlike many other NER systems that need to retrain their statistical\nmodels as new entities arrive, our approach does not require such retraining,\nwhich makes it more adaptable for types of entities that are frequently\nupdated. For this preliminary study, we focus on one entity type, movie title,\nusing data collected from Twitter. Our system is tested on two evaluation sets,\none including only entities corresponding to movies in our training set, and\nthe other excluding any of those entities. Our final model shows F1-scores of\n76.19% and 78.70% on these evaluation sets, which gives strong evidence that\nour approach is completely unbiased to any par- ticular set of entities found\nduring training.\n",
        "  In this paper, we present a new method to generate an instantaneous\nvolumetric image using a single x-ray projection. To fully extract motion\ninformation hidden in projection images, we partitioned a projection image into\nsmall patches. We utilized a sparse learning method to automatically select\npatches that have a high correlation with principal component analysis (PCA)\ncoefficients of a lung motion model. A model that maps the patch intensity to\nthe PCA coefficients is built along with the patch selection process. Based on\nthis model, a measured projection can be used to predict the PCA coefficients,\nwhich are further used to generate a motion vector field and hence a volumetric\nimage. We have also proposed an intensity baseline correction method based on\nthe partitioned projection, where the first and the second moments of pixel\nintensities at a patch in a simulated image are matched with those in a\nmeasured image via a linear transformation. The proposed method has been valid\nin simulated data and real phantom data. The algorithm is able to identify\npatches that contain relevant motion information, e.g. diaphragm region. It is\nfound that intensity correction step is important to remove the systematic\nerror in the motion prediction. For the simulation case, the sparse learning\nmodel reduced prediction error for the first PCA coefficient to 5%, compared to\nthe 10% error when sparse learning is not used. 95th percentile error for the\npredicted motion vector is reduced from 2.40 mm to 0.92mm. In the phantom case,\nthe predicted tumor motion trajectory is successfully reconstructed with 0.82\nmm mean vector field error compared to 1.66 mm error without using the sparse\nlearning method. The algorithm robustness with respect to sparse level, patch\nsize, and existence of diaphragm, as well as computation time, has also been\nstudied.\n",
        "  We present the strain and temperature dependence of an anomalous nematic\nphase in optimally doped BaFe$_2$(As,P)$_2$. Polarized ultrafast optical\nmeasurements reveal broken 4-fold rotational symmetry in a temperature range\nabove $T_c$ in which bulk probes do not detect a phase transition. Using\nultrafast microscopy, we find that the magnitude and sign of this nematicity\nvary on a ${50{-}100}~\\mu$m length scale, and the temperature at which it\nonsets ranges from 40 K near a domain boundary to 60 K deep within a domain.\nScanning Laue microdiffraction maps of local strain at room temperature\nindicate that the nematic order appears most strongly in regions of weak,\nisotropic strain. These results indicate that nematic order arises in a genuine\nphase transition rather than by enhancement of local anisotropy by a strong\nnematic susceptibility. We interpret our results in the context of a proposed\nsurface nematic phase.\n",
        "  We report on CO (J = 2 - 1) mapping with the IRAM 30-m HERA receiver array of\nCGCG 97-079, an irregular galaxy in the merging galaxy cluster Abell 1367 (z =\n0.022). We find that $\\sim$ 80% of the detected CO (J = 2 - 1) is projected\nwithin a 16 arcsec$^{2}$ (6.5 kpc$^{2}$) region to the north and west of the\noptical/NIR centre, with the intensity maximum offset $\\sim 10$ arcsec (4 kpc)\nNW of the optical/NIR centre and $\\sim$ 7 arcsec (3 kpc) south-east of the HI\nintensity maximum. Evolutionary synthesis models indicate CGCG 97-079\nexperienced a burst of star formation $\\sim$ 10$^8$ yr ago, most likely\ntriggered by a tidal interaction with CGCG 97-073. For CGCG 97-079 we deduce an\ninfall velocity to the cluster of $\\sim$ 1000 km s$^{-1}$ and moderate ram\npressure (P$_\\mathrm{ram} \\sim 10^{-11}$ dyn cm$^{-2}$). The observed offset in\nCGCG 97-079 of the highest density HI and CO (J = 2 - 1) from the stellar\ncomponents has not previously been observed in galaxies currently undergoing\nram pressure stripping, although previous detailed studies of gas morphology\nand kinematics during ram pressure stripping were restricted to significantly\nmore massive galaxies with deeper gravitational potential wells. We conclude\nthe observed cold gas density maxima offsets are most likely the result of ram\npressure and/or the high-speed tidal interaction with CGCG 97-073. However ram\npressure stripping is likely to be playing a major role in the perturbation of\nlower density gas.\n",
        "  Three steps aid in the analysis of selection. First, describe phenotypes by\ntheir component causes. Components include genes, maternal effects, symbionts,\nand any other predictors of phenotype that are of interest. Second, describe\nfitness by its component causes, such as an individual's phenotype, its\nneighbors' phenotypes, resource availability, and so on. Third, put the\npredictors of phenotype and fitness into an exact equation for evolutionary\nchange, providing a complete expression of selection and other evolutionary\nprocesses. The complete expression separates the distinct causal roles of the\nvarious hypothesized components of phenotypes and fitness. Traditionally, those\ncomponents are given by the covariance, variance, and regression terms of\nevolutionary models. I show how to interpret those statistical expressions with\nrespect to information theory. The resulting interpretation allows one to read\nthe fundamental equations of selection and evolution as sentences that express\nhow various causes lead to the accumulation of information by selection and the\ndecay of information by other evolutionary processes. The interpretation in\nterms of information leads to a deeper understanding of selection and\nheritability, and a clearer sense of how to formulate causal hypotheses about\nevolutionary process. Kin selection appears as a particular type of causal\nanalysis that partitions social effects into meaningful components.\n",
        "  Using Monte Carlo model of biological evolution we have discovered that\npopulations can switch between two different strategies of their genomes'\nevolution; Darwinian purifying selection and complementing the haplotypes. The\nfirst one is exploited in the large panmictic populations while the second one\nin the small highly inbred populations. The choice depends on the crossover\nfrequency. There is a power law relation between the critical value of\ncrossover frequency and the size of panmictic population. Under the constant\ninbreeding this critical value of crossover does not depend on the population\nsize and has a character of phase transition. Close to this value sympatric\nspeciation is observed.\n",
        "  Monte Carlo (MC) simulation is considered as the most accurate method for\nradiation dose calculations. Accuracy of a source model for a linear\naccelerator is critical for the overall dose calculation accuracy. In this\npaper, we presented an analytical source model that we recently developed for\nGPU-based MC dose calculations. A key concept called phase-space-ring (PSR) was\nproposed. It contained a group of particles that are of the same type and close\nin energy and radial distance to the center of the phase-space plane. The model\nparameterized probability densities of particle location, direction and energy\nfor each primary photon PSR, scattered photon PSR and electron PSR. For a\nprimary photon PSRs, the particle direction is assumed to be from the beam\nspot. A finite spot size is modeled with a 2D Gaussian distribution. For a\nscattered photon PSR, multiple Gaussian components were used to model the\nparticle direction. The direction distribution of an electron PSRs was also\nmodeled as a 2D Gaussian distribution with a large standard deviation. We also\ndeveloped a method to analyze a phase-space file and derive corresponding model\nparameters. To test the accuracy of our linac source model, dose distributions\nof different open fields in a water phantom were calculated using our source\nmodel and compared to those directly calculated using the reference phase-space\nfile. The average distance-to-agreement (DTA) was within 1 mm for the depth\ndose in the build-up region and beam penumbra regions. The root-mean-square\n(RMS) dose difference was within 1.1% for dose profiles at inner and outer beam\nregions. The maximal relative difference of output factors was within 0.5%.\nGood agreements were also found in an IMRT prostate patient case and an IMRT\nhead-and-neck case. These results demonstrated the efficacy of our source model\nin terms of accurately representing a reference phase-space file.\n",
        "  In this paper, we present a novel approach -- called WaterFowl -- for the\nstorage of RDF triples that addresses some key issues in the contexts of big\ndata and the Semantic Web. The architecture of our prototype, largely based on\nthe use of succinct data structures, enables the representation of triples in a\nself-indexed, compact manner without requiring decompression at query answering\ntime. Moreover, it is adapted to efficiently support RDF and RDFS entailment\nregimes thanks to an optimized encoding of ontology concepts and properties\nthat does not require a complete inference materialization or extensive query\nrewriting algorithms. This approach implies to make a distinction between the\nterminological and the assertional components of the knowledge base early in\nthe process of data preparation, i.e., preprocessing the data before storing it\nin our structures. The paper describes the complete architecture of this system\nand presents some preliminary results obtained from evaluations conducted on\nour first prototype.\n",
        "  A cylindrical stretch line is a stretch line, in the sense of Thurston, whose\nhorocyclic lamination is a weighted multicurve. In this paper, we show that two\ncorrectly parameterized cylindrical lines are parallel if and only if these\nlines converge towards the same point in Thurston's boundary of Teichm\\\"uller\nspace.\n",
        "  The Ebola virus in West Africa has infected almost 30,000 and killed over\n11,000 people. Recent models of Ebola Virus Disease (EVD) have often made\nassumptions about how the disease spreads, such as uniform transmissibility and\nhomogeneous mixing within a population. In this paper, we test whether these\nassumptions are necessarily correct, and offer simple solutions that may\nimprove disease model accuracy. First, we use data and models of West African\nmigration to show that EVD does not homogeneously mix, but spreads in a\npredictable manner. Next, we estimate the initial growth rate of EVD within\ncountry administrative divisions and find that it significantly decreases with\npopulation density. Finally, we test whether EVD strains have uniform\ntransmissibility through a novel statistical test, and find that certain\nstrains appear more often than expected by chance.\n",
        "  This posting announces public availability of version 1.2 of the DiskFit\nsoftware package developed by the authors, which may be used to fit simple\nnon-axisymmetric models either to images or to velocity fields of disk\ngalaxies. Here we give an outline of the capability of the code and provide the\nlink to downloading executables, the source code, and a comprehensive on-line\nmanual. We argue that in important respects the code is superior to rotcur for\nfitting kinematic maps and to galfit for fitting multi-component models to\nphotometric images.\n",
        "  Storytelling serves many different social functions, e.g. stories are used to\npersuade, share troubles, establish shared values, learn social behaviors, and\nentertain. Moreover, stories are often told conversationally through dialog,\nand previous work suggests that information provided dialogically is more\nengaging than when provided in monolog. In this paper, we present algorithms\nfor converting a deep representation of a story into a dialogic storytelling,\nthat can vary aspects of the telling, including the personality of the\nstorytellers. We conduct several experiments to test whether dialogic\nstorytellings are more engaging, and whether automatically generated variants\nin linguistic form that correspond to personality differences can be recognized\nin an extended storytelling dialog.\n",
        "  Confidence is an essential ingredient of success in a wide range of domains\nranging from job performance and mental health, to sports, business, and\ncombat. Some authors have suggested that not just confidence but\noverconfidence-believing you are better than you are in reality-is advantageous\nbecause it serves to increase ambition, morale, resolve, persistence, or the\ncredibility of bluffing, generating a self-fulfilling prophecy in which\nexaggerated confidence actually increases the probability of success. However,\noverconfidence also leads to faulty assessments, unrealistic expectations, and\nhazardous decisions, so it remains a puzzle how such a false belief could\nevolve or remain stable in a population of competing strategies that include\naccurate, unbiased beliefs. Here, we present an evolutionary model showing\nthat, counter-intuitively, overconfidence maximizes individual fitness and\npopulations will tend to become overconfident, as long as benefits from\ncontested resources are sufficiently large compared to the cost of competition.\nIn contrast, \"rational\" unbiased strategies are only stable under limited\nconditions. The fact that overconfident populations are evolutionarily stable\nin a wide range of environments may help to explain why overconfidence remains\nprevalent today, even if it contributes to hubris, market bubbles, financial\ncollapses, policy failures, disasters, and costly wars.\n",
        "  To show an optimization method of element substitution for cuprate\nsuperconductors, we investigate Fermi surface shape of TlR2A2Cu3O9 with R=La,\nY, and A=Li, Na, K, Rb, Cs. We adopt the generalized gradient approximation in\nthe density-functional theory (DFT-GGA) for the study of over-doped phases of\nthese unknown cuprates. The electronic structures of crystals optimized by\nDFT-GGA show systematic element dependence in a Fermi surface shape controlling\nparameter, r, of Cu dx2-y2 bands, where nearly absent dz2 component at the\nFermi level and smaller r keeping t1 suggest enhancement of the superconducting\ntransition temperature within the spin-fluctuation mechanism. For TlYRb2Cu3O9,\nsmaller r by a reduction factor larger than 10% compared to a reference system\nof TlBa2Ca2Cu3O9 (TBCCO) appears in the outer CuO2 plane, but with 12 %\nreduction in t1. For TlR2Li2Cu3O9, (R=Y, La), smaller r by a factor larger than\n1% appears keeping t1 as large as TBCCO in the inner plane. Our method may be\nused to predict an optimized material structure referencing known cuprate\nsuperconductors.\n",
        "  The task of event extraction has long been investigated in a supervised\nlearning paradigm, which is bound by the number and the quality of the training\ninstances. Existing training data must be manually generated through a\ncombination of expert domain knowledge and extensive human involvement.\nHowever, due to drastic efforts required in annotating text, the resultant\ndatasets are usually small, which severally affects the quality of the learned\nmodel, making it hard to generalize. Our work develops an automatic approach\nfor generating training data for event extraction. Our approach allows us to\nscale up event extraction training instances from thousands to hundreds of\nthousands, and it does this at a much lower cost than a manual approach. We\nachieve this by employing distant supervision to automatically create event\nannotations from unlabelled text using existing structured knowledge bases or\ntables.We then develop a neural network model with post inference to transfer\nthe knowledge extracted from structured knowledge bases to automatically\nannotate typed events with corresponding arguments in text.We evaluate our\napproach by using the knowledge extracted from Freebase to label texts from\nWikipedia articles. Experimental results show that our approach can generate a\nlarge number of high quality training instances. We show that this large volume\nof training data not only leads to a better event extractor, but also allows us\nto detect multiple typed events.\n",
        "  Developing parallel corpora is an important and a difficult activity for\nMachine Translation. This requires manual annotation by Human Translators.\nTranslating same text again is a useless activity. There are tools available to\nimplement this for European Languages, but no such tool is available for Indian\nLanguages. In this paper we present a tool for Indian Languages which not only\nprovides automatic translations of the previously available translation but\nalso provides multiple translations, in cases where a sentence has multiple\ntranslations, in ranked list of suggestive translations for a sentence.\nMoreover this tool also lets translators have global and local saving options\nof their work, so that they may share it with others, which further lightens\nthe task.\n",
        "  We use a quantitative convergent beam electron diffraction (CBED) based\nmethod to image the valence electron density distribution in\nBa(Fe$_{1-x}$Co$_x$)$_2$As$_2$. We show a remarkable increase in both the\ncharge quadrupole of the Fe cations and the charge dipole of the arsenic anions\nupon Co doping from $x=0$ ($T_c=0$ K) to $x=0.1$ ($T_c=22.5$ K). Our data\nsuggest that an unexpected electronic correlation effect, namely strong\ncoupling of Fe orbital fluctuation and anion electronic polarization, is\npresent in iron-based superconductors.\n",
        "  An increasing number of dissident voices claim that the standard\nneo-Darwinian view of genes as 'leaders' and phenotypes as 'followers' during\nthe process of adaptive evolution should be turned on its head. This idea is\nolder than the rediscovery of Mendel's laws of inheritance and has been given\nseveral names before its final 'Baldwin effect' label. A condition for this\neffect is that environmentally induced variation such as phenotypic plasticity\nor learning is crucial for the initial establishment of a population. This\ngives the necessary time for natural selection to act on genetic variation and\nthe adaptive trait can be eventually encoded in the genotype. An influential\npaper published in the late 1980s showed the Baldwin effect to happen in\ncomputer simulations, and claimed that it was crucial to solve a difficult\nadaptive task. This generated much excitement among scholars in various\ndisciplines that regard neo-Darwinian accounts to explain the evolutionary\nemergence of high-order phenotypic traits such as consciousness or language\nalmost hopeless. Here, we use analytical and computational approaches to show\nthat a standard population genetics treatment can easily crack what the\nscientific community has granted as an unsolvable adaptive problem without\nlearning. The Baldwin effect is once again in need of convincing theoretical\nfoundations.\n",
        "  In most of the under developed nations, access to basic health care is\nscarce. A heater is an indispensable device for many applications, especially\nfor medical purposes. As such a simple and easy to make heater from readily\navailable materials becomes a valuable equipment to help alleviate the human\nhealth problems. An ultra low cost heater from readily available materials like\npaper/cellulose acetate sheet and a pencil was designed and fabricated.\nManually, using high graphite contained pencil; paper/cellulose surface was\narbitrarily coated by applying numerous pencil strokes on the surface. Contacts\nwere connected using either a wire or Copper/Aluminum foils over the coated\nsurface. Maximum temperature attended by the heater was around 140 degree\ncelcius. The main application of this heater was explored in the laboratory and\ntherapeutic uses.\n",
        "  The first part of this paper completes the classification of Whitney towers\nin the 4-ball that was started in three related papers. We provide an algebraic\nframework allowing the computations of the graded groups associated to\ngeometric filtrations of classical link concordance by order n (twisted)\nWhitney towers in the 4-ball. Higher-order Sato-Levine invariants and\nhigher-order Arf invariants are defined and shown to be the obstructions to\nframing a twisted Whitney tower. In the second part of this paper, a general\ntheory of quadratic forms is developed and then specialized from the\nnon-commutative to the commutative to finally, the symmetric settings. The\nintersection invariant for twisted Whitney towers is shown to be the universal\nsymmetric refinement of the framed intersection invariant. UPDATE: The results\nof the first six sections of this paper have been subsumed into the paper\n\"Whitney tower concordance of classical links.\"\n",
        "  We report on magnetization data obtained as a function of temperature and\nmagnetic field in Li2 (Pd0.8Pt0.2)3B and Li2Pd3B non-centro-symmetric\nsuperconductors. Reversible magnetization curves were plotted as M1/2 vs. T.\nThis allows study of the asymptotic behavior of the averaged order parameter\namplitude (gap) near the superconducting transition. Results of the analysis\nshow, as expected, a mean field superconducting transition for Li2Pd3B. On\ncontrary, a large deviation from the mean field behavior is revealed for\nLi2(Pd0.8Pt0.2)3B. This is interpreted as due to the strength of the non s-wave\nspin-triplet pairing in this Pt-containing compound which produces nodes in the\norder parameter and consequently, phase fluctuations. The diamagnetic signal\nabove Tc(H) in Li2Pd3B is well explained by superconducting Gaussian\nfluctuations, which agrees with the observed mean field transition. For\nLi2(Pd0.8Pt0.2)3B the diamagnetic signal above Tc(H) is much higher than the\nexpected Gaussian values and appears to be well explained by three dimensional\ncritical fluctuations of the lowest-Landau-level type, which somehow agrees\nwith the scenario of a phase mediated transition.\n",
        "  We develop a theory of the tunneling spectroscopy for superconducting\ntopological insulators (STIs), where the surface Andreev bound states (SABSs)\nappear as helical Majorana fermions. Based on the symmetry and topological\nnature of parent topological insulators, we find that the SABSs in the STIs\nhave a profound structural transition in the energy dispersions. The transition\nresults in a variety of Majorana fermions, by tuning the chemical potential and\nthe effective mass of the energy band. We clarify that Majorana fermions in the\nvicinity of the transitions give rise to robust zero bias peaks in the\ntunneling conductance between normal metal/STI junctions.\n",
        "  Data from a multi-parametric MRI study of patients with possible early-stage\nprostate cancer was assessed with a view to creating an efficient clinical\nprotocol. Based on a correlation analysis suggesting that diffusion-weighted\nimaging (DWI) scores are more strongly correlated with overall PIRADS scores\nthan other modalities such as dynamic contrast enhanced imaging or\nspectroscopy, we investigate the combination of T2-weighted imaging (T2w) and\nDWI as a potential diagnostic tool for prostate cancer detection, staging and\nguided biopsies. Quantification of the noise floor in the DWI images and\ncareful fitting of the data suggests that the mono-exponential model provides a\nvery good fit to the data and there is no evidence of non-Gaussian diffusion\nfor $b$-values up to 1000 s/mm$^2$. This precludes the use of kurtosis or other\nnon-Gaussian measures as a biomarker for prostate cancer in our case. However,\nthe ADC scores for healthy and probably malignant regions are significantly\nlower for the latter in all 20 but one patient. The results suggest that a\nsimplified mp-MRI protocol combining T2w and DWI may be a good compromise for a\ncost and time efficient, early-stage prostate cancer diagnostic programme,\ncombining robust MR biomarkers for prostate cancer that can be reliably\nquantified and appear well-suited for general clinical practice.\n",
        "  The temperature-dependent optical reflectivity and complex transmissivity of\nan epitaxially grown Ba(Fe$_{0.9}$Co$_{0.1}$)$_2$As$_2$ thin film were measured\nand the optical conductivity and permittivity evaluated over a wide frequency\nrange. The opening of the superconducting gap $2\\Delta_0 = 3.7$ meV below\n$T_c\\approx 20$ K is {\\em directly} observed by a completely vanishing optical\nconductivity. The temperature and frequency dependent electrodynamic properties\nof Ba(Fe$_{0.9}$Co$_{0.1}$)$_2$As$_2$ in the superconducting state agree well\nwith the BCS predictions with no nodes in the order parameter. The spectral\nweight of the condensate $1.94\\times 10^7 {\\rm cm}^{-2}$ corresponds to a\nLondon penetration depth $\\lambda_L=3600$ \\AA.\n",
        "  In this paper, we systematically evaluate the performance of adaptive\nadjustment of the relaxation parameters of various iterative algorithms for\nX-ray CT reconstruction relying on sparsity priors. Sparsity prior has been\nfound to be an efficient strategy in CT reconstruction where significantly\nfewer attenuation measurements are available. Sparsity prior CT reconstruction\nrelies on iterative algorithms such as the algebraic reconstruction technique\n(ART) to produce a crude reconstruction based on which a sparse approximation\nis performed. Data driven adjustment of relaxation has been found to ensure\nbetter convergence than traditional relaxation for ART. In this paper, we study\nthe performance of such data driven relaxation on a (CS) compressed sensing\nenvironment. State-of-the-art algorithms are implemented and their performance\nanalyzed in regard to conventional and data-driven relaxation. Experiments are\nperformed both on simulated and real environments. For the simulated case,\nexperiments are conducted with and without the presence of noise. Correlation\ncoefficients, root mean square error, structural similarity index and\nperceptual dissimilarity metric were used for the quantitative comparisons of\nthe results. Experiments reveal that data driven relaxation also ensures\noverall better quality reconstruction in a CS environment compared to\ntraditional relaxation. However, when the data are corrupted by noise,\ninconsistencies emerge in the convergence unless a threshold is imposed on the\nmaximum amendments. Data driven relaxation seems a logical choice to more\nrapidly reach the solution. In a compressed sensing environment, especially\nwhen the data are corrupted by noise, a threshold to specify the maximum\namendments needs to be specified. Experimentally, we have set the threshold as\n20% of the previous value and thus have ensured more consistency in the\nconvergence.\n",
        "  Background: A deterministic model is developed for the spatial spread of an\nepidemic disease in a geographical setting. The disease is borne by vectors to\nsusceptible hosts through criss-cross dynamics. The model is focused on an\nepidemic outbreak that initiates from a small number of cases in a small\nsub-region of the geographical setting. Methods: Partial differential equations\nare formulated to describe the interaction of the model compartments. Results:\nThe partial differential equations of the model are analyzed and proven to be\nwell-posed. The epidemic outcomes of the model are correlated to the spatially\ndependent parameters and initial conditions of the model. Conclusions: A\nversion of the model is applied to the 2015-2016 Zika outbreak in the Rio de\nJaneiro Municipality in Brazil.\n",
        "  Point-contact spectroscopy was originally developed for the determination of\nthe electron-phonon spectral function in normal metals. However, in the past 20\nyears it has become an important tool in the investigation of superconductors.\nAs a matter of fact, point contacts between a normal metal and a superconductor\ncan provide information on the amplitude and symmetry of the energy gap that,\nin the superconducting state, opens up at the Fermi level. In this paper we\nreview the experimental and theoretical aspects of point-contact spectroscopy\nin superconductors, and we give an experimental survey of the most recent\napplications of this technique to anisotropic and multiband superconductors.\n",
        "  Query-by-example search often uses dynamic time warping (DTW) for comparing\nqueries and proposed matching segments. Recent work has shown that comparing\nspeech segments by representing them as fixed-dimensional vectors --- acoustic\nword embeddings --- and measuring their vector distance (e.g., cosine distance)\ncan discriminate between words more accurately than DTW-based approaches. We\nconsider an approach to query-by-example search that embeds both the query and\ndatabase segments according to a neural model, followed by nearest-neighbor\nsearch to find the matching segments. Earlier work on embedding-based\nquery-by-example, using template-based acoustic word embeddings, achieved\ncompetitive performance. We find that our embeddings, based on recurrent neural\nnetworks trained to optimize word discrimination, achieve substantial\nimprovements in performance and run-time efficiency over the previous\napproaches.\n",
        "  The theory of the high temperature superconducting cuprates, which is based\non the condensation of holes into strings in checker-board geometry, was\nsuccessful to explain the elastically scattered Neutrons by spin waves. Here it\nis extended to analyze the inward dispersion curve of its inelastic\ncounterpart, up to the resonance energy- . This extension is done by applying\nthe perturbation theory of the linear response to the condensed strings. The\napproximated susceptibility is derived by means of the ring diagram. The\ndispersion relation is obtained from the dispersion of the poles of the\nsusceptibility integral. It is found that the particle anti-particle pair that\nyields the susceptibility is the time reversal pair where the particle momentum\nis in phase A, and the anti-particle momentum is in phase B. The dispersion is\nfound to be in agreement with experiment, subject to some suggested\ncorrections. The weak intensity by the resonance energy, as well as the\ndispersion, is speculated to be modified due to interference with spin waves\nthat are caused by direct spin flip, as in the mother undoped materials.\n",
        "  In this paper, the following three are shown. (1) For a $C^\\infty$ convex\nintegrand $\\gamma: S^n\\to \\mathbb{R}_+$, its dual convex integrand $\\delta:\nS^n\\to \\mathbb{R}_+$ is of class $C^\\infty$. (2) For a stable convex integrand\n$\\gamma: S^n\\to \\mathbb{R}_+$, its dual convex integrand $\\delta: S^n\\to\n\\mathbb{R}_+$ is stable. (3) Let $\\gamma: S^n\\to \\mathbb{R}_+$ be a stable\nconvex integrand. Then, for any $i$ $(0\\le i\\le n)$, $\\theta_0\\in S^n$ is a\nnon-degenerate critical point of $\\gamma$ with Morse index $i$ if and only if\n$-\\theta_0\\in S^n$ is a non-degenerate critical point of the dual convex\nintegrand ${\\delta}$ with Morse index $(n-i)$.\n",
        "  The inference of the evolutionary history of a collection of organisms is a\nproblem of fundamental importance in evolutionary biology. The abundance of DNA\nsequence data arising from genome sequencing projects has led to significant\nchallenges in the inference of these phylogenetic relationships. Among these\nchallenges is the inference of the evolutionary history of a collection of\nspecies based on sequence information from several distinct genes sampled\nthroughout the genome. It is widely accepted that each individual gene has its\nown phylogeny, which may not agree with the species tree. Many possible causes\nof this gene tree incongruence are known. The best studied is incomplete\nlineage sorting, which is commonly modeled by the coalescent process. Numerous\nmethods based on the coalescent process have been proposed for estimation of\nthe phylogenetic species tree given DNA sequence data. However, use of these\nmethods assumes that the phylogenetic species tree can be identified from DNA\nsequence data at the leaves of the tree, although this has not been formally\nestablished. We prove that the unrooted topology of the n-leaf phylogenetic\nspecies tree is generically identifiable given observed data at the leaves of\nthe tree that are assumed to have arisen from the coalescent process under a\ntime-reversible substitution process with the possibility of site-specific rate\nvariation modeled by the discrete gamma distribution and a proportion of\ninvariable sites.\n",
        "  Lyman- and Werner-band absorption of molecular hydrogen (H$_2$) is detected\nin $\\sim$50% of low redshift ($z<1$) DLAs/sub-DLAs with $N$(H$_2$) >\n10$^{14.4}$ cm$^{-2}$. However the true origin(s) of the H$_2$ bearing gas\nremain elusive. Here we report a new detection of an H$_{2}$ absorber at $z=$\n0.4298 in the HST/COS spectra of quasar PKS 2128-123. The total $N$(HI) of\n10$^{19.50\\pm0.15}$ cm$^{-2}$ classifies the absorber as a sub-DLA. H$_{2}$\nabsorption is detected up to the $J=3$ rotational level with a total $\\log\nN$(H$_{2}$) = 16.36$\\pm$0.08 corresponding to a molecular fraction of log\n$f$(H$_{2}$) = $-$2.84$\\pm$0.17. The excitation temperature of $T_{ex}$ =\n206$\\pm$6K indicates the presence of cold gas. Using detailed ionization\nmodelling we obtain a near-solar metallicity (i.e., [O/H]= $-$0.26$\\pm$0.19)\nand a dust-to-gas ratio of $\\log \\kappa \\sim -0.45$ for the H$_{2}$ absorbing\ngas. The host-galaxy of the sub-DLA is detected at an impact parameter of $\\rho\n\\sim$ 48 kpc with an inclination angle of $i \\sim$ 48 degree and an azimuthal\nangle of $\\Phi \\sim$ 15 degree with respect to the QSO sightline. We show that\nco-rotating gas in an extended disk cannot explain the observed kinematics of\nMg II absorption. Moreover, the inferred high metallicity is not consistent\nwith the scenario of gas accretion. An outflow from the central region of the\nhost-galaxy, on the other hand, would require a large opening angle (i.e.,\n2$\\theta>$150 degree), much larger than the observed outflow opening angles in\nSeyfert galaxies, in order to intercept the QSO sightline. We thus favor a\nscenario in which the H$_2$ bearing gas is stemming from a dwarf-satellite\ngalaxy, presumably via tidal and/or ram-pressure stripping. Detection of a\ndwarf galaxy candidate in the HST/WFPC2 image at an impact parameter of\n$\\sim$12 kpc reinforces such an idea.\n",
        "  The ranked retrieval model has rapidly become the de facto way for search\nquery processing in client-server databases, especially those on the web.\nDespite of the extensive efforts in the database community on designing better\nranking functions/mechanisms, many such databases in practice still fail to\naddress the diverse and sometimes contradicting preferences of users on tuple\nranking, perhaps (at least partially) due to the lack of expertise and/or\nmotivation for the database owner to design truly effective ranking functions.\nThis paper takes a different route on addressing the issue by defining a novel\n{\\em query reranking problem}, i.e., we aim to design a third-party service\nthat uses nothing but the public search interface of a client-server database\nto enable the on-the-fly processing of queries with any user-specified ranking\nfunctions (with or without selection conditions), no matter if the ranking\nfunction is supported by the database or not. We analyze the worst-case\ncomplexity of the problem and introduce a number of ideas, e.g., on-the-fly\nindexing, domination detection and virtual tuple pruning, to reduce the\naverage-case cost of the query reranking algorithm. We also present extensive\nexperimental results on real-world datasets, in both offline and live online\nsystems, that demonstrate the effectiveness of our proposed techniques.\n",
        "  The Apache Spark stack has enabled fast large-scale data processing. Despite\na rich library of statistical models and inference algorithms, it does not give\ndomain users the ability to develop their own models. The emergence of\nprobabilistic programming languages has showed the promise of developing\nsophisticated probabilistic models in a succinct and programmatic way. These\nframeworks have the potential of automatically generating inference algorithms\nfor the user defined models and answering various statistical queries about the\nmodel. It is a perfect time to unite these two great directions to produce a\nprogrammable big data analysis framework. We thus propose, InferSpark, a\nprobabilistic programming framework on top of Apache Spark. Efficient\nstatistical inference can be easily implemented on this framework and inference\nprocess can leverage the distributed main memory processing power of Spark.\nThis framework makes statistical inference on big data possible and speed up\nthe penetration of probabilistic programming into the data engineering domain.\n",
        "  We report on measurements on Y1-xPrxBa2Cu3O7-delta single crystals, with x\nvarying from 0 to 2.4%. The upper and the lower critical fields, Hc2 and Hc1,\nthe Ginzburg-Landau parameter and the critical current density, Jc(B), were\ndetermined from magnetization measurements and the effective media approach\nscaling method. We present the influence of Pr substitution on the pinning\nforce density as well as on the trapped field profiles analyzed by Hall probe\nscanning.\n",
        "  The study of young massive clusters can provide key information for the\nformation of globular clusters, as they are often considered analogues. A\ncurrently unanswered question in this field is how long these massive clusters\nremain embedded in their natal gas, with important implications for the\nformation of multiple populations that have been used to explain phenomena\nobserved in globular clusters. We present an analysis of ages and masses of the\nyoung massive cluster population of M83. Through visual inspection of the\nclusters, and comparison of their SEDs and position in colour-colour space, the\nclusters are all exposed (no longer embedded) by < 4 Myr, most likely less,\nindicating that current proposed age spreads within older clusters are\nunlikely. We also present several methods of constraining the ages of very\nyoung massive clusters. This can often be difficult using SED fitting due to a\nlack of information to disentangle age-extinction degeneracies and possible\ninaccurate assumptions in the models used for the fitting. The individual\nmorphology of the Halpha around each cluster has a significant effect on the\nmeasured fluxes, which contributes to inaccuracies in the age estimates for\nclusters younger than 10 Myr using SED fitting. This is due to model\nuncertainties and aperture effects. Our methods to help constrain ages of young\nclusters include using the near-infrared and spectral features, such as\nWolf-Rayet stars.\n",
        "  This paper provides an overview and a tutorial of molecular clock dating\nusing MrBayes, which is a software for Bayesian inference of phylogeny. Two\nmodern approaches, total-evidence dating and node dating, are demonstrated\nusing a dataset of Hymenoptera with molecular sequences and morphological\ncharacters. The similarity and difference of the two methods are compared and\ndiscussed. Besides, a non-clock analysis is performed on the same dataset to\ncompare with the molecular clock dating analyses.\n",
        "  Genome-wide patterns of genetic divergence reveal mechanisms of adaptation\nunder gene flow. Empirical data show that divergence is mostly concentrated in\nnarrow genomic regions. This pattern may arise because differentiated loci\nprotect nearby mutations from gene flow, but recent theory suggests this\nmechanism is insufficient to explain the emergence of concentrated\ndifferentiation during biologically realistic timescales. Critically, earlier\ntheory neglects an inevitable consequence of genetic drift: stochastic loss of\nlocal genomic divergence. Here we demonstrate that the rate of stochastic loss\nof weak local differentiation increases with recombination distance to a\nstrongly diverged locus and, above a critical recombination distance, local\nloss is faster than local `gain' of new differentiation. Under high migration\nand weak selection this critical recombination distance is much smaller than\nthe total recombination distance of the genomic region under selection.\nConsequently, divergence between populations increases by net gain of new\ndifferentiation within the critical recombination distance, resulting in\ntightly-linked clusters of divergence. The mechanism responsible is the balance\nbetween stochastic loss and gain of weak local differentiation, a mechanism\nacting universally throughout the genome. Our results will help to explain\nempirical observations and lead to novel predictions regarding changes in\ngenomic architectures during adaptive divergence.\n",
        "  In this manuscript, we investigated the accuracy of the entire chain of phase\nspace files generated by the TrueBeam VirtuaLinac system in clinical treatment\nplans. Good agreement was obtained between the MC simulation and the clinical\ngolden data for both flattened and FFF beams. The relative magnitude of\nagreement between MC and golden beam data presented in this research may assist\na physicist in terms of the percent dose deviation that one may expect when\nusing MC data for verification purposes.\n",
        "  Evolutionary graph theory is a well established framework for modelling the\nevolution of social behaviours in structured populations. An emerging consensus\nin this field is that graphs that exhibit heterogeneity in the number of\nconnections between individuals are more conducive to the spread of cooperative\nbehaviours. In this article we show that such a conclusion largely depends on\nthe individual-level interactions that take place. In particular, averaging\npayoffs garnered through game interactions rather than accumulating the payoffs\ncan altogether remove the cooperative advantage of heterogeneous graphs while\nsuch a difference does not affect the outcome on homogeneous structures. In\naddition, the rate at which game interactions occur can alter the evolutionary\noutcome. Less interactions allow heterogeneous graphs to support more\ncooperation than homogeneous graphs, while higher rates of interactions make\nhomogeneous and heterogeneous graphs virtually indistinguishable in their\nability to support cooperation. Most importantly, we show that common measures\nof evolutionary advantage used in homogeneous populations, such as a comparison\nof the fixation probability of a rare mutant to that of the resident type, are\nno longer valid in heterogeneous populations. Heterogeneity causes a bias in\nwhere mutations occur in the population which affects the mutant's fixation\nprobability. We derive the appropriate measures for heterogeneous populations\nthat account for this bias.\n",
        "  Events of various kinds are mentioned and discussed in text documents,\nwhether they are books, news articles, blogs or microblog feeds. The paper\nstarts by giving an overview of how events are treated in linguistics and\nphilosophy. We follow this discussion by surveying how events and associated\ninformation are handled in computationally. In particular, we look at how\ntextual documents can be mined to extract events and ancillary information.\nThese days, it is mostly through the application of various machine learning\ntechniques. We also discuss applications of event detection and extraction\nsystems, particularly in summarization, in the medical domain and in the\ncontext of Twitter posts. We end the paper with a discussion of challenges and\nfuture directions.\n",
        "  The field of machine translation faces an under-recognized problem because of\ninconsistency in the reporting of scores from its dominant metric. Although\npeople refer to \"the\" BLEU score, BLEU is in fact a parameterized metric whose\nvalues can vary wildly with changes to these parameters. These parameters are\noften not reported or are hard to find, and consequently, BLEU scores between\npapers cannot be directly compared. I quantify this variation, finding\ndifferences as high as 1.8 between commonly used configurations. The main\nculprit is different tokenization and normalization schemes applied to the\nreference. Pointing to the success of the parsing community, I suggest machine\ntranslation researchers settle upon the BLEU scheme used by the annual\nConference on Machine Translation (WMT), which does not allow for user-supplied\nreference processing, and provide a new tool, SacreBLEU, to facilitate this.\n",
        "  We propose a sampling scheme on the sphere and develop a corresponding\nspherical harmonic transform (SHT) for the accurate reconstruction of the\ndiffusion signal in diffusion magnetic resonance imaging (dMRI). By exploiting\nthe antipodal symmetry, we design a sampling scheme that requires the optimal\nnumber of samples on the sphere, equal to the degrees of freedom required to\nrepresent the antipodally symmetric band-limited diffusion signal in the\nspectral (spherical harmonic) domain. Compared with existing sampling schemes\non the sphere that allow for the accurate reconstruction of the diffusion\nsignal, the proposed sampling scheme reduces the number of samples required by\na factor of two or more. We analyse the numerical accuracy of the proposed SHT\nand show through experiments that the proposed sampling allows for the accurate\nand rotationally invariant computation of the SHT to near machine precision\naccuracy.\n",
        "  A potential application for spectral computed tomography (CT) with\nmulti-energy-window photon-counting detectors is quantitative medical imaging\nwith K-edge contrast agents. Image reconstruction for spectral CT with such\ncontrast agents necessitates expression of the X-ray linear attenuation map in\nat least three expansion functions, for example, bone/water/K-edge-material or\nphoto-electric- process/Compton-process/K-edge-material. The use of three\nexpansion functions can result in slow convergence for iterative image\nreconstruction (IIR) algorithms applied to spectral CT. We propose a\nblock-diagonal step-preconditioner for use with a primal-dual iterative image\nreconstruction framework that we have been developing for spectral CT. We\ndemonstrate the advantage of the new step-preconditioner on a sensitive\nspectral CT simulation where the test object has low concentration of\nGadolinium (Gd) contrast agent and the X-ray attenuation map is represented by\nthree materials - PMMA, a soft-tissue equivalent, Aluminum, a bone equivalent,\nand Gd.\n",
        "  An original experimental procedure is presented to measure the mechanical\ninteraction between tongue and teeth and palate during speech production. It\nconsists in using edentulous people as subjects and to insert pressure sensors\nin the structure of a replication of their dental prosthesis. This is assumed\nto induce no speech production perturbation for subjects who are used to speak\nwith their prosthesis. Data collected from 4 subjects of French demonstrate the\nusability of the system.\n",
        "  Based on the model of the space $Pol_3(n)$ of polygons in $R^3$ with limited\nnumber of vertex, which was proposed by Jean-Claude Hausmann and Allen Knutson,\nand developed by several authors: Jason Cantarella, Alexander Y. Grosberg,\nRobert Kusner, and Clayton Shonkwiler, we prove that there exists an isometric\nisotopy of $Pol_3(n)$, $n=7$, into itself, which transforms an arbitrary\npolygon to its mirror copy, and, additionally, preserves lengths of projections\nof polygons into the two coordinate planes, and keeps projection of polygons\nonto the line. The proof is based on elementary arguments with Cayley numbers.\nA possible generalization of the statement for greater $n$ is related with a\ntheorem by I.James on strong Kervaire invariants in stable homotopy of spheres.\n",
        "  General description of an on-line procedure of calibration for IGRT (Image\nGuided Radiotherapy) is given. The algorithm allows to improve targeting cancer\nby estimating its position in space and suggests appropriate correction of the\nposition of the patient. The description is given in the Geometric Algebra\nlanguage which significantly simplifies calculations and clarifies\npresentation.\n",
        "  Theory of self induced resonances in asymmetric two-junction interferometer\ndevice is presented. In real devices it is impossible to have an ideal\ninterferometer free of imperfections. Thus, we extended previous theoretical\napproaches introducing a model which contains several asymmetries: Josephson\ncurrent $\\epsilon$, capacitances $\\chi$ and dissipation $\\rho$ presented in an\nequivalent circuit. Moreover, non conventional symmetry of the order parameter\nin high temperature superconducting quantum interference devices forced us to\ninclude phase asymmetries. Therefore, the model has been extended to the case\nof $\\pi$-shift interferometers, where a phase shift is present in one of the\njunctions.\n",
        "  We present deep number counts at 450 and 850 $\\mu$m using the SCUBA-2 camera\non the James Clerk Maxwell Telescope. We combine data for six lensing cluster\nfields and three blank fields to measure the counts over a wide flux range at\neach wavelength. Thanks to the lensing magnification, our measurements extend\nto fluxes fainter than 1 mJy and 0.2 mJy at 450 $\\mu$m and 850 $\\mu$m,\nrespectively. Our combined data highly constrain the faint end of the number\ncounts. Integrating our counts shows that the majority of the extragalactic\nbackground light (EBL) at each wavelength is contributed by faint sources with\n$L_{\\rm IR} < 10^{12} L_{\\odot }$, corresponding to luminous infrared galaxies\n(LIRGs) or normal galaxies. By comparing our result with the 500 $\\mu$m\nstacking of $K$-selected sources from the literature, we conclude that the\n$K$-selected LIRGs and normal galaxies still cannot fully account for the EBL\nthat originates from sources with $L_{\\rm IR} < 10^{12} L_{\\odot }$. This\nsuggests that many faint submillimeter galaxies may not be included in the UV\nstar formation history. We also explore the submillimeter flux ratio between\nthe two bands for our 450 $\\mu$m and 850 $\\mu$m selected sources. At 850\n$\\mu$m, we find a clear relation between the flux ratio and the observed flux.\nThis relation can be explained by a redshift evolution, where galaxies at\nhigher redshifts have higher luminosities and star formation rates. In\ncontrast, at 450 $\\mu$m, we do not see a clear relation between the flux ratio\nand the observed flux.\n",
        "  Magnetic resonance spectroscopic imaging (SI) is a unique imaging technique\nthat provides biochemical information from in vivo tissues. The 1H spectra\nacquired from several spatial regions are quantified to yield metabolite\nconcentrations reflective of tissue metabolism. However, since these\nmetabolites are found in tissues at very low concentrations, SI is often\nacquired with limited spatial resolution. In this work we test the hypothesis\nthat deep learning is able to upscale low resolution SI, together with the\nT1-weighted (T1w) image, to reconstruct high resolution SI. We report a novel\ndensely connected Unet (D-Unet) architecture capable of producing\nsuper-resolution spectroscopic images. The inputs for the D-UNet are the T1w\nimage and the low resolution SI image while the output is the high resolution\nSI. The results of the D-UNet are compared both qualitatively and\nquantitatively to simulated and in vivo high resolution SI. It is found that\nthis deep learning approach can produce high quality spectroscopic images and\nreconstruct entire 1H spectra from low resolution acquisitions, which can\ngreatly advance the current SI workflow.\n",
        "  We describe the natural gluing map on sutured Floer homology which is induced\nby the inclusion of one sutured manifold (M',\\Gamma') into a larger sutured\nmanifold (M,\\Gamma), together with a contact structure on M-M'. As an\napplication of this gluing map, we produce a (1+1)-dimensional TQFT by\ndimensional reduction and study its properties.\n",
        "  We introduce a new set of eight Milky Way-sized cosmological simulations\nperformed using the AMR code ART + Hydrodynamics in a LCDM cosmology. The set\nof zoom-in simulations covers present-day virial masses in the 0.83-1.56 x\n10^12 msun range and is carried out with our simple but effective deterministic\nstar formation (SF) and ``explosive' stellar feedback prescriptions. The work\nis focused on showing the goodness of the simulated set of ``field' Milky\nWay-sized galaxies. Our results are as follows. (a) The circular velocity\ncurves of our simulated galaxies are nearly flat. (b) Runs ending with a\nsignificant disk component, for their stellar masses, have V_max, radius, SF\nrate, gas fraction, and specific angular momentum values consistent with\nobservations of late-type galaxies. (C) The two most spheroid-dominated\ngalaxies formed in halos with late active merger histories, but other run that\nends also as spheroid-dominated, never had major mergers. (d) Our simulations\nare consistent with the empirical stellar-to-halo mass correlation, and those\nthat end as disk-dominated, evolve mostly along the low-mass branch of this\ncorrelation. (e) Moreover, since the last 6.5-10 Gyr, the baryonic/stellar and\nhalo mass growth histories are proportional. (f) Within Rvir ~ 25-50% of the\nbaryons are missed. (g) The z ~ 0 gas velocity dispersion profiles, sigma_z(r),\nare nearly flat and can be mostly explained by the kinetic energy injected by\nstars. (h) The average values of sigma_z increase at higher redshifts,\nfollowing roughly the shape of the SF history.\n",
        "  Evolutionary game dynamics describes the spreading of successful strategies\nin a population of reproducing individuals. Typically, the microscopic\ndefinition of strategy spreading is stochastic, such that the dynamics becomes\ndeterministic only in infinitely large populations. Here, we introduce a new\nmicroscopic birth--death process that has a fully deterministic strong\nselection limit in well--mixed populations of any size. Additionally, under\nweak selection, from this new process the frequency dependent Moran process is\nrecovered. This makes it a natural extension of the usual evolutionary dynamics\nunder weak selection. We find simple expressions for the fixation probabilities\nand average fixation times of the new process in evolutionary games with two\nplayers and two strategies. For cyclic games with two players and three\nstrategies, we show that the resulting deterministic dynamics crucially depends\non the initial condition in a non--trivial way.\n",
        "  Purpose - The purpose of this paper is to describe finite element modelling\nfor fracture and fatigue behaviour of zirconia toughened alumina\nmicrostructures. Design/methodology/approach - A two-dimensional finite element\nmodel is developed with an actual $Al{_2}O{_3}$ - 10 vol% $ZrO{_2}$\nmicrostructure. A bilinear, time-independent cohesive zone law is implemented\nfor describing fracture behaviour of grain boundaries. Simulation conditions\nare similar to those found at contact between a head and a cup of hip\nprosthesis. Residual stresses arisen from the mismatch of thermal coefficient\nbetween grains are determined. Then, effects of a micro-void and contact stress\nmagnitude are investigated with models containing residual stresses. For the\npurpose of simulating fatigue behaviour, cyclic loadings are applied to the\nmodels. Findings - Results show that crack density is gradually increased with\nincreasing magnitude of contact stress or number of fatigue cycles. It is also\nidentified that a micro-void brings about the increase of crack density rate.\nSocial implications - This paper is the first step for predicting the lifetime\nof ceramic implants. The social implications would appear in the next few years\nabout health issues. Originality/value - This proposed finite element method\nallows describing fracture and fatigue behaviours of alumina-zirconia\nmicrostructures for hip prosthesis, provided that a microstructure image is\navailable.\n",
        "  Let K be the space of long j-knots in R^n. In this paper we introduce a graph\ncomplex D and a linear map I from D to the de Rham complex of K via\nconfiguration space integral, and prove that (1) when both n>j>=3 are odd, the\nmap I is a cochain map if restricted to graphs with at most one loop component,\n(2) when n-j>=2 is even, the map I is a cochain map if restricted to tree\ngraphs, and (3) when n-j >=3 is odd, the map I added a correction term produces\na (2n-3j-3)-cocycle of K which gives a new formulation of the Haefliger\ninvariant when n=6k, j=4k-1 for some k.\n",
        "  We demonstrate that replacing an LSTM encoder with a self-attentive\narchitecture can lead to improvements to a state-of-the-art discriminative\nconstituency parser. The use of attention makes explicit the manner in which\ninformation is propagated between different locations in the sentence, which we\nuse to both analyze our model and propose potential improvements. For example,\nwe find that separating positional and content information in the encoder can\nlead to improved parsing accuracy. Additionally, we evaluate different\napproaches for lexical representation. Our parser achieves new state-of-the-art\nresults for single models trained on the Penn Treebank: 93.55 F1 without the\nuse of any external data, and 95.13 F1 when using pre-trained word\nrepresentations. Our parser also outperforms the previous best-published\naccuracy figures on 8 of the 9 languages in the SPMRL dataset.\n",
        "  Most current word prediction systems make use of n-gram language models (LM)\nto estimate the probability of the following word in a phrase. In the past\nyears there have been many attempts to enrich such language models with further\nsyntactic or semantic information. We want to explore the predictive powers of\nLatent Semantic Analysis (LSA), a method that has been shown to provide\nreliable information on long-distance semantic dependencies between words in a\ncontext. We present and evaluate here several methods that integrate LSA-based\ninformation with a standard language model: a semantic cache, partial\nreranking, and different forms of interpolation. We found that all methods show\nsignificant improvements, compared to the 4-gram baseline, and most of them to\na simple cache model as well.\n",
        "  Scalable and highly available systems often require data stores that offer\nweaker consistency guarantees than traditional relational databases systems.\nThe correctness of these applications highly depends on the resilience of the\napplication model against data inconsistencies. In particular regarding\napplication security, it is difficult to determine which inconsistencies can be\ntolerated and which might lead to security breaches.\n  In this paper, we discuss the problem of how to develop an access control\nlayer for applications using weakly consistent data stores without loosing the\nperformance benefits gained by using weaker consistency models. We present\nACGreGate, a Java framework for implementing correct access control layers for\napplications using weakly consistent data stores. Under certain requirements on\nthe data store, ACGreGate ensures that the access control layer operates\ncorrectly with respect to dynamically adaptable security policies. We used\nACGreGate to implement the access control layer of a student management system.\nThis case study shows that practically useful security policies can be\nimplemented with the framework incurring little overhead. A comparison with a\nsetup using a centralized server shows the benefits of using ACGreGate for\nscalability of the service to geo-scale.\n",
        "  Recently, a new organic superconductor, K-intercalated Picene with high\ntransition temperatures $T_c$ (up to 18\\,K) has been discovered. We have\ninvestigated the electronic properties of the undoped relative, solid picene,\nusing a combination of experimental and theoretical methods. Our results\nprovide detailed insight into the occuopied and unoccupied electronic states.\n",
        "  In this study, we introduce a new approach for learning language models by\ntraining them to estimate word-context pointwise mutual information (PMI), and\nthen deriving the desired conditional probabilities from PMI at test time.\nSpecifically, we show that with minor modifications to word2vec's algorithm, we\nget principled language models that are closely related to the well-established\nNoise Contrastive Estimation (NCE) based language models. A compelling aspect\nof our approach is that our models are trained with the same simple negative\nsampling objective function that is commonly used in word2vec to learn word\nembeddings.\n",
        "  Fungal symbionts are often overlooked in studies of plant invasion.\nNevertheless, their role could be essential to the competitive success of the\ninvader. We studied fungal endophytes in the widespread invasive Centaurea\nstoebe (common knapweed). A preliminary experiment showed that endophytes in\nroots of C. stoebe significantly reduced the biomass of evolutionarily na\\\"ive\nneighbours (Festuca idahoensis), compared to endophyte-free C. stoebe. In the\nmain experiment non-clavicipitaceous endophytes belonging to six phylotypes,\nwere employed as root inoculants. Each of these endophytes again reduced the\ngrowth of na\\\"ive neighbours (F. idahoensis); and remarkably, each also\nincreased the growth of adapted neighbours (F. ovina) that were tested for the\nfirst time. Four of the six endophytes caused C. stoebe to gain a competitive\nadvantage over its na\\\"ive neighbour that was significantly greater than the\nendophyte-free C. stoebe over that same neighbour. However, endophyte-free C.\nstoebe had no greater competitive advantage over F. idahoensis than it had over\nF. ovina. Therefore, plant-plant interactions were dramatically affected by the\npresence of endophytes in a way that would favor invasion.\n",
        "  In Distributional Semantic Models (DSMs), Vector Cosine is widely used to\nestimate similarity between word vectors, although this measure was noticed to\nsuffer from several shortcomings. The recent literature has proposed other\nmethods which attempt to mitigate such biases. In this paper, we intend to\ninvestigate APSyn, a measure that computes the extent of the intersection\nbetween the most associated contexts of two target words, weighting it by\ncontext relevance. We evaluated this metric in a similarity estimation task on\nseveral popular test sets, and our results show that APSyn is in fact highly\ncompetitive, even with respect to the results reported in the literature for\nword embeddings. On top of it, APSyn addresses some of the weaknesses of Vector\nCosine, performing well also on genuine similarity estimation.\n",
        "  In this work, a new indexing technique of data streams called BSTree is\nproposed. This technique uses the method of data discretization, SAX [4], to\nreduce online the dimensionality of data streams. It draws on Btree to build\nthe index and finally uses an LRV (least Recently visited) pruning technique to\nrid the index structure from data whose last visit time exceeds a threshold\nvalue and thus minimizes response time for similarity search queries.\n",
        "  In response to a published assertion to the contrary, this paper briefly\nreviews many studies that document remote wounding effects of ballistic\npressure waves including experiments in pigs and dogs that find brain injury\nresulting from animal models shot in the thigh and case studies in humans that\ndocument both remote brain and spinal cord injuries ascribed to ballistic\npressure waves.\n",
        "  Purpose: To determine whether alternative HDR prostate brachytherapy catheter\npatterns can result in improved dose distributions while providing better\naccess and reducing trauma.\n  Methods: Prostate HDR brachytherapy uses a grid of parallel needle positions\nto guide the catheter insertion. This geometry does not easily allow the\nphysician to avoid piercing the critical structures near the penile bulb nor\ndoes it provide position flexibility in the case of pubic arch interference. On\nCT data from ten previously-treated patients new catheters were digitized\nfollowing three catheter patterns: conical, bi-conical, and fireworks. The\nconical patterns were used to accommodate a robotic delivery using a single\nentry point. The bi-conical and fireworks patterns were specifically designed\nto avoid the critical structures near the penile bulb. For each catheter\ndistribution, a plan was optimized with the inverse planning algorithm, IPSA,\nand compared with the plan used for treatment. Irrelevant of catheter geometry,\na plan must fulfill the RTOG-0321 dose criteria for target dose coverage.\n  Results: Thirty plans from ten patients were optimized. All non-standard\npatterns fulfilled the RTOG criteria when the clinical plan did. In some cases,\nthe dose distribution was improved by better sparing the organs-at-risk.\n  Conclusion: Alternative catheter patterns can provide the physician with\nadditional ways to treat patients previously considered unsuited for\nbrachytherapy treatment (pubic arch interference) and facilitate robotic\nguidance of catheter insertion. In addition, alternative catheter patterns may\ndecrease toxicity by avoidance of the critical structures near the penile bulb\nwhile still fulfilling the RTOG criteria.\n",
        "  Provenance refers to the documentation of an object's lifecycle. This\ndocumentation (often represented as a graph) should include all the information\nnecessary to reproduce a certain piece of data or the process that led to it.\nIn a dynamic world, as data changes, it is important to be able to get a piece\nof data as it was, and its provenance graph, at a certain point in time.\nSupporting time-aware provenance querying is challenging and requires: (i)\nexplicitly representing the time information in the provenance graphs, and (ii)\nproviding abstractions and efficient mechanisms for time-aware querying of\nprovenance graphs over an ever growing volume of data. The existing provenance\nmodels treat time as a second class citizen (i.e. as an optional annotation).\nThis makes time-aware querying of provenance data inefficient and sometimes\ninaccessible. We introduce an extended provenance graph model to explicitly\nrepresent time as an additional dimension of provenance data. We also provide a\nquery language, novel abstractions and efficient mechanisms to query and\nanalyze timed provenance graphs. The main contributions of the paper include:\n(i) proposing a Temporal Provenance Model (TPM) as a timed provenance model;\nand (ii) introducing two concepts of timed folder, as a container of related\nset of objects and their provenance relationship over time, and timed paths, to\nrepresent the evolution of objects tracing information over time, for analyzing\nand querying TPM graphs. We have implemented the approach on top of FPSPARQL, a\nquery engine for large graphs, and have evaluated for querying TPM models. The\nevaluation shows the viability and efficiency of our approach.\n",
        "  When two superconductors are coupled via an insulator the phase difference\nbetween them is expected to be zero in the ground state and such Josephson\njunctions are known as \"0\" type Josephson junctions . But in the presence of\nmagnetic impurities in the barrier or ferromagnetism, the phase difference in\nthe ground state could become pi; such Josephson junctions are known as \"pi\"\ntype Josephson junctions1-4. Experimental results have confirmed the\nobservation of a 0 to pi transition as a function of temperature in Josephson\njunctions such as Nb/Cu1-xNix/Nb based on ferromagnetic weak links with low\nexchange energy5. Recently, observation of possible magnetic behaviour in\noxides has been reported6,7, and it would, therefore, be interesting to study\nthe Josephson coupling in YBCO/gallium oxide multilayers. This work reports the\nobservation of a very anomalous superconducting behaviour in YBCO/gallium oxide\nmultilayers. A transition to the superconducting state has been observed with\nan enhanced superconducting transition temperature of 98 K in the YBCO/gallium\noxide multilayer followed by a second transition temperature at 90 K with the\nmultilayer losing its superconductivity at intermediate temperatures. This\nbehaviour is explained by assuming that the Josephson coupling between the\nsuperconducting YBCO layers in the multilayer changes from pi type to 0 type.\nThis is the first experimental observation of the 0 to pi transition in\nHTSC/gallium oxide/HTSC thin film structures.\n",
        "  The effects of absence of inversion symmetry on superconducting states are\ninvestigated theoretically. In particular we focus on the noncentrosymmetric\ncompounds which have the cubic symmetry $O$ like Li$_2$Pt$_3$B. An appropriate\nand isotropic spin-orbital interaction is added in the Hamiltonian and it acts\nlike a magnetic monopole in the momentum space. The consequent pairing\nwavefunction has an additional triplet component in the pseudospin space, and a\nZeeman magnetic field $\\bf{B}$ can induce a collinear supercurrent $\\bf{J}$\nwith a coefficient $\\kappa(T)$. The effects of anisotropy embedded in the cubic\nsymmetry and the nodal superconducting gap function on $\\kappa(T)$ are also\nconsidered. From the macroscopic perspectives, the pair of mutually induced\n$\\bf{J}$ and magnetization ${\\bf{M}}$ can affect the distribution of magnetic\nfield in such noncentrosymmetric superconductors, which is studied through\nsolving the Maxwell equation in the Meissner geometry as well as the case of a\nsingle vortex line. In both cases, magnetic fields perpendicular to the\nexternal ones emerge as a signature of the broken symmetry.\n",
        "  The present study is part of a broader programme, exploring the possibility\nof involving the Microsoft Kinect$^{\\rm TM}$ sensor in the analysis of human\nmotion. We examine the output obtained from the two available versions of this\nsensor in relation to the variability of the estimates of the lengths of eight\nbones belonging to the subject's extremities: of the humerus (upper arm), ulna\n(lower arm, forearm), femur (upper leg), and tibia (lower leg, shank). Large\nsystematic effects in the output of the two sensors have been observed.\n",
        "  We describe a family of 4-dimensional hyperbolic orbifolds, constructed by\ndeforming an infinite volume orbifold obtained from the ideal, hyperbolic\n24-cell by removing two walls. This family provides an infinite number of\ninfinitesimally rigid, infinite covolume, geometrically finite discrete\nsubgroups of the isometry group of hyperbolic 4-space. It also leads to finite\ncovolume Coxeter groups which are the homomorphic image of the group of\nreflections in the hyperbolic 24-cell. The examples are constructed very\nexplicitly, both from an algebraic and a geometric point of view. The method\nused can be viewed as a 4-dimensional, but infinite volume, analog of\n3-dimensional hyperbolic Dehn filling.\n",
        "  Dual-energy computed tomography (DECT) has shown great potential and\npromising applications in advanced imaging fields for its capabilities of\nmaterial decomposition. However, image reconstructions and decompositions under\nsparse views dataset suffers severely from multi factors, such as\ninsufficiencies of data, appearances of noise, and inconsistencies of\nobservations. Under sparse views, conventional filtered back-projection type\nreconstruction methods fails to provide CT images with satisfying quality.\nMoreover, direct image decomposition is unstable and meet with noise boost even\nwith full views dataset. This paper proposes an iterative image reconstruction\nalgorithm and a practical image domain decomposition method for DECT. On one\nhand, the reconstruction algorithm is formulated as an optimization problem,\nwhich containing total variation regularization term and data fidelity term.\nThe alternating direction method is utilized to design the corresponding\nalgorithm which shows faster convergence speed compared with the existing ones.\nOn the other hand, the image domain decomposition applies the penalized least\nsquare (PLS) estimation on decomposing the material mappings. The PLS includes\nlinear combination term and the regularization term which enforces the\nsmoothness on estimation images. The authors implement and evaluate the\nproposed joint method on real DECT projections and compare the method with\ntypical and state-of-the-art reconstruction and decomposition methods. The\nexperiments on dataset of an anthropomorphic head phantom show that our methods\nhave advantages on noise suppression and edge reservation, without blurring the\nfine structures in the sinus area in the phantom. Compared to the existing\napproaches, our method achieves a superior performance on DECT imaging with\nrespect to reconstruction accuracy and decomposition quality.\n",
        "  Most work in relation extraction forms a prediction by looking at a short\nspan of text within a single sentence containing a single entity pair mention.\nThis approach often does not consider interactions across mentions, requires\nredundant computation for each mention pair, and ignores relationships\nexpressed across sentence boundaries. These problems are exacerbated by the\ndocument- (rather than sentence-) level annotation common in biological text.\nIn response, we propose a model which simultaneously predicts relationships\nbetween all mention pairs in a document. We form pairwise predictions over\nentire paper abstracts using an efficient self-attention encoder. All-pairs\nmention scores allow us to perform multi-instance learning by aggregating over\nmentions to form entity pair representations. We further adapt to settings\nwithout mention-level annotation by jointly training to predict named entities\nand adding a corpus of weakly labeled data. In experiments on two Biocreative\nbenchmark datasets, we achieve state of the art performance on the Biocreative\nV Chemical Disease Relation dataset for models without external KB resources.\nWe also introduce a new dataset an order of magnitude larger than existing\nhuman-annotated biological information extraction datasets and more accurate\nthan distantly supervised alternatives.\n",
        "  Let L be a link in an thickened annulus. We specify the embedding of this\nannulus in the three sphere, and consider its complement thought of as the axis\nto L. In the right circumstances this axis lifts to a null-homologous knot in\nthe double branched cover of the three sphere, branched over the embedded copy\nof L. This paper shows that the knot Floer homology of this lift, with mod 2\ncoefficients, can be computed from a spectral sequence starting at a type of\nKhovanov homology already described by Asaeda, Przytycki, and Sikora. We extend\nthe known results about this type of Khovanov homology, and use it to provide a\nvery simple explanation of the case when L is alternating for the obvious\nprojection.\n",
        "  TimeML is an XML-based schema for annotating temporal information over\ndiscourse. The standard has been used to annotate a variety of resources and is\nfollowed by a number of tools, the creation of which constitute hundreds of\nthousands of man-hours of research work. However, the current state of\nresources is such that many are not valid, or do not produce valid output, or\ncontain ambiguous or custom additions and removals. Difficulties arising from\nthese variances were highlighted in the TempEval-3 exercise, which included its\nown extra stipulations over conventional TimeML as a response.\n  To unify the state of current resources, and to make progress toward easy\nadoption of its current incarnation ISO-TimeML, this paper introduces\nTimeML-strict: a valid, unambiguous, and easy-to-process subset of TimeML. We\nalso introduce three resources -- a schema for TimeML-strict; a validator tool\nfor TimeML-strict, so that one may ensure documents are in the correct form;\nand a repair tool that corrects common invalidating errors and adds\ndisambiguating markup in order to convert documents from the laxer TimeML\nstandard to TimeML-strict.\n",
        "  In today's competitive scenario in corporate world, \"Customer Retention\"\nstrategy in Customer Relationship Management (CRM) is an increasingly pressed\nissue. Data mining techniques play a vital role in better CRM. This paper\nattempts to bring a new perspective by focusing the issue of data mining\napplications, opportunities and challenges in CRM. It covers the topic such as\ncustomer retention, customer services, risk assessment, fraud detection and\nsome of the data mining tools which are widely used in CRM.\n",
        "  In this paper, we empirically explore the effects of various kinds of skip\nconnections in stacked bidirectional LSTMs for sequential tagging. We\ninvestigate three kinds of skip connections connecting to LSTM cells: (a) skip\nconnections to the gates, (b) skip connections to the internal states and (c)\nskip connections to the cell outputs. We present comprehensive experiments\nshowing that skip connections to cell outputs outperform the remaining two.\nFurthermore, we observe that using gated identity functions as skip mappings\nworks pretty well. Based on this novel skip connections, we successfully train\ndeep stacked bidirectional LSTM models and obtain state-of-the-art results on\nCCG supertagging and comparable results on POS tagging.\n",
        "  In this paper, we study stable equivalence of exotically knotted surfaces in\n4-manifolds, surfaces that are topologically isotopic but not smoothly\nisotopic. We prove that any pair of embedded surfaces in the same homology\nclass become smoothly isotopic after stabilizing them by handle additions in\nthe ambient 4-manifold, which can moreover assumed to be attached in a standard\nway (locally and unknottedly) in many favorable situations. In particular, any\nexotically knotted pair of surfaces with cyclic fundamental group complements\nbecome smoothly isotopic after a same number of standard stabilizations -\nanalogous to C.T.C. Wall's celebrated result on the stable equivalence of\nsimply-connected 4-manifolds. We moreover show that all constructions of exotic\nknottings of surfaces we are aware of, which display a good variety of\ntechniques and ideas, produce surfaces that become smoothly isotopic after a\nsingle stabilization.\n",
        "  We describe a simple approach to semantic parsing based on a tensor product\nkernel. We extract two feature vectors: one for the query and one for each\ncandidate logical form. We then train a classifier using the tensor product of\nthe two vectors. Using very simple features for both, our system achieves an\naverage F1 score of 40.1% on the WebQuestions dataset. This is comparable to\nmore complex systems but is simpler to implement and runs faster.\n",
        "  We summarize the accomplishments of a multi-disciplinary workshop exploring\nthe computational and scientific issues surrounding the discovery of linguistic\nunits (subwords and words) in a language without orthography. We study the\nreplacement of orthographic transcriptions by images and/or translated text in\na well-resourced language to help unsupervised discovery from raw speech.\n",
        "  Let C_T be the subgroup of the smooth knot concordance group generated by\ntopologically slice knots and let C_D be the subgroup generated by knots with\ntrivial Alexander polynomial. We prove the quotient C_T/C_D is infinitely\ngenerated, and uncover similar structure in the 3-dimensional rational spin\nbordism group. Our methods also lead to the construction of links that are\ntopologically, but not smoothly, concordant to boundary links.\n",
        "  By considering appropriate finite covering spaces of closed non-orientable\nsurfaces, we construct linear representations of their mapping class group\nwhich have finite index image in certain big arithmetic groups.\n",
        "  Different spatial objects that vary in their characteristics, such as\nmolecular biology and geography, are presented in spatial areas. Methods to\norganize, manage, and maintain those objects in a structured manner are\nrequired. Data mining raised different techniques to overcome these\nrequirements. There are many major tasks of data mining, but the mostly used\ntask is clustering. Data set within the same cluster share common features that\ngive each cluster its characteristics. In this paper, an implementation of\nApproximate kNN-based spatial clustering algorithm using the K-d tree is\nproposed. The major contribution achieved by this research is the use of the\nk-d tree data structure for spatial clustering, and comparing its performance\nto the brute-force approach. The results of the work performed in this paper\nrevealed better performance using the k-d tree, compared to the traditional\nbrute-force approach.\n",
        "  Many applications need to process massive streams of spatio-textual data in\nreal-time against continuous spatio-textual queries. For example, in\nlocation-aware ad targeting publish/subscribe systems, it is required to\ndisseminate millions of ads and promotions to millions of users based on the\nlocations and textual profiles of users. In this paper, we study indexing of\ncontinuous spatio-textual queries. There exist several related spatio-textual\nindexes that typically integrate a spatial index with a textual index. However,\nthese indexes usually have a high demand for main-memory and assume that the\nentire vocabulary of keywords is known in advance. Also, these indexes do not\nsuccessfully capture the variations in the frequencies of keywords across\ndifferent spatial regions and treat frequent and infrequent keywords in the\nsame way. Moreover, existing indexes do not adapt to the changes in workload\nover space and time. For example, some keywords may be trending at certain\ntimes in certain locations and this may change as time passes. This affects the\nindexing and searching performance of existing indexes significantly. In this\npaper, we introduce FAST, a Frequency-Aware Spatio-Textual index for continuous\nspatio-textual queries. FAST is a main-memory index that requires up to one\nthird of the memory needed by the state-of-the-art index. FAST does not assume\nprior knowledge of the entire vocabulary of indexed objects. FAST adaptively\naccounts for the difference in the frequencies of keywords within their\ncorresponding spatial regions to automatically choose the best indexing\napproach that optimizes the insertion and search times. Extensive experimental\nevaluation using real and synthetic datasets demonstrates that FAST is up to 3x\nfaster in search time and 5x faster in insertion time than the state-of-the-art\nindexes.\n",
        "  We present numerical simulations of molecular clouds (MCs) with\nself-consistent CO gas-phase and isotope chemistry in various environments. The\nsimulations are post-processed with a line radiative transfer code to obtain\n$^{12}$CO and $^{13}$CO emission maps for the $J=1\\rightarrow0$ rotational\ntransition. The emission maps are analysed with commonly used observational\nmethods, i.e. the $^{13}$CO column density measurement, the virial mass\nestimate and the so-called $X_{\\textrm{CO}}$ (also CO-to-H$_2$) conversion\nfactor, and then the inferred quantities (i.e. mass and column density) are\ncompared to the physical values. We generally find that most methods examined\nhere recover the CO-emitting H$_{2}$ gas mass of MCs within a factor of two\nuncertainty if the metallicity is not too low. The exception is the $^{13}$CO\ncolumn density method. It is affected by chemical and optical depth issues, and\nit measures both the true H$_{2}$ column density distribution and the molecular\nmass poorly. The virial mass estimate seems to work the best in the considered\nmetallicity and radiation field strength range, even when the overall virial\nparameter of the cloud is above the equilibrium value. This is explained by a\nsystematically lower virial parameter (i.e. closer to equilibrium) in the\nCO-emitting regions; in CO emission, clouds might seem (sub-)virial, even when,\nin fact, they are expanding or being dispersed. A single CO-to-H$_{2}$\nconversion factor appears to be a robust choice over relatively wide ranges of\ncloud conditions, unless the metallicity is low. The methods which try to take\nthe metallicity dependence of the conversion factor into account tend to\nsystematically overestimate the true cloud masses.\n",
        "  Searching for words in Sanskrit E-text is a problem that is accompanied by\ncomplexities introduced by features of Sanskrit such as euphonic conjunctions\nor sandhis. A word could occur in an E-text in a transformed form owing to the\noperation of rules of sandhi. Simple word search would not yield these\ntransformed forms of the word. Further, there is no search engine in the\nliterature that can comprehensively search for words in Sanskrit E-texts taking\neuphonic conjunctions into account. This work presents an optimal binary\nrepresentational schema for letters of the Sanskrit alphabet along with\nalgorithms to efficiently process the sandhi rules of Sanskrit grammar. The\nwork further presents an algorithm that uses the sandhi processing algorithm to\nperform a comprehensive word search on E-text.\n",
        "  We study the novel problem of finding new, prominent situational facts, which\nare emerging statements about objects that stand out within certain contexts.\nMany such facts are newsworthy---e.g., an athlete's outstanding performance in\na game, or a viral video's impressive popularity. Effective and efficient\nidentification of these facts assists journalists in reporting, one of the main\ngoals of computational journalism. Technically, we consider an ever-growing\ntable of objects with dimension and measure attributes. A situational fact is a\n\"contextual\" skyline tuple that stands out against historical tuples in a\ncontext, specified by a conjunctive constraint involving dimension attributes,\nwhen a set of measure attributes are compared. New tuples are constantly added\nto the table, reflecting events happening in the real world. Our goal is to\ndiscover constraint-measure pairs that qualify a new tuple as a contextual\nskyline tuple, and discover them quickly before the event becomes yesterday's\nnews. A brute-force approach requires exhaustive comparison with every tuple,\nunder every constraint, and in every measure subspace. We design algorithms in\nresponse to these challenges using three corresponding ideas---tuple reduction,\nconstraint pruning, and sharing computation across measure subspaces. We also\nadopt a simple prominence measure to rank the discovered facts when they are\nnumerous. Experiments over two real datasets validate the effectiveness and\nefficiency of our techniques.\n",
        "  Stack Overflow (SO) has been a great source of natural language questions and\ntheir code solutions (i.e., question-code pairs), which are critical for many\ntasks including code retrieval and annotation. In most existing research,\nquestion-code pairs were collected heuristically and tend to have low quality.\nIn this paper, we investigate a new problem of systematically mining\nquestion-code pairs from Stack Overflow (in contrast to heuristically\ncollecting them). It is formulated as predicting whether or not a code snippet\nis a standalone solution to a question. We propose a novel Bi-View Hierarchical\nNeural Network which can capture both the programming content and the textual\ncontext of a code snippet (i.e., two views) to make a prediction. On two\nmanually annotated datasets in Python and SQL domain, our framework\nsubstantially outperforms heuristic methods with at least 15% higher F1 and\naccuracy. Furthermore, we present StaQC (Stack Overflow Question-Code pairs),\nthe largest dataset to date of ~148K Python and ~120K SQL question-code pairs,\nautomatically mined from SO using our framework. Under various case studies, we\ndemonstrate that StaQC can greatly help develop data-hungry models for\nassociating natural language with programming language.\n",
        "  By Torelli topology the author understands aspects of the topology of\nsurfaces (potentially) relevant to the study of Torelli groups. The present\npaper is devoted to a new approach to the results of W. Vautaw about Dehn\nmulti-twists in Torelli groups and abelian subgroups of Torelli groups. The new\nproofs are more transparent and lead to stronger estimates of the rank of an\nabelian subgroup when some additional information is available. An unexpected\napplication of our methods is a new proof of the algebraic characterization of\nthe Dehn twist about separating circles and the Dehn-Johnson twists about\nbounding pairs of circles. This proof is much shorter than the original one and\nbypasses one of the main difficulties, which may be called the extension\nproblem, specific to Torelli groups as opposed to the Teichm\\\"uller modular\ngroups. The extension problem is discussed in details in the second paper of\nthis series.\n",
        "  We generalize an algorithm of Rudolph to establish that every link is\ntopologically concordant to a strongly quasipositive link.\n",
        "  We conduct a pilot investigation to determine the optimal combination of\ncolor and variability information to identify quasars in current and future\nmulti-epoch optical surveys. We use a Bayesian quasar selection algorithm\n(Richards et al. 2004) to identify 35,820 type 1 quasar candidates in a 239\nsquare degree field of the Sloan Digital Sky Survey (SDSS) Stripe 82, using a\ncombination of optical photometry and variability. Color analysis is performed\non 5-band single- and multi-epoch SDSS optical photometry to a depth of r\n~22.4. From these data, variability parameters are calculated by fitting the\nstructure function of each object in each band with a power law model using 10\nto >100 observations over timescales from ~1 day to ~8 years. Selection was\nbased on a training sample of 13,221 spectroscopically-confirmed type-1\nquasars, largely from the SDSS. Using variability alone, colors alone, and\ncombining variability and colors we achieve 91%, 93%, and 97% quasar\ncompleteness and 98%, 98%, and 97% efficiency respectively, with particular\nimprovement in the selection of quasars at 2.7<z<3.5 where quasars and stars\nhave similar optical colors. The 22,867 quasar candidates that are not\nspectroscopically confirmed reach a depth of i ~22.0; 21,876 (95.7%) are dimmer\nthan coadded i-band magnitude of 19.9, the cut off for spectroscopic follow-up\nfor SDSS on Stripe 82. Brighter than 19.9, we find 5.7% more quasar candidates\nwithout confirming spectra in sky regions otherwise considered complete. The\nresulting quasar sample has sufficient purity (and statistically correctable\nincompleteness) to produce a luminosity function comparable to those determined\nby spectroscopic investigations. We discuss improvements that can be made to\nthe process in preparation for performing similar photometric selection and\nscience on data from post-SDSS sky surveys.\n",
        "  This article proposes a method to extract dependency structures from\nphrase-structure level parsing with Interaction Grammars. Interaction Grammars\nare a formalism which expresses interactions among words using a polarity\nsystem. Syntactical composition is led by the saturation of polarities.\nInteractions take place between constituents, but as grammars are lexicalized,\nthese interactions can be translated at the level of words. Dependency\nrelations are extracted from the parsing process: every dependency is the\nconsequence of a polarity saturation. The dependency relations we obtain can be\nseen as a refinement of the usual dependency tree. Generally speaking, this\nwork sheds new light on links between phrase structure and dependency parsing.\n",
        "  Galaxies represent one of the preferred candidate sources to drive the\nreionization of the universe. Even as gains are made in mapping the galaxy UV\nluminosity density to z>6, significant uncertainties remain regarding the\nconversion to the implied ionizing emissivity. The relevant unknowns are the\nLyman-continuum (LyC) photon production efficiency xi_{ion} and the escape\nfraction f_{esc}. As we show here, the first of these unknowns is directly\nmeasureable in z=4-5 galaxies, based on the impact the Halpha line has on the\nobserved IRAC fluxes. By computing a LyC photon production rate from the\nimplied Halpha luminosities for a broad selection of z=4-5 galaxies and\ncomparing this against the dust-corrected UV-continuum luminosities, we provide\nthe first-ever direct estimates of the LyC photon production efficiency\nxi_{ion} for the z>~4 galaxy population. We find log_{10} xi_{ion}/[Hz/ergs] to\nhave a mean value of 25.27_{-0.03}^{+0.03} and 25.34_{-0.02}^{+0.02} for sub-L*\nz=4-5 galaxies adopting Calzetti and SMC dust laws, respectively. Reassuringly,\nboth values are consistent with standardly assumed xi_{ion}'s in reionization\nmodels, with a slight preference for higher xi_{ion}'s (by ~0.1 dex) adopting\nthe SMC dust law. A modest ~0.03-dex increase in these estimates would result\nif the escape fraction for ionizing photons is non-zero and galaxies dominate\nthe ionizing emissivity at z~4.4. High values of xi_{ion} (~25.5-25.8 dex) are\nderived for the bluest galaxies (beta<-2.3) in our samples, independent of dust\nlaw and consistent with results for a z=7.045 galaxy. Such elevated values of\nxi_{ion} would have important consequences, indicating that f_{esc} cannot be\nin excess of 13% unless the galaxy UV luminosity function does not extend down\nto -13 mag or the clumping factor is greater than 3. A low escape fraction\nwould fit well with the low rate of LyC leakage observed at z~3.\n",
        "  Quantifying interaction strength between species is of interest in food web\nstudies for understanding population dynamics. Theory has run ahead of\nexperiment in solving equations describing ecological systems (the\nLotka-Volterra equations, for example). Modeling approaches depend upon\nquantitative knowledge of interaction strengths among species, yet there are\nfew methods available for estimating strengths of interactions between species,\nespecially in the field. Competition and predation in fishes is traditionally\nstudied with a variety of methods, most requiring extensive sampling to\ndetermine stock densities or extensive stomach content or isotope analysis.\nAverage relative weight of fish species in an ecosystem can usually be\naccurately determined with much smaller sample sizes than required for accurate\ndetermination of stock densities. Correlations of mean annual relative weights\nmay be used to test hypotheses and establish interaction strengths regarding\ncompetition and predation among fishes. The trends are sufficiently suggestive\nto use a strong positive correlation as supporting evidence of competition and\npossibly indicative of the magnitude of competition if other evidence such as\nanalysis of stomach contents is also present. Likewise, a strong negative\ncorrelation might be interpreted as relevant when forming a hypothesis of\npredation and as supporting evidence of the magnitude of predation if other\nevidence is also present. In systems where regular weight and length survey\ndata are available, this method can suggest the likely strength of competitive\nand predatory relationships and may be more cost effective than stomach content\nor isotope analysis.\n",
        "  We consider hyperbolic 3-manifolds with either non-empty compact geodesic\nboundary, or some toric cusps, or both. For any such M we analyze what portion\nof the volume of M can be recovered by inserting in M boundary collars and cusp\nneighbourhoods with disjoint embedded interiors. Our main result is that this\nportion can only be maximal in some combinatorially extremal configurations.\nThe techniques we employ are VERY elementary but the result is in our opinion\nof some interest.\n",
        "  High quality epitaxial NbN/MgO/NbN Josephson junctions have been realized\nwith MgO barriers up to a thickness of d=1 nm. The junction properties\ncoherently scale with the size of barrier, and low critical current densities\ndown to 3 A/cm$^2$ have been achieved for larger barriers. In this limit,\njunctions exhibit macroscopic quantum phenomena for temperatures lower than 90\nmK. Measurements and junction parameters support the notion of a possible use\nof these devices for multiphoton quantum experiments, taking advantage of the\nfast non equilibrium electron-phonon relaxation times of NbN.\n",
        "  ID is derived from the word identity, derived from the first two characters\nin the word. ID is used to distinguish between an entity to another entity.\nStudent ID (SID) is the key differentiator between a student with other\nstudents. On the concept of database, the differentiator is unique. SID can be\nnumbers, letters, or a combination of both (alphanumeric). Viewed from the\ndaily context, it is not important to determine which a SID belongs to the type\nof data. However, when reviewed on database design, determining the type of\ndata, including SID in this case, is important. Problems arise because there is\na contradiction between the data type viewed from the data characteristic and\npractical needs. Type of data for SID is a string, if it is evaluated from the\nbasic concepts and its characteristic. It is acceptable because SID consists of\na set of numbers which will not be meaningful if applied arithmetic operations\nlike addition, subtraction, multiplication and division. But in terms of\ncomputer organization, data representation type will determine how much data\nspace requirements, speed of access, and speed of operation. By considering the\nconstraints of space and speed on the experiments conducted, SID is better\nexpressed as an integer rather than a set of characters.\n  KEYWORDS aphanumeric,representation, string, integer, space, speed\n",
        "  The $\\log 3$ Theorem, proved by Culler and Shalen, states that every point in\nthe hyperbolic 3-space is moved a distance at least $\\log 3$ by one of the\nnon-commuting isometries $\\xi$ or $\\eta$ provided that $\\xi$ and $\\eta$\ngenerate a torsion-free, discrete group which is not co-compact and contains no\nparabolic. This theorem lies in the foundation of many techniques that provide\nlower estimates for the volumes of orientable, closed hyperbolic 3-manifolds\nwhose fundamental group has no 2-generator subgroup of finite index and, as a\nconsequence, gives insights into the topological properties of these manifolds.\n  In this paper, we show that every point in the hyperbolic 3-space is moved a\ndistance at least $(1/2)\\log(5+3\\sqrt{2})$ by one of the isometries in\n$\\{\\xi,\\eta,\\xi\\eta\\}$ when $\\xi$ and $\\eta$ satisfy the conditions given in\nthe $\\log 3$ Theorem.\n",
        "  This paper reports on ongoing research investigating more expressive\napproaches to spatial-temporal trajectory clustering. Spatial-temporal data is\nincreasingly becoming universal as a result of widespread use of GPS and mobile\ndevices, which makes mining and predictive analyses based on trajectories a\ncritical activity in many domains. Trajectory analysis methods based on\nclustering techniques heavily often rely on a similarity definition to properly\nprovide insights. However, although trajectories are currently described in\nterms of its two dimensions (space and time), their representation is limited\nin that it is not expressive enough to capture, in a combined way, the\nstructure of space and time as well as the contextual and semantic trajectory\nproperties. Moreover, the massive amounts of available trajectory data make\ntrajectory mining and analyses very challenging. In this paper, we briefly\ndiscuss (i) an improved trajectory representation that takes into consideration\nspace-time structures, context and semantic properties of trajectories; (ii)\nnew forms of relations between the dimensions of a pair of trajectories; and\n(iii) big data approaches that can be used to develop a novel spatial-temporal\nclustering framework.\n",
        "  This article presents a fragment of a new comparative dictionary \"A\ncomparative dictionary of names of expansive action in Russian and Bulgarian\nlanguages\". Main features of the new web-based comparative dictionary are\nplaced, the principles of its formation are shown, primary links between the\nword-matches are classified. The principal difference between translation\ndictionaries and the model of double comparison is also shown. The\nclassification scheme of the pages is proposed. New concepts and keywords have\nbeen introduced. The real prototype of the dictionary with a few key pages is\npublished. The broad debate about the possibility of this prototype to become a\nversion of Russian-Bulgarian comparative dictionary of a new generation is\navailable.\n",
        "  In-situ processing has been proposed as a novel data exploration solution in\nmany domains generating massive amounts of raw data, e.g., astronomy, since it\nprovides immediate SQL querying over raw files. The performance of in-situ\nprocessing across a query workload is, however, limited by the speed of full\nscan, tokenizing, and parsing of the entire data. Online aggregation (OLA) has\nbeen introduced as an efficient method for data exploration that identifies\nuninteresting patterns faster by continuously estimating the result of a\ncomputation during the actual processing---the computation can be stopped as\nearly as the estimate is accurate enough to be deemed uninteresting. However,\nexisting OLA solutions have a high upfront cost of randomly shuffling and/or\nsampling the data. In this paper, we present OLA-RAW, a bi-level sampling\nscheme for parallel online aggregation over raw data. Sampling in OLA-RAW is\nquery-driven and performed exclusively in-situ during the runtime query\nexecution, without data reorganization. This is realized by a novel\nresource-aware bi-level sampling algorithm that processes data in random chunks\nconcurrently and determines adaptively the number of sampled tuples inside a\nchunk. In order to avoid the cost of repetitive conversion from raw data,\nOLA-RAW builds and maintains a memory-resident bi-level sample synopsis\nincrementally. We implement OLA-RAW inside a modern in-situ data processing\nsystem and evaluate its performance across several real and synthetic datasets\nand file formats. Our results show that OLA-RAW chooses the sampling plan that\nminimizes the execution time and guarantees the required accuracy for each\nquery in a given workload. The end result is a focused data exploration process\nthat avoids unnecessary work and discards uninteresting data.\n",
        "  Data replication and deployment of local SPARQL endpoints improve scalability\nand availability of public SPARQL endpoints, making the consumption of Linked\nData a reality. This solution requires synchronization and specific query\nprocessing strategies to take advantage of replication. However, existing\nreplication aware techniques in federations of SPARQL endpoints do not consider\ndata dynamicity. We propose Fedra, an approach for querying federations of\nendpoints that benefits from replication. Participants in Fedra federations can\ncopy fragments of data from several datasets, and describe them using\nprovenance and views. These descriptions enable Fedra to reduce the number of\nselected endpoints while satisfying user divergence requirements. Experiments\non real-world datasets suggest savings of up to three orders of magnitude.\n",
        "  Consistent query answering is an inconsistency tolerant approach to obtaining\nsemantically correct answers from a database that may be inconsistent with\nrespect to its integrity constraints. In this work we formalize the notion of\nconsistent query answer for spatial databases and spatial semantic integrity\nconstraints. In order to do this, we first characterize conflicting spatial\ndata, and next, we define admissible instances that restore consistency while\nstaying close to the original instance. In this way we obtain a repair\nsemantics, which is used as an instrumental concept to define and possibly\nderive consistent query answers. We then concentrate on a class of spatial\ndenial constraints and spatial queries for which there exists an efficient\nstrategy to compute consistent query answers. This study applies inconsistency\ntolerance in spatial databases, rising research issues that shift the goal from\nthe consistency of a spatial database to the consistency of query answering.\n",
        "  Although, the fair amount of works in sentiment analysis (SA) and opinion\nmining (OM) systems in the last decade and with respect to the performance of\nthese systems, but it still not desired performance, especially for\nmorphologically-Rich Language (MRL) such as Arabic, due to the complexities and\nchallenges exist in the nature of the languages itself. One of these challenges\nis the detection of idioms or proverbs phrases within the writer text or\ncomment. An idiom or proverb is a form of speech or an expression that is\npeculiar to itself. Grammatically, it cannot be understood from the individual\nmeanings of its elements and can yield different sentiment when treats as\nseparate words. Consequently, In order to facilitate the task of detection and\nclassification of lexical phrases for automated SA systems, this paper presents\nAIPSeLEX a novel idioms/ proverbs sentiment lexicon for modern standard Arabic\n(MSA) and colloquial. AIPSeLEX is manually collected and annotated at sentence\nlevel with semantic orientation (positive or negative). The efforts of manually\nbuilding and annotating the lexicon are reported. Moreover, we build a\nclassifier that extracts idioms and proverbs, phrases from text using n-gram\nand similarity measure methods. Finally, several experiments were carried out\non various data, including Arabic tweets and Arabic microblogs (hotel\nreservation, product reviews, and TV program comments) from publicly available\nArabic online reviews websites (social media, blogs, forums, e-commerce web\nsites) to evaluate the coverage and accuracy of AIPSeLEX.\n",
        "  The area of interests of nuclear physics are studies of reactions, wherein\natomic nuclei of projectile collide with target nuclei. An amount of energy\nlost by projectile nucleus during its passing through the target is a major\nissue - it is important to know how charged particles interact with matter. It\nis possible to afford this knowledge by using theoretical programs, which\ncalculate energy loss applying Bethe-Bloch equation.\n  Hadrontherapy, which is a field of still growing interest, is based on\ninteractions of charged particles with matter. Therefore exists a need of\ncreating a simple model, which could be used to the calculation of dose\ndistributions in biological matter.\n  Two programs (SRIM, Xeloss), used to the calculation of energy loss by\nnuclear physicist, have been adapted to determine dose distributions in analogs\nof human tissues. Results of the calculations with those programs for beams\nused in hadrontherapy (e.g. $^1$H, $^{12}$C) will be compared with experimental\ndata available in references.\n",
        "  The Cooper pairing mechanism of heavy-fermion superconductors, while long\nhypothesized as due to spin fluctuations, has not been determined. It is the\nmomentum space (k-space) structure of the superconducting energy gap delta(k)\nthat encodes specifics of this pairing mechanism. However, because the energy\nscales are so low, it has not been possible to directly measure delta(k) for\nany heavy-fermion superconductor. Bogoliubov quasiparticle interference (QPI)\nimaging, a proven technique for measuring the energy gaps of high-Tc\nsuperconductors, has recently been proposed as a new method to measure delta(k)\nin heavy-fermion superconductors, specifically CeCoIn5. By implementing this\nmethod, we immediately detect a superconducting energy gap whose nodes are\noriented along k||(+-1, +-1)pi/a0 directions. Moreover, we determine the\ncomplete k-space structure of the delta(k) of a heavy-fermion superconductor.\nFor CeCoIn5, this novel information includes: the complex band structure and\nFermi surface of the hybridized heavy bands, the fact that highest magnitude\ndelta(k) opens on a high-k band so that gap nodes occur at quite unanticipated\nk-space locations, and that the Bogoliubov quasiparticle interference patterns\nare most consistent with dx2-y2 gap symmetry. The availability of such\nquantitative heavy band- and gap-structure data will be critical in identifying\nthe microscopic mechanism of heavy fermion superconductivity in this material,\nand perhaps in general.\n",
        "  We have studied electronic and magnetic properties of the K0.8Fe1.6Se2\nsuperconductor by x-ray absorption and emission spectroscopy. Detailed\ntemperature dependent measurements alongwith a direct comparison with the\nbinary FeSe system have revealed coexisting electronic phases: a majority phase\nwith high spin 3+Fe state and a minority phase with intermediate spin 2+Fe\nstate. The effect of high temperature annealing suggests that the compressed\nphase with lower spin 2+Fe state is directly related with the high Tc\nsuperconductivity in the title system. The results clearly underline glassy\nnature of superconductivity in the electronically inhomogeneous K0.8Fe1.6Se2,\nsimilar to the superconductivity in granular phases.\n",
        "  The active galaxy NGC 4151 has a crucial role as one of only two active\ngalactic nuclei for which black hole mass measurements based on emission line\nreverberation mapping can be calibrated against other dynamical methods.\nUnfortunately, effective calibration requires an accurate distance to NGC 4151,\nwhich is currently not available. Recently reported distances range from 4 to\n29 megaparsecs (Mpc). Strong peculiar motions make a redshift-based distance\nvery uncertain, and the geometry of the galaxy and its nucleus prohibit\naccurate measurements using other techniques. Here we report a dust-parallax\ndistance to NGC 4151 of $D_A = 19.0^{+2.4}_{-2.6}$ Mpc. The measurement is\nbased on an adaptation of a geometric method proposed previously using the\nemission line regions of active galaxies. Since this region is too small for\ncurrent imaging capabilities, we use instead the ratio of the\nphysical-to-angular sizes of the more extended hot dust emission as determined\nfrom time-delays and infrared interferometry. This new distance leads to an\napproximately 1.4-fold increase in the dynamical black hole mass, implying a\ncorresponding correction to emission line reverberation masses of black holes\nif they are calibrated against the two objects with additional dynamical\nmasses.\n",
        "  We study the Thurston-Bennequin number of complete and complete bipartite\nLegendrian graphs. We define a new invariant called the total\nThurston-Bennequin number of the graph. We show that this invariant is\ndetermined by the Thurston-Bennequin numbers of 3-cycles for complete graphs\nand by the Thurston-Bennequin number of 4-cycles for complete bipartite graphs.\nWe discuss the consequences of these results for K_4, K_5 and K_{3,3}.\n",
        "  This paper presents the Norwegian Review Corpus (NoReC), created for training\nand evaluating models for document-level sentiment analysis. The full-text\nreviews have been collected from major Norwegian news sources and cover a range\nof different domains, including literature, movies, video games, restaurants,\nmusic and theater, in addition to product reviews across a range of categories.\nEach review is labeled with a manually assigned score of 1-6, as provided by\nthe rating of the original author. This first release of the corpus comprises\nmore than 35,000 reviews. It is distributed using the CoNLL-U format,\npre-processed using UDPipe, along with a rich set of metadata. The work\nreported in this paper forms part of the SANT initiative (Sentiment Analysis\nfor Norwegian Text), a project seeking to provide resources and tools for\nsentiment analysis and opinion mining for Norwegian. As resources for sentiment\nanalysis have so far been unavailable for Norwegian, NoReC represents a highly\nvaluable and sought-after addition to Norwegian language technology.\n",
        "  Given an $L^2$-acyclic connected finite $CW$-complex, we define its universal\n$L^2$-torsion in terms of the chain complex of its universal covering. It takes\nvalues in the weak Whitehead group $\\operatorname{Wh}^w(G)$. We study its main\nproperties such as homotopy invariance, sum formula, product formula and\nPoincar\\'e duality. Under certain assumptions, we can specify certain\nhomomorphisms from the weak Whitehead group $\\operatorname{Wh}^w(G)$ to abelian\ngroups such as the real numbers or the Grothendieck group of integral\npolytopes, and the image of the universal $L^2$-torsion can be identified with\nmany invariants such as the $L^2$-torsion, the $L^2$-torsion function, twisted\n$L^2$-Euler characteristics and, in the case of a $3$-manifold, the dual\nThurston norm polytope.\n",
        "  Deuterium fractionation is considered as an important process to infer the\nchemical ages of prestellar cores in filaments. We present here the first\nmagneto-hydrodynamical simulations including a chemical network to study\ndeuterium fractionation in magnetized and turbulent filaments and their\nsubstructures. The filaments typically show widespread deuterium fractionation\nwith average values $\\gtrsim0.01$. For individual cores of similar age, we\nobserve the deuteration fraction to increase with time, but also to be\nindependent of their average properties such as density, virial or\nmass-to-magnetic flux ratio. We further find a correlation of the deuteration\nfraction with core mass, average H$_2$ density and virial parameter only at\nlate evolutionary stages of the filament and attribute this to the lifetime of\nthe individual cores. Specifically, chemically old cores reveal higher\ndeuteration fractions. Within the radial profiles of selected cores, we notice\ndifferences in the structure of the deuteration fraction or surface density,\nwhich we can attribute to their different turbulent properties. High\ndeuteration fractions of the order $0.01-0.1$ may be reached within\napproximately $200$~kyrs, corresponding to two free-fall times, as defined for\ncylindrical systems, of the filaments\n",
        "  Compositionality in language refers to how much the meaning of some phrase\ncan be decomposed into the meaning of its constituents and the way these\nconstituents are combined. Based on the premise that substitution by synonyms\nis meaning-preserving, compositionality can be approximated as the semantic\nsimilarity between a phrase and a version of that phrase where words have been\nreplaced by their synonyms. Different ways of representing such phrases exist\n(e.g., vectors [1] or language models [2]), and the choice of representation\naffects the measurement of semantic similarity.\n  We propose a new compositionality detection method that represents phrases as\nranked lists of term weights. Our method approximates the semantic similarity\nbetween two ranked list representations using a range of well-known distance\nand correlation metrics. In contrast to most state-of-the-art approaches in\ncompositionality detection, our method is completely unsupervised. Experiments\nwith a publicly available dataset of 1048 human-annotated phrases shows that,\ncompared to strong supervised baselines, our approach provides superior\nmeasurement of compositionality using any of the distance and correlation\nmetrics considered.\n",
        "  Given a virtual knot $K$, we construct a group $VG_K$ called the virtual knot\ngroup, and we use the elementary ideals of $VG_K$ to define invariants of $K$\ncalled the virtual Alexander invariants. For instance, associated to the $k=0$\nideal is a polynomial $H_K(s,t,q)$ in three variables which we call the virtual\nAlexander polynomial, and we show that it is closely related to the generalized\nAlexander polynomial $G_K(s,t)$ introduced by Sawollek, Kauffman-Radford, and\nSilver-Williams. We define a natural normalization of the virtual Alexander\npolynomial and show it satisfies a skein formula. We also introduce the twisted\nvirtual Alexander polynomial associated to a virtual knot $K$ and a\nrepresentation $\\varrho \\colon VG_K \\to GL_n(R)$, and we define a normalization\nof the twisted virtual Alexander polynomial. As applications we derive bounds\non the virtual crossing numbers of virtual knots from the virtual Alexander\npolynomial and twisted virtual Alexander polynomial.\n",
        "  Beam search is a widely used approximate search strategy for neural network\ndecoders, and it generally outperforms simple greedy decoding on tasks like\nmachine translation. However, this improvement comes at substantial\ncomputational cost. In this paper, we propose a flexible new method that allows\nus to reap nearly the full benefits of beam search with nearly no additional\ncomputational cost. The method revolves around a small neural network actor\nthat is trained to observe and manipulate the hidden state of a\npreviously-trained decoder. To train this actor network, we introduce the use\nof a pseudo-parallel corpus built using the output of beam search on a base\nmodel, ranked by a target quality metric like BLEU. Our method is inspired by\nearlier work on this problem, but requires no reinforcement learning, and can\nbe trained reliably on a range of models. Experiments on three parallel corpora\nand three architectures show that the method yields substantial improvements in\ntranslation quality and speed over each base system.\n",
        "  This paper presents an abstract data model for linguistic annotations and its\nimplementation using XML, RDF and related standards; and to outline the work of\na newly formed committee of the International Standards Organization (ISO),\nISO/TC 37/SC 4 Language Resource Management, which will use this work as its\nstarting point. The primary motive for presenting the latter is to solicit the\nparticipation of members of the research community to contribute to the work of\nthe committee.\n",
        "  We study relations between global characteristics of low-redshift (0 < z < 1)\ncompact star-forming galaxies, including absolute optical magnitudes, Hbeta\nemission-line luminosities (or equivalently star-formation rates), stellar\nmasses, and oxygen abundances. The sample consists of 5182 galaxies with\nhigh-excitation HII regions selected from the SDSS DR7 and SDSS/BOSS DR10\nsurveys adopting a criterion [OIII]4959/Hbeta > 1. These data were combined\nwith the corresponding data for high-redshift (2 < z < 3) star-forming\ngalaxies. We find that in all diagrams low-z and high-z star-forming galaxies\nare closely related indicating a very weak dependence of metallicity on stellar\nmass, redshift, and star-formation rate. This finding argues in favour of the\nuniversal character of the global relations for compact star-forming galaxies\nwith high-excitation HII regions over redshifts 0 < z < 3.\n",
        "  Tree transducers are formal automata that transform trees into other trees.\nMany varieties of tree transducers have been explored in the automata theory\nliterature, and more recently, in the machine translation literature. In this\npaper I review T and xT transducers, situate them among related formalisms, and\nshow how they can be used to implement rules for machine translation systems\nthat cover all of the cross-language structural divergences described in Bonnie\nDorr's influential article on the topic. I also present an implementation of xT\ntransduction, suitable and convenient for experimenting with translation rules.\n",
        "  Recently, different transport coefficients have been measured in High-Tc\nsuperconductors to pinpoint the nature of the pseudogap phase. In particular,\nthe thermoelectric coefficients received a considerable attention both\ntheoretically and experimentally. We numerically simulate the Nernst effect in\nextreme type-II superconductors using the time-dependent Ginzburg-Landau\nequations. We report the sign reversal of the thermoelectric coefficient,\nalpha_xy, at temperatures close to the mean-field transition temperature,\nTc^{MF}(H), which qualitatively agrees with recent experiments on high-Tc\nmaterials. We also discuss the noise power spectrum of alpha_xy, which shows\n1/f^beta behavior. Based on this observation, we propose an experiment to\ndistinguish among different regimes of vortex dynamics by measuring the noise\ncorrelations of the Nernst signal.\n",
        "  This is the documentation of the tomographic X-ray data of a carved cheese\nslice. Data are available at www.fips.fi/dataset.php, and can be freely used\nfor scientific purposes with appropriate references to them, and to this\ndocument in http://arxiv.org/. The data set consists of (1) the X-ray sinogram\nof a single 2D slice of the cheese slice with three different resolutions and\n(2) the corresponding measurement matrices modeling the linear operation of the\nX-ray transform. Each of these sinograms was obtained from a measured\n360-projection fan-beam sinogram by down-sampling and taking logarithms. The\noriginal (measured) sinogram is also provided in its original form and\nresolution.\n",
        "  We present results of Raman spectroscopic studies of urine to determine the\nsuitability of near-infrared Raman spectroscopy for quantitative estimation of\nurinary urea. The Raman spectra were acquired from the urine samples with an\ninbuilt Raman spectroscopy setup that employs a 785-nm diode laser as the Raman\nexcitation source. A multivariate algorithm based on partial least square (PLS)\nregression was developed to predict the concentration of urea depending on the\nmeasured sets of Raman spectra and the reference urea concentration. The\ncomputed results shows that Raman spectroscopy in amalgamation with PLS-based\nmultivariate chemometric algorithm can detect urea in urine samples with an\naccuracy of >90 %\n",
        "  An association rule is statistically significant, if it has a small\nprobability to occur by chance. It is well-known that the traditional\nfrequency-confidence framework does not produce statistically significant\nrules. It can both accept spurious rules (type 1 error) and reject significant\nrules (type 2 error). The same problem concerns other commonly used\ninterestingness measures and pruning heuristics.\n  In this paper, we inspect the most common measure functions - frequency,\nconfidence, degree of dependence, $\\chi^2$, correlation coefficient, and\n$J$-measure - and redundancy reduction techniques. For each technique, we\nanalyze whether it can make type 1 or type 2 error and the conditions under\nwhich the error occurs. In addition, we give new theoretical results which can\nbe use to guide the search for statistically significant association rules.\n",
        "  Subjective language detection is one of the most important challenges in\nSentiment Analysis. Because of the weight and frequency in opinionated texts,\nadjectives are considered a key piece in the opinion extraction process. These\nsubjective units are more and more frequently collected in polarity lexicons in\nwhich they appear annotated with their prior polarity. However, at the moment,\nany polarity lexicon takes into account prior polarity variations across\ndomains. This paper proves that a majority of adjectives change their prior\npolarity value depending on the domain. We propose a distinction between domain\ndependent and domain independent adjectives. Moreover, our analysis led us to\npropose a further classification related to subjectivity degree: constant,\nmixed and highly subjective adjectives. Following this classification, polarity\nvalues will be a better support for Sentiment Analysis.\n",
        "  We extend the black hole (BH) feedback models of Ciotti, Ostriker, and Proga\nto two dimensions. In this paper, we focus on identifying the differences\nbetween the one-dimensional and two-dimensional hydrodynamical simulations. We\nexamine a normal, isolated $L_*$ galaxy subject to the cooling flow instability\nof gas in the inner regions. Allowance is made for subsequent star formation,\nType Ia and Type II supernovae, radiation pressure, and inflow to the central\nBH from mildly rotating galactic gas which is being replenished as a normal\nconsequence of stellar evolution. The central BH accretes some of the infalling\ngas and expels a conical wind with mass, momentum, and energy flux derived from\nboth observational and theoretical studies. The galaxy is assumed to have low\nspecific angular momentum in analogy with the existing one-dimensional case in\norder to isolate the effect of dimensionality. The code then tracks the\ninteraction of the outflowing radiation and winds with the galactic gas and\ntheir effects on regulating the accretion. After matching physical modeling to\nthe extent possible between the one-dimensional and two-dimensional treatments,\nwe find essentially similar results in terms of BH growth and duty cycle\n(fraction of the time above a given fraction of the Eddington luminosity). In\nthe two-dimensional calculations, the cool shells forming at 0.1--1 kpc from\nthe center are Rayleigh--Taylor unstable to fragmentation, leading to a\nsomewhat higher accretion rate, less effective feedback, and a more irregular\npattern of bursting compared to the one-dimensional case.\n",
        "  Interpretability of a predictive model is a powerful feature that gains the\ntrust of users in the correctness of the predictions. In word sense\ndisambiguation (WSD), knowledge-based systems tend to be much more\ninterpretable than knowledge-free counterparts as they rely on the wealth of\nmanually-encoded elements representing word senses, such as hypernyms, usage\nexamples, and images. We present a WSD system that bridges the gap between\nthese two so far disconnected groups of methods. Namely, our system, providing\naccess to several state-of-the-art WSD models, aims to be interpretable as a\nknowledge-based system while it remains completely unsupervised and\nknowledge-free. The presented tool features a Web interface for all-word\ndisambiguation of texts that makes the sense predictions human readable by\nproviding interpretable word sense inventories, sense representations, and\ndisambiguation results. We provide a public API, enabling seamless integration.\n",
        "  We propose that the orbital angular momentum of the conduction electrons in\nthe Iron-based superconductors is activated in their low energy physics. Using\na five-band tight-binding model derived from fitting the LDA band structure, we\nfind that the orbital magnetic susceptibility of the conduction electrons in\nsuch a multi-orbital system is several times larger than the Pauli spin\nsusceptibility and is comparable in magnitude to the observed total magnetic\nsusceptibility. The orbital magnetic susceptibility in the Fe-As\nplane($\\chi^{x}_{L}$) is found to be larger than that perpendicular to the\nFe-As plane($\\chi^{z}_{L}$) by a factor about two and the total magnetic\nsusceptibility in the normal state can be fitted with formula\n$\\chi(T,\\theta)\\approx \\chi_{s}(T)+\\chi_{L}(\\theta)$, where $\\chi_{s}(T)$ is\nthe temperature dependent isotropic part due to spin and $\\chi_{L}(\\theta)$ is\nthe temperature independent anisotropic part due to orbital. In the\nsuperconducting state, $\\chi^{x}_{L}$ is found to be significantly reduced as\nthe pairing gap develops, while $\\chi^{z}_{L}$ is almost not affected by the\nsuperconducting transition. We argue the large anisotropy observed in the bulk\nmagnetic susceptibility and the Knight shift in the Iron-based superconductors\nshould be attributed to the orbital magnetic response of their conduction\nelectrons.\n",
        "  We study the problem of spectrum estimation from transmission data of a known\nphantom. The goal is to reconstruct an x-ray spectrum that can accurately model\nthe x-ray transmission curves and reflects a realistic shape of the typical\nenergy spectra of the CT system. To this end, spectrum estimation is posed as\nan optimization problem with x-ray spectrum as unknown variables, and a\nKullback-Leibler (KL) divergence constraint is employed to incorporate prior\nknowledge of the spectrum and enhance numerical stability of the estimation\nprocess. The formulated constrained optimization problem is convex and can be\nsolved efficiently by use of the exponentiated-gradient (EG) algorithm. We\ndemonstrate the effectiveness of the proposed approach on the simulated and\nexperimental data. The comparison to the expectation-maximization (EM) method\nis also discussed. In simulations, the proposed algorithm is seen to yield\nx-ray spectra that closely match the ground truth and represent the attenuation\nprocess of x-ray photons in materials, both included and not included in the\nestimation process. In experiments, the calculated transmission curve is in\ngood agreement with the measured transmission curve, and the estimated spectra\nexhibits physically realistic looking shapes. The results further show the\ncomparable performance between the proposed optimization-based approach and EM.\nIn conclusion, our formulation of a constrained optimization provides an\ninterpretable and flexible framework for spectrum estimation. Moreover, a\nKL-divergence constraint can include a prior spectrum and appears to capture\nimportant features of x-ray spectrum, allowing accurate and robust estimation\nof x-ray spectrum in CT imaging.\n",
        "  Here we explore the evolution of galaxy ensembles at early times by writing\nthe in situ stellar mass growth of galaxies purely as a stationary stochastic\n(e.g., quasi-steady state) process. By combining the mathematics of such\nprocesses with Newtonian gravity and a mean local star formation efficiency, we\nshow that the stellar mass evolution of galaxy ensembles is directly related to\nthe average acceleration of baryons onto dark matter halos at the onset of star\nformation, with explicit dependencies on initial local matter densities and\nhalo mass. The density term specifically implies more rapid average rates of\ngrowth in higher density regions of the universe compared to low density\nregions, i.e., assembly bias. With this framework, using standard cosmological\nparameters, a mean star formation efficiency derived by other authors, and\nknowledge of the shape of the cosmological matter power spectrum at small\nscales, we analytically derive (1) the characteristic stellar masses of\ngalaxies (M*), (2) the power-law low-mass slope (alpha) and normalization\n(phi*) of the stellar mass function, and (3) the evolution of the stellar mass\nfunction in time over 12.5 > z > 2. Correspondingly, the rise in the cosmic\nstar formation rate density over these epochs, while the universe can sustain\nunabated fueling of star formation, also emerges naturally. All of our findings\nare consistent with the deepest available data, including the expectation of\nalpha~-7/5; i.e., a stellar mass function low-mass slope that is notably\nshallower than that of the halo mass function, and with no systematic\ndeviations from a mean star formation efficiency with density or mass, nor any\nexplicit, additional feedback mechanisms. These derivations yield a compelling\nrichness and complexity but also show that very few astrophysical details are\nrequired to understand the evolution of cosmic ensemble of galaxies at early\ntimes.\n",
        "  Distributional word representation methods exploit word co-occurrences to\nbuild compact vector encodings of words. While these representations enjoy\nwidespread use in modern natural language processing, it is unclear whether\nthey accurately encode all necessary facets of conceptual meaning. In this\npaper, we evaluate how well these representations can predict perceptual and\nconceptual features of concrete concepts, drawing on two semantic norm datasets\nsourced from human participants. We find that several standard word\nrepresentations fail to encode many salient perceptual features of concepts,\nand show that these deficits correlate with word-word similarity prediction\nerrors. Our analyses provide motivation for grounded and embodied language\nlearning approaches, which may help to remedy these deficits.\n",
        "  Formalizing an RDF abstract graph model to be compatible with the RDF formal\nsemantics has remained one of the foundational problems in the Semantic Web. In\nthis paper, we propose a new formal graph model for RDF datasets. This model\nallows us to express the current model-theoretic semantics in the form of a\ngraph. We also propose the concepts of resource path and triple path as well as\nan algorithm for traversing the new graph. We demonstrate the feasibility of\nthis graph model through two implementations: one is a new graph engine called\nGraphKE, and the other is extended from RDF-3X to show that existing systems\ncan also benefit from this model. In order to evaluate the empirical aspect of\nour graph model, we choose the shortest path algorithm and implement it in the\nGraphKE and the RDF-3X. Our experiments on both engines for finding the\nshortest paths in the YAGO2S-SP dataset give decent performance in terms of\nexecution time. The empirical results show that our graph model with\nwell-defined semantics can be effectively implemented.\n",
        "  Differential privacy is a promising privacy-preserving paradigm for\nstatistical query processing over sensitive data. It works by injecting random\nnoise into each query result, such that it is provably hard for the adversary\nto infer the presence or absence of any individual record from the published\nnoisy results. The main objective in differentially private query processing is\nto maximize the accuracy of the query results, while satisfying the privacy\nguarantees. Previous work, notably the matrix mechanism, has suggested that\nprocessing a batch of correlated queries as a whole can potentially achieve\nconsiderable accuracy gains, compared to answering them individually. However,\nas we point out in this paper, the matrix mechanism is mainly of theoretical\ninterest; in particular, several inherent problems in its design limit its\naccuracy in practice, which almost never exceeds that of naive methods. In\nfact, we are not aware of any existing solution that can effectively optimize a\nquery batch under differential privacy. Motivated by this, we propose the\nLow-Rank Mechanism (LRM), the first practical differentially private technique\nfor answering batch queries with high accuracy, based on a low rank\napproximation of the workload matrix. We prove that the accuracy provided by\nLRM is close to the theoretical lower bound for any mechanism to answer a batch\nof queries under differential privacy. Extensive experiments using real data\ndemonstrate that LRM consistently outperforms state-of-the-art query processing\nsolutions under differential privacy, by large margins.\n",
        "  Superconductivity with zero resistance transition temperature (Tc) up to 8.4\nK and 8.3 K can be obtained by doping cobalt and sodium in alfa-FeSe with the\nnominal composition of Fe0.92Co0.08Se and Na0.1FeSe, respectively. The\nelectrical resistivity and AC magnetic susceptibility of the prepared samples,\nmeasured with physical property measurement system (PPMS), unambiguously\nconsistent with each other to indicate that the samples are superconductive.\nThe respective doping mechanisms for cobalt and sodium into the parent\nalfa-FeSe are the Fe-site substitution and the interlayer insertion. It is the\nfirst time that alfa-FeSe can be induced to be a superconductor with Na+\nintercalated into the interlayers.\n",
        "  Bacterial communities have rich social lives. A well-established interaction\ninvolves the exchange of a public good in Pseudomonas populations, where the\niron-scavenging compound pyoverdine, synthesized by some cells, is shared with\nthe rest. Pyoverdine thus mediates interactions between producers and\nnon-producers and can constitute a public good. This interaction is often used\nto test game theoretical predictions on the \"social dilemma\" of producers. Such\nan approach, however, underestimates the impact of specific properties of the\npublic good, for example consequences of its accumulation in the environment.\nHere, we experimentally quantify costs and benefits of pyoverdine production in\na specific environment, and build a model of population dynamics that\nexplicitly accounts for the changing significance of accumulating pyoverdine as\nchemical mediator of social interactions. The model predicts that, in an\nensemble of growing populations (metapopulation) with different initial\nproducer fractions (and consequently pyoverdine contents), the global producer\nfraction initially increases. Because the benefit of pyoverdine declines at\nsaturating concentrations, the increase need only be transient. Confirmed by\nexperiments on metapopulations, our results show how a changing benefit of a\npublic good can shape social interactions in a bacterial population.\n",
        "  Neural Machine Translation models have replaced the conventional phrase based\nstatistical translation methods since the former takes a generic, scalable,\ndata-driven approach rather than relying on manual, hand-crafted features. The\nneural machine translation system is based on one neural network that is\ncomposed of two parts, one that is responsible for input language sentence and\nother part that handles the desired output language sentence. This model based\non encoder-decoder architecture also takes as input the distributed\nrepresentations of the source language which enriches the learnt dependencies\nand gives a warm start to the network. In this work, we transform Roman-Urdu to\nUrdu transliteration into sequence to sequence learning problem. To this end,\nwe make the following contributions. We create the first ever parallel corpora\nof Roman-Urdu to Urdu, create the first ever distributed representation of\nRoman-Urdu and present the first neural machine translation model that\ntransliterates text from Roman-Urdu to Urdu language. Our model has achieved\nthe state-of-the-art results using BLEU as the evaluation metric. Precisely,\nour model is able to correctly predict sentences up to length 10 while\nachieving BLEU score of 48.6 on the test set. We are hopeful that our model and\nour results shall serve as the baseline for further work in the domain of\nneural machine translation for Roman-Urdu to Urdu using distributed\nrepresentation.\n",
        "  We show that fundamental groups of compact, orientable, irreducible\n3-manifolds with toroidal boundary are Grothendieck rigid.\n",
        "  Genetic interactions can strongly influence the fitness effects of individual\nmutations, yet the impact of these epistatic interactions on evolutionary\ndynamics remains poorly understood. Here we investigate the evolutionary role\nof epistasis over 50,000 generations in a well-studied laboratory evolution\nexperiment in E. coli. The extensive duration of this experiment provides a\nunique window into the effects of epistasis during long-term adaptation to a\nconstant environment. Guided by analytical results in the weak-mutation limit,\nwe develop a computational framework to assess the compatibility of a given\nepistatic model with the observed patterns of fitness gain and mutation\naccumulation through time. We find that a decelerating fitness trajectory alone\nprovides little power to distinguish between competing models, including those\nthat lack any direct epistatic interactions between mutations. However, when\ncombined with the mutation trajectory, these observables place strong\nconstraints on the set of possible models of epistasis, ruling out many\nexisting explanations of the data. Instead, we find that the data are\nconsistent with \"two-epoch\" model of adaptation, in which an initial burst of\ndiminishing returns epistasis is followed by a steady accumulation of mutations\nunder a constant distribution of fitness effects. Our results highlight the\nneed for additional DNA sequencing of these populations, as well as for more\nsophisticated models of epistasis that are compatible with all of the\nexperimental data.\n",
        "  The tetrus is a sort of big brother to the tripus, W.P. Thurston's example of\na compact hyperbolic 3-manifold with totally geodesic boundary. We describe a\nsixfold cover of the double of the tetrus, itself a double, which fibers over\nthe circle with fiber a closed surface of genus 19. We also record\narithmeticity of the doubles and certain twisted doubles of the tripus and\ntetrus, and point out some consequences regarding families of covers.\n",
        "  The goal of this article is to invite the reader to get to know and to get\ninvolved into higher Teichm\\\"uller theory by describing some of its many\nfacets.\n",
        "  Let $X$ be the space of isometry classes of ordered sextuples of points in\nthe hyperbolic plane such that the product of the six corresponding rotations\nof angle $\\pi$ is the identity. This space $X$ is closely related to the\nPSL$_2(\\mathbb{R})$-character variety of the genus 2 surface $\\Sigma$. In this\narticle we study the topology and the natural symplectic structure on $X$, and\nwe describe the action of the mapping class group of $\\Sigma$ on $X$. This\ncompletes the classification of the ergodic components of the character variety\nin genus 2 initiated in our previous work.\n",
        "  We derive a linear estimate of the signature of positive knots, in terms of\ntheir genus. As an application, we show that every knot concordance class\ncontains at most finitely many positive knots.\n",
        "  In this paper we define Crossing Change Alternating Knots (CCA knots) and\ntheir generalization: $k$-CCA knots.\n",
        "  This thesis focuses on the applications of mathematical tools and concepts\nbrought from nonequilibrium statistical physics to the modeling of ecological\nproblems.\n  The first part provides a short introduction where the theoretical concepts\nand mathematical tools that are going to be used in subsequent chapters are\npresented. Firstly, the different levels of description usually employed in the\nmodels are explained. Secondly, the mathematical relationships among them are\npresented. Finally, the notation and terminology that will be used later on are\nexplained.\n  The second part is devoted to studying vegetation pattern formation in\nregions where precipitations are not frequent and resources for plant growth\nare scarce. This part comprises two chapters.\n  The third part of the thesis develops a series of mathematical models\ndescribing the collective movement and behavior of some animal species. Its\nprimary objective is to investigate the effect that communication among\nforagers has on searching times and the formation of groups. It consists of two\nchapters.\n  The fourth part covers the effect of stochastic temporal disorder, mimicking\nclimate and environmental variability, on systems formed by many interacting\nparticles. These models may serve as an example of ecosystems. The thesis ends\nwith a summary and devising future research lines.\n",
        "  In many modern applications, data are received as infinite, rapid,\nunpredictable and time- variant data elements that are known as data streams.\nSystems which are able to process data streams with such properties are called\nData Stream Management Systems (DSMS). Due to the unpredictable and time-\nvariant properties of data streams as well as system, adaptivity of the DSMS is\na major requirement for each DSMS. Accordingly, determining parameters which\nare effective on the most important performance metric of a DSMS (i.e.,\nresponse time) and analysing them will affect on designing an adaptive DSMS. In\nthis paper, effective parameters on response time of DSMS are studied and\nanalysed and a solution is proposed for DSMSs' adaptivity. The proposed\nadaptive DSMS architecture includes a learning unit that frequently evaluates\nsystem to adjust the optimal value for each of tuneable effective. Learning\nAutomata is used as the learning mechanism of the learning unit to adjust the\nvalue of tuneable effective parameters. So, when system faces some changes, the\nlearning unit increases performance by tuning each of tuneable effective\nparameters to its optimum value. Evaluation results illustrate that after a\nwhile, parameters reach their optimum value and then DSMS's adaptivity will be\nimproved considerably.\n",
        "  Federated query engines allow data consumers to execute queries over the\nfederation of Linked Data (LD). However, as federated queries are decomposed\ninto potentially thousands of subqueries distributed among SPARQL endpoints,\ndata providers do not know federated queries, they only know subqueries they\nprocess. Consequently, unlike warehousing approaches, LD data providers have no\naccess to secondary data. In this paper, we propose FETA (FEderated query\nTrAcking), a query tracking algorithm that infers Basic Graph Patterns (BGPs)\nprocessed by a federation from a shared log maintained by data providers.\nConcurrent execution of thousand subqueries generated by multiple federated\nquery engines makes the query tracking process challenging and uncertain.\nExperiments with Anapsid show that FETA is able to extract BGPs which, even in\na worst case scenario, contain BGPs of original queries.\n",
        "  In todays digital world automated Machine Translation of one language to\nanother has covered a long way to achieve different kinds of success stories.\nWhereas Babel Fish supports a good number of foreign languages and only Hindi\nfrom Indian languages, the Google Translator takes care of about 10 Indian\nlanguages. Though most of the Automated Machine Translation Systems are doing\nwell but handling Indian languages needs a major care while handling the local\nproverbs/ idioms. Most of the Machine Translation system follows the direct\ntranslation approach while translating one Indian language to other. Our\nresearch at KMIT R&D Lab found that handling the local proverbs/idioms is not\ngiven enough attention by the earlier research work. This paper focuses on two\nof the majorly spoken Indian languages Marathi and Telugu, and translation\nbetween them. Handling proverbs and idioms of both the languages have been\ngiven a special care, and the research outcome shows a significant achievement\nin this direction.\n",
        "  Thin films of single-crystal\nPb$_{1-y}$Bi$_{y}$Sr$_{2}$Y$_{1-x}$Ca$_{x}$Cu$_{2}$O$_{7+\\delta}$ (PbBi1212)\nwere grown on SrTiO$_{3}$ (100) substrates by a two-step growth technique in\nwhich an amorphous film is annealed at 970 $^\\circ$C in a closed ceramic\ncontainer prepared using the same material as the film. We find that PbBi1212\nexhibits superconductivity when the Ca concentration $x$ exceeds 0.3. The\neffective number of holes per Cu atom $n_{\\rm eff}$ is well described as\n$n_{\\rm eff}=0.34x$. The highest onset temperature for the superconducting\ntransition attained in the present study is 65 K. The resistivity measurement\nin a magnetic field reveals that the coherence lengths of Pb1212 ($y=0$) are\napproximately 25 and 2.7 \\AA, along the $ab$ plane and the $c$ axis,\nrespectively.\n",
        "  We report the first detections of OH$^+$ emission in planetary nebulae (PNe).\nAs part of an imaging and spectroscopy survey of 11 PNe in the far-IR using the\nPACS and SPIRE instruments aboard the Herschel Space Observatory, we performed\na line survey in these PNe over the entire spectral range between 51 and\n672$\\mu$m to look for new detections. OH$^+$ rotational emission lines at\n152.99, 290.20, 308.48, and 329.77$\\mu$m were detected in the spectra of three\nplanetary nebulae: NGC 6445, NGC 6720, and NGC 6781. Excitation temperatures\nand column densities derived from these lines are in the range of 27 to 47 K\nand 2$\\times$10$^{10}$ to 4 $\\times$10$^{11}$ cm$^{-2}$, respectively. In PNe,\nthe OH+ rotational line emission appears to be produced in the\nphotodissociation region (PDR) in these objects. The emission of OH+ is\nobserved only in PNe with hot central stars (T$_{eff}$ > 100000 K), suggesting\nthat high-energy photons may play a role in the OH+ formation and its line\nexcitation in these objects, as it seems to be the case for ultraluminous\ngalaxies.\n",
        "  We call a closed, connected, orientable manifold in one of the categories\nTOP, PL or DIFF chiral if it does not admit an orientation-reversing\nautomorphism and amphicheiral otherwise. Moreover, we call a manifold strongly\nchiral if it does not admit a self-map of degree -1. We prove that there are\nstrongly chiral, smooth manifolds in every oriented bordism class in every\ndimension greater than two. We also produce simply-connected, strongly chiral\nmanifolds in every dimension greater than six. For every positive integer k, we\nexhibit lens spaces with an orientation-reversing self-diffeomorphism of order\n2^k but no self-map of degree -1 of smaller order.\n",
        "  In this manuscript we apply stochastic modeling to investigate the risk of\nreactivation of latent mycobacterial infections in patients undergoing\ntreatment with tumor necrosis factor inhibitors. First, we review the\nperspective proposed by one of the authors in a previous work and which\nconsists in predicting the occurrence of reactivation of latent tuberculosis\ninfection or newly acquired tuberculosis during treatment; this is based on\nvariational procedures on a simple set of parameters (e.g. rate of reactivation\nof a latent infection). Then, we develop a full analytical study of this\napproach through a Markov chain analysis and we find an exact solution for the\ntemporal evolution of the number of cases of tuberculosis infection\n(re)activation. The analytical solution is compared with Monte Carlo\nsimulations and with experimental data, showing overall excellent agreement.\nThe generality of this theoretical framework allows to investigate also the\ncase of non-tuberculous mycobacteria infections; in particular, we show that\nreactivation in that context plays a minor role. This may suggest that, while\nthe screening for tuberculous is necessary prior to initiating biologics, when\nconsidering non-tuberculous mycobacteria only a watchful monitoring during the\ntreatment is recommended. The framework outlined in this paper is quite general\nand could be extremely promising in further researches on drug-related adverse\nevents.\n",
        "  A possible explanation for the existence of the cuprate \"pseudogap\" state is\nthat it is a d-wave superconductor without quantum phase rigidity. Transport\nand thermodynamic studies provide compelling evidence that supports this\nproposal, but few spectroscopic explorations of it have been made. One\nspectroscopic signature of d-wave superconductivity is the particle-hole\nsymmetric \"octet\" of dispersive Bogoliubov quasiparticle interference\nmodulations. Here we report on this octet's evolution from low temperatures to\nwell into the underdoped pseudogap regime. No pronounced changes occur in the\noctet phenomenology at the superconductor's critical temperature Tc, and it\nsurvives up to at least temperature T ~ 1.5Tc. In the pseudogap regime, we\nobserve the detailed phenomenology that was theoretically predicted for\nquasiparticle interference in a phase-incoherent d-wave superconductor. Thus,\nour results not only provide spectroscopic evidence to confirm and extend the\ntransport and thermodynamics studies, but they also open the way for\nspectroscopic explorations of phase fluctuation rates, their effects on the\nFermi arc, and the fundamental source of the phase fluctuations that suppress\nsuperconductivity in underdoped cuprates.\n",
        "  - Genome-scan methods are used for screening genome-wide patterns of DNA\npolymorphism to detect signatures of positive selection. There are two main\ntypes of methods: (i) 'outlier' detection methods based on Fst that detect loci\nwith high differentiation compared to the rest of the genomes, and (ii)\nenvironmental association methods that test the association between allele\nfrequencies and environmental variables.\n  - We present a new Fst-based genome-scan method, BayeScEnv, which\nincorporates environmental information in the form of 'environmental\ndifferentiation'. It is based on the F-model, but, as opposed to existing\napproaches, it considers two locus-specific effects; one due to divergent\nselection, and another one due to various other processes different from local\nadaptation (e.g. range expansions, differences in mutation rates across loci or\nbackground selection). The method was developped in C++ and is avaible at\nhttp://github.com/devillemereuil/bayescenv.\n  - Simulation studies shows that our method has a much lower false positive\nrate than an existing Fst-based method, BayeScan, under a wide range of\ndemographic scenarios. Although it has lower power, it leads to a better\ncompromise between power and false positive rate.\n  - We apply our method to human and salmon datasets and show that it can be\nused successfully to study local adaptation. We discuss its scope and compare\nits mechanics to other existing methods.\n",
        "  As Resource Description Framework (RDF) is becoming a popular data modelling\nstandard, the challenges of efficient processing of Basic Graph Pattern (BGP)\nSPARQL queries (a.k.a. SQL inner-joins) have been a focus of the research\ncommunity over the past several years. In our recently published work we\nbrought community's attention to another equally important component of SPARQL,\ni.e., OPTIONAL pattern queries (a.k.a. SQL left-outer-joins). We proposed novel\noptimization techniques -- first of a kind -- and showed experimentally that\nour techniques perform better for the low-selectivity queries, and give at par\nperformance for the highly selective queries, compared to the state-of-the-art\nmethods.\n  BGPs and OPTIONALs (BGP-OPT) make the basic building blocks of the SPARQL\nquery language. Thus, in this paper, treating our BGP-OPT query optimization\ntechniques as the primitives, we extend them to handle other broader components\nof SPARQL such as such as UNION, FILTER, and DISTINCT. We mainly focus on the\nprocedural (algorithmic) aspects of these extensions. We also make several\nimportant observations about the structural aspects of complex SPARQL queries\nwith any intermix of these clauses, and relax some of the constraints regarding\nthe cyclic properties of the queries proposed earlier. We do so without\naffecting the correctness of the results, thus providing more flexibility in\nusing the BGP-OPT optimization techniques.\n",
        "  We report the observation of a new superfluid phase of 3He -- polar phase.\nThis phase appears in 3He confined in a new type of aerogel with nearly\nparallel arrangement of strands which play a role of ordered impurities. Our\nobservations qualitatively agree with theoretical predictions and suggest that\nin other systems with unconventional Cooper pairing (e.g. in unconventional\nsuperconductors) similar phenomena may be found in presence of anisotropic\nimpurities.\n",
        "  In 1999, Kauffman-Harary conjectured that every non-trivial Fox $p$-coloring\nof a reduced, alternating knot diagram with prime determinant $p$ is\nheterogeneous. Ten years later this conjecture was proved by W. Mattman and P.\nSolis. Mathew Williamson generalized this conjecture to alternating virtual\nknots and proved it for certain families of virtual knots. In the present note,\nwe use the methods of W. Mattman and P. Solis to give an affirmative answer to\nthe Kauffman-Harary conjecture for alternating virtual knots.\n",
        "  Hybrid superconducting/magnetic nanostructures on Si substrates have been\nbuilt with identical physical dimensions but different magnetic configurations.\nBy constructing arrays based on Co-dots with in-plane, out-of-plane, and vortex\nstate magnetic configurations, the stray fields are systematically tuned.\nDissipation in the mixed state of superconductors can be decreased (increased)\nby several orders of magnitude by decreasing (increasing) the stray magnetic\nfields. Furthermore, ordering of the stray fields over the entire array helps\nto suppress dissipation and enhance commensurability effects increasing the\nnumber of dissipation minima.\n",
        "  Previous research on unstable footwear has suggested that it may induce\nplantar mechanical noise during walking. The purpose of this study was to\nexplore whether unstable footwear could be considered as a noise-based training\ngear to exercise body center of mass (CoM) motion during walking or not. Ground\nreaction forces were collected among 24 healthy young women walking at speeds\nbetween 3 and 6 km h-1 with control running shoes and unstable rocker-bottom\nshoes. The external mechanical work, the recovery of mechanical energy of the\nCoM during and within the step cycles, and the phase shift between potential\nand kinetic energy curves of the CoM were computed. Our findings support the\nidea that unstable rocker-bottom footwear could serve as a speed-dependent\nnoise- based training gear to exercise CoM motion during walking. At slow\nspeed, it acts as a stochastic resonance or facilitator, whereas at brisk speed\nit acts as a constraint.\n",
        "  Hydrides are key ingredients of interstellar chemistry since they are the\ninitial products of chemical networks that lead to the formation of more\ncomplex molecules. The fundamental rotational transitions of light hydrides\nfall into the submillimeter wavelength range. Using the APEX telescope, we\nobserved the long sought hydrides SH+ and OH+ in absorption against the strong\ncontinuum source Sagittarius B2(M). Both, absorption from Galactic center gas\nas well as from diffuse clouds in intervening spiral arms over a large velocity\nrange is observed. The detected absorption of a continuous velocity range on\nthe line of sight shows these hydrides to be an abundant component of diffuse\nclouds. In addition, we used the strongest submillimeter dust continuum sources\nin the inner Galaxy to serve as background candles for a systematic census of\nthese hydrides in diffuse clouds and massive star forming regions of our Galaxy\nand initial results of this survey are presented.\n",
        "  We report detailed studies of the upper critical field and low-temperature\nspecific heat in the two-gap superconductor Lu$_{2}$Fe$_{3}$Si$_{5}$. The\nanisotropy of the upper critical field suggests that the active band is\nquasi-one-dimensional. Low-temperature specific heat in magnetic fields reveals\nthat the virtual $H_{c2}$ in the passive band is almost isotropic. These\nresults strongly indicate that the two bands have two different anisotropies,\nsimilar to the typical two-gap superconductor MgB$_{2}$, and their interplay\nmay be essential to the two-gap superconductivity in Lu$_{2}$Fe$_{3}$Si$_{5}$.\n",
        "  We reformulate the eigenvalue problem for the selection--mutation equilibrium\ndistribution in the case of a haploid asexually reproduced population in the\nform of an equation for an unknown probability generating function of this\ndistribution. The special form of this equation in the infinite sequence limit\nallows us to obtain analytically the steady state distributions for a number of\nparticular cases of the fitness landscape. The general approach is illustrated\nby examples; theoretical findings are compared with numerical calculations.\n",
        "  Creating accurate meta-embeddings from pre-trained source embeddings has\nreceived attention lately. Methods based on global and locally-linear\ntransformation and concatenation have shown to produce accurate\nmeta-embeddings. In this paper, we show that the arithmetic mean of two\ndistinct word embedding sets yields a performant meta-embedding that is\ncomparable or better than more complex meta-embedding learning methods. The\nresult seems counter-intuitive given that vector spaces in different source\nembeddings are not comparable and cannot be simply averaged. We give insight\ninto why averaging can still produce accurate meta-embedding despite the\nincomparability of the source vector spaces.\n",
        "  We show that the Kauffman bracket skein modules of certain manifolds obtained\nfrom integral surgery on a (2,2b) torus link are finitely generated, and list\nthe generators for select examples.\n",
        "  Suppose the knot group G(K) of a knot K has a non-abelian representation \\rho\non A_4 \\subset GL(4,Z). We conjecture that the twisted Alexander polynomial of\nK associated to \\rho is of the form: \\Delta_K(t)/(1-t) \\phi(t^3), where\n\\Delta_K (t) is the Alexander polynomial of K and \\phi(t^3) is an integer\npolynomial in t^3. We prove the conjecture for 2-bridge knots K whose group\nG(K) can be mapped onto a free product Z/2*Z/3. Later, we discuss more general\nmetabelian representations of the knot groups and propose a similar conjecture\non the form of the twisted Alexander polynomials.\n",
        "  The outcomes of evolution are determined by which mutations occur and fix. In\nrapidly adapting microbial populations, this process is particularly hard to\npredict because lineages with different beneficial mutations often spread\nsimultaneously and interfere with one another's fixation. Hence to predict the\nfate of any individual variant, we must know the rate at which new mutations\ncreate competing lineages of higher fitness. Here, we directly measured the\neffect of this interference on the fates of specific adaptive variants in\nlaboratory Saccharomyces cerevisiae populations and used these measurements to\ninfer the distribution of fitness effects of new beneficial mutations. To do\nso, we seeded marked lineages with different fitness advantages into replicate\npopulations and tracked their subsequent frequencies for hundreds of\ngenerations. Our results illustrate the transition between strongly\nadvantageous lineages which decisively sweep to fixation and more moderately\nadvantageous lineages that are often outcompeted by new mutations arising\nduring the course of the experiment. We developed an approximate likelihood\nframework to compare our data to simulations and found that the effects of\nthese competing beneficial mutations were best approximated by an exponential\ndistribution, rather than one with a single effect size. We then used this\ninferred distribution of fitness effects to predict the rate of adaptation in a\nset of independent control populations. Finally, we discuss how our\nexperimental design can serve as a screen for rare, large-effect beneficial\nmutations.\n",
        "  We show that the number of noncommensurable lattices, hence also that of\nmaximal lattices in SO(1,n) is at least exponential. To do so we construct\nlarge families of noncommensurable hybrid hyperbolic (Gromov/Piatetski-Shapiro)\nmanifolds.\n",
        "  The web of data has brought forth the need to preserve and sustain evolving\ninformation within linked datasets; however, a basic requirement of data\npreservation is the maintenance of the datasets' structural characteristics as\nwell. As open data are often found using different and/or heterogeneous data\nmodels and schemata from one source to another, there is a need to reconcile\nthese mismatches and provide common denominations of interpretation on a\nmultitude of levels, in order to be able to preserve and manage the evolution\nof the generated resources. In this paper, we present a linked data approach\nfor the preservation and archiving of open heterogeneous datasets that evolve\nthrough time, at both the structural and the semantic layer. We first propose a\nset of re-quirements for modelling evolving linked datasets. We then proceed on\nconcep-tualizing a modelling framework for evolving entities and place these in\na 2x2 model space that consists of the semantic and the temporal dimensions.\n",
        "  Observations reveal massive amounts of OVI around star-forming $L_*$\ngalaxies, with covering fractions of near unity extending to the host halo's\nvirial radius. This OVI absorption is typically kinematically centered upon\nphotoionized gas, with line widths that are suprathermal and kinematically\noffset from the galaxy. We discuss various scenarios and whether they could\nresult in the observed phenomenology (cooling gas flows, boundary layers,\nshocks, virialized gas, photoionized clouds in thermal equilibrium). If\npredominantly collisionally ionized, as we argue is most probable, the OVI\nobservations require that the circumgalactic medium (CGM) of $L_*$ galaxies\nholds nearly all the associated baryons within a virial radius ($\\sim\n10^{11}M_\\odot$) and hosts massive flows of cooling gas with\n$\\approx30[nT/30{\\rm~cm^{-3}K}]~M_\\odot~$yr$^{-1}$, which must be largely\nprevented from accreting onto the host galaxy. Cooling and feedback energetics\nconsiderations require $10 <\\langle nT\\rangle<100{\\rm~cm^{-3}K}$ for the warm\nand hot halo gases. We argue that virialized gas, boundary layers, hot winds,\nand shocks are unlikely to directly account for the bulk of the OVI.\nFurthermore, we show that there is a robust constraint on the number density of\nmany of the photoionized $\\sim10^4$K absorption systems that yields upper\nbounds in the range $n<(0.1-3)\\times10^{-3}(Z/0.3)$cm$^{-3}$, where $Z$ is the\nmetallicity, suggestive that the dominant pressure in some photoionized clouds\nis nonthermal. This constraint, which requires minimal ionization modeling, is\nin accord with the low densities inferred from more complex photoionization\nmodeling. The large amount of cooling gas that is inferred could re-form these\nclouds in a fraction of the halo dynamical time, as some arguments require, and\nit requires much of the feedback energy available from supernovae and stellar\nwinds to be dissipated in the CGM.\n",
        "  We consider the problem of adapting neural paragraph-level question answering\nmodels to the case where entire documents are given as input. Our proposed\nsolution trains models to produce well calibrated confidence scores for their\nresults on individual paragraphs. We sample multiple paragraphs from the\ndocuments during training, and use a shared-normalization training objective\nthat encourages the model to produce globally correct output. We combine this\nmethod with a state-of-the-art pipeline for training models on document QA\ndata. Experiments demonstrate strong performance on several document QA\ndatasets. Overall, we are able to achieve a score of 71.3 F1 on the web portion\nof TriviaQA, a large improvement from the 56.7 F1 of the previous best system.\n",
        "  We report the electrical, magneto transport and specific heat of the layered\npolycrystalline RECoPO (RE = La, Nd and Sm) samples. These compounds are\niso-structural to recently discovered superconductor LaFeAs(O/F). Bulk\npolycrystalline samples are synthesized by solid state reaction route in an\nevacuated sealed quartz tube. All these compounds are crystallized in a\ntetragonal structure with space group P4/nmm. The Cobalt in these compounds is\nin itinerant state with its paramagnetic moment above 1.4\\muB and the same\norders ferromagnetically (FM) with saturation moment of around 0.20\\muB below\nsay 80K. Though, LaCoPO shows single paramagnetic (PM) to ferromagnetic (FM)\ntransition near 35K, the NdCoPO and SmCoPO exhibit successive PM-FM-AFM\ntransitions. Both FM and AFM transition temperatures vary with applied field.\nAlthough the itinerant ferromagnetism occurs with small saturation moment,\ntypical anti-ferromagnetic (AFM) transitions (TN1, TN2) are observed at 69K and\n14K for Nd and 57K and 45K for Sm. This FM-AFM transition of Co spins in NdCoPO\nand SmCoPO is both field and temperature dependent. The Magneto-transport of\nNdCoPO and SmCoPO distinctly follows their successive PM-FM-AFM transitions. It\nis clear that Sm/Nd (4f) interacts with the Co (3d) in first time synthesized\nSm/NdCoPO.\n",
        "  The 3-strand pretzel knots and links are a well-studied source of examples in\nknot theory. However, while there have been computations of the Khovanov\nhomology of some sub-families of 3-strand pretzel knots, no general formula has\nbeen given for all of them. We give a general formula for the unreduced\nKhovanov homology of all 3-strand pretzel links, over the rational numbers.\n",
        "  Supervised training of abstractive language generation models results in\nlearning conditional probabilities over language sequences based on the\nsupervised training signal. When the training signal contains a variety of\nwriting styles, such models may end up learning an 'average' style that is\ndirectly influenced by the training data make-up and cannot be controlled by\nthe needs of an application. We describe a family of model architectures\ncapable of capturing both generic language characteristics via shared model\nparameters, as well as particular style characteristics via private model\nparameters. Such models are able to generate language according to a specific\nlearned style, while still taking advantage of their power to model generic\nlanguage phenomena. Furthermore, we describe an extension that uses a mixture\nof output distributions from all learned styles to perform on-the fly style\nadaptation based on the textual input alone. Experimentally, we find that the\nproposed models consistently outperform models that encapsulate single-style or\naverage-style language generation capabilities.\n",
        "  We define families of invariants for elements of the mapping class group of\nS, a compact orientable surface. Fix any characteristic subgroup H of pi_1(S)\nand restrict to J(H), any subgroup of mapping classes that induce the identity\nmodulo H. To any unitary representation, r of pi_1(S)/H we associate a\nhigher-order rho_r-invariant and a signature 2-cocycle sigma_r. These signature\ncocycles are shown to be generalizations of the Meyer cocycle. In particular\neach rho_r is a quasimorphism and each sigma_r is a bounded 2-cocycle on J(H).\nIn one of the simplest non-trivial cases, by varying r, we exhibit infinite\nfamilies of linearly independent quasimorphisms and signature cocycles. We show\nthat the rho_r restrict to homomorphisms on certain interesting subgroups. Many\nof these invariants extend naturally to the full mapping class group and some\nextend to the monoid of homology cylinders based on S.\n",
        "  Often corporations need tools to improve their decision making in a\ncompetitive market. In general, these tools are based on data warehouse\nplatforms to mange and analyze large amounts of data. However, several of these\ncorporations do not have enough resources to buy such platforms because of the\nhigh cost. This work is dedicated to a feasibility study of a low cost platform\nto data warehouse. We consider as a low cost platform the use of open source\nsoftware like the PostgreSQL database system and the GNU/Linux operational\nsystem. We verify the feasibility of this platform by executing two benchmarks\nthat simulate a data warehouse workload. The workload reproduces a multi-user\nenvironment with the execution of complex queries, which executes:\naggregations, nested sub queries, multi joins, in-line views and more.\nConsidering the results we were able to highlight some problems on the\nPostgreSQL database system, and discuss improvements in the context of data\nwarehouse.\n",
        "  The work of J{\\o}rgensen and Thurston shows that there is a finite number\nN(v) of orientable hyperbolic 3-manifolds with any given volume v. In this\npaper, we construct examples showing that the number of hyperbolic knot\ncomplements with a given volume v can grow at least factorially fast with v. A\nsimilar statement holds for closed hyperbolic 3-manifolds, obtained via Dehn\nsurgery. Furthermore, we give explicit estimates for lower bounds of N(v) in\nterms of v for these examples. These results improve upon the work of Hodgson\nand Masai, which describes examples that grow exponentially fast with v. Our\nconstructions rely on performing volume preserving mutations along Conway\nspheres and on the classification of Montesinos knots.\n",
        "  Gas-phase complex organic molecules are commonly detected toward high-mass\nprotostellar hot cores. Detections toward low-mass protostars and outflows are\ncomparatively rare, and a larger sample is key to investigate how the chemistry\nresponds to its environment. Guided by the prediction that complex organic\nmolecules form in CH3OH-rich ices and thermally or non-thermally evaporate with\nCH3OH, we have identified three sight-lines in the Serpens core - SMM1, SMM4\nand SMM4-W - which are likely to be rich in complex organics. Using the IRAM\n30m telescope, narrow lines (FWHM of 1-2 km s-1) of CH3CHO and CH3OCH3 are\ndetected toward all sources, HCOOCH3 toward SMM1 and SMM4-W, and C2H5OH not at\nall. Beam-averaged abundances of individual complex organics range between 0.6\nand 10% with respect to CH3OH when the CH3OH rotational temperature is applied.\nThe summed complex organic abundances also vary by an order of magnitude, with\nthe richest chemistry toward the most luminous protostar SMM1. The range of\nabundances compare well with other beam-averaged observations of low-mass\nsources. Complex organic abundances are of the same order of magnitude toward\nlow-mass protostars and high-mass hot cores, but HCOOCH3 is relatively more\nimportant toward low-mass protostars. This is consistent with a sequential ice\nphotochemistry, dominated by CHO-containing products at low temperatures and\nearly times.\n",
        "  Analytic and computational methods developed within statistical physics have\nfound applications in numerous disciplines. In this letter, we use such methods\nto solve a long-standing problem in statistical genetics. The problem, posed by\nHaldane and Waddington [J.B.S. Haldane and C.H. Waddington, Genetics 16,\n357-374 (1931)], concerns so-called recombinant inbred lines (RILs) produced by\nrepeated inbreeding. Haldane and Waddington derived the probabilities of RILs\nwhen considering 2 and 3 genes but the case of 4 or more genes has remained\nelusive. Our solution uses two probabilistic frameworks relatively unknown\noutside of physics: Glauber's formula and self-consistent equations of the\nSchwinger-Dyson type. Surprisingly, this combination of statistical formalisms\nunveils the exact probabilities of RILs for any number of genes. Extensions of\nthe framework may have applications in population genetics and beyond.\n",
        "  The standard actions of finite groups on spheres S^d are linear actions, i.e.\nby finite subgroups of the orthogonal group O(d+1). We prove that, in each\ndimension d>5, there is a finite group G which admits a faithful, topological\naction on a sphere S^d but is not isomorphic to a subgroup of O(d+1). The\nsituation remains open for smooth actions.\n",
        "  The purpose of this study is to measure the internal radiation dose using a\nhuman blood sample. In the literature, there is no process that allows the\ndirect measurement of the internal radiation dose received by a person. The\nluminescence counts from a blood sample having a laboratory-injected radiation\ndose and the waste blood of the patient injected with a radiopharmaceutical for\ndiagnostic purposes were both measured. The decay and dose-response curves were\nplotted for the different doses. The doses received by the different blood\naliquots can be determined by interpolating the luminescence counts to the\ndose-response curve. This study shows that the dose received by a person can be\nmeasured directly, simply and retrospectively by using only a very small amount\nof blood sample. The results will have important ramifications for the medicine\nand healthcare fields in particular. This will also be very important in cases\nof suspicion of radiation poisoning, malpractice and so on.\n",
        "  Defaults in vascular (VN) and neuronal networks of spinal cord are\nresponsible for serious neurodegenerative pathologies. Because of inadequate\ninvestigation tools, the lacking knowledge of the complete fine structure of VN\nand neuronal systems is a crucial problem. Conventional 2D imaging yields\nincomplete spatial coverage leading to possible data misinterpretation, whereas\nstandard 3D computed tomography imaging achieves insufficient resolution and\ncontrast. We show that X-ray high-resolution phase-contrast tomography allows\nthe simultaneous visualization of three-dimensional VN and neuronal systems of\nmouse spinal cord at scales spanning from millimeters to hundreds of\nnanometers, with neither contrast agent nor a destructive sample-preparation.\nWe image both the 3D distribution of micro-capillary network and the\nmicrometric nerve fibers, axon-bundles and neuron soma. Our approach is a\ncrucial tool for pre-clinical investigation of neurodegenerative pathologies\nand spinal-cord-injuries. In particular, it should be an optimal tool to\nresolve the entangled relationship between VN and neuronal system.\n",
        "  For an oriented knot $K$, we construct a functor from the category of pointed\nquandles to the category of quandles in three different ways. We also extend\nthe quandle cocycle invariants of knots by using these quandle-valued invariant\nof knots, and study their properties.\n",
        "  The accuracy of deformable image registration (DIR) has a significant\ndosimetric impact in radiation treatment planning. We evaluated accuracy of\nvarious DIR algorithms using variations of the deformation point and volume.\nThe reference image (Iref) and volume (Vref) was first generated with virtual\ndeformation QA software (ImSimQA, Oncology System Limited, UK). We deformed\nIref with axial movement of deformation point and Vref depending on the types\nof deformation that are the deformation1 is to increase the Vref (relaxation)\nand the deformation 2 is to decrease . The deformed image (Idef) and volume\n(Vdef) acquired by ImSimQA software were inversely deformed to Iref and Vref\nusing DIR algorithms. As a result, we acquired deformed image (Iid) from Idef\nand volume (Vid) from Vdef. The DIR algorithms were the Horn Schunk optical\nflow (HS), Iterative Optical Flow (IOF), Modified Demons (MD) and Fast Demons\n(FD) with the Deformable Image Registration and Adaptive Radiotherapy Toolkit\n(DIRART) of MATLAB. The image similarity between Iref and Iid was calculated\nusing the metrics that were Normalized Mutual Information (NMI) and Normalized\nCross Correlation (NCC). When moving distance of deformation point was 4 mm,\nthe value of NMI was above 1.81 and NCC was above 0.99 in all DIR\nalgorithms.When the Vref increased or decreased about 12%, the difference\nbetween Vref and Vid was within 5% regardless of the type of deformation.The\nvalue of Dice Similarity Coefficient (DSC) was above 0.95 in deformation1\nexcept for the MD algorithm. In case of deformation 2, that of DSC was above\n0.95 in all DIR algorithms. The Idef and Vdef have not been completely restored\nto Iref and Vref and the accuracy of DIR algorithms was different depending on\nthe degree of deformation. Hence, the performance of DIR algorithms should be\nverified for the desired applications.\n",
        "  The purpose of this study was to develop a method for tissue elasticity\nimaging using tagged magnetic resonance imaging (MRI). First, we developed a\ncyclic pressure device that used air to remotely transmit the power to generate\ncyclic deformation in an object. The pressure induced by the cyclic pressure\ndevice was measured by MRI-compatible force sensors. Second, we developed a\nsoftware to calculate Young's modulus from tagged MRI data using the harmonic\nphase (HARP) method and the finite element method (FEM). We also developed a\nsoftware to extract tag-cross points from tagged MRI data. Finally, we\nevaluated the usefulness of our method using three homogeneous silicone gel\nphantoms with different degrees of stiffness in comparison with Young's moduli\nmeasured by a material testing machine. The coefficient of variation of the\npressure data measured by MRI-compatible force sensors was within 5 %,\nindicating that the reproducibility of the pressure generated by our cyclic\npressure device was good. The Young's moduli obtained by the material testing\nmachine, the HARP method, and the FEM increased with increasing stiffness of\nthe phantoms. There were relatively good agreements among the three methods.\nThese results suggest that our method is useful for tissue elasticity imaging\nand for quantifying the stiffness of tissues.\n",
        "  Two major sources of stochasticity in the dynamics of neutral alleles result\nfrom resampling of finite populations (genetic drift) and the random genetic\nbackground of nearby selected alleles on which the neutral alleles are found\n(linked selection). There is now good evidence that linked selection plays an\nimportant role in shaping polymorphism levels in a number of species. One of\nthe best investigated models of linked selection is the recurrent full sweep\nmodel, in which newly arisen selected alleles fix rapidly. However, the bulk of\nselected alleles that sweep into the population may not be destined for rapid\nfixation. Here we develop a general model of recurrent selective sweeps in a\ncoalescent framework, one that generalizes the recurrent full sweep model to\nthe case where selected alleles do not sweep to fixation. We show that in a\nlarge population, only the initial rapid increase of a selected allele affects\nthe genealogy at partially linked sites, which under fairly general assumptions\nare unaffected by the subsequent fate of the selected allele. We also apply the\ntheory to a simple model to investigate the impact of recurrent partial sweeps\non levels of neutral diversity, and find that for a given reduction in\ndiversity, the impact of recurrent partial sweeps on the frequency spectrum at\nneutral sites is determined primarily by the frequencies achieved by the\nselected alleles. Consequently, recurrent sweeps of selected alleles to low\nfrequencies can have a profound effect on levels of diversity but can leave the\nfrequency spectrum relatively unperturbed. In fact, the limiting coalescent\nmodel under a high rate of sweeps to low frequency is identical to the standard\nneutral model. The general model of selective sweeps we describe goes some way\ntowards providing a more flexible framework to describe genomic patterns of\ndiversity than is currently available.\n",
        "  We exhibit an infinite family of knots with isomorphic knot Heegaard Floer\nhomology. Each knot in this infinite family admits a nontrivial genus two\nmutant which shares the same total dimension in both knot Floer homology and\nKhovanov homology. Each knot is distinguished from its genus two mutant by both\nknot Floer homology and Khovanov homology as bigraded groups. Additionally, for\nboth knot Heegaard Floer homology and Khovanov homology, the genus two mutation\ninterchanges the groups in $\\delta$-gradings $k$ and $-k$.\n",
        "  This paper proposes a novel approach to create an unit set for CTC based\nspeech recognition systems. By using Byte Pair Encoding we learn an unit set of\nan arbitrary size on a given training text. In contrast to using characters or\nwords as units this allows us to find a good trade-off between the size of our\nunit set and the available training data. We evaluate both Crossword units,\nthat may span multiple word, and Subword units. By combining this approach with\ndecoding methods using a separate language model we are able to achieve state\nof the art results for grapheme based CTC systems.\n",
        "  The individual k\\parallel and k\\perp stripe excitations in fluctuating\nspin-charge stripes have not been observed yet. In Raman scattering if we set,\nfor example, incident and scattered light polarizations to two possible stripe\ndirections, we can observe the fluctuating stripe as if it is static. Using the\ndifferent symmetry selection rule between the B1g two-magnon scattering and the\nB1g and B2g isotropic electronic scattering, we succeeded to obtain the\nk\\parallel and k\\perp strip magnetic excitations separately in La2-xSrxCuO4.\nOnly the k\\perp stripe excitations appear in the wide-energy isotropic\nelectronic Raman scattering, indicating that the charge transfer is restricted\nto the direction perpendicular to the stripe. This is the same as the Burgers\nvector of an edge dislocation which easily slides perpendicularly to the\nstripe. Hence charges at the edge dislocation move together with the\ndislocation perpendicularly to the stripe, while other charges are localized. A\nlooped edge dislocation has lower energy than a single edge dislocation. The\nsuperconducting coherence length is close to the inter-charge stripe distance\nat x \\le 0.2. Therefore we conclude that Cooper pairs are formed at looped edge\ndislocations. The restricted charge transfer direction naturally explains the\nopening of a pseudogap around (0, {\\pi}) for the stripe parallel to the b axis\nand the reconstruction of the Fermi surface to have a flat plane near (0,\n{\\pi}). They break the four-fold rotational symmetry. Furthermore the\nsystematic experiments revealed the carrier density dependence of the isotropic\nand anisotropic electronic excitations, the spin density wave and/or charge\ndensity wave gap near ({\\pi}/2, {\\pi}/2), and the strong coupling between the\nelectronic states near ({\\pi}/2, {\\pi}/2) and the zone boundary phonons at\n({\\pi}, {\\pi}).\n",
        "  The Bose-Einstein condensation (BEC) critical temperature in a relativistic\nideal Bose gas of identical bosons, with and without the antibosons expected to\nbe pair-produced abundantly at sufficiently hot temperatures, is exactly\ncalculated for all boson number-densities, all boson point rest masses, and all\ntemperatures. The Helmholtz free energy at the critical BEC temperature is\nfound to be lower, thus implying that the omission of antibosons always leads\nto the computation of a metastable state.\n",
        "  We provide a formula for the Dubrovnik polynomial of a rational knot in terms\nof the entries of the tuple associated with a braid-form diagram of the knot.\nOur calculations can be easily carried out using a computer algebra system.\n",
        "  We report on the young massive clump (G35.20w) in W48 that previous molecular\nline and dust observations have revealed to be in the very early stages of star\nformation. Based on virial analysis, we find that a strong field of ~1640\nmicroG is required to keep the clump in pressure equilibrium. We performed a\ndeep Zeeman effect measurement of the 113 GHz CN (1-0) line towards this clump\nwith the IRAM 30 m telescope. We combine simultaneous fitting of all CN\nhyperfines with Monte Carlo simulations for a large range in realization of the\nmagnetic field to obtain a constraint on the line-of-sight field strength of\n-687 +/- 420 microG. We also analyze archival dust polarization observations\ntowards G35.20w. A strong magnetic field is implied by the remarkably ordered\nfield orientation that is perpendicular to the longest axis of the clump. Based\non this, we also estimate the plane-of-sky component of the magnetic field to\nbe ~740 microG. This allows for a unique comparison of the two orthogonal\nmeasurements of magnetic field strength of the same region and at similar\nspatial scales. The expected total field strength shows no significant conflict\nbetween the observed field and that required for pressure equilibrium. By\nproducing a probability distribution for a large range in field geometries, we\nshow that plane-of-sky projections are much closer to the true field strengths\nthan line-of-sight projections. This can present a significant challenge for\nZeeman measurements of magnetized structures, even with ALMA. We also show that\nCN molecule does not suffer from depletion on the observed scales in the\npredominantly cold and highly deuterated core in an early stage of high-mass\nstar formation and is thus a good tracer of the dense gas.\n",
        "  We seek to address the lack of labeled data (and high cost of annotation) for\ntextual entailment in some domains. To that end, we first create (for\nexperimental purposes) an entailment dataset for the clinical domain, and a\nhighly competitive supervised entailment system, ENT, that is effective (out of\nthe box) on two domains. We then explore self-training and active learning\nstrategies to address the lack of labeled data. With self-training, we\nsuccessfully exploit unlabeled data to improve over ENT by 15% F-score on the\nnewswire domain, and 13% F-score on clinical data. On the other hand, our\nactive learning experiments demonstrate that we can match (and even beat) ENT\nusing only 6.6% of the training data in the clinical domain, and only 5.8% of\nthe training data in the newswire domain.\n",
        "  We have investigated the suppression of the superconducting transition\ntemperature, Tc, with an increase of the residual resistivity, \\rho_0, through\nthe substitution of M (M = Co, Ni, Zn) for Fe in Fe1-yMySe0.3Te0.7 single\ncrystals, in order to clarify the symmetry of the superconducting gap in\nFeSe1-xTex. Small, large and very small suppression of Tc have been observed\nthrough the Co, Ni and Zn substitution, respectively. The magnitude of the\nsuppression rate is hardly explained in terms of the pair-breaking effect due\nto potential scattering calculated based on the Abrikosov-Gor'kov theory, even\nif errors in the estimation of the carrier concentration, effective mass and\n\\rho_0 were taken into account. Accordingly, these results suggest that the\nsuperconducting symmetry is the s++-wave in FeSe1-xTex.\n",
        "  We present an analysis of the deepest Herschel images in four major\nextragalactic fields GOODS-North, GOODS-South, UDS and COSMOS obtained within\nthe GOODS-Herschel and CANDELS-Herschel key programs. The picture provided by\n10497 individual far-infrared detections is supplemented by the stacking\nanalysis of a mass-complete sample of 62361 star-forming galaxies from the\nCANDELS-HST H band-selected catalogs and from two deep ground-based Ks\nband-selected catalogs in the GOODS-North and the COSMOS-wide fields, in order\nto obtain one of the most accurate and unbiased understanding to date of the\nstellar mass growth over the cosmic history. We show, for the first time, that\nstacking also provides a powerful tool to determine the dispersion of a\nphysical correlation and describe our method called \"scatter stacking\" that may\nbe easily generalized to other experiments. We demonstrate that galaxies of all\nmasses from z=4 to 0 follow a universal scaling law, the so-called main\nsequence of star-forming galaxies. We find a universal close-to-linear slope of\nthe logSFR-logM* relation with evidence for a flattening of the main sequence\nat high masses (log(M*/Msun) > 10.5) that becomes less prominent with\nincreasing redshift and almost vanishes by z~2. This flattening may be due to\nthe parallel stellar growth of quiescent bulges in star-forming galaxies.\nWithin the main sequence, we measure a non varying SFR dispersion of 0.3 dex.\nThe specific SFR (sSFR=SFR/M*) of star-forming galaxies is found to\ncontinuously increase from z=0 to 4. Finally we discuss the implications of our\nfindings on the cosmic SFR history and show that more than 2/3 of present-day\nstars must have formed in a regime dominated by the main sequence mode. As a\nconsequence we conclude that, although omnipresent in the distant Universe,\ngalaxy mergers had little impact in shaping the global star formation history\nover the last 12.5 Gyr.\n",
        "  Let M be an oriented irreducible 3-manifold with infinite fundamental group\nand empty or toroidal boundary. Consider any element \\phi in the first\ncohomology of M with integral coefficients. Then one can define the\n\\phi-twisted L^2-torsion function of the universal covering which is a function\nfrom the set of positive real numbers to the set of real numbers. By earlier\nwork of the second author and Schick the evaluation at t=1 determines the\nvolume.\n  In this paper we show that its degree, which is a number extracted from its\nasymptotic behavior at 0 and at infinity, agrees with the Thurston norm of\n\\phi.\n",
        "  Copying mechanism shows effectiveness in sequence-to-sequence based neural\nnetwork models for text generation tasks, such as abstractive sentence\nsummarization and question generation. However, existing works on modeling\ncopying or pointing mechanism only considers single word copying from the\nsource sentences. In this paper, we propose a novel copying framework, named\nSequential Copying Networks (SeqCopyNet), which not only learns to copy single\nwords, but also copies sequences from the input sentence. It leverages the\npointer networks to explicitly select a sub-span from the source side to target\nside, and integrates this sequential copying mechanism to the generation\nprocess in the encoder-decoder paradigm. Experiments on abstractive sentence\nsummarization and question generation tasks show that the proposed SeqCopyNet\ncan copy meaningful spans and outperforms the baseline models.\n",
        "  Neural word segmentation has attracted more and more research interests for\nits ability to alleviate the effort of feature engineering and utilize the\nexternal resource by the pre-trained character or word embeddings. In this\npaper, we propose a new neural model to incorporate the word-level information\nfor Chinese word segmentation. Unlike the previous word-based models, our model\nstill adopts the framework of character-based sequence labeling, which has\nadvantages on both effectiveness and efficiency at the inference stage. To\nutilize the word-level information, we also propose a new long short-term\nmemory (LSTM) architecture over directed acyclic graph (DAG). Experimental\nresults demonstrate that our model leads to better performances than the\nbaseline models.\n",
        "  We introduce a method for transliteration generation that can produce\ntransliterations in every language. Where previous results are only as\nmultilingual as Wikipedia, we show how to use training data from Wikipedia as\nsurrogate training for any language. Thus, the problem becomes one of ranking\nWikipedia languages in order of suitability with respect to a target language.\nWe introduce several task-specific methods for ranking languages, and show that\nour approach is comparable to the oracle ceiling, and even outperforms it in\nsome cases.\n",
        "  We discuss bound states appearing at the interface between two different\nsuperconductors characterized by different nontrivial topological numbers such\nas one-dimensional winding numbers and Chern numbers. The one-dimensional\nwinding number characterizes d_{xy} and p_x wave superconductors. The Chern\nnumber characterizes chiral-p, chiral-d, and chiral-f wave superconductors. The\ninterfacial bound state appears at the zero-energy when the topological numbers\nof the two superconductors are different from each other. When the two\nsuperconductors are characterized by the Chern numbers n and m, for example,\nthe number of the zero-energy bound is |n-m| independent of junction parameters\nsuch as the phase difference across the junction and the transmission\nprobability of interface. We generalize a concept of bulk-boundary\ncorrespondence to the Josephson junctions consisting of two topological\nsuperconductors.\n",
        "  In query optimisation accurate cardinality estimation is essential for\nfinding optimal query plans. It is especially challenging for RDF due to the\nlack of explicit schema and the excessive occurrence of joins in RDF queries.\nExisting approaches typically collect statistics based on the counts of triples\nand estimate the cardinality of a query as the product of its join components,\nwhere errors can accumulate even when the estimation of each component is\naccurate. As opposed to existing methods, we propose PRESTO, a cardinality\nestimation method that is based on the counts of subgraphs instead of triples\nand uses a probabilistic method to estimate cardinalities of RDF queries as a\nwhole. PRESTO avoids some major issues of existing approaches and is able to\naccurately estimate arbitrary queries under a bound memory constraint. We\nevaluate PRESTO with YAGO and show that PRESTO is more accurate for both simple\nand complex queries.\n",
        "  The concept of matching dependencies (mds) is recently pro- posed for\nspecifying matching rules for object identification. Similar to the functional\ndependencies (with conditions), mds can also be applied to various data quality\napplications such as violation detection. In this paper, we study the problem\nof discovering matching dependencies from a given database instance. First, we\nformally define the measures, support and confidence, for evaluating utility of\nmds in the given database instance. Then, we study the discovery of mds with\ncertain utility requirements of support and confidence. Exact algorithms are\ndeveloped, together with pruning strategies to improve the time performance.\nSince the exact algorithm has to traverse all the data during the computation,\nwe propose an approximate solution which only use some of the data. A bound of\nrelative errors introduced by the approximation is also developed. Finally, our\nexperimental evaluation demonstrates the efficiency of the proposed methods.\n",
        "  The effect of the intersite and interplane Coulomb interactions between the\nDirac fermions on the formation of the Kohn-Luttinger superconductivity in\nbilayer doped graphene is studied disregarding the effects of the van der Waals\npotential of the substrate and both magnetic and non-magnetic impurities. The\nphase diagram determining the boundaries of superconductive domains with\ndifferent types of symmetry of the order parameter is built using the extended\nHubbard model in the Born weak-coupling approximation with allowance for the\nintratomic, interatomic, and interlayer Coulomb interactions between electrons.\nIt is shown that the Kohn-Luttinger polarization contributions up to the second\norder of perturbation theory in the Coulomb interaction inclusively and an\naccount for the long-range intraplane Coulomb interactions significantly affect\nthe competition between the superconducting $f-$, $p+ip-$, and $d+id-$wave\npairings. It is demonstrated that the account for the interplane Coulomb\ninteraction enhances the critical temperature of the transition to the\nsuperconducting phase.\n",
        "  A Coxeter $n$-orbifold is an $n$-dimensional orbifold based on a polytope\nwith silvered boundary facets. Each pair of adjacent facets meet on a ridge of\nsome order $m$, whose neighborhood is locally modeled on ${\\mathbb R}^n$ modulo\nthe dihedral group of order $2m$ generated by two reflections. For $n \\geq 3$,\nwe study the deformation space of real projective structures on a compact\nCoxeter $n$-orbifold $Q$ admitting a hyperbolic structure. Let $e_+(Q)$ be the\nnumber of ridges of order $\\geq 3$. A neighborhood of the hyperbolic structure\nin the deformation space is a cell of dimension $e_+(Q) - n$ if $n=3$ and $Q$\nis weakly orderable, i.e., the faces of $Q$ can be ordered so that each face\ncontains at most $3$ edges of order $2$ in faces of higher indices, or $Q$ is\nbased on a truncation polytope.\n",
        "  This paper briefly reviews past milestones in the field of medical image\nreconstruction and describes some future directions. It is part of an overview\npaper on \"open problems in signal processing\" that will appear in IEEE Signal\nProcessing Magazine, but presented here with citations and equations.\n",
        "  The Maximum Entropy Theory of Ecology (METE) is a unified theory of\nbiodiversity that predicts a large number of macroecological patterns using\nonly information on the species richness, total abundance, and total metabolic\nrate of the community. We evaluated four major predictions of METE\nsimultaneously at an unprecedented scale using data from 60 globally\ndistributed forest communities including over 300,000 individuals and nearly\n2000 species. METE successfully captured 96% and 89% of the variation in the\nspecies abundance distribution and the individual size distribution, but\nperformed poorly when characterizing the size-density relationship and\nintraspecific distribution of individual size. Specifically, METE predicted a\nnegative correlation between size and species abundance, which is weak in\nnatural communities. By evaluating multiple predictions with large quantities\nof data, our study not only identifies a mismatch between abundance and body\nsize in METE, but also demonstrates the importance of conducting strong tests\nof ecological theories.\n",
        "  The Resource Description Framework (RDF) represents information as\nsubject-predicate-object triples. These triples are commonly interpreted as a\ndirected labelled graph. We propose an alternative approach, interpreting the\ndata as a 3-way Boolean tensor. We show how SPARQL queries - the standard\nqueries for RDF - can be expressed as elementary operations in Boolean algebra,\ngiving us a complete re-interpretation of RDF and SPARQL. We show how the\nBoolean tensor interpretation allows for new optimizations and analyses of the\ncomplexity of SPARQL queries. For example, estimating the size of the results\nfor different join queries becomes much simpler.\n",
        "  We study nanodevices based on ultrathin superconducting nanowires connected\nin parallel to form nanowire SQUIDs. The function of the critical current\nversus magnetic field, $I_{C}(B)$, is multivalued, asymmetric and its maxima\nand minima are shifted from the usual integer and half integer flux quantum\npoints. The nanowire interference device is qualitatively distinct from\nconventional SQUIDs because nanowires do not obey the same current-phase\nrelationship (CPR) as Josephson junctions. We demonstrate that the results can\nbe explained assuming that (i) the CPR is linear and (ii) that each wire is\ncharacterized by a sample-specific critical phase, which is usually much larger\nthan $\\pi/2$. Our proposed model offers accurate fits to $I_{C}(B)$. It\nexplains the single-valuedness regions where only one vorticity (i.e., the\norder parameter winding number) is stable as well as regions where multiple\nvorticity values are allowed for the SQUIDs. We also observe and explain\nregions in which the standard deviation of the switching current is independent\nof the magnetic field. We develop a technique that allows a reliable detection\nof hidden phase-slips. Using this technique we find that our model correctly\npredicts the boundaries of vorticity regions, even at low currents where\n$I_C(B)$ is not directly measurable.\n",
        "  The Intel Science and Technology Center for Big Data is developing a\nreference implementation of a Polystore database. The BigDAWG (Big Data Working\nGroup) system supports \"many sizes\" of database engines, multiple programming\nlanguages and complex analytics for a variety of workloads. Our recent efforts\ninclude application of BigDAWG to an ocean metagenomics problem and\ncontainerization of BigDAWG. We intend to release an open source BigDAWG v1.0\nin the Spring of 2017. In this article, we will demonstrate a number of\npolystore applications developed with oceanographic researchers at MIT and\ndescribe our forthcoming open source release of the BigDAWG system.\n",
        "  Developers across the world use autonumber or auto sequences field of the\nbackend databases for developing both the desktop and web based data centric\napplications which is easier to use at the development and deployment purpose\nbut can create a lot of problems under varied situations. This paper examines\nhow a database independent autonumber could be developed and reused solving all\nthe problems as well as providing the same degree of easy to use features of\nautonumber offered by modern Relational Database Systems.\n",
        "  Providing an appropriate level of accessibility to and tracking of data or\nprocess elements in large volumes of medical data, is an essential requirement\nin the Big Data era. Researchers require systems that provide traceability of\ninformation through provenance data capture and management to support their\nclinical analyses. We present an approach that has been adopted in the neuGRID\nand N4U projects, which aimed to provide detailed traceability to support\nresearch analysis processes in the study of biomarkers for Alzheimers disease,\nbut is generically applicable across medical systems. To facilitate the\norchestration of complex, large-scale analyses in these projects we have\nadapted CRISTAL, a workflow and provenance tracking solution. The use of\nCRISTAL has provided a rich environment for neuroscientists to track and manage\nthe evolution of data and workflow usage over time in neuGRID and N4U.\n",
        "  Comparative genetic studies of non-model organisms are transforming rapidly\ndue to major advances in sequencing technology. A limiting factor in these\nstudies has been the identification and screening of orthologous loci across an\nevolutionarily distant set of taxa. Here, we evaluate the efficacy of genomic\nmarkers targeting ultraconserved DNA elements (UCEs) for analyses at shallow\nevolutionary timescales. Using sequence capture and massively parallel\nsequencing to generate UCE data for five co-distributed Neotropical rainforest\nbird species, we recovered 776-1,516 UCE loci across the five species. Across\nspecies, 53-77 percent of the loci were polymorphic, containing between 2.0 and\n3.2 variable sites per polymorphic locus, on average. We performed species tree\nconstruction, coalescent modeling, and species delimitation, and we found that\nthe five co-distributed species exhibited discordant phylogeographic histories.\nWe also found that species trees and divergence times estimated from UCEs were\nsimilar to those obtained from mtDNA. The species that inhabit the understory\nhad older divergence times across barriers, contained a higher number of\ncryptic species, and exhibited larger effective population sizes relative to\nspecies inhabiting the canopy. Because orthologous UCEs can be obtained from a\nwide array of taxa, are polymorphic at shallow evolutionary time scales, and\ncan be generated rapidly at low cost, they are effective genetic markers for\nstudies investigating evolutionary patterns and processes at shallow time\nscales.\n",
        "  Falciparum malaria is a major parasitic disease causing widespread morbidity\nand mortality globally. Artemisinin derivatives---the most effective and\nwidely-used antimalarials that have helped reduce the burden of malaria by 60%\nin some areas over the past decade---have recently been found to induce growth\nretardation of blood-stage Plasmodium falciparum when applied at clinically\nrelevant concentrations. To date, no model has been designed to quantify the\ngrowth retardation effect and to predict the influence of this property on in\nvivo parasite killing. Here we introduce a mechanistic model of parasite growth\nfrom the ring to trophozoite stage of the parasite's life cycle, and by\nmodelling the level of staining with an RNA-binding dye, we demonstrate that\nthe model is able to reproduce fluorescence distribution data from in vitro\nexperiments using the laboratory 3D7 strain. We quantify the dependence of\ngrowth retardation on drug concentration and demonstrate the model's utility as\na platform to propose experimentally-testable mechanisms of growth retardation.\nFurthermore we illustrate that a drug-induced delay in growth may significantly\ninfluence in vivo parasite dynamics, demonstrating the importance of\nconsidering growth retardation in the design of optimal artemisinin-based\ndosing regimens.\n",
        "  Many clinical applications depend critically on the accurate differentiation\nand classification of different types of materials in patient anatomy. This\nwork introduces a unified framework for accurate nonlinear material\ndecomposition and applies it, for the first time, in the concept of\ntriple-energy CT (TECT) for enhanced material differentiation and\nclassification as well as dual-energy CT. The triple-energy data acquisition is\nimplemented at the scales of micro-CT and clinical CT imaging with commercial\n\"TwinBeam\" dual-source DECT configuration and a fast kV switching DECT\nconfiguration. Material decomposition and quantitative comparison with a photon\ncounting detector and with the presence of a bow-tie filter are also performed.\nThe proposed method provides quantitative material- and energy-selective images\nexamining realistic configurations for both dual- and triple-energy CT\nmeasurements. Compared to the polychromatic kV CT images, virtual monochromatic\nimages show superior image quality. For the mouse phantom, quantitative\nmeasurements show that the differences between gadodiamide and iodine\nconcentrations obtained using TECT and idealized photon counting CT (PCCT) are\nsmaller than 8 mg/mL and 1 mg/mL, respectively. TECT outperforms DECT for\nmulti-contrast CT imaging and is robust with respect to spectrum estimation.\nFor the thorax phantom, the differences between the concentrations of the\ncontrast map and the corresponding true reference values are smaller than 7\nmg/mL for all of the realistic configurations. A unified framework for both\ndual- and triple-energy CT imaging has been established for the accurate\nextraction of material compositions; considering currently available commercial\nDECT configurations. The novel technique is promising to provide an urgently\nneeded solution for several CT-based diagnosis and therapy applications.\n",
        "  For each integer k > 1, Johnson gave a 3-manifold with Heegaard splittings of\ngenera 2k and 2k-1 such that any common stabilization of these two surfaces has\ngenus at least 3k-1. We modify his argument to produce a 3-manifold with two\nHeegaard splitings of genus 2k such that any common stabilization of them has\ngenus at least 3k.\n",
        "  We have obtained K-band spectra at R~5,000 and angular resolution 0.3\" of a\nsection of the Herbig-Haro 7 (HH7) bow shock, using the Near-Infrared Integral\nField Spectrograph at Gemini North. Present in the portion of the data cube\ncorresponding to the brightest part of the bow shock are emission lines of H2\nwith upper state energies ranging from ~6,000 K up to the dissociation energy\nof H2, ~50,000 K. Because of low signal-to-noise ratios, the highest excitation\nlines cannot be easily seen elsewhere in the observed region. However,\nexcitation temperatures, measured throughout much of the observed region using\nlines from levels as high as 25,000 K, are a strong function of upper level\nenergy, indicating that the very highest levels are populated throughout. The\nlevel populations in the brightest region are well fit by a two-temperature\nmodel, with 98.5% of the emitting gas at T=1800 K and 1.5% at T=5200 K. The\nbulk of the H2 line emission in HH7, from the 1,800 K gas, has previously been\nwell modeled by a continuous shock, but the 5,200 K component is inconsistent\nwith standalone standard continuous shock models. We discuss various possible\norigins for the hot component and suggest that this component is H2 newly\nreformed on dust grains and then ejected from them, presumably following\ndissociation of some of the H2 by the shock.\n",
        "  We investigate the multiquantum vortex states in type-II superconductor both\nin \"clean\" and \"dirty\" regimes defined by impurity scattering rate. Within\nquasiclassical approach we calculate self-consistently the order parameter\ndistributions and electronic local density of states (LDOS) profiles. In the\nclean case we find the low temperature vortex core anomaly predicted\nanalytically in G.E. Volovik, JETP Lett. {\\bf 58}, 455 (1993) and obtain the\ninterference patterns in LDOS distributions analogous to found previously in\nthe framework of Bogolubov-de Gennes theory. In dirty regime the multiquantum\nvortices feature a peculiar plateau in the zero-energy LDOS profile which can\nbe considered as an experimental hallmark of multiquantum vortex formation in\nmesoscopic superconductors.\n",
        "  In this paper we present deep-learning models that submitted to the\nSemEval-2018 Task~1 competition: \"Affect in Tweets\". We participated in all\nsubtasks for English tweets. We propose a Bi-LSTM architecture equipped with a\nmulti-layer self attention mechanism. The attention mechanism improves the\nmodel performance and allows us to identify salient words in tweets, as well as\ngain insight into the models making them more interpretable. Our model utilizes\na set of word2vec word embeddings trained on a large collection of 550 million\nTwitter messages, augmented by a set of word affective features. Due to the\nlimited amount of task-specific training data, we opted for a transfer learning\napproach by pretraining the Bi-LSTMs on the dataset of Semeval 2017, Task 4A.\nThe proposed approach ranked 1st in Subtask E \"Multi-Label Emotion\nClassification\", 2nd in Subtask A \"Emotion Intensity Regression\" and achieved\ncompetitive results in other subtasks.\n",
        "  Nuclear magnetic resonance (NMR) experiments on single crystals of\nHgBa$_{2}$CuO$_{4+\\delta}$ are presented that identify two distinct\ntemperature-dependent spin susceptibilities: one is due to a spin component\nthat is temperature-dependent above the critical temperature for\nsuperconductivity ($T_{\\rm c}$) and reflects pseudogap behavior; the other is\nFermi-liquid-like in that it is temperature independent above $T_{\\rm c}$ and\nvanishes rapidly below $T_{\\rm c}$. In addition, we demonstrate the existence\nof a third, hitherto undetected spin susceptibility: it is temperature\nindependent at higher temperatures, vanishes at lower temperatures (below\n$T_0\\neq T_{\\rm c}$), and changes sign near optimal doping. This susceptibility\neither arises from the coupling between the two spin components, or it could be\ngiven by a distinct third spin component.\n",
        "  Selective accumulation of B-10 compound in tumour tissue is a fundamental\ncondition for the achievement of BNCT (Boron Neutron Capture Therapy), since\nthe effectiveness of therapy irradiation derives just from neutron capture\nreaction of B-10. Hence, the determination of the B-10 concentration ratio,\nbetween tumour and healthy tissue, and a control of this ratio, during the\ntherapy, are essential to optimise the effectiveness of the BNCT, which it is\nknown to be based on the selective uptake of B-10 compound. In this work,\nexperimental methods are proposed and evaluated for the determination in vivo\nof B-10 compound in biological samples, in particular based on neutron\nradiography and gammaray spectroscopy by telescopic system. Measures and Monte\nCarlo calculations have been performed to investigate the possibility of\nexecuting imaging of the 10B distribution, both by radiography with thermal\nneutrons, using 6LiF/ZnS:Ag scintillator screen and a CCD camera, and by\nspectroscopy, based on the revelation of gamma-ray reaction products from B-10\nand the H. A rebuilding algorithm has been implemented. The present study has\nbeen done for the standard case of B-10 uptake, as well as for proposed case in\nwhich, to the same carrier, is also synthesized Gd-157, in the amount of is\nused like a contrast agent in NMRI.\n",
        "  Band structure calculations are presented for large supercells of\nBa$_2$CuO$_4$ (BCO) with O-vacancies in planar or apic al positions, and of\nsuperoxygenated La$_2$CuO$_4$ (LCO) with oxygen interstitials in the\nLa$_2$O$_2$ layers. It is foun d that apical oxygen vacancies in BCO act as\nelectron dopants and makes the electronic structure similar to that of hole\ndoped LCO. Excess oxygen interstitials forming wires in the La$_2$O$_2$ layers\nof LCO are shown to yield a much larger density-of-states at the Fermi energy\nthan for the stoichiometric compound related with a segmentation of the Fermi\nsurface. Anti-ferromagnetic (AFM) spin fluctuations are strengthened by\nO-vacancies in BCO as well as by oxygen interstitials in LCO, but are strongly\nsuppressed in O-deficient LCO. Our results indicate the complexity of doping by\nO-vacancies, and by ordered defects that are a significant factor contr olling\nthe electronic properties of cuprates.\n",
        "  We introduce and analyze coupled, multi-strain epidemic models designed to\nsimulate the emergence and dissemination of mutant (e.g. drug-resistant)\npathogen strains. In particular, we investigate the mathematical and biological\nproperties of a general class of multi-strain epidemic models in which the\ninfectious compartments of each strain are coupled together in a general\nmanner. We derive explicit expressions for the basic reproduction number of\neach strain and highlight their importance in regulating the system dynamics\n(e.g. the potential for an epidemic outbreak) and the existence of nonnegative\nendemic solutions. Importantly, we find that the basic reproduction number of\neach strain is independent of the mutation rates between the strains --- even\nunder quite general assumptions for the form of the infectious compartment\ncoupling. Moreover, we verify that the coupling term promotes strain\ncoexistence (as an extension of the competitive exclusion principle) and\ndemonstrate that the strain with the greatest reproductive capacity is not\nnecessarily the most prevalent. Finally, we briefly discuss the implications of\nour results for public health policy and planning.\n",
        "  We show that for rational surface singularities with odd determinant the\nmu-bar invariant defined by W. Neumann is an obstruction for the link of the\nsingularity to bound a rational homology 4-ball. We identify the mu-bar\ninvariant with the corresponding correction term in Heegaard Floer theory.\n",
        "  We develop a multi-patch and multi-group model that captures the dynamics of\nan infectious disease when the host is structured into an arbitrary number of\ngroups and interacts into an arbitrary number of patches where the infection\ntakes place. In this framework, we model host mobility that depends on its\nepidemiological status, by a Lagrangian approach. This framework is applied to\na general SEIRS model and the basic reproduction number $\\mathcal{R_0}$ is\nderived. The effects of heterogeneity in groups, patches and mobility patterns\non $\\mathcal{R_0}$ and disease prevalence are explored. Our results show that\nfor a fixed number of groups, the basic reproduction number increases with\nrespect to the number of patches and the host mobility patterns. Moreover, when\nthe mobility matrix of susceptible individuals is of rank one, the basic\nreproduction number is explicitly determined and was found to be independent of\nthe latter. The cases where mobility matrices are of rank one capture important\nmodeling scenarios. Additionally, we study the global analysis of equilibria\nfor some special cases. Numerical simulations are carried out to showcase the\nramifications of mobility pattern matrices on disease prevalence and basic\nreproduction number.\n",
        "  This paper presents a numerical study on a fast marching method based back\nprojection reconstruction algorithm for photoacoustic tomography in\nheterogeneous media. Transcranial imaging is used here as a case study. To\ncorrect for the phase aberration from the heterogeneity (i.e., skull), the fast\nmarching method is adopted to compute the phase delay based on the known speed\nof sound distribution, and the phase delay is taken into account by the back\nprojection algorithm for more accurate reconstructions. It is shown that the\nproposed algorithm is more accurate than the conventional back projection\nalgorithm, but slightly less accurate than the time reversal algorithm\nparticularly in the area close to the skull. However, the image reconstruction\ntime for the proposed algorithm can be as little as 124 ms when implemented by\na GPU (512 sensors, 21323 pixels reconstructed), which is two orders of\nmagnitude faster than the time reversal reconstruction. The proposed algorithm,\ntherefore, not only corrects for the phase aberration, but can be also\npotentially implemented in a real-time manner.\n",
        "  The Change detection based on analysis and samples are analyzed. Land\nuse/cover change detection based on SDM is discussed.\n",
        "  The difference between the phases of superconducting order parameter plays in\nsuperconducting circuits the role similar to that played by the electrostatic\npotential difference required to drive a current in conventional circuits. This\nfundamental property can be altered by inserting in a superconducting circuit a\nparticular type of weak link, the so-called Josephson $\\pi$-junction having\ninverted current-phase relation and enabling a shift of the phase by $\\pi$. We\ndemonstrate the operation of three superconducting circuits -- two of them are\nclassical and one quantum -- which all utilize such $\\pi$-phase shifters\nrealized using superconductor-ferromagnet-superconductor sandwich technology.\nThe classical circuits are based on single-flux-quantum cells, which are shown\nto be scalable and compatible with conventional niobium-based superconducting\nelectronics. The quantum circuit is a $\\pi$-phase biased qubit, for which we\nobserve coherent Rabi oscillations and compare the measured coherence time with\nthat of conventional superconducting phase qubits.\n",
        "  We report vortex matching phenomenon in rectangular antidot array fabricated\non epitaxial NbN thin film. The antidot array was fabricated using Focussed Ion\nBeam milling technique. The magneto-transport measurements points to a period\ndoubling transition at higher magnetic field for rectangular lattices. The\nresults are discussed within the light of several models including the\nmulti-vortex model, the matched lattice model and the super-matched lattice\nmodel.\n",
        "  Relational lattice is a formal mathematical model for Relational algebra. It\nreduces the set of six classic relational algebra operators to two: natural\njoin and inner union. We continue to investigate Relational lattice properties\nwith emphasis onto axiomatic definition. New results include additional axioms,\nequational definition for set difference (more generally anti-join), and case\nstudy demonstrating application of the relational lattice theory for query\ntransformations.\n",
        "  Starting with an ideal triangulation of the interior of a compact 3-manifold\nM with boundary, no component of which is a 2-sphere, we provide a\nconstruction, called an inflation of the ideal triangulation, to obtain a\nstrongly related triangulations of M itself. Besides a step-by-step algorithm\nfor such a construction, we provide examples of an inflation of the\ntwo-tetrahedra ideal triangulation of the complement of the figure-eight knot\nin the 3-sphere, giving a minimal triangulation, having ten tetrahedra, of the\nfigure-eight knot exterior. As another example, we provide an inflation of the\none-tetrahedron Gieseking manifold giving a minimal triangulation, having seven\ntetrahedra, of a nonorientable compact 3-manifold with Klein bottle boundary.\nSeveral applications of inflations are discussed.\n",
        "  For machine translation to tackle discourse phenomena, models must have\naccess to extra-sentential linguistic context. There has been recent interest\nin modelling context in neural machine translation (NMT), but models have been\nprincipally evaluated with standard automatic metrics, poorly adapted to\nevaluating discourse phenomena. In this article, we present hand-crafted,\ndiscourse test sets, designed to test the models' ability to exploit previous\nsource and target sentences. We investigate the performance of recently\nproposed multi-encoder NMT models trained on subtitles for English to French.\nWe also explore a novel way of exploiting context from the previous sentence.\nDespite gains using BLEU, multi-encoder models give limited improvement in the\nhandling of discourse phenomena: 50% accuracy on our coreference test set and\n53.5% for coherence/cohesion (compared to a non-contextual baseline of 50%). A\nsimple strategy of decoding the concatenation of the previous and current\nsentence leads to good performance, and our novel strategy of multi-encoding\nand decoding of two sentences leads to the best performance (72.5% for\ncoreference and 57% for coherence/cohesion), highlighting the importance of\ntarget-side context.\n",
        "  We present deep Magellan/Megacam stellar photometry of four recently\ndiscovered faint Milky Way satellites: Sagittarius II (Sgr II), Reticulum II\n(Ret II), Phoenix II (Phe II), and Tucana III (Tuc III). Our photometry reaches\n~2-3 magnitudes deeper than the discovery data, allowing us to revisit the\nproperties of these new objects (e.g., distance, structural properties,\nluminosity measurements, and signs of tidal disturbance). The satellite\ncolor-magnitude diagrams show that they are all old (~13.5 Gyr) and metal-poor\n([Fe/H]$\\lesssim-2.2$). Sgr II is particularly interesting as it sits in an\nintermediate position between the loci of dwarf galaxies and globular clusters\nin the size-luminosity plane. The ensemble of its structural parameters is more\nconsistent with a globular cluster classification, indicating that Sgr II is\nthe most extended globular cluster in its luminosity range. The other three\nsatellites land directly on the locus defined by Milky Way ultra-faint dwarf\ngalaxies of similar luminosity. Ret II is the most elongated nearby dwarf\ngalaxy currently known for its luminosity range. Our structural parameters for\nPhe II and Tuc III suggest that they are both dwarf galaxies. Tuc III is known\nto be associated with a stellar stream, which is clearly visible in our\nmatched-filter stellar density map. The other satellites do not show any clear\nevidence of tidal stripping in the form of extensions or distortions. Finally,\nwe also use archival HI data to place limits on the gas content of each object.\n",
        "  Phylogenetic methods typically rely on an appropriate model of how data\nevolved in order to infer an accurate phylogenetic tree. For molecular data,\nstandard statistical methods have provided an effective strategy for extracting\nphylogenetic information from aligned sequence data when each site (character)\nis subject to a common process. However, for other types of data (e.g.\nmorphological data), characters can be too ambiguous, homoplastic or saturated\nto develop models that are effective at capturing the underlying process of\nchange. To address this, we examine the properties of a classic but neglected\nmethod for inferring splits in an underlying tree, namely, maximum\ncompatibility. By adopting a simple and extreme model in which each character\neither fits perfectly on some tree, or is entirely random (but it is not known\nwhich class any character belongs to) we are able to derive exact and explicit\nformulae regarding the performance of maximum compatibility. We show that this\nmethod is able to identify a set of non-trivial homoplasy-free characters, when\nthe number $n$ of taxa is large, even when the number of random characters is\nlarge. By contrast, we show that a method that makes more uniform use of all\nthe data --- maximum parsimony --- can provably estimate trees in which {\\em\nnone} of the original homoplasy-free characters support splits.\n",
        "  Electromagnetic tracking (EMT) is a promising technology for automated\ncatheter and applicator reconstruc- 10 tions in brachytherapy. In this work, a\nproof-of-concept is presented for reconstruction of the individual channels of\na shielded tandem applicator dedicated to intensity modulated brachytherapy.\nAll six channels of a straight prototype was reconstructed and the distance\nbetween two opposite channels was measured. A study was also conducted on the\ninfluence of the shield on the data fluctuation of the EMT system. The\ndifferences with the CAD specified dimensions are under 2 mm. The pair of\nchannels which has one of it more distant from the generator have 15 higher\ninter-channel distance with higher variability. In the first 110 cm\nreconstruction, all inter-channel distances are within the geometrical\ntolerances. According to a paired Student t-test, the data given by the EM\nsystem with and without the shield applicator tip are not significantly\ndifferent. This study shows that the reconstruction of channel path within the\nmechanical accuracy of the applicator is possible.\n",
        "  In this paper we present the state of advancement of the French ANR WebStand\nproject. The objective of this project is to construct a customizable XML based\nwarehouse platform to acquire, transform, analyze, store, query and export data\nfrom the web, in particular mailing lists, with the final intension of using\nthis data to perform sociological studies focused on social groups of World\nWide Web, with a specific emphasis on the temporal aspects of this data. We are\ncurrently using this system to analyze the standardization process of the W3C,\nthrough its social network of standard setters.\n",
        "  The notion of a braid is generalized into two and three dimensions.\nTwo-dimensional braids are described by braid monodromies or graphics called\ncharts. In this paper we introduce the notion of curtains, and show that\nthree-dimensional braids are described by braid monodromies or curtains.\n",
        "  We characterize sequences of Kleinian surface groups with convergent\nsubsequences in terms of the asymptotic behavior of the ending invariants of\nthe associated hyperbolic 3-manifolds. Asymptotic behavior of end invariants in\na convergent sequence predicts the parabolic locus of the algebraic limit as\nwell as how the algebraic limit wraps within the geometric limit under the\nnatural locally isometric covering map.\n",
        "  We give a necessary and sufficient condition for a hyperbolic Coxeter group\nwith planar nerve to have Sierpi\\'nski curve as its Gromov boundary.\n",
        "  Shortly after the discovery in 1994 of superconductivity in Sr2RuO4, it was\nproposed on theoretical grounds that the superconducting state may have chiral\np-wave symmetry analogous to the A phase of superfluid He-3. Substantial\nexperimental evidence has since accumulated in favor of this pairing symmetry,\nincluding several interesting recent results related to broken time reversal\nsymmetry and vortices with half of the usual superconducting flux quantum.\nGreat interest surrounds the possibility of chiral p-wave order in Sr2RuO4,\nsince this state may exhibit topological order analogous to that of a quantum\nHall state, and can support such exotic physics as Majorana fermions and\nnon-Abelian winding statistics, which have been proposed as one route to a\nquantum computer. However, serious discrepancies remain in trying to connect\nthe experimental results to theoretical predictions for chiral p-wave order. In\nthis paper, I review a broad range of experiments on Sr2RuO4 that are sensitive\nto p-wave pairing, triplet superconductivity and time-reversal symmetry\nbreaking and compare these experiments to each other and to theoretical\npredictions. In this context, the evidence for triplet pairing is strong,\nalthough some puzzles remain. The \"smoking gun\" experimental results for chiral\np-wave, those which directly look for evidence of broken time-reversal symmetry\nin the superconducting state of Sr2RuO4, are most perplexing when the results\nare compared to each other and to theoretical predictions. Consequently, the\ncase for chiral p-wave in Sr2RuO4 remains unresolved, suggesting the need to\nconsider either significant modifications to the standard chiral p-wave models\nor possible alternative pairing symmetries. Recent ideas along these lines are\ndiscussed.\n",
        "  Notional anaphors are pronouns which disagree with their antecedents'\ngrammatical categories for notional reasons, such as plural to singular\nagreement in: 'the government ... they'. Since such cases are rare and conflict\nwith evidence from strictly agreeing cases ('the government ... it'), they\npresent a substantial challenge to both coreference resolution and referring\nexpression generation. Using the OntoNotes corpus, this paper takes an ensemble\napproach to predicting English notional anaphora in context on the basis of the\nlargest empirical data to date. In addition to state of the art prediction\naccuracy, the results suggest that theoretical approaches positing a plural\nconstrual at the antecedent's utterance are insufficient, and that\ncircumstances at the anaphor's utterance location, as well as global factors\nsuch as genre, have a strong effect on the choice of referring expression.\n",
        "  This article describes the systems jointly submitted by Institute for\nInfocomm (I$^2$R), the Laboratoire d'Informatique de l'Universit\\'e du Maine\n(LIUM), Nanyang Technology University (NTU) and the University of Eastern\nFinland (UEF) for 2015 NIST Language Recognition Evaluation (LRE). The\nsubmitted system is a fusion of nine sub-systems based on i-vectors extracted\nfrom different types of features. Given the i-vectors, several classifiers are\nadopted for the language detection task including support vector machines\n(SVM), multi-class logistic regression (MCLR), Probabilistic Linear\nDiscriminant Analysis (PLDA) and Deep Neural Networks (DNN).\n",
        "  In this paper, we propose a new approach for fast processing of SPARQL\nqueries on large RDF datasets containing RDF quadruples (or quads). Our\napproach called RIQ employs a decrease-and-conquer strategy: Rather than\nindexing the entire RDF dataset, RIQ identifies groups of similar RDF graphs\nand indexes each group separately. During query processing, RIQ uses a novel\nfiltering index to first identify candidate groups that may contain matches for\nthe query. On these candidates, it executes optimized queries using a\nconventional SPARQL processor to produce the final results. Our initial\nperformance evaluation results are promising: Using a synthetic and a real\ndataset, each containing about 1.4 billion quads, we show that RIQ outperforms\nRDF-3X and Jena TDB on a variety of SPARQL queries.\n",
        "  We performed nuclear magnetic resonance (NMR) measurements to investigate the\nevolution of spin-density-wave (SDW) and superconducting (SC) states upon\nelectron doping in CaFe_{1-x}Co_{x}AsF, which exhibits an intermediate phase\ndiagram between those of LaFeAsO_{1-x}F_x and Ba(Fe_{1-x}Co_x)_2As_2. We found\nthat homogeneous coexistence of the incommensurate SDW and SC states occurs\nonly in a narrow doping region around the crossover regime, which supports\nS_{+-}-wave symmetry. However, only the structural phase transition survives\nupon further doping, which agrees with predictions from orbital fluctuation\ntheory. The transitional features upon electron doping imply that both spin and\norbital fluctuations are involved in the superconducting mechanism.\n",
        "  We use Nathanson's $g$-adic representation of integers to relate metric\nproperties of Cayley graphs of the integers with respect to various infinite\ngenerating sets $S$ to problems in additive number theory. If $S$ consists of\nall powers of a fixed integer $g$, we find explicit formulas for the smallest\npositive integer of a given length. This is related to finding the smallest\npositive integer expressible as a fixed number of sums and differences of\npowers of $g$. We also consider $S$ to be the set of all powers of all primes\nand bound the diameter of Cayley graph by relating it to Goldbach's conjecture.\n",
        "  We present Herschel observations of far-infrared (FIR) fine-structure (FS)\nlines [CII]158$\\mu$m, [OI]63$\\mu$m, [OIII]52$\\mu$m, and [SiII]35$\\mu$m in the\nz=2.56 Cloverleaf quasar, and combine them with published data in an analysis\nof the dense interstellar medium (ISM) in this system. Observed [CII]158$\\mu$m,\n[OI]63$\\mu$m, and FIR continuum flux ratios are reproduced with\nphotodissociation region (PDR) models characterized by moderate far-ultraviolet\n(FUV) radiation fields $G_0=$ 0.3-1$\\times10^3$ and atomic gas densities\n$n_{\\rm H}=$ 3-5$\\times10^3$ cm$^{-3}$, depending on contributions to\n[CII]158$\\mu$m from ionized gas. We assess the contribution to [CII]158$\\mu$m\nflux from an active galactic nucleus (AGN) narrow line region (NLR) using\nground-based measurements of the [NII]122$\\mu$m transition, finding that the\nNLR can contribute at most 20-30% of the observed [CII]158$\\mu$m flux. The PDR\ndensity and far-UV radiation fields inferred from the atomic lines are not\nconsistent with the CO emission, indicating that the molecular gas excitation\nis not solely provided via UV-heating from local star-formation, but requires\nan additional heating source. X-ray heating from the AGN is explored, and we\nfind that X-ray dominated region (XDR) models, in combination with PDR models,\ncan match the CO cooling without overproducing observed FS line emission. While\nthis XDR/PDR solution is favored given the evidence for both X-rays and\nstar-formation in the Cloverleaf, we also investigate alternatives for the warm\nmolecular gas, finding that either mechanical heating via low-velocity shocks\nor an enhanced cosmic-ray ionization rate may also contribute. Finally, we\ninclude upper limits on two other measurements attempted in the Herschel\nprogram: [CII]158$\\mu$m in FSC~10214 and [OI]63$\\mu$m in APM~08279+5255.\n",
        "  This study presents and validates a novel (non-ECG-triggered) MRI sequence\nbased on SPAtial Modulation of the Magnetization (SPAMM) to non-invasively\nmeasure 3D (quasi-static) soft tissue deformations using only six acquisitions\n(three static and three indentations). In current SPAMM tagged MRI approaches\ndata is typically constructed from many repeated motion cycles. This has so far\nrestricted its application to the measurement of highly repeatable and periodic\nmovements (e.g. cardiac deformation). In biomechanical applications where soft\ntissue deformation is artificially induced, often by indentation, significant\nrepeatability constraints exist and, for clinical applications, discomfort and\nhealth issues generally preclude a large number of repetitions.\n",
        "  Anisotropy effects on flux pinning and flux flow are strongly effective in\ncuprate as well as iron-based superconductors due to their intrinsically\nlayered crystallographic structure. However $\\textrm{Fe(Se,Te)}$ thin films\ngrown on $\\textrm{CaF}_2$ substrate result less anisotropic with respect to all\nthe other iron based superconductors. We present the first study on the angular\ndependence of the flux flow instability, which occurs in the flux flow regime\nas a current driven transition to the normal state at the instability point\n($I^*$,$V^*$) in the current-voltage characteristics. The voltage jumps are\nsystematically investigated as a function of the temperature, the external\nmagnetic field, and the angle between the field and the $\\textrm{Fe(Se,Te)}$\nfilm. The scaling procedure based on the anisotropic Ginzburg-Landau approach\nis successfully applied to the observed angular dependence of the critical\nvoltage $V^*$. Anyway, we find out that $\\textrm{Fe(Se,Te)}$ represents the\ncase study of a layered material characterized by a weak anisotropy of its\nstatic superconducting properties, but with an increased anisotropy in its\nvortex dynamics due to the predominant perpendicular component of the external\napplied magnetic field. Indeed, $I^*$ shows less sensitivity to angle\nvariations, thus being promising for high field applications.\n",
        "  Let $K$ be a simple $2q$-knot with exterior $X$. We show directly how the\nFarber quintuple $(A,\\Pi,\\alpha,\\ell,\\psi)$ determines the homotopy type of $X$\nif the torsion subgroup of $A=\\pi_q(X)$ has odd order. We comment briefly on\nthe possible role of the EHP sequence in recovering the boundary inclusion from\nthe duality pairings $\\ell $ and $\\psi$. Finally we reformulate the Farber\nquintuple as an hermitian self-duality of an object in an additive category\nwith involution.\n",
        "  Developments in the educational landscape have spurred greater interest in\nthe problem of automatically scoring short answer questions. A recent shared\ntask on this topic revealed a fundamental divide in the modeling approaches\nthat have been applied to this problem, with the best-performing systems split\nbetween those that employ a knowledge engineering approach and those that\nalmost solely leverage lexical information (as opposed to higher-level\nsyntactic information) in assigning a score to a given response. This paper\naims to introduce the NLP community to the largest corpus currently available\nfor short-answer scoring, provide an overview of methods used in the shared\ntask using this data, and explore the extent to which more\nsyntactically-informed features can contribute to the short answer scoring task\nin a way that avoids the question-specific manual effort of the knowledge\nengineering approach.\n",
        "  An array of resistively and capacitively shunted Josephson junctions with\nnonsinusoidal current-phase relation is considered for modelling the transition\nin high-T$_c$ superconductors. The emergence of higher harmonics, besides the\nsimple sinusoid $I_{c}\\sin\\phi$, is expected for dominant \\emph{d}-wave\nsymmetry of the Cooper pairs, random distribution of potential drops, dirty\ngrains, or nonstationary conditions. We show that additional cosine and sine\nterms act respectively by modulating the global resistance and by changing the\nJosephson coupling of the mixed superconductive-normal states. First, the\napproach is applied to simulate the transition in disordered granular\nsuperconductors with the weak-links characterized by nonsinusoidal\ncurrent-phase relation. In granular superconductors, the emergence of\nhigher-order harmonics affects the slope of the transition. Then, arrays of\nintrinsic Josephson junctions, naturally formed by the CuO$_2$ planes in\ncuprates, are considered. The critical temperature suppression, observed at\nvalues of hole doping close to $p=1/8$, is investigated. Such suppression,\nrelated to the sign change and modulation of the Josephson coupling across the\narray, is quantified in terms of the intensities of the first and second\nsinusoids of the current-phase relation. Applications are envisaged for the\ndesign and control of quantum devices based on stacks of intrinsic Josephson\njunctions.\n",
        "  The plethora of existing data models and specific data modeling techniques is\nnot only confusing but leads to complex, eclectic and inefficient designs of\nsystems for data management and analytics. The main goal of this paper is to\ndescribe a unified approach to data modeling, called the concept-oriented model\n(COM), by using functions as a basis for its formalization. COM tries to answer\nthe question what is data and to rethink basic assumptions underlying this and\nrelated notions. Its main goal is to unify major existing views on data\n(generality), using only a few main notions (simplicity) which are very close\nto how data is used in real life (naturalness).\n",
        "  Ecosystems can undergo sudden shifts to undesirable states, but recent\nstudies with simple single species ecosystems have demonstrated that advance\nwarning can be provided by the slowing down of population dynamics near a\ntipping point. However, it is not clear how this effect of critical slowing\ndown will manifest in ecosystems with strong interactions between their\ncomponents. Here we probe the dynamics of an experimental producer parasite\necosystem as it approaches a catastrophic collapse. Surprisingly, the producer\npopulation grows in size as the environment deteriorates, highlighting that\npopulation size can be a misleading measure of ecosystem stability. By\nanalyzing the oscillatory producer parasite dynamics for over ~100 generations\nin multiple environmental conditions, we found that the collective ecosystem\ndynamics slows down as the tipping point is approached. Analysis of the coupled\ndynamics of interacting populations may therefore be necessary to provide\nadvance warning of collapse in complex communities.\n",
        "  In this paper, an improved scatter correction with moving beam stop array\n(BSA) for x-ray cone beam (CB) CT is proposed. Firstly, correlation between\nneighboring CB views is deduced based on John's Equation. Then,\ncorrelation-based algorithm is presented to complement the incomplete views by\nusing the redundancy (over-determined information) in CB projections. Finally,\ncombining the algorithm with scatter correction method using moving BSA, where\npart of primary radiation is blocked and incomplete projections are acquired,\nan improved correction method is proposed. Effectiveness and robustness is\nvalidated by Monte Carlo (MC) simulation with EGSnrc on humanoid phantom.\n",
        "  \\noindent Given a Riemann surface $M$, the \\emph{complexity} of a branched\ncover of $M$ to the Riemann sphere $S^2$, of degree $d$ and with branching set\nof cardinality $n \\geq 3$, is defined as $d$ times the hyperbolic area of the\ncomplement of its branching set in $S^2$. A branched cover $p \\colon M \\to S^2$\nof degree $d$ is \\emph{simple} if the cardinality of the pre-image $p^{-1}(y)$\nis at least $d-1$ for all $y \\in S^2$. The \\emph{(simple) complexity} of $M$ is\ndefined as the infimum of the complexities of all (simple) branched covers of\n$M$ to $S^2$. We prove that if $M$ is a closed, connected, orientable Riemann\nsurface of genus $g \\geq 1$, then: (1) its simple complexity equals $8\\pi g$,\nand (2) its complexity equals $2\\pi(m_{\\text{min}}+2g-2)$, where\n$m_{\\text{min}}$ is the minimum total length of a branch datum realizable by a\nbranched cover $p \\colon M \\to S^2$.\n",
        "  In this paper we show how to lift Lagrangian immersions in $\\mathbb{C}\nP^{n-1}$ to produce Lagrangian cones in $\\mathbb{C} ^n$, and use this process\nto produce several families of examples of Lagrangian cones and special\nLagrangian cones. Moreover we show how to produce Lagrangian cones, isotopic to\nthe Harvey-Lawson and trivial cones, whose projections to $\\mathbb{C} P^{n-1}$\nare immersions with few transverse double points.\n",
        "  We define a new notion of thin position for a graph in a 3-manifold which\ncombines the ideas of thin position for manifolds first originated by\nScharlemann and Thompson with the idea of thin position for knots first\noriginated by Gabai. This thin position has the property that connect summing\nannuli and pairs-of-pants show up as thin levels. In a forthcoming paper, this\nnew thin position allows us to define two new families of invariants of knots,\nlinks, and graphs in 3-manifolds. The invariants in one family are similar to\nbridge number and the invariants in the other family are similar to Gabai's\nwidth for knots in the 3-sphere. The invariants in both families detect the\nunknot and are additive under connected sum and trivalent vertex sum.\n",
        "  With the rise of XML as a standard for representing business data, XML data\nwarehouses appear as suitable solutions for Web-based decision-support\napplications. In this context, it is necessary to allow OLAP analyses over XML\ndata cubes (XOLAP). Thus, XQuery extensions are needed. To help define a formal\nframework and allow much-needed performance optimizations on analytical queries\nexpressed in XQuery, having an algebra at one's disposal is desirable. However,\nXOLAP approaches and algebras from the literature still largely rely on the\nrelational model and/or only feature a small number of OLAP operators. In\nopposition, we propose in this paper to express a broad set of OLAP operators\nwith the TAX XML algebra.\n",
        "  This article presents a study on different informative modalities of a\nperceptual supplementation device aiming at reducing overpressure at the\nbuttock area. Visual, audio and tactile modalities are analysed and compared\nwith a non-biofeedback session. In conclusion, modalities have a positive and\nequal effect, but they are not equally judged in term of comfort and\ndisturbance with some other activities\n",
        "  The field of Natural Language Processing (NLP) is growing rapidly, with new\nresearch published daily along with an abundance of tutorials, codebases and\nother online resources. In order to learn this dynamic field or stay up-to-date\non the latest research, students as well as educators and researchers must\nconstantly sift through multiple sources to find valuable, relevant\ninformation. To address this situation, we introduce TutorialBank, a new,\npublicly available dataset which aims to facilitate NLP education and research.\nWe have manually collected and categorized over 6,300 resources on NLP as well\nas the related fields of Artificial Intelligence (AI), Machine Learning (ML)\nand Information Retrieval (IR). Our dataset is notably the largest\nmanually-picked corpus of resources intended for NLP education which does not\ninclude only academic papers. Additionally, we have created both a search\nengine and a command-line tool for the resources and have annotated the corpus\nto include lists of research topics, relevant resources for each topic,\nprerequisite relations among topics, relevant sub-parts of individual\nresources, among other annotations. We are releasing the dataset and present\nseveral avenues for further research.\n",
        "  We present the original computer code for the simulation of multi-element\ndetection system of the compact positron-emission tomograph based on a\nscintillator-photodiode type of detection elements. The use of such type of\ndetection elements allows obtaining a high spatial resolution at a relatively\nsmall total size of the tomograph. This program gives an opportunity to choose\nthe optimal geometry of detection system depending on the parameters of its\nelements, and analyze the efficiency of different image reconstruction\nalgorithms. The handy interface for the designed compact positron-emission\ntomograph operation has been created.\n",
        "  Evolutionary hypotheses to explain the greater numbers of species in the\ntropics than the temperate zone include greater age and area, higher\ntemperature and metabolic rates, and greater ecological opportunity. These\nideas make contrasting predictions about the relationship between speciation\nprocesses and latitude, which I elaborate and evaluate. Available data suggest\nthat per capita speciation rates are currently highest in the temperate zone,\nand that diversification rates (speciation minus extinction) are similar\nbetween latitudes. In contrast, clades whose oldest analyzed dates precede the\nEocene thermal maximum, when the extent of the tropics was much greater than\ntoday, tend to show highest speciation and diversification rates in the\ntropics. These findings are consistent with age and area, which is alone among\nhypotheses in predicting a time trend. Higher recent speciation rates in the\ntemperate zone than the tropics suggest an additional response to high\necological opportunity associated with low species diversity. These broad\npatterns are compelling but provide limited insights into underlying\nmechanisms, arguing that studies of speciation processes along the latitudinal\ngradient will be vital. Using threespine stickleback in depauperate northern\nlakes as an example, I show how high ecological opportunity can lead to rapid\nspeciation. The results support a role for ecological opportunity in\nspeciation, but its importance in the evolution of the latitudinal gradient\nremains uncertain. I conclude that per-capita evolutionary rates are no longer\nhigher in the tropics than the temperate zone. Nevertheless, the vast numbers\nof species that have already accumulated in the tropics ensure that total rate\nof species production remains highest there. Thus, tropical evolutionary\nmomentum helps to perpetuate the steep latitudinal biodiversity gradient.\n",
        "  We characterize all groups which can occur as the topological symmetry group\nor the orientation preserving topological symmetry group of some embedding of\nthe Petersen graph in S^3.\n",
        "  We show that the center of the Goldman algebra associated to a closed\noriented hyperbolic surface is trivial. For a hyperbolic surface of finite type\nwith nonempty boundary, the center consists of closed curves which are\nhomotopic to boundary components or punctures.\n",
        "  Monolingual data have been demonstrated to be helpful in improving\ntranslation quality of both statistical machine translation (SMT) systems and\nneural machine translation (NMT) systems, especially in resource-poor or domain\nadaptation tasks where parallel data are not rich enough. In this paper, we\npropose a novel approach to better leveraging monolingual data for neural\nmachine translation by jointly learning source-to-target and target-to-source\nNMT models for a language pair with a joint EM optimization method. The\ntraining process starts with two initial NMT models pre-trained on parallel\ndata for each direction, and these two models are iteratively updated by\nincrementally decreasing translation losses on training data. In each iteration\nstep, both NMT models are first used to translate monolingual data from one\nlanguage to the other, forming pseudo-training data of the other NMT model.\nThen two new NMT models are learnt from parallel data together with the pseudo\ntraining data. Both NMT models are expected to be improved and better\npseudo-training data can be generated in next step. Experiment results on\nChinese-English and English-German translation tasks show that our approach can\nsimultaneously improve translation quality of source-to-target and\ntarget-to-source models, significantly outperforming strong baseline systems\nwhich are enhanced with monolingual data for model training including\nback-translation.\n",
        "  A finite nonabelian simple group does not admit a free action on a homology\nsphere, and the only finite simple group which acts on a homology sphere with\nat most 0-dimensional fixed point sets (\"pseudofree action\") is the alternating\ngroup A_5 acting on the 2-sphere. Our first main theorem is the finiteness\nresult that there are only finitely many finite simple groups which admit a\nsmooth action on a homology sphere with at most d-dimensional fixed points\nsets, for a fixed d. We then go on proving that the finite simple groups acting\non a homology sphere with at most 1-dimensional fixed point sets are the\nalternating group A_5 in dimensions 2, 3 and 5, the linear fractional group\nPSL_2(7) in dimension 5, and possibly the unitary group PSU_3(3) in dimension 5\n(we conjecture that it does not admit any action on a homology 5-sphere but\ncannot exclude it at present). Finally, we discuss the situation for arbitrary\nfinite groups which admit an action on a homology 3-sphere.\n",
        "  Comparison of optimization algorithms for inverse treatment planning requires\nobjective function value evaluation.\n",
        "  Extracting relations from text corpora is an important task in text mining.\nIt becomes particularly challenging when focusing on weakly-supervised relation\nextraction, that is, utilizing a few relation instances (i.e., a pair of\nentities and their relation) as seeds to extract more instances from corpora.\nExisting distributional approaches leverage the corpus-level co-occurrence\nstatistics of entities to predict their relations, and require large number of\nlabeled instances to learn effective relation classifiers. Alternatively,\npattern-based approaches perform bootstrapping or apply neural networks to\nmodel the local contexts, but still rely on large number of labeled instances\nto build reliable models. In this paper, we study integrating the\ndistributional and pattern-based methods in a weakly-supervised setting, such\nthat the two types of methods can provide complementary supervision for each\nother to build an effective, unified model. We propose a novel co-training\nframework with a distributional module and a pattern module. During training,\nthe distributional module helps the pattern module discriminate between the\ninformative patterns and other patterns, and the pattern module generates some\nhighly-confident instances to improve the distributional module. The whole\nframework can be effectively optimized by iterating between improving the\npattern module and updating the distributional module. We conduct experiments\non two tasks: knowledge base completion with text corpora and corpus-level\nrelation extraction. Experimental results prove the effectiveness of our\nframework in the weakly-supervised setting.\n",
        "  With the rapid development of crowdsourcing platforms that aggregate the\nintelligence of Internet workers, crowdsourcing has been widely utilized to\naddress problems that require human cognitive abilities. Considering great\ndynamics of worker arrival and departure, it is of vital importance to design a\ntask assignment scheme to adaptively select the most beneficial tasks for the\navailable workers. In this paper, in order to make the most efficient\nutilization of the worker labor and balance the accuracy of answers and the\noverall latency, we a) develop a parameter estimation model that assists in\nestimating worker expertise, question easiness and answer confidence; b)\npropose a \\textit{quality-assured synchronized task assignment scheme} that\nexecutes in batches and maximizes the number of potentially completed questions\n(MCQ) within each batch. We prove that MCQ problem is NP-hard and present two\ngreedy approximation solutions to address the problem. The effectiveness and\nefficiency of the approximation solutions are further evaluated through\nextensive experiments on synthetic and real datasets. The experimental results\nshow that the accuracy and the overall latency of the MCQ approaches outperform\nthe existing online task assignment algorithms in the synchronized task\nassignment scenario.\n",
        "  The possible control of competitive invasion by infection of the invader and\nmultiplicative noise is studied. The basic model is the Lotka-Volterra\ncompetition system with emergent carrying capacities. Several stationary\nsolutions of the non-infected and infected system are identified as well as\nparameter ranges of bistability. The latter are used for the numerical study of\ninvasion phenomena. The diffusivities, the infection but in particular the\nwhite and colored multiplicative noise are the control parameters. It is shown\nthat not only competition, possible infection and mobilities are important\ndrivers of the invasive dynamics but also the noise and especially its color\nand the functional response of populations to the emergence of noise.\n",
        "  The contribution to an organism's phenotype from one genetic locus may depend\nupon the status of other loci. Such epistatic interactions among loci are now\nrecognized as fundamental to shaping the process of adaptation in evolving\npopulations. Although little is known about the structure of epistasis in most\norganisms, recent experiments with bacterial populations have concluded that\nantagonistic interactions abound and tend to de-accelerate the pace of\nadaptation over time. Here, we use a broad class of mathematical fitness\nlandscapes to examine how natural selection biases the mutations that\nsubstitute during evolution based on their epistatic interactions. We find\nthat, even when beneficial mutations are rare, these biases are strong and\nchange substantially throughout the course of adaptation. In particular,\nepistasis is less prevalent than the neutral expectation early in adaptation\nand much more prevalent later, with a concomitant shift from predominantly\nantagonistic interactions early in adaptation to synergistic and sign epistasis\nlater in adaptation. We observe the same patterns when re-analyzing data from a\nrecent microbial evolution experiment. Since these biases depend on the\npopulation size and other parameters, they must be quantified before we can\nhope to use experimental data to infer an organism's underlying fitness\nlandscape or to understand the role of epistasis in shaping its adaptation. In\nparticular, we show that when the order of substitutions is not known to an\nexperimentalist, then standard methods of analysis may suggest that epistasis\nretards adaptation when in fact it accelerates it.\n",
        "  We used UKIRT near infrared (NIR) broad band JHK photometry, narrow band\nimaging of the 1-0S(1) molecular hydrogen emission line and mid infrared\nSpitzer IRAC data to investigate the nature of the young cluster Mercer14.\nForeground star counts in decontaminated NIR photometry and a comparison with\nthe Besancon Galaxy Model are performed to estimate the cluster distance. This\nmethod yields a distance of 2.5kpc with an uncertainty of about 10% and can be\napplied to other young and embedded clusters. Mercer14 shows clear signs of\nongoing star formation with several detected molecular hydrogen outflows, a\nhigh fraction of infrared excess sources and an association to a small gas and\ndust cloud. Hence, the cluster is less than 4Myrs old and has a line of sight\nextinction of A_K=0.8mag. Based on the most massive cluster members we find\nthat Mercer14 is an intermediate mass cluster with about 500Mo.\n",
        "  Functional dependencies are an integral part of database design. However,\nthey are only defined when we exclude null markers. Yet we commonly use null\nmarkers in practice. To bridge this gap between theory and practice,\nresearchers have proposed definitions of functional dependencies over relations\nwith null markers. Though sound, these definitions lack some qualities that we\nfind desirable. For example, some fail to satisfy Armstrong's axioms---while\nthese axioms are part of the foundation of common database methodologies. We\npropose a set of properties that any extension of functional dependencies over\nrelations with null markers should possess. We then propose two new extensions\nhaving these properties. These extensions attempt to allow null markers where\nthey make sense to practitioners.\n  They both support Armstrong's axioms and provide realizable null markers: at\nany time, some or all of the null markers can be replaced by actual values\nwithout causing an anomaly. Our proposals may improve database designs.\n",
        "  Extracting location names from informal and unstructured social media data\nrequires the identification of referent boundaries and partitioning compound\nnames. Variability, particularly systematic variability in location names\n(Carroll, 1983), challenges the identification task. Some of this variability\ncan be anticipated as operations within a statistical language model, in this\ncase drawn from gazetteers such as OpenStreetMap (OSM), Geonames, and DBpedia.\nThis permits evaluation of an observed n-gram in Twitter targeted text as a\nlegitimate location name variant from the same location-context. Using n-gram\nstatistics and location-related dictionaries, our Location Name Extraction tool\n(LNEx) handles abbreviations and automatically filters and augments the\nlocation names in gazetteers (handling name contractions and auxiliary\ncontents) to help detect the boundaries of multi-word location names and\nthereby delimit them in texts.\n  We evaluated our approach on 4,500 event-specific tweets from three targeted\nstreams to compare the performance of LNEx against that of ten state-of-the-art\ntaggers that rely on standard semantic, syntactic and/or orthographic features.\nLNEx improved the average F-Score by 33-179%, outperforming all taggers.\nFurther, LNEx is capable of stream processing.\n",
        "  Dispersal is a key ecological process, that enables local populations to form\nspatially extended systems called metapopulations. In the present study, we\ninvestigate how dispersal affects the linear stability of a general\nsingle-species metapopulation model. We discuss both the influence of local\nwithin-patch dynamics and the effects of various dispersal behaviors on\nstability. We find that positive density-dependent dispersal and positive\ndensity-dependent settlement are destabilizing dispersal behaviors while\nnegative density-dependent dispersal and negative density-dependent settlement\nare stabilizing. It is also shown that dispersal has a stabilizing impact on\nheterogeneous metapopulations that correlates positively with the number of\npatches and the connectance of metapopulation networks.\n",
        "  To guide experimental work on the search for Majorana zero-energy modes, we\ncalculate the superconducting pairing symmetry of a three-dimensional\ntopological insulator in combination with an s-wave superconductor. In analogy\nto the case of nanowires with strong spin-orbit coupling we show how the\npairing symmetry changes across different topological regimes. We demonstrate\nthat a dominant p-wave pairing relation is not sufficient to realize a Majorana\nzero-energy mode useful for quantum computation. Our main result of this paper\nis the relation between odd-frequency pairing and Majorana zero energy modes by\nusing Green functions techniques in three-dimensional topological insulators in\nthe so-called Majorana regime. We discuss thereafter how the pairing relations\nin the different regimes can be observed in the shape of the tunneling\nconductance of an s-wave proximized three-dimensional topological insulator. We\nwill discuss the necessity to incorporate a ferromagnetic insulator to localize\nthe zero-energy bound state to the interface as a Majorana mode.\n",
        "  In this work, the time evolution of the free induction decay caused by the\nlocal dipole field of a spherical magnetic perturber is analyzed. The\ncomplicated treatment of the diffusion process is considered by the\nstrong-collision approximation that allows a determination of the free\ninduction decay in dependence of the underlying microscopic tissue parameters\nsuch as diffusion coefficient, sphere radius and susceptibility difference. The\ninterplay between susceptibility- and diffusion-mediated effects yields several\ndephasing regimes of which, so far, only the classical regimes of motional\nnarrowing and static dephasing for dominant and negligible diffusion,\nrespectively, were extensively examined. Due to the asymmetric form of the\nthree-dimensional dipole field for spherical objects, the free induction decay\nexhibits a complex component in contradiction to the cylindrical case, where\nthe symmetric local two-dimensional dipole field only causes a purely real\ninduction decay. Knowledge of the shape of the corresponding frequency\ndistribution is necessary for the evaluation of more sophisticated pulse\nsequences and a detailed understanding of the off-resonance distribution allows\nimproved quantification of transverse relaxation.\n",
        "  Cone-beam computed tomography (CBCT) has been widely used in medical imaging\nand industrial nondestructive testing, but the presence of scattered radiation\nwill cause significant reduction of image quality. In this article, a robust\nscatter correction method for CBCT using an interlacing-slit plate (ISP) is\ncarried out for convenient practice. Firstly, a Gaussian filtering method is\nproposed to compensate the missing data of the inner scatter image, and\nsimultaneously avoid too-large values of calculated inner scatter and smooth\nthe inner scatter field. Secondly, an interlacing-slit scan without detector\ngain correction is carried out to enhance the practicality and convenience of\nthe scatter correction method. Finally, a denoising step for scatter-corrected\nprojection images is added in the process flow to control the noise\namplification. The experimental results show that the improved method can not\nonly make the scatter correction more robust and convenient, but also achieve a\ngood quality of scatter-corrected slice images.\n",
        "  We solve a well known, long-standing open problem in relational databases\ntheory, showing that the conjunctive query determinacy problem (in its\n\"unrestricted\" version) is undecidable.\n",
        "  We study Question 7.9 in the paper \"Monoids in the mapping class group\" by\nEtnyre and Van Horn-Morris; whether a symmetric mapping class admitting a\npositive factorization is a lift of a quasi-positive braid. We answer\naffirmatively for mapping classes satisfying certain cyclic conditions.\n",
        "  Relation Extraction is an important sub-task of Information Extraction which\nhas the potential of employing deep learning (DL) models with the creation of\nlarge datasets using distant supervision. In this review, we compare the\ncontributions and pitfalls of the various DL models that have been used for the\ntask, to help guide the path ahead.\n",
        "  Accurate distances are fundamental to interpreting many measured properties\nof galaxies. Surprisingly, many of the best-studied spiral galaxies in the\nLocal Volume have distance uncertainties that are much larger than can be\nachieved with modern observations. Using Hubble Space Telescope optical\nimaging, we use the tip of the red giant branch method to measure the distances\nto six galaxies that are included in the Spitzer Infrared Nearby Galaxies\nSurvey (SINGS) program and its offspring surveys. The sample includes M63, M74,\nNGC 1291, NGC 4559, NGC 4625, and NGC 5398. We compare our results with\ndistances reported to these galaxies based on a variety of methods. Depending\non the technique, there can be a wide range in published distances,\nparticularly from the Tully-Fisher relation. In addition, differences between\nthe Planetary Nebula Luminosity Function and Surface Brightness Fluctuation\ntechniques can vary between galaxies suggesting inaccuracies that cannot be\nexplained by systematics in the calibrations. Our distances improve upon\nprevious results as we use a well-calibrated, stable distance indicator,\nprecision photometry in an optimally selected field of view, and a Bayesian\nMaximum Likelihood technique that reduces measurement uncertainties.\n",
        "  It is common practice for data scientists to acquire and integrate disparate\ndata sources to achieve higher quality results. But even with a perfectly\ncleaned and merged data set, two fundamental questions remain: (1) is the\nintegrated data set complete and (2) what is the impact of any unknown (i.e.,\nunobserved) data on query results?\n  In this work, we develop and analyze techniques to estimate the impact of the\nunknown data (a.k.a., unknown unknowns) on simple aggregate queries. The key\nidea is that the overlap between different data sources enables us to estimate\nthe number and values of the missing data items. Our main techniques are\nparameter-free and do not assume prior knowledge about the distribution.\nThrough a series of experiments, we show that estimating the impact of unknown\nunknowns is invaluable to better assess the results of aggregate queries over\nintegrated data sources.\n",
        "  Resolving the microscopic pairing mechanism and its experimental\nidentification in unconventional superconductors is among the most vexing\nproblems of contemporary condensed matter physics. We show that Raman\nspectroscopy provides an avenue for this quest by probing the structure of the\npairing interaction at play in an unconventional superconductor. As we study\nthe spectra of the prototypical Fe-based superconductor ${\\rm\nBa_{1-x}K_xFe_2As_2}$ for $0.22\\le x \\le 0.70$ in all symmetry channels, Raman\nspectroscopy allows us to distill the leading $s$-wave state. In addition, the\nspectra collected in the $B_{1g}$ symmetry channel reveal the existence of two\ncollective modes which are indicative of the presence of two competing, yet\nsub-dominant, pairing tendencies of $d_{x^2-y^2}$ symmetry type. A\ncomprehensive functional Renormalization Group (fRG) and random-phase\napproximation (RPA) study on this compound confirms the presence of the two\nsub-leading channels, and consistently matches the experimental doping\ndependence of the related modes. The synopsis of experimental evidence and\ntheoretical modelling supports a spin-fluctuation mediated superconducting\npairing mechanism.\n",
        "  We propose to prototype the first CT-MRI scanner for radiation therapy and\nbasic research, demonstrate its transformative biomedical potential, and\ninitiate a paradigm shift in multimodality imaging. Our design consists of a\ndouble donut-shaped pair of permanent magnets to form a regionally uniform\n~0.5T magnetic field and leave room for a stationary 9-source interior CT\ngantry at 3 tube voltages (triple-energy CT). Image reconstruction will be in a\ncompressive sensing framework. Please discuss with Dr. Ge Wang\n(ge-wang@ieee.org) if you are interested in collaborative opportunities.\n",
        "  The classical approaches to the modeling of sex ratio evolution can be\ndivided into two classes. The first class contains the static strategic models\nrelated to the Dusing Fisher Shaw Mohler fitness measure, based on the\nreproductive value of the offspring of the focal female. The second class\ncontains the population genetic models focused on the dynamics of the allele\nfrequencies. The approaches are not fully compatible because the strategic\nmodels disregard the role of the male individuals as the passive carriers of\nthe strategy genes. In the previous two papers in this cycle, a new model\ncombining the strategic analysis with more rigorous genetics was presented. The\nnew model shows that sex ratio self-regulation is a multistage complex process\nwhich can be regarded as an example of multilevel selection. One of the\nelements of this process is the dynamic equilibrium between male and female\ngene carriers associated with convergence of the dynamics to the manifold\ntermed the male subpopulation equilibrium (MSE). This paper attempts to explain\nthis phenomenon and analyze its properties. We show that the MSE phenomenon\naffects every stage of sex ratio self-regulation (Lemmas 1-4). The MSE plays a\ncrucial role in synchronizing two levels of selection in the double-level\nselection process. We show that the MSE condition can be generalized as an\ninteresting synergistic property allowing for the estimation of the primary sex\nratio of the entire population according to the state of some arbitrarily\nchosen subpopulation (Lemma 5). We also show that the classical Dusing Fisher\nShaw Mohler fitness measure is a biased approximation of the new approach, but\nthat it produces compatible strategic predictions (Lemma 6).\n",
        "  We argue that odd-frequency triplet superconductivity can be conveniently\nrealized in hybrid superconductor-ferromagnet (SF) structures with a\nferromagnetic vortex. We demonstrate that due to proximity-induced long-range\ntriplet pairing such SFS junctions can sustain appreciable supercurrent which\ncan be directly measured in experiments.\n",
        "  Purpose: The mechanics of the mitral valve leaflet as a nonlinear, inelastic\nand anisotropic soft tissue results from an integrated response of many\nmathematical/physical indexes' that illustrate the tissue. In the past decade,\nfinite element modeling of complete heart valves has greatly aided evaluation\nof heart valve surgery, design of bioprosthetic valve replacements, and general\nunderstanding of healthy and abnormal cardiac function. Such a model must be\nbased on an accurate description of the mechanical behavior of the valve\nmaterial. It is essential to calculate velocity/displacement and strain\nrate/strain at a component level that is to work at the cellular level. In this\nstudy we developed the first three-dimensional displacement vectors field in\nthe characterization of mitral valve leaflets in continuum equations of\ninelasticity framework based on echocardiography.\n  Method: Much of our knowledge of abnormal mitral valve function is based on\nsurgical and post-mortem studies while these studies are quantitative in some\ncases, they are limited by evaluation of valve anatomy in a fixed and\nnonfunctioning state. A more sophisticated analysis method is necessary to gain\na full considerate of mitral valve function. Several groups attempted to model\nmitral valve anatomy and function by mathematical/physical equations.\n  Result: Preliminary results concerning a different aspect of MVL\nbiomechanics, such as leaflets dynamics, displacements/velocities and strain\nrates/strains of points on leaflets, were in good agreement with in\nechocardiographic observations.\n",
        "  Fluorescence molecular tomography (FMT) has potential of providing high\ncontrast images for breast tumor detection. Computational phantom provides a\nconvenient way to a wide variety of fluorophore distribution configurations in\npatients and perform comprehensive evaluation of the imaging systems and\nmethods for FMT. In this study, a digital breast phantom was used to compare\nthe performance of a novel sparsity-based reconstruction method and Tikhonov\nregularization method for resolving tumors with different amount of separation.\nThe results showed that the proposed sparse reconstruction method yielded\nbetter performance. This simulation-based approach with computational phantoms\nenabled an evaluation of the reconstruction methods for FMT for breast-cancer\ndetection.\n",
        "  Studying materials informatics from a data mining perspective can be\nbeneficial for manufacturing and other industrial engineering applications.\nPredictive data mining technique and machine learning algorithm are combined to\ndesign a knowledge discovery system for the selection of engineering materials\nthat meet the design specifications. Predictive method-Naive Bayesian\nclassifier and Machine learning Algorithm - Pearson correlation coefficient\nmethod were implemented respectively for materials classification and\nselection. The knowledge extracted from the engineering materials data sets is\nproposed for effective decision making in advanced engineering materials design\napplications.\n",
        "  We describe a resource-based method of morphological annotation of written\nKorean text. Korean is an agglutinative language. The output of our system is a\ngraph of morphemes annotated with accurate linguistic information. The language\nresources used by the system can be easily updated, which allows us-ers to\ncontrol the evolution of the per-formances of the system. We show that\nmorphological annotation of Korean text can be performed directly with a\nlexicon of words and without morpho-logical rules.\n",
        "  Neurosurgery interventions involve complex tracking systems because a tissue\ndeformation takesplace. The neuronavigation system relies only on preoperative\nimages. In order to overcome the soft tissue deformations and guarantee the\naccuracy of the navigation a biomechanical model can be used during surgery to\nsimulate the deformation of the brain. Therefore, a mesh generation for an\noptimal real-time Finite Element Model (FEM) becomes crucial. In this work we\npresent different alternatives from a meshgeneration point of view that were\nevaluated to optimize the process in terms of elements quantity and quality as\nwell as constraints of a intraoperative application and patient specific data.\n",
        "  Epidemic dynamics in a stochastic network of interacting epidemic centers is\nconsidered. The epidemic and migration processes are modelled by Markov's\nchains. Explicit formulas for probability distribution of the migration process\nare derived. Dependence of outbreak parameters on initial parameters,\npopulation, coupling parameters is examined analytically and numerically. The\nmean field approximation for a general migration process is derived. An\napproximate method allowing computation of statistical moments for networks\nwith highly populated centres is proposed and tested numerically.\n",
        "  Purpose: The aim of this work was to create tissue-mimicking gel phantoms\nappropriate for diffusion kurtosis imaging (DKI) for quality assurance,\nprotocol optimization and sequence development.\n  Methods: A range of agar, agarose and polyvinyl alcohol phantoms with\nconcentrations ranging from 1.0% to 3.5%, 0.5% to 3.0% and 10% to 20%,\nrespectively, and up to 3 g of glass microspheres per 100 ml were created.\nDiffusion coefficients, excess kurtosis values and relaxation rates were\nexperimentally determined.\n  Results: The kurtosis values for the plain gels ranged from 0.05 with 95%\nconfidence interval (CI) of $(0.029, 0.071)$ to $0.216(0.185, 0.246)$, well\nbelow the kurtosis values reported in the literature for various tissues. The\naddition of glass microspheres increased the kurtosis of the gels with values\nup to $0.523(0.465, 0.581)$ observed for gels with the highest concentration of\nmicrospheres. Repeat scans of some of the gels after more than six months of\nstorage at room temperature indicate changes in the diffusion parameters of\nless than 10%. The addition of the glass microspheres reduces the apparent\ndiffusion coefficients (ADCs) and increases the longitudinal and transverse\nrelaxation rates but the values remain comparable to those for plain gels and\ntissue, with ADCs observed ranging from $818(585, 1053) \\times 10^{-6}$\nmm$^2$/s to $2257(2118, 2296) \\times 10^{-6}$ mm$^2$/s, and R1 values ranging\nfrom $0.34(0.32, 0.35)$ 1/s to $0.51(0.50, 0.52)$ 1/s, and R2 values ranging\nfrom $9.69(9.34, 10.04)$ 1/s to $33.07(27.10, 39.04)$ 1/s.\n  Conclusions: Glass microspheres can be used to effectively modify diffusion\nproperties of gel phantoms and achieve a range of kurtosis values comparable to\nthose reported for a variety of tissues.\n",
        "  We model the responses of Tissue-Equivalent Proportional Counters (TEPC) to\nradiation fields of therapeutic C-12 beams in a water phantom and to\nquasi-monoenergetic neutrons in a PMMA phantom. Simulations are performed with\nthe Monte Carlo model for Heavy Ion Therapy (MCHIT) based on the Geant4\ntoolkit. The shapes of the calculated lineal energy spectra agree well with\nmeasurements in both cases. The influence of fragmentation reactions on the\nTEPC response to a narrow pencil-like beam with its width smaller than the TEPC\ndiameter is investigated by Monte Carlo modeling. It is found that total lineal\nenergy spectra are not very sensitive to the choice of the nuclear\nfragmentation model used. The calculated frequency-mean lineal energy y_f\ndiffers from the data on the axis of a therapeutic beam by less than 10% and by\n10-20% at other TEPC positions. The validation of MCHIT with neutron beams\ngives us confidence in estimating the contributions to lineal energy spectra\ndue to secondary neutrons produced in water by C-12 nuclei. As found, the\nneutron contribution at 10 cm distance from the beam axis amounts to ~ 50%\nclose the entrance to the phantom and decreases to ~ 25% at the depth of the\nBragg peak and beyond it. The presented results can help in evaluating\nbiological out-of-field doses in carbon-ion therapy.\n",
        "  We introduce an invariant of tangles in Khovanov homology by considering a\nnatural inverse system of Khovanov homology groups. As application, we derive\nan invariant of strongly invertible knots; this invariant takes the form of a\ngraded vector space that vanishes if and only if the strongly invertible knot\nis trivial. While closely tied to Khovanov homology -- and hence the Jones\npolynomial -- we observe that this new invariant detects non-amphicheirality in\nsubtle cases where Khovanov homology fails to do so. In fact, we exhibit\nexamples of knots that are not distinguished by Khovanov homology but, owing to\nthe presence of a strong inversion, may be distinguished using our invariant.\nThis work suggests a strengthened relationship between Khovanov homology and\nHeegaard Floer homology by way of two-fold branched covers that we formulate in\na series of conjectures.\n",
        "  The first stage of every knowledge base question answering approach is to\nlink entities in the input question. We investigate entity linking in the\ncontext of a question answering task and present a jointly optimized neural\narchitecture for entity mention detection and entity disambiguation that models\nthe surrounding context on different levels of granularity. We use the Wikidata\nknowledge base and available question answering datasets to create benchmarks\nfor entity linking on question answering data. Our approach outperforms the\nprevious state-of-the-art system on this data, resulting in an average 8%\nimprovement of the final score. We further demonstrate that our model delivers\na strong performance across different entity categories.\n",
        "  Consideration is given to thermodynamical properties of a three-dimensional\nBose-condensate of translation-invariant bipolarons (TI-bipolarons) in magnetic\nfield. The critical temperature of transition, critical magnetic fields,\nenergy, heat capacity and the transition heat of TI-bipolaron gas are\ncalculated. Such values as maximum magnetic field, London penetration depth and\ntheir temperature dependencies are calculated. The results obtained are used to\nexplain experiments on high-temperature superconductors.\n",
        "  Mutational neighbourhoods in genotype-phenotype (GP) maps are widely believed\nto be more likely to share characteristics than expected from random chance.\nSuch genetic correlations should, as John Maynard Smith famously pointed out,\nstrongly influence evolutionary dynamics. We explore and quantify these\nintuitions by comparing three GP maps - RNA SS, HP for tertiary, Polyominoes\nfor protein quaternary structure - to a simple random null model that maintains\nthe number of genotypes mapping to each phenotype, but assigns genotypes\nrandomly. The mutational neighbourhood of a genotype in these GP maps is much\nmore likely to contain (mutationally neutral) genotypes mapping to the same\nphenotype than in the random null model. These neutral correlations can\nincrease the robustness to mutations by orders of magnitude over that of the\nnull model, raising robustness above the critical threshold for the formation\nof large neutral networks that enhance the capacity for neutral exploration. We\nalso study {\\em non-neutral correlations}: Compared to the null model, i) If a\nparticular (non-neutral) phenotype is found once in the 1-mutation\nneighbourhood of a genotype, then the chance of finding that phenotype multiple\ntimes in this neighbourhood is larger than expected; ii) If two genotypes are\nconnected by a single neutral mutation, then their respective non-neutral\n1-mutation neighbourhoods are more likely to be similar; iii) If a genotype\nmaps to a folding or self-assembling phenotype, then its non-neutral neighbours\nare less likely to be a potentially deleterious non-folding or non-assembling\nphenotype. Non-neutral correlations of type i) and ii) reduce the rate at which\nnew phenotypes can be found by neutral exploration, and so may diminish\nevolvability, while non-neutral correlations of type iii) may instead\nfacilitate evolutionary exploration and so increase evolvability.\n",
        "  The Gaia-ESO survey is a large public spectroscopic survey aimed at\ninvestigating the origin and formation history of our Galaxy by collecting\nspectroscopy of representative samples (about 10^5 Milky Way stars) of all\nGalactic stellar populations, in the field and in clusters. The survey uses\nglobular clusters as intra- and inter-survey calibrators, deriving stellar\natmospheric parameters and abundances of a significant number of stars in\nclusters, along with radial velocity determinations. We used precise radial\nvelocities of a large number of stars in seven globular clusters (NGC 1851, NGC\n2808, NGC 4372, NGC 4833, NGC 5927, NGC 6752, and NGC 7078) to validate\npipeline results and to preliminarily investigate the cluster internal\nkinematics. Radial velocity measurements were extracted from FLAMES/GIRAFFE\nspectra processed by the survey pipeline as part of the second internal data\nrelease of data products to ESO. We complemented our sample with ESO archival\ndata obtained with different instrument configurations. Reliable radial\nvelocity measurements for 1513 bona fide cluster star members were obtained in\ntotal. We measured systemic rotation, estimated central velocity dispersions,\nand present velocity dispersion profiles of all the selected clusters,\nproviding the first velocity dispersion curve and the first estimate of the\ncentral velocity dispersion for the cluster NGC~5927. Finally, we explore the\npossible link between cluster kinematics and other physical parameters. The\nanalysis we present here demonstrates that Gaia-ESO survey data are\nsufficiently accurate to be used in studies of kinematics of stellar systems\nand stellar populations in the Milky Way.\n",
        "  The dependence of the Josephson Plasma Resonance (JPR) frequency in heavily\nunderdoped Bi2Sr2CaCu2O8+\\delta on temperature and controlled pointlike\ndisorder, introduced by high-energy electron irradiation, is cross-correlated\nand compared to the behavior of the ab-plane penetration depth. It is found\nthat the zero temperature plasma frequency, representative of the superfluid\ncomponent of the c-axis spectral weight, decreases proportionally with T_c when\nthe disorder is increased. The temperature dependence of the JPR frequency is\nthe same for all disorder levels, including pristine crystals. The reduction of\nthe c-axis superfluid density as function of disorder is accounted for by\npair-breaking induced by impurity scattering in the CuO2 planes, rather than by\nquantum fluctuations of the superconducting phase. The reduction of the c-axis\nsuperfluid density as function of temperature follows a T^{2}--law and is\naccounted for by quasi-particle hopping through impurity induced interlayer\nstates.\n",
        "  The Monoceros Loop (SNR G205.5+0.5) is a large shell-type supernova remnant\nlocated in the Rosette Complex region. It was suggested to be interacting with\nthe Rosette Nebula. We aim to re-examine the radio spectral index and its\nspatial variation over the Monoceros SNR, and study its properties of evolution\nwithin the complex interstellar medium. We extracted radio continuum data for\nthe Monoceros complex region from the Effelsberg 21 cm and 11 cm surveys and\nthe Urumqi 6 cm polarization survey. We used the new Arecibo GALFA-HI survey\ndata with much higher resolution and sensitivity than that previously available\nto identify the HI shell related with the SNR. Multi-wavelengths data are\nincluded to investigate the properties of the SNR. The spectral index $\\alpha$\n($S_{\\nu}\\propto\\nu^{\\alpha}$) averaged over the SNR is $-0.41 \\pm$0.16. The\nTT-plots and the distribution of $\\alpha$ over the SNR show spatial variations\nwhich steepen towards the inner western filamentary shell. Polarized emission\nis prominent on the western filamentary shell region. The RM there is estimated\nto be about 30$\\pm$77n rad m$^{-2}$, where the n=1 solution is preferred, and\nthe magnetic field has a strength of about 9.5 $\\mu$G. From the HI channel\nmaps, further evidence is provided for an interaction between the Monoceros SNR\nand the Rosette Nebula. We identify partial neutral hydrogen shell structures\nin the northwestern region at LSR velocities of +15 km s$^{-1}$ circumscribing\nthe continuum emission. The HI shell has swept up a mass of about 4000\nM$_{\\odot}$ for a distance of 1.6 kpc. The western HI shell, well associated\nwith the dust mission, is found to lie outside of the radio shell. We suggest\nthat the Monoceros SNR is evolving within a cavity blown-out by the progenitor,\nand has triggered part of the star formation in the Rosette Nebula.\n",
        "  For $n$ an even number, we study representations of the mapping class group\nof the $n$-punctured sphere arising from $\\mathrm{SU}(2)$-TQFT when all\npunctures are colored by the same integer $N \\geq 1$. We prove that the\nconjecture of Andersen, Masbaum and Ueno holds for the $4$-punctured sphere for\nall $N \\geq 2$. In the case $n \\geq 6$ of punctures, we prove it for the\npseudo-Anosovs satisfying a homological condition, namely they should act with\na non trivial stretching factor on certain eigenspaces of homology of a\n$\\frac{n}{2}$-fold branched cover considered by McMullen. The main idea is to\nconsider the kernel space which is the kernel of the natural map from the skein\nmodule to the $\\mathrm{SU}(2)$-TQFT. Our main theorem identifies, as\nrepresentations of mapping class groups, certain of these kernel spaces with\nhomology eigenspaces considered by McMullen. Our results concerning the AMU\nconjecture are obtained by taking appropriate limits of the quantum\nrepresentations in which such a kernel space may appear as a subspace of the\nlimit representation.\n",
        "  We use point contact spectroscopy to probe the superconducting and normal\nstate properties of the iron-based superconductor\n$\\rm{NaFe_{1-\\textit{x}}Co_{\\textit{x}}As}$ with $\\rm{\\textit{x} = 0, 0.02,\n0.06}$. Andreev spectra corresponding to multiple superconducting gaps are\ndetected in the superconducting phase. For $\\rm{\\textit{x} = 0.02}$, a broad\nconductance enhancement around zero bias voltage is detected in both the normal\nand the superconducting phase. Such a feature is not present in the\n$\\rm{\\textit{x} = 0.06}$ samples. We suspect that this enhancement is caused by\norbital fluctuations, as previously detected in underdoped\n$\\rm{Ba(Fe_{1-\\textit{x}}Co_\\textit{x})_2As_2}$ (Phys. Rev. B 85, 214515\n(2012)). Occasionally, the superconducting phase shows a distinct asymmetric\nconductance feature instead of Andreev reflection. We discuss the possible\norigins of this feature. NaFeAs (the parent compound) grown by two different\ntechniques is probed. Melt-grown NaFeAs shows a normal state conductance\nenhancement. On the other hand, at low temperatures, flux-grown NaFeAs shows a\nsharp dip in the conductance at zero bias voltage. The compounds are very\nreactive in air and the different spectra are likely a reflection of their\ndifferent oxidation and purity levels.\n",
        "  In the companion paper of this set (Capitan and Cuesta, 2010) we have\ndeveloped a full analytical treatment of the model of species assembly\nintroduced in Capitan et al. (2009). This model is based on the construction of\nan assembly graph containing all viable configurations of the community, and\nthe definition of a Markov chain whose transitions are the transformations of\ncommunities by new species invasions. In the present paper we provide an\nexhaustive numerical analysis of the model, describing the average time to the\nrecurrent state, the statistics of avalanches, and the dependence of the\nresults on the amount of available resource. Our results are based on the fact\nthat the Markov chain provides an asymptotic probability distribution for the\nrecurrent states, which can be used to obtain averages of observables as well\nas the time variation of these magnitudes during succession, in an exact\nmanner. Since the absorption times into the recurrent set are found to be\ncomparable to the size of the system, the end state is quickly reached (in\nunits of the invasion time). Thus, the final ecosystem can be regarded as a\nfluctuating complex system where species are continually replaced by newcomers\nwithout ever leaving the set of recurrent patterns. The assembly graph is\ndominated by pathways in which most invasions are accepted, triggering small\nextinction avalanches. Through the assembly process, communities become less\nresilient (e.g., have a higher return time to equilibrium) but become more\nrobust in terms of resistance against new invasions.\n",
        "  Every closed geodesic $\\gamma$ on a surface has a canonically associated knot\n$\\widehat\\gamma$ in the projective unit tangent bundle. We study, for $\\gamma$\nfilling, the volume of the associated knot complement with respect to its\nunique complete hyperbolic metric. We provide a lower bound for the volume\nrelative to the number of homotopy classes of $\\gamma$-arcs in each pair of\npants of a pants decomposition of the surface.\n",
        "  A meromorphic quadratic differential on a punctured Riemann surface induces\nhorizontal and vertical measured foliations with pole-singularities. In a\nneighborhood of a pole such a foliation comprises foliated strips and\nhalf-planes, and its leaf-space determines a metric graph. We introduce the\nnotion of an asymptotic direction at each pole, and show that for a punctured\nsurface equipped with a choice of such asymptotic data, any compatible pair of\nmeasured foliations uniquely determines a complex structure and a meromorphic\nquadratic differential realizing that pair. This proves the analogue of a\ntheorem of Gardiner-Masur, for meromorphic quadratic differentials. We also\nprove an analogue of the Hubbard-Masur theorem, namely, for a fixed punctured\nRiemann surface there exists a meromorphic quadratic differential with any\nprescribed horizontal foliation, and such a differential is unique provided we\nprescribe the singular-flat geometry at the poles.\n",
        "  Compressed bitmap indexes are used in systems such as Git or Oracle to\naccelerate queries. They represent sets and often support operations such as\nunions, intersections, differences, and symmetric differences. Several\nimportant systems such as Elasticsearch, Apache Spark, Netflix's Atlas,\nLinkedIn's Pinot, Metamarkets' Druid, Pilosa, Apache Hive, Apache Tez,\nMicrosoft Visual Studio Team Services and Apache Kylin rely on a specific type\nof compressed bitmap index called Roaring. We present an optimized software\nlibrary written in C implementing Roaring bitmaps: CRoaring. It benefits from\nseveral algorithms designed for the single-instruction-multiple-data (SIMD)\ninstructions available on commodity processors. In particular, we present\nvectorized algorithms to compute the intersection, union, difference and\nsymmetric difference between arrays. We benchmark the library against a wide\nrange of competitive alternatives, identifying weaknesses and strengths in our\nsoftware. Our work is available under a liberal open-source license.\n",
        "  Monte Carlo (MC) simulation of linacs depends on the accurate geometrical\ndescription of the head. The geometry of the Varian TrueBeam (TB) linac is not\navailable to researchers. Instead, the company distributes phase-space files\n(PSFs) of the flattening-filter-free (FFF) beams tallied upstream the jaws.\nYet, MC simulations based on third party tallied PSFs are subject to\nlimitations. We present an experimentally-based geometry developed for the\nsimulation of the FFF beams of the TB linac. The upper part of the TB linac was\nmodeled modifying the Clinac 2100 geometry. The most important modification is\nthe replacement of the standard flattening filters by ad hoc thin filters which\nwere modeled by comparing dose measurements and simulations. The experimental\ndose profiles for the 6MV and 10MV FFF beams were obtained from the Varian\nGolden Data Set and from in-house measurements for radiation fields ranging\nfrom 3X3 to 40X40 cm2. Indicators of agreement between the experimental data\nand the simulation results obtained with the proposed geometrical model were\nthe dose differences, the root-mean-square error and the gamma index. The same\ncomparisons were done for dose profiles obtained from MC simulations using the\nsecond generation of PSFs distributed by Varian for the TB linac. Results of\ncomparisons show a good agreement of the dose for the ansatz geometry similar\nto that obtained for the simulations with the TB PSFs for all fields\nconsidered, except for the 40X40 cm2 field where the ansatz geometry was able\nto reproduce the measured dose more accurately. Our approach makes possible to:\n(i) adapt the initial beam parameters to match measured dose profiles; (ii)\nreduce the statistical uncertainty to arbitrarily low values; and (iii) assess\nsystematic uncertainties by employing different MC codes.\n",
        "  Machine translation is research based area where evaluation is very important\nphenomenon for checking the quality of MT output. The work is based on the\nevaluation of English to Urdu Machine translation. In this research work we\nhave evaluated the translation quality of Urdu language which has been\ntranslated by using different Machine Translation systems like Google, Babylon\nand Ijunoon. The evaluation process is done by using two approaches - Human\nevaluation and Automatic evaluation. We have worked for both the approaches\nwhere in human evaluation emphasis is given to scales and parameters while in\nautomatic evaluation emphasis is given to some automatic metric such as BLEU,\nGTM, METEOR and ATEC.\n",
        "  We report measurements of the phonon density-of-states in iron oxypnictide\nsuperconductors by inelastic x-ray scattering. A good agreement with ab-initio\ncalculations that do not take into account strong electronic correlations is\nfound, and an unpredicted softening of phonon branches under F doping of these\ncompounds is observed. Raman scattering experiments lead us to conclude that\nthis softening is not related to zone center phonons, and consequently imply an\nimportant softening of the relevant phonon branches at finite momentum transfer\nQ.\n",
        "  Emission from metal resonant lines has recently emerged as a potentially\npowerful probe of the structure of galactic winds at low and high redshift. In\nthis work, we present only the second example of spatially resolved\nobservations of NaI D emission from a galactic wind in a nearby galaxy (and the\nfirst 3D observations at any redshift). F05189-2524, a nearby (z=0.043) ultra\nluminous infrared galaxy powered by a quasar, was observed with the integral\nfield unit on the Gemini Multi-Object Spectrograph (GMOS) at Gemini North. NaI\nD absorption in the system traces dusty filaments on the near side of an\nextended, AGN-driven galactic wind (with projected velocities up to 2000 km/s).\nThese filaments (A_V < 4) and N(H) < 10^22 cm^-2) simultaneously obscure the\nstellar continuum and NaI D emission lines. The NaI D emission lines serve as a\ncomplementary probe of the wind; they are strongest in regions of low\nforeground obscuration and extend up to the limits of the field of view\n(galactocentric radii of 4 kpc). An azimuthally symmetric Sersic model\nextincted by the same foreground screen as the stellar continuum reproduces the\nNaI D emission line surface brightness distribution except in the inner regions\nof the wind, where some emission-line filling of absorption lines may occur.\nThe presence of detectable NaI D emission in F05189-2524 may be due to its high\ncontinuum surface brightness at the rest wavelength of NaI D. These data\nuniquely constrain current models of cool gas in galactic winds and serve as a\nbenchmark for future observations and models.\n",
        "  A database system optimized for in-memory storage can support much higher\ntransaction rates than current systems. However, standard concurrency control\nmethods used today do not scale to the high transaction rates achievable by\nsuch systems. In this paper we introduce two efficient concurrency control\nmethods specifically designed for main-memory databases. Both use\nmultiversioning to isolate read-only transactions from updates but differ in\nhow atomicity is ensured: one is optimistic and one is pessimistic. To avoid\nexpensive context switching, transactions never block during normal processing\nbut they may have to wait before commit to ensure correct serialization\nordering. We also implemented a main-memory optimized version of single-version\nlocking. Experimental results show that while single-version locking works well\nwhen transactions are short and contention is low performance degrades under\nmore demanding conditions. The multiversion schemes have higher overhead but\nare much less sensitive to hotspots and the presence of long-running\ntransactions.\n",
        "  The ability to efficiently find relevant subgraphs and paths in a large graph\nto a given query is important in many applications including scientific data\nanalysis, social networks, and business intelligence. Currently, there is\nlittle support and no efficient approaches for expressing and executing such\nqueries. This paper proposes a data model and a query language to address this\nproblem. The contributions include supporting the construction and selection\nof: (i) folder nodes, representing a set of related entities, and (ii) path\nnodes, representing a set of paths in which a path is the transitive\nrelationship of two or more entities in the graph. Folders and paths can be\nstored and used for future queries. We introduce FPSPARQL which is an extension\nof the SPARQL supporting folder and path nodes. We have implemented a query\nengine that supports FPSPARQL and the evaluation results shows its viability\nand efficiency for querying large graph datasets.\n",
        "  The Asian chestnut gall wasp \\emph{Dryocosmus kuriphilus}, native of China,\nhas become a pest when it appeared in Japan, Korea, and the United States. In\nEurope it was first found in Italy, in 2002. In 1982 the host-specific\nparasitoid \\emph{Torymus sinensis} was introduced in Japan, in an attempt to\nachieve a biological control of the pest. After an apparent initial success,\nthe two species seem to have locked in predator-prey cycles of decadal length.\nWe have developed a spatially explicit mathematical model that describes the\nseasonal time evolution of the adult insect populations, and the competition\nfor finding egg deposition sites. In a spatially homogeneous situation the\nmodel reduces to an iterated map for the egg density of the two species. While\nthe map would suggest, for realistic parameters, that both species should\nbecome locally extinct (somewhat corroborating the hypothesis of biological\ncontrol), the full model, for the same parameters, shows that the introduction\nof \\emph{T. sinensis} sparks a traveling wave of the parasitoid population that\ndestroys the pest on its passage. Depending on the value of the diffusion\ncoefficients of the two species, the pest can later be able to re-colonize the\nempty area left behind the wave. When this occurs the two populations do not\nseem to attain a state of spatial homogeneity, but produce an ever-changing\npattern of traveling waves.\n",
        "  Routine ergonomic assessment of postures and gestures in the workplace are\nmostly conducted by visual observations, either direct or based on video\nrecordings. Nowadays, low-cost three-dimensional cameras like Microsoft Kinect\nopen the possibility of recording the full kinematics of workers in a\nnon-intrusive way, providing a more precise, and reliable assessment of their\nmotor strategies. As an illustration, we focus on a peculiar kind of workers:\nprofessional musicians (violinists), whose playing is representative of a work\nsituation involving repeated gestures and postures that can be described as\nnon-ergonomic. We show that the Microsoft Kinect can be efficiently used to\nquantify the motion performed by these musicians. Moreover, we argue that\nlow-cost three-dimensional cameras can be a useful aid in ergonomic risk\nassessment of developing musculoskeletal disorders and give the example of the\nrepetition of movements and postural items included in the OCRA checklist,\nwhose scoring can be facilitated by such a device, as addressed in our TRACK\nTMS research project.\n",
        "  Using techniques from the theory of Kirby calculus we give an explicit\nconstruction of a four dimensional hyperbolic link complement in a 4-manifold\nthat is diffeomorphic to a standard $S^2 \\times S^2$.\n",
        "  The finding of a double red clump in the luminosity function of the Milky Way\nbulge has been interpreted as evidence for an X-shaped structure. Recently, an\nalternative explanation has been suggested, where the double red clump is an\neffect of multiple stellar populations in a classical spheroid. In this letter\nwe provide an observational assessment of this scenario and show that it is not\nconsistent with the behaviour of the red clump across different lines of sight,\nparticularly at high distances from the Galactic plane. Instead, we confirm\nthat the shape of the red clump magnitude distribution closely follows the\ndistance distribution expected for an X-shaped bulge at critical Galactic\nlatitudes. We also emphasize some key observational properties of the bulge red\nclump that should not be neglected in the search for alternative scenarios.\n",
        "  The present study revealed that populations of climbing perch (Anabas\ntestudineus) inhabiting river, backwater, shallow water channel, ponds with and\nwithout vegetation cover, marsh, sewage canal and aquaculture tank varied\nsignificantly in the number of food items consumed. Chironomous larvae, organic\ndebris and filamentous algae were the common ingredients of the menu of this\nspecies across focal ecosystems, whereas sewage canal population was found\nsurviving solely on insect larvae and organic debris.\n",
        "  The Star Formation Rate (SFR) is one of the main parameters used to analyze\nthe evolution of galaxies through time. The need for recovering the light\nreprocessed by dust commonly requires the use of low spatial resolution\nfar-infrared data. Recombination-line luminosities provide an alternative,\nalthough uncertain dust-extinction corrections based on narrow-band imaging or\nlong-slit spectroscopy have traditionally posed a limit to their applicability.\nIntegral Field Spectroscopy (IFS) is clearly the way to overcome such\nlimitation. We obtain integrated H{\\alpha}, ultraviolet (UV) and infrared\n(IR)-based SFR measurements for 272 galaxies from the CALIFA survey at 0.005 <\nz < 0.03 using single-band and hybrid tracers. We provide updated calibrations,\nboth global and split by properties (including stellar mass and morphological\ntype), referred to H{\\alpha}. The extinction-corrected H{\\alpha} luminosity\nagrees with the updated hybrid SFR estimators based on either UV or H{\\alpha}\nplus IR luminosity over the full range of SFRs (0.03-20 M$_{\\odot}$ yr$^{-1}$).\nThe coefficient that weights the amount of energy produced by newly-born stars\nthat is reprocessed by dust on the hybrid tracers, a$_{IR}$, shows a large\ndispersion. However, it does not became increasingly small at high\nattenuations, as expected if significant highly-obscured H$\\alpha$ emission\nwould be missed. Lenticulars, early-type spirals and type-2 AGN host galaxies\nshow smaller coefficients due to the contribution of optical photons and AGN to\ndust heating. In the Local Universe the H{\\alpha} luminosity derived from IFS\nobservations can be used to measure SFR, at least in statistically-significant,\noptically-selected galaxy samples. The analysis of the SFR calibrations by\ngalaxies properties could be potentially used by other works to study the\nimpact of different selection criteria in the SFR values derived.\n",
        "  In this paper we enumerate and classify the ``simplest'' pairs (M,G) where M\nis a closed orientable 3-manifold and G is a trivalent graph embedded in M.\n  To enumerate the pairs we use a variation of Matveev's definition of\ncomplexity for 3-manifolds, and we consider only (0,1,2)-irreducible pairs,\nnamely pairs (M,G) such that any 2-sphere in M intersecting G transversely in\nat most 2 points bounds a ball in M either disjoint from G or intersecting G in\nan unknotted arc. To classify the pairs our main tools are geometric invariants\ndefined using hyperbolic geometry. In most cases, the graph complement admits a\nunique hyperbolic structure with parabolic meridians; this structure was\ncomputed and studied using Heard's program Orb and Goodman's program Snap.\n  We determine all (0,1,2)-irreducible pairs up to complexity 5, allowing\ndisconnected graphs but forbidding components without vertices in complexity 5.\nThe result is a list of 129 pairs, of which 123 are hyperbolic with parabolic\nmeridians. For these pairs we give detailed information on hyperbolic\ninvariants including volumes, symmetry groups and arithmetic invariants.\nPictures of all hyperbolic graphs up to complexity 4 are provided. We also\ninclude a partial analysis of knots and links.\n  The theoretical framework underlying the paper is twofold, being based on\nMatveev's theory of spines and on Thurston's idea (later developed by several\nauthors) of constructing hyperbolic structures via triangulations. Many of our\nresults were obtained (or suggested) by computer investigations.\n",
        "  Radiation dose is an important performance indicator of a dedicated breast CT\n(DBCT). In this paper, the method of putting thermoluminescent dosimeters (TLD)\ninto a breast shaped PMMA phantom to study the dose distribution in breasts was\nimproved by using smaller TLDs and a new half-ellipsoid PMMA phantom. Then the\nweighted CT dose index (CTDIw) was introduced to average glandular assessment\nin DBCT for the first time and two measurement modes were proposed for\ndifferent sizes of breasts. The dose deviations caused by using cylindrical\nphantoms were simulated using the Monte Carlo method and a set of correction\nfactors were calculated. The results of the confirmatory measurement with a\ncylindrical phantom (11cm/8cm) show that CTDIw gives a relatively conservative\noverestimate of the average glandular dose comparing to the results of Monte\nCarlo simulation and TLDs measurement. But with better practicability and\nstability, the CTDIw is suitable for dose evaluations in daily clinical\npractice. Both of the TLDs and CTDIw measurements demonstrate that the\nradiation dose of our DBCT system is lower than conventional two-view\nmammography.\n",
        "  The electronic Raman scattering in overdoped (Y,Ca)Ba2Cu3Oy was investigated\nwith changing hole concentration in the superconducting state. It was found\nthat the superconducting responses such as the pair-breaking peaks in the A1g\nand B1g spectra and the anisotropy of the pair-breaking peak in XX and YY\npolarizations radically change at around the carrier doping p=0.19. Since both\na- and c-axis resistivities strongly suggest the closing of pseudogap at\np~0.18, the observed change at p=0.19 in superconducting Raman response is\nattributed to the electronic crossover due to the collapse of the pseudogap.\n",
        "  The science of happiness is an area of positive psychology concerned with\nunderstanding what behaviors make people happy in a sustainable fashion.\nRecently, there has been interest in developing technologies that help\nincorporate the findings of the science of happiness into users' daily lives by\nsteering them towards behaviors that increase happiness. With the goal of\nbuilding technology that can understand how people express their happy moments\nin text, we crowd-sourced HappyDB, a corpus of 100,000 happy moments that we\nmake publicly available. This paper describes HappyDB and its properties, and\noutlines several important NLP problems that can be studied with the help of\nthe corpus. We also apply several state-of-the-art analysis techniques to\nanalyze HappyDB. Our results demonstrate the need for deeper NLP techniques to\nbe developed which makes HappyDB an exciting resource for follow-on research.\n",
        "  $2$-stratifolds are a generalization of $2$-manifolds in that there are\ndisjoint simple closed curves where several sheets meet. We show that the word\nproblem for fundamental groups of $2$-stratifolds is solvable.\n",
        "  Emergent pattern formation in self-propelled particle (SPP) systems is\nextensively studied because it addresses a range of swarming phenomena which\noccur without leadership. Here we present a dynamic SPP model in which a\nsensory blind zone is introduced into each particle's zone of interaction.\nUsing numerical simulations we discovered that the degradation of milling\npatterns with increasing blind zone ranges undergoes two distinct transitions,\nincluding a new, spatially nonhomogeneous transition that involves cessation of\nparticles' motion caused by broken symmetries in their interaction fields. Our\nresults also show the necessity of nearly complete panoramic sensory ability\nfor milling behavior to emerge in dynamic SPP models, suggesting a possible\nrelationship between collective behavior and sensory systems of biological\norganisms.\n",
        "  The physical processes responsible of sweeping up the surrounding gas in the\nhost galaxy of an AGN, and able in some circumstances to expel it from the\ngalaxy, are not yet well known. The various mechanisms are briefly reviewed:\nquasar or radio modes, either momentum-conserving outflows, energy-conserving\noutflows, or intermediate. They are confronted to observations, to know whether\nthey can explain the M-sigma relation, quench the star formation or whether\nthey can also provide some positive feedback and how the black hole accretion\nhistory is related to that of star formation.\n",
        "  This paper reviews the history and evidence related to remote wounding\neffects of ballistic pressure waves imparted to tissue by the impact of a\nbullet. Such remote effects are often referred to as hydraulic or hydrostatic\nshock. In spite of considerable published evidence and a long history, some\nmedical professionals continue to regard the ability of a bullet to injure\ntissue that is not directly crushed or stretched as mythical (Jandial R,\nReichwage B, Levy M, Duenas V, Sturdivan L. Ballistics for the neurosurgeon.\nNeurosurgery. 2008:62:472-480.) Early references to these effects date to the\n19th century; however, compelling experimental support and medical findings in\nhuman case studies did not become available until the late 20th and early 21st\ncentury.\n",
        "  We define a concordance invariant, epsilon(K), associated to the knot Floer\ncomplex of K, and give a formula for the Ozsv\\'ath-Szab\\'o concordance\ninvariant tau of K_{p,q}, the (p,q)-cable of a knot K, in terms of p, q,\ntau(K), and epsilon(K). We also describe the behavior of epsilon under cabling,\nallowing one to compute tau of iterated cables. Various properties and\napplications of epsilon are also discussed.\n",
        "  The upper critical field Hc2(T) of the multiband superconductor KFe2As2 has\nbeen studied via low-temperature thermal expansion and magnetostriction\nmeasurements. We present compelling evidence for Pauli-limiting effects\ndominating Hc2(T) for H || a, as revealed by a crossover from second- to\nfirst-order phase transitions to the superconducting state in the\nmagnetostriction measurements down to 50 mK. Corresponding features were absent\nfor H || c. To our knowledge, this crossover constitutes the first confirmation\nof Pauli limiting of the Hc2(T) of a multiband superconductor. The results are\nsupported by modeling Pauli limits for single-band and multiband cases.\n",
        "  In state-of-the-art Neural Machine Translation, an attention mechanism is\nused during decoding to enhance the translation. At every step, the decoder\nuses this mechanism to focus on different parts of the source sentence to\ngather the most useful information before outputting its target word. Recently,\nthe effectiveness of the attention mechanism has also been explored for\nmultimodal tasks, where it becomes possible to focus both on sentence parts and\nimage regions. Approaches to pool two modalities usually include element-wise\nproduct, sum or concatenation. In this paper, we evaluate the more advanced\nMultimodal Compact Bilinear pooling method, which takes the outer product of\ntwo vectors to combine the attention features for the two modalities. This has\nbeen previously investigated for visual question answering. We try out this\napproach for multimodal image caption translation and show improvements\ncompared to basic combination methods.\n",
        "  Speaker change detection (SCD) is an important task in dialog modeling. Our\npaper addresses the problem of text-based SCD, which differs from existing\naudio-based studies and is useful in various scenarios, for example, processing\ndialog transcripts where speaker identities are missing (e.g., OpenSubtitle),\nand enhancing audio SCD with textual information. We formulate text-based SCD\nas a matching problem of utterances before and after a certain decision point;\nwe propose a hierarchical recurrent neural network (RNN) with static\nsentence-level attention. Experimental results show that neural networks\nconsistently achieve better performance than feature-based approaches, and that\nour attention-based model significantly outperforms non-attention neural\nnetworks.\n",
        "  The acquisition and use of mobile phone is tremendously increasing especially\nin developing countries, but not without a concern. The greater concern among\nthe public is principally over the proximity of mobile base stations (MBS) to\nresidential areas rather than the use of handsets. In this paper, we present an\nassessment of Radio-Frequency (RF) radiation exposure level measurements and\nanalysis of radiation power density (in W/sq m) from mobile base stations\nrelative to radial distance (in metre). The minimum average power density from\nindividual base station in the town was about 47uW/sq m while the average\nmaximum was about 1.5mW/sq m. Our result showed that average power density of a\nbase station decreased with increase in distance (away from base station) and\nthat radiation intensity varies from one base station to another even at the\nsame distance away. Our result (obtained signature of power density variation\nfrom data) was also compared the with an 'expected' signature. It was found\nthat radiation from external sources (indicative) interfered with the reference\nbase station and accounted for the deviation observed in this study. Finally,\nour result showed that the RF exposure hazard index in the town of Lokoja was\nbelow the permitted RF exposure limit to the general public recommended by\nICRNIP. Useful recommendations were also made to the Policy and Regulatory\nAgencies responsible to Telephony in Nigeria.\n",
        "  This note will become part of a new paper with more authors.\n",
        "  In this note, we show that, if a pseudo-Anosov map $\\phi:S\\to S$ admits a\nfinite cover whose action on the first homology has spectral radius greater\nthan $1$, then the monodromy of any fibered structure of any finite cover of\nthe mapping torus $M_{\\phi}$ has the same property.\n",
        "  We consider manifolds $M^{2n}$ which admit smooth maps into a connected sum\nof $S^1\\times S^n$ with only finitely many critical points, for\n$n\\in\\{2,4,8\\}$, and compute the minimal number of critical points.\n",
        "  Leo P is a low-luminosity dwarf galaxy discovered through the blind HI\nArecibo Legacy Fast ALFA (ALFALFA) survey. The HI and follow-up optical\nobservations have shown that Leo P is a gas-rich dwarf galaxy with active star\nformation, an underlying older population, and an extremely low oxygen\nabundance. We have obtained optical imaging with the Hubble Space Telescope to\ntwo magnitudes below the red clump in order to study the evolution of Leo P. We\nrefine the distance measurement to Leo P to be 1.62+/-0.15 Mpc, based on the\nluminosity of the horizontal branch stars and 10 newly identified RR Lyrae\ncandidates. This places the galaxy at the edge of the Local Group, ~0.4 Mpc\nfrom Sextans B, the nearest galaxy in the NGC 3109 association of dwarf\ngalaxies of which Leo P is clearly a member. The star responsible for ionizing\nthe HII region is most likely an O7V or O8V spectral type, with a stellar mass\n>25 Msun. The presence of this star provides observational evidence that\nmassive stars at the upper-end of the initial mass function are capable of\nbeing formed at star formation rates as low as ~10^-5 Msun/yr. The best-fitting\nstar formation history derived from the resolved stellar populations of Leo P\nusing the latest PARSEC models shows a relatively constant star formation rate\nover the lifetime of the galaxy. The modeled luminosity characteristics of Leo\nP at early times are consistent with low-luminosity dSph Milky Way satellites,\nsuggesting that Leo P is what a low-mass dSph would look like if it evolved in\nisolation and retained its gas. Despite the very low mass of Leo P, the imprint\nof reionization on its star formation history is subtle at best, and consistent\nwith being totally negligible. The isolation of Leo P, and the total quenching\nof star formation of Milky Way satellites of similar mass, implies that local\nenvironment dominates the quenching of the Milky Way satellites.\n",
        "  Dual-energy computed tomography (CT) is to reconstruct images of an object\nfrom two projection datasets generated from two distinct x-ray source energy\nspectra. It can provide more accurate attenuation quantification than\nconventional CT with a single x-ray energy spectrum. In the diagnostic energy\nrange, x-ray energy-dependent attenuation can be approximated as a linear\ncombination of photoelectric absorption and Compton scattering. Hence, two\nphysical components of x-ray attenuation can be determined from two spectral\ninformative projection datasets to achieve monochromatic imaging and material\ndecomposition. In this paper, a projection decomposition method is proposed for\nthe image reconstruction in dual-energy CT. This method combines both an\nanalytical algorithm and a single-variable optimization method to solve the\nnon-linear polychromatic x-ray integral model, allowing accurate quantification\nof photoelectric absorption and Compton scattering components. Numerical tests\nare performed to illustrate the merit of the proposed method by comparing with\nclassical projection decomposition methods.\n",
        "  We use time-of-flight (ToF) inelastic neutron scattering (INS) spectroscopy\nto investigate the doping dependence of magnetic excitations across the phase\ndiagram of NaFe$_{1-x}$Co$_x$As with $x=0, 0.0175, 0.0215, 0.05,$ and $0.11$.\nThe effect of electron-doping by partially substituting Fe by Co is to form\nresonances that couple with superconductivity, broaden and suppress low energy\n($E\\le 80$ meV) spin excitations compared with spin waves in undoped NaFeAs.\nHowever, high energy ($E> 80$ meV) spin excitations are weakly Co-doping\ndependent. Integration of the local spin dynamic susceptibility\n$\\chi^{\\prime\\prime}(\\omega)$ of NaFe$_{1-x}$Co$_x$As reveals a total\nfluctuating moment of 3.6 $\\mu_B^2$/Fe and a small but systematic reduction\nwith electron doping. The presence of a large spin gap in the Co-overdoped\nnonsuperconducting NaFe$_{0.89}$Co$_{0.11}$As suggests that Fermi surface\nnesting is responsible for low-energy spin excitations. These results parallel\nNi-doping evolution of spin excitations in BaFe$_{2-x}$Ni$_x$As$_2$, confirming\nthe notion that low-energy spin excitations coupling with itinerant electrons\nare important for superconductivity, while weakly doping dependent high-energy\nspin excitations result from localized moments.\n",
        "  We show that hole-doped cuprates can harbor {\\it Fermi-surface-free\nsuperconductivity} similar to the case of the pnictides. This occurs near the\ndoping at which a new Fermi surface pocket appears in the antinodal region. The\nchange in Fermi surface topology is accompanied by a characteristic rise in\nspectral weight. Our results support the presence of a trisected\nsuperconducting dome, and suggest that superconductivity is responsible for\nstabilizing the $(\\pi,\\pi)$ magnetic order.\n",
        "  We investigated the annealing effects on the physical properties of\nSrFe$_2$(As$_{1-x}$P$_x$)$_2$ ($x=0.35$). The superconducting transition\ntemperature ($T_c$) increased from 26 K to 33 K by annealing. The X-ray\ndiffraction measurement suggested that the annealed crystals have the\nshorter/longer $a/c$-axes and the larger pnictogen height $h_\\mathrm{Pn}$. This\nmust be linked to the $T_c$-enhancement by annealing. Moreover, it was found\nthat the post-annealing decreased the electronic specific heat coefficient at\n$T$=0 K, $\\gamma_r$, and changed the magnetic field ($H$) dependence from\nsub-linear $\\gamma_r\\propto H^{0.7}$ to $H$-linear $\\gamma_r\\propto H$. This\ncan be attributed the electronic change from dirty to clean superconductors\nwith $s_{\\pm}$ gap.\n",
        "  The Zebrafish Model Organism Database (ZFIN) provides a Web resource of\nzebrafish genomic, genetic, developmental, and phenotypic data. Four different\nontologies are currently used to annotate data to the most specific term\navailable facilitating a better comparison between inter-species data. In\naddition, ontologies are used to help users find and cluster data more quickly\nwithout the need of knowing the exact technical name for a term.\n",
        "  Interventional C-arm systems allow flexible 2-D imaging of a 3-D scene while\nbeing capable of cone beam computed tomography. Due to the flexible structure\nof the C-arm, the rotation speed is limited, increasing the acquisition time\ncompared to conventional computed tomography. Therefore, patient motion\nfrequently occurs during data acquisition inducing inconsistencies in the\nprojection raw data. A framework using Grangeat's theorem and epipolar\nconsistency was successfully applied for compensating rigid motion. This\nalgorithm was efficiently parallelized, however, before each iteration, the\npseudoinverse of each projection matrix must be calculated. We present a\ngeometric modification of the presented algorithm which can be used without a\npseudo-inverse. As such, the complete algorithm can be implemented for\nlow-level hardware without the need of a linear algebra package that supports\nthe calculation of matrix inverse. Both algorithms are applied for head motion\ncompensation and the runtime of both is compared.\n",
        "  Bae and Park found an upper bound on the arc index of prime links in terms of\nthe minimal crossing number. In this paper, we extend the definition of the arc\npresentation to spatial graphs and find an upper bound on the arc index $\\alpha\n(G)$ of any spatial graph $G$ as $$\\alpha(G) \\leq c(G)+e+b,$$ where $c(G)$ is\nthe minimal crossing number of $G$, $e$ is the number of edges, and $b$ is the\nnumber of bouquet cut-components. This upper bound is lowest possible.\n",
        "  Suppose a knot in a $3$-manifold is in $n$-bridge position. We consider a\nreduction of the knot along a bridge disk $D$ and show that the result is an\n$(n-1)$-bridge position if and only if there is a bridge disk $E$ such that\n$(D, E)$ is a cancelling pair. We apply this to an unknot $K$, in $n$-bridge\nposition with respect to a bridge sphere $S$ in the $3$-sphere, to consider the\nrelationship between a bridge disk $D$ and a disk in the $3$-sphere that $K$\nbounds. We show that if a reduction of $K$ along $D$ yields an $(n-1)$-bridge\nposition, then $K$ bounds a disk that contains $D$ as a subdisk and intersects\n$S$ in $n$ arcs.\n",
        "  Arterial tissue consists of multiple structurally important constituents that\nhave individual material properties and associated stress-free configurations\nthat evolve over time. This gives rise to residual stresses contributing to the\nhomoeostatic state of stress in vivo as well as adaptations to perturbed loads,\ndisease or injury. The existence of residual stresses in an intact but\nload-free excised arterial segment suggests compressive and tensile stresses,\nrespectively, in the inner and outer walls. Accordingly, an artery ring springs\nopen into a sector after a radial cut. The measurement of the opening angle is\ncommonly used to deduce the residual stresses, which are the stresses required\nto close back the ring. The opening angle method provides an average estimate\nof circumferential residual stresses but it gives no information on local\ndistributions through the thickness and along the axial direction. To address\nthis lack, a new method is proposed in this article to derive maps of residual\nstresses using an approach based on the contour method. A piece of freshly\nexcised tissue is carefully cut into the specimen, and the local distribution\nof residual strains and stresses is determined from whole-body digital image\ncorrelation measurements using an inverse approach based on a finite element\nmodel.\n",
        "  Understanding the nature of spiral structure in disk galaxies is one of the\nmain, and still unsolved questions in galactic astronomy. However, theoretical\nworks are proposing new testable predictions whose detection is becoming\nfeasible with recent development in instrumentation. In particular, streaming\nmotions along spiral arms are expected to induce azimuthal variations in the\nchemical composition of a galaxy at a given galactic radius. In this letter we\nanalyse the gas content in NGC 6754 with VLT/MUSE data to characterise its 2D\nchemical composition and H$\\alpha$ line-of-sight velocity distribution. We find\nthat the trailing (leading) edge of the NGC 6754 spiral arms show signatures of\ntangentially-slower, radially-outward (tangentially-faster, radially-inward)\nstreaming motions of metal-rich (poor) gas over a large range of radii. These\nresults show direct evidence of gas radial migration for the first time. We\ncompare our results with the gas behaviour in a $N$-body disk simulation\nshowing spiral morphological features rotating with a similar speed as the gas\nat every radius, in good agreement with the observed trend. This indicates that\nthe spiral arm features in NGC 6754 may be transient and rotate similarly as\nthe gas does at a large range of radii.\n",
        "  Using techniques from the theory of Kirby calculus we give an explicit\nconstruction of a four dimensional hyperbolic link complement in a 4-manifold\nthat is diffeomorphic to the standard 4-sphere.\n",
        "  Pattern matching of streaming time series with lower latency under limited\ncomputing resource comes to a critical problem, especially as the growth of\nIndustry 4.0 and Industry Internet of Things. However, against traditional\nsingle pattern matching model, a pattern may contain multiple subpatterns\nrepresenting different physical meanings in the real world. Hence, we formulate\na new problem, called \"consecutive subpatterns matching\", which allows users to\nspecify a pattern containing several consecutive subpatterns with various\nspecified thresholds. We propose a novel representation Equal-Length Block\n(ELB) together with two efficient implementations, which work very well under\nall Lp-Norms without false dismissals. Extensive experiments are performed on\nsynthetic and real-world datasets to illustrate that our approach outperforms\nthe brute-force method and MSM, a multi-step filter mechanism over the\nmulti-scaled representation by orders of magnitude.\n",
        "  We report a study of the normal- and superconducting-state electronic\nproperties of the centrosymmetric compound SrPt3P via 31P\nnuclear-magnetic-resonance (NMR) and magnetometry investigations. Essential\nfeatures such as a sharp drop of the Knight shift at T < Tc and an exponential\ndecrease of the NMR spin-lattice relaxation ratio 1/(T1T) below Tc are\nconsistent with an s-wave electron pairing in SrPt3P, although a direct\nconfirmation in the form of a Hebel-Slichter-type peak is lacking. Normal-state\nNMR data at T < 50 K indicate conventional features of the conduction\nelectrons, typical of simple metals such as lithium or silver. Our data are\nfinally compared with available NMR results for the noncentrosymmetric\nsuperconductors LaPt$_3$Si and CePt$_3$Si, which adopt similar crystal\nstructures.\n",
        "  The antibody repertoire of each individual is continuously updated by the\nevolutionary process of B cell receptor mutation and selection. It has recently\nbecome possible to gain detailed information concerning this process through\nhigh-throughput sequencing. Here, we develop modern statistical molecular\nevolution methods for the analysis of B cell sequence data, and then apply them\nto a very deep short-read data set of B cell receptors. We find that the\nsubstitution process is conserved across individuals but varies significantly\nacross gene segments. We investigate selection on B cell receptors using a\nnovel method that side-steps the difficulties encountered by previous work in\ndifferentiating between selection and motif-driven mutation; this is done\nthrough stochastic mapping and empirical Bayes estimators that compare the\nevolution of in-frame and out-of-frame rearrangements. We use this new method\nto derive a per-residue map of selection, which provides a more nuanced view of\nthe constraints on framework and variable regions.\n",
        "  The superconducting Cu_xBi_2Se_3 is an electron-doped topological insulator\nand is a prime candidate of the topological superconductor which still awaits\ndiscovery. The electrochemical intercalation technique for synthesizing\nCu_xBi2Se3 offers good control of restricting Cu into the van-der-Waals gap and\nyields samples with shielding fractions of up to ~50%. We report essential\ndetails of this synthesis technique and present the established superconducting\nphase diagram of T_c vs x, along with a diagram of the shielding fraction vs x.\nIntriguingly, those diagrams suggest that there is a tendency to spontaneously\nform small islands of optimum superconductor in this material.\n",
        "  We study the similarity search problem which aims to find the similar query\nresults according to a set of given data and a query string. To balance the\nresult number and result quality, we combine query result diversity with query\nrelaxation. Relaxation guarantees the number of the query results, returning\nmore relevant elements to the query if the results are too few, while the\ndiversity tries to reduce the similarity among the returned results. By making\na trade-off of similarity and diversity, we improve the user experience. To\nachieve this goal, we define a novel goal function combining similarity and\ndiversity. Aiming at this goal, we propose three algorithms. Among them,\nalgorithms genGreedy and genCluster perform relaxation first and select part of\nthe candidates to diversify. The third algorithm CB2S splits the dataset into\nsmaller pieces using the clustering algorithm of k-means and processes queries\nin several small sets to retrieve more diverse results. The balance of\nsimilarity and diversity is determined through setting a threshold, which has a\ndefault value and can be adjusted according to users' preference. The\nperformance and efficiency of our system are demonstrated through extensive\nexperiments based on various datasets.\n",
        "  Given two \\'etale groupoids $\\Cal G$ and $\\Cal G'$, we consider the set of\npointed morphisms from $\\Cal G$ to $\\Cal G'$. Under suitable hypothesis we\nintroduce on this set a structure of Banach manifold which can be considered as\nthe space of objects of an \\'etale groupoid whose space of orbits is the space\nof morphisms from $\\Cal G$ to $\\Cal G'$.\n",
        "  We present the IIT Bombay English-Hindi Parallel Corpus. The corpus is a\ncompilation of parallel corpora previously available in the public domain as\nwell as new parallel corpora we collected. The corpus contains 1.49 million\nparallel segments, of which 694k segments were not previously available in the\npublic domain. The corpus has been pre-processed for machine translation, and\nwe report baseline phrase-based SMT and NMT translation results on this corpus.\nThis corpus has been used in two editions of shared tasks at the Workshop on\nAsian Language Translation (2016 and 2017). The corpus is freely available for\nnon-commercial research. To the best of our knowledge, this is the largest\npublicly available English-Hindi parallel corpus.\n",
        "  With an ever increasing size of text present on the Internet, automatic\nsummary generation remains an important problem for natural language\nunderstanding. In this work we explore a novel full-fledged pipeline for text\nsummarization with an intermediate step of Abstract Meaning Representation\n(AMR). The pipeline proposed by us first generates an AMR graph of an input\nstory, through which it extracts a summary graph and finally, generate summary\nsentences from this summary graph. Our proposed method achieves\nstate-of-the-art results compared to the other text summarization routines\nbased on AMR. We also point out some significant problems in the existing\nevaluation methods, which make them unsuitable for evaluating summary quality.\n",
        "  We exhibit a certain infinite family of three-stranded quasi-alternating\npretzel knots which are counterexamples to Lobb's conjecture that the sl_3-knot\nconcordance invariant s_3 (suitably normalised) should be equal to the\nRasmussen invariant s_2. For this family, |s_3| < |s_2|. However, we also find\nother knots for which |s_3| > |s_2|. The main tool is an implementation of\nMorrison and Nieh's algorithm to calculate Khovanov's sl_3-foam link homology.\nOur C++-program is fast enough to calculate the integral homology of e.g. the\n(6,5)-torus knot in six minutes. Furthermore, we propose a potential\nimprovement of the algorithm by gluing sub-tangles in a more flexible way.\n",
        "  Magnetic resonance imaging (MRI) is the cornerstone technique for diagnostic\nmedicine, biology, and neuroscience. This imaging method is highly innovative,\nnoninvasive and its impact continues to grow. It can be used for measuring\nchanges in the brain after enhanced neural activity, detecting early cancerous\ncells in tissue, as well as for imaging nanoscale biological structures, and\ncontrolling fluid dynamics, and it can be beneficial for cardiovascular\nimaging. The MRI performance is characterized by a signal-to-noise ratio,\nhowever the spatial resolution and image contrast depend strongly on the\nscanner design. Here, we reveal how to exploit effectively the unique\nproperties of metasurfaces for the substantial improvement of MRI efficiency.\nWe employ a metasurface created by an array of wires placed inside the MRI\nscanner under an object, and demonstrate a giant enhancement of the magnetic\nfield by means of subwavelength near-field manipulation with the metasurface,\nthus strongly increasing the scanner sensitivity, signal-to-noise ratio, and\nimage resolution. We demonstrate experimentally this effect for a commercially\navailable MRI scanner and a biological tissue sample. Our results are\ncorroborated by measured and simulated characteristics of the metasurface\nresonator, and our approach can enhance dramatically functionalities of widely\navailable low-field MRI devices.\n",
        "  The c-axis optical response of the underdoped cuprates is qualitatively\ndifferent from its in-plane counterpart. The features of the pseudogap show\nthemselves more prominently in the c-axis than in-plane. We compute both the\nc-axis and in-plane optical conductivity using the Yang-Rice-Zhang model of the\nunderdoped cuprates. This model combined with coherent interlayer tunnelling is\nenough to explain the qualitative differences between the in-plane and c-axis\ndata. We show how pseudogap features manifest themselves in the infrared and\nmicrowave conductivity within this model.\n",
        "  In this paper a novel fragile watermarking scheme is proposed to detect,\nlocalize and recover malicious modifications in relational databases. In the\nproposed scheme, all tuples in the database are first securely divided into\ngroups. Then watermarks are embedded and verified group-by-group independently.\nBy using the embedded watermark, we are able to detect and localize the\nmodification made to the database and even we recover the true data from the\ndatabase modified locations. Our experimental results show that this scheme is\nso qualified; i.e. distortion detection and true data recovery both are\nperformed successfully.\n",
        "  Background: The 2014 Ebola outbreak in West Africa was the largest on record,\nresulting in over 25,000 total infections and 15,000 total deaths. Mathematical\nmodeling can be used to investigate the mechanisms driving transmission during\nthis outbreak -- in particular, burial practices appear to have been major\nsource of infections. Methodology/Principal Findings: We developed a\nmulti-stage model of Ebola virus transmission linked to a game-theoretic model\nof population burial practice selection. We fit our model to cumulative\nincidence and mortality data from Guinea, Liberia, and Sierra Leone from\nJanuary 2014 to March, 2016. The inclusion of behavior change substantially\nimproved best fit estimates and final size prediction compared to a reduced\nmodel with fixed burials. Best fit trajectories suggest that the majority of\nsanitary burial adoption occurred between July, 2014 and October, 2014.\nHowever, these simulations also indicated that continued sanitary burial\npractices waned following the resolution of the outbreak.\nConclusions/Significance: Surveillance data from the 2014 outbreak appears to\nhave a signal of changes in the dominant burial practices in all three\ncountries. Increased adoption of sanitary burials likely attenuated\ntransmission, but these changes occurred too late to prevent the explosive\ngrowth of the outbreak during its early phase. For future outbreaks, explicitly\nmodeling behavior change and collecting data on transmission-related behaviors\nmay improve intervention planning and outbreak response.\n",
        "  X-ray spectral imaging provides quantitative imaging of trace elements in\nbiological sample with high sensitivity. We propose a novel algorithm to\npromote the signal-to-noise ratio (SNR) of X-ray spectral images that have low\nphoton counts. Firstly, we estimate the image data area that belongs to the\nhomogeneous parts through confidence interval testing. Then, we apply the\nPoisson regression through its maximum likelihood estimation on this area to\nestimate the true photon counts from the Poisson noise corrupted data. Unlike\nother denoising methods based on regression analysis, we use the bootstrap\nresampling method to ensure the accuracy of regression estimation. Finally, we\nuse a robust local nonparametric regression method to estimate the baseline and\nsubsequently subtract it from the X-ray spectral data to further improve the\nSNR of the data. Experiments on several real samples show that the proposed\nmethod performs better than some state-of-the-art approaches to ensure accuracy\nand precision for quantitative analysis of the different trace elements in\nstandard reference biological sample.\n",
        "  Ultrasound speckle is a granular texture pattern appearing in ultrasound\nimaging. It can be used to distinguish tissues and identify pathologies.\nLorentz force electrical impedance tomography is an ultrasound-based medical\nimaging technique of the tissue electrical conductivity. It is based on the\napplication of an ultrasound wave in a medium placed in a magnetic field and on\nthe measurement of the induced electric current due to Lorentz force. Similarly\nto ultrasound imaging, we hypothesized that a speckle could be observed with\nLorentz force electrical impedance tomography imaging. In this study, we first\nassessed the theoretical similarity between the measured signals in Lorentz\nforce electrical impedance tomography and in ultrasound imaging modalities. We\nthen compared experimentally the signal measured in both methods using an\nacoustic and electrical impedance interface. Finally, a bovine muscle sample\nwas imaged using the two methods. Similar speckle patterns were observed. This\nindicates the existence of an \"acousto-electrical speckle\" in the Lorentz force\nelectrical impedance tomography with spatial characteristics driven by the\nacoustic parameters but due to electrical impedance inhomogeneities instead of\nacoustic ones as is the case of ultrasound imaging.\n",
        "  In this study we analyzed a corpus of 8 million words academic literature\nfrom Computational lingustics' academic literature. the lexical bundles from\nthis corpus are categorized based on structures and functions.\n",
        "  One of the key challenges in natural language processing (NLP) is to yield\ngood performance across application domains and languages. In this work, we\ninvestigate the robustness of the mention detection systems, one of the\nfundamental tasks in information extraction, via recurrent neural networks\n(RNNs). The advantage of RNNs over the traditional approaches is their capacity\nto capture long ranges of context and implicitly adapt the word embeddings,\ntrained on a large corpus, into a task-specific word representation, but still\npreserve the original semantic generalization to be helpful across domains. Our\nsystematic evaluation for RNN architectures demonstrates that RNNs not only\noutperform the best reported systems (up to 9\\% relative error reduction) in\nthe general setting but also achieve the state-of-the-art performance in the\ncross-domain setting for English. Regarding other languages, RNNs are\nsignificantly better than the traditional methods on the similar task of named\nentity recognition for Dutch (up to 22\\% relative error reduction).\n",
        "  The selection of beam orientations, which is a key step in radiation\ntreatment planning, is particularly challenging for non-coplanar radiotherapy\nsystems due to the large number of candidate beams. In this paper, we report\nprogress on the group sparsity approach to beam orientation optimization,\nwherein beam angles are selected by solving a large scale fluence map\noptimization problem with an additional group sparsity penalty term that\nencourages most candidate beams to be inactive. The optimization problem is\nsolved using an accelerated proximal gradient method, the Fast Iterative\nShrinkage-Thresholding Algorithm (FISTA). We derive a closed-form expression\nfor a relevant proximal operator which enables the application of FISTA. The\nproposed algorithm is used to create non-coplanar treatment plans for four\ncases (including head and neck, lung, and prostate cases), and the resulting\nplans are compared with clinical plans. The dosimetric quality of the group\nsparsity treatment plans is superior to that of the clinical plans. Moreover,\nthe runtime for the group sparsity approach is typically about 5 minutes.\nProblems of this size could not be handled using the previous group sparsity\nmethod for beam orientation optimization, which was slow to solve much smaller\ncoplanar cases. This work demonstrates for the first time that the group\nsparsity approach, when combined with an accelerated proximal gradient method\nsuch as FISTA, works effectively for non-coplanar cases with 500-800 candidate\nbeams.\n",
        "  Studying star formation in spiral arms tells us not only about the evolution\nof star formation, and molecular clouds, but can also tell us about the nature\nof spiral structure in galaxies. I will address both these topics using the\nresults of recent simulations and observations. Galactic scale simulations are\nbeginning to examine in detail the evolution of GMCs as they form in spiral\narms, and then disperse by stellar feedback or shear. The overall timescale for\nthis process appears comparable to the crossing time of the GMCs, a few Myrs\nfor $10^5$ M$_{\\odot}$ clouds, 20 Myr or so for more massive GMCs. Both\nsimulations and observations show that the massive clouds are found in the\nspiral arms, likely as a result of cloud-cloud collisions. Simulations\nincluding stars should also tell us about the stellar age distribution in GMCs,\nand across spiral arms. More generally, recent work on spiral galaxies suggests\nthat the dynamics of gas flows in spiral arms are different in longlived and\ntransient spiral arms, resulting in different age patterns in the stars. Such\nresults could be used to help establish the main driver of spiral structure in\nthe Milky Way (Toomre instabilities, the bar, or nearby companion galaxies) in\nconjunction with future surveys.\n",
        "  We present spectroscopic follow-up observations of CR7 with ALMA, targeted at\nconstraining the infrared (IR) continuum and [CII]$_{158 \\mu \\rm m}$\nline-emission at high spatial resolution matched to the HST/WFC3 imaging. CR7\nis a luminous Ly$\\alpha$ emitting galaxy at $z=6.6$ that consists of three\nseparated UV-continuum components. Our observations reveal several\nwell-separated components of [CII] emission. The two most luminous components\nin [CII] coincide with the brightest UV components (A and B), blue-shifted by\n$\\approx 150$ km s$^{-1}$ with respect to the peak of Ly$\\alpha$ emission.\nOther [CII] components are observed close to UV clumps B and C and are\nblue-shifted by $\\approx300$ and $\\approx80$ km s$^{-1}$ with respect to the\nsystemic redshift. We do not detect FIR continuum emission due to dust with a\n3$\\sigma$ limiting luminosity L$_{\\rm IR} (T_d = 35 \\rm \\, K) <\n3.1\\times10^{10}$ L$_{\\odot}$. This allows us to mitigate uncertainties in the\ndust-corrected SFR and derive SFRs for the three UV clumps A, B and C of 28, 5\nand 7 M$_{\\odot}$ yr$^{-1}$. All clumps have [CII] luminosities consistent\nwithin the scatter observed in the local relation between SFR and L$_{\\rm\n[CII]}$, implying that strong Ly$\\alpha$ emission does not necessarily\nanti-correlate with [CII] luminosity. Combining our measurements with the\nliterature, we show that galaxies with blue UV slopes have weaker [CII]\nemission at fixed SFR, potentially due to their lower metallicities and/or\nhigher photoionisation. Comparison with hydrodynamical simulations suggests\nthat CR7's clumps have metallicities of $0.1<\\rm Z/Z_{\\odot}<0.2$. The observed\nISM structure of CR7 indicates that we are likely witnessing the build up of a\ncentral galaxy in the early Universe through complex accretion of satellites.\n",
        "  The goal of the present chapter is to explore the possibility of providing\nthe research (but also the industrial) community that commonly uses spoken\ncorpora with a stable portfolio of well-documented standardised formats that\nallow a high re-use rate of annotated spoken resources and, as a consequence,\nbetter interoperability across tools used to produce or exploit such resources.\n",
        "  Background: Electronic health record (EHR) notes contain abundant medical\njargon that can be difficult for patients to comprehend. One way to help\npatients is to reduce information overload and help them focus on medical terms\nthat matter most to them.\n  Objective: The aim of this work was to develop FIT (Finding Important Terms\nfor patients), an unsupervised natural language processing (NLP) system that\nranks medical terms in EHR notes based on their importance to patients.\n  Methods: We built FIT on a new unsupervised ensemble ranking model derived\nfrom the biased random walk algorithm to combine heterogeneous information\nresources for ranking candidate terms from each EHR note. Specifically, FIT\nintegrates four single views for term importance: patient use of medical\nconcepts, document-level term salience, word-occurrence based term relatedness,\nand topic coherence. It also incorporates partial information of term\nimportance as conveyed by terms' unfamiliarity levels and semantic types. We\nevaluated FIT on 90 expert-annotated EHR notes and compared it with three\nbenchmark unsupervised ensemble ranking methods.\n  Results: FIT achieved 0.885 AUC-ROC for ranking candidate terms from EHR\nnotes to identify important terms. When including term identification, the\nperformance of FIT for identifying important terms from EHR notes was 0.813\nAUC-ROC. It outperformed the three ensemble rankers for most metrics. Its\nperformance is relatively insensitive to its parameter.\n  Conclusions: FIT can automatically identify EHR terms important to patients\nand may help develop personalized interventions to improve quality of care. By\nusing unsupervised learning as well as a robust and flexible framework for\ninformation fusion, FIT can be readily applied to other domains and\napplications.\n",
        "  Mining labeled subgraph is a popular research task in data mining because of\nits potential application in many different scientific domains. All the\nexisting methods for this task explicitly or implicitly solve the subgraph\nisomorphism task which is computationally expensive, so they suffer from the\nlack of scalability problem when the graphs in the input database are large. In\nthis work, we propose FS^3, which is a sampling based method. It mines a small\ncollection of subgraphs that are most frequent in the probabilistic sense. FS^3\nperforms a Markov Chain Monte Carlo (MCMC) sampling over the space of a\nfixed-size subgraphs such that the potentially frequent subgraphs are sampled\nmore often. Besides, FS^3 is equipped with an innovative queue manager. It\nstores the sampled subgraph in a finite queue over the course of mining in such\na manner that the top-k positions in the queue contain the most frequent\nsubgraphs. Our experiments on database of large graphs show that FS^3 is\nefficient, and it obtains subgraphs that are the most frequent amongst the\nsubgraphs of a given size.\n",
        "  The SPARQL query language is a recent W3C standard for processing RDF data, a\nformat that has been developed to encode information in a machine-readable way.\nWe investigate the foundations of SPARQL query optimization and (a) provide\nnovel complexity results for the SPARQL evaluation problem, showing that the\nmain source of complexity is operator OPTIONAL alone; (b) propose a\ncomprehensive set of algebraic query rewriting rules; (c) present a framework\nfor constraint-based SPARQL optimization based upon the well-known chase\nprocedure for Conjunctive Query minimization. In this line, we develop two\nnovel termination conditions for the chase. They subsume the strongest\nconditions known so far and do not increase the complexity of the recognition\nproblem, thus making a larger class of both Conjunctive and SPARQL queries\namenable to constraint-based optimization. Our results are of immediate\npractical interest and might empower any SPARQL query optimizer.\n",
        "  We develop a primal-dual algorithm that allows for one-step inversion of\nspectral CT transmission photon counts data to a basis map decomposition. The\nalgorithm allows for image constraints to be enforced on the basis maps during\nthe inversion. The derivation of the algorithm makes use of a local upper\nbounding quadratic approximation to generate descent steps for non-convex\nspectral CT data discrepancy terms, combined with a new convex-concave\noptimization algorithm. Convergence of the algorithm is demonstrated on\nsimulated spectral CT data. Simulations with noise and anthropomorphic phantoms\nshow examples of how to employ the constrained one-step algorithm for spectral\nCT data.\n",
        "  Knots are commonly found in molecular chains such as DNA and proteins, and\nthey have been considered to be useful models for structural analysis of these\nmolecules. One interested quantity is the minimum number of monomers necessary\nto realize a molecular knot. The minimum lattice length $\\mbox{Len}(K)$ of a\nknot $K$ indicates the minimum length necessary to construct $K$ in the cubic\nlattice. Another important quantity in physical knot theory is the ropelength\nwhich is one of knot energies measuring the complexity of knot conformation.\nThe minimum ropelength $\\mbox{Rop}(K)$ is the minimum length of an ideally\nflexible rope necessary to tie a given knot $K$.\n  Much effort has been invested in the research project for finding upper\nbounds on both quantities in terms of the minimum crossing number $c(K)$ of the\nknot. It is known that $\\mbox{Len}(K)$ and $\\mbox{Rop}(K)$ lie between\n$\\mbox{O}(c(K)^{\\frac{3}{4}})$ and $\\mbox{O}(c(K) [\\ln (c(K))]^5)$, but unknown\nyet whether any family of knots has superlinear growth. In this paper, we focus\non 2-bridge knots and links. Linear growth upper bounds on the minimum lattice\nlength and minimum ropelength for nontrivial 2-bridge knots or links are\npresented:\n  $\\mbox{Len}(K) \\leq 8 c(K) + 2$.\n  $\\mbox{Rop}(K) \\leq 11.39 c(K) + 12.37$.\n",
        "  Analysis of demographic catastrophes shows that, with the exception of\nperhaps only two critical events, they were too weak to influence the growth of\nhuman population. These results reinforce the conclusion that the concept of\nthe Epoch of Malthusian Stagnation, the alleged first stage of growth claimed\nby the Demographic Transition Theory, is not supported by empirical evidence.\nThey show that even if we assume that Malthusian positive checks are capable of\nsuppressing the growth of population their impact was too weak to create the\nEpoch of Malthusian Stagnation.\n",
        "  Using the functional integral method, we construct a theory of heterotic SIS\nJosephson junctions between single- and two-gap superconductors. The theory\npredicts the presence of in-phase and out-of-phase collective oscillation modes\nof superconducting phases. The former corresponds to the Josephson plasma mode\nwhose frequency is drastically reduced for $\\pm$ s-wave symmetry, and the\nlatter is a counterpart of Leggett's mode in Josephson junctions. We also\nreveal that the critical current and the Fraunhofer pattern strongly depend on\nthe symmetry type of the two-gap superconductor.\n",
        "  Magnetic Particle Imaging (MPI) is an emerging imaging modality that maps the\nspatial distribution of magnetic nanoparticles. The x-space reconstruction in\nMPI results in highly blurry images, where the resolution depends on both\nsystem parameters and nanoparticle type. Previous techniques to counteract this\nblurring rely on the knowledge of the imaging point spread function (PSF),\nwhich may not be available or may require additional measurements. This work\nproposes a blind deconvolution algorithm for MPI to recover the precise spatial\ndistribution of nanoparticles. The proposed algorithm exploits the observation\nthat the imaging PSF in MPI has zero phase in Fourier domain. Thus, even though\nthe reconstructed images are highly blurred, phase remains unaltered. We\nleverage this powerful property to iteratively enforce consistency of phase and\nbounded l1 energy information, using an orthogonal Projections Onto Convex Sets\n(POCS) algorithm. To demonstrate the method, comprehensive simulations were\nperformed without and with nanoparticle relaxation effects, and at various\nnoise levels. In addition, imaging experiments were performed on an in-house\nMPI scanner using a three-vial phantom that contained different nanoparticle\ntypes. Image quality was compared with conventional deconvolution methods,\nWiener deconvolution and Lucy-Richardson method, which explicitly rely on the\nknowledge of PSF. Both the simulation results and experimental imaging results\nshow that the proposed blind deconvolution algorithm outperforms the\nconventional deconvolution methods. Without utilizing the imaging PSF, the\nproposed algorithm improves image quality and resolution even in the case of\ndifferent nanoparticle types, while displaying reliable performance against\nloss of the fundamental harmonic, nanoparticle relaxation effects, and noise.\n",
        "  We perform current(I)-voltage(V) measurements on low resistance\nsuperconductor-normal-superconductor arrays in finite magnetic fields, focusing\non the dilute vortex population regime. We observe significant deviations from\npredicted behavior, notably the absence of a differential resistance peak near\nthe vortex de-pinning current, and a broad linear I-V region with an\nextrapolated I intercept equal to the de-pinning current. Comparing these\nresults to an overdamped molecular vortex model, we find that this behavior can\nbe explained by the presence of a history dependent dissipative force. This\napproach has not been considered previously, yet is crucial for obtaining a\ncorrect description of the vortex dynamics in superconducting arrays.\n",
        "  Suppose that $\\mathcal F$ is a transversely oriented, codimension one\nfoliation of a connected, closed, oriented 3-manifold. Suppose also that\n$\\mathcal F$ has continuous tangent plane field and is {\\sl taut}; that is,\nclosed smooth transversals to $\\mathcal F$ pass through every point of $M$. We\nshow that if $\\mathcal F$ is not the product foliation $S^1\\times S^2$, then\n$\\mathcal F$ can be $C^0$ approximated by weakly symplectically fillable,\nuniversally tight, contact structures. This extends work of Eliashberg-Thurston\non approximations of taut, transversely oriented $C^2$ foliations to the class\nof foliations that often arise in branched surface constructions of foliations.\nThis allows applications of contact topology and Floer theory beyond the\ncategory of $C^2$ foliated spaces.\n",
        "  Disconnection of mobile clients from server, in an unclear time and for an\nunknown duration, due to mobility of mobile clients, is the most important\nchallenges for concurrency control in mobile database with client-server model.\nApplying pessimistic common classic methods of concurrency control (like 2pl)\nin mobile database leads to long duration blocking and increasing waiting time\nof transactions. Because of high rate of aborting transactions, optimistic\nmethods aren`t appropriate in mobile database. In this article, OPCOT\nconcurrency control algorithm is introduced based on optimistic concurrency\ncontrol method. Reducing communications between mobile client and server,\ndecreasing blocking rate and deadlock of transactions, and increasing\nconcurrency degree are the most important motivation of using optimistic method\nas the basis method of OPCOT algorithm. To reduce abortion rate of\ntransactions, in execution time of transactions` operators a timestamp is\nassigned to them. In other to checking commitment ordering property of\nscheduler, the assigned timestamp is used in server on time of commitment. In\nthis article, serializability of OPCOT algorithm scheduler has been proved by\nusing serializability graph. Results of evaluating simulation show that OPCOT\nalgorithm decreases abortion rate and waiting time of transactions in compare\nto 2pl and optimistic algorithms.\n",
        "  In concurrent chemoradiotherapy, chemotherapeutic agents are administered\nduring the course of radiotherapy to enhance the primary tumor control.\nHowever, that often comes at the expense of increased risk of normal-tissue\ncomplications. The additional biological damage is mainly attributed to two\nmechanisms of action, which are the independent cytotoxic activity of\nchemotherapeutic agents and their interactive cooperation with radiation. The\ngoal of this study is to develop a mathematical framework to obtain drug and\nradiation administration schedules that maximize the therapeutic gain for\nconcurrent chemoradiotherapy. In particular, we analyze the impact of\nincorporating these two mechanisms into the radiation fractionation problem.\nConsidering each mechanism individually, we first derive closed-form\nexpressions for the optimal radiation fractionation regimen and the\ncorresponding drug administration schedule. We next study the case in which\nboth mechanisms are simultaneously present and develop a dynamic programming\nframework to determine optimal treatment regimens. Results show that those\nchemotherapeutic agents that interact with radiation may change optimal\nradiation fractionation regimens. Moreover, administration of chemotherapeutic\nagents possessing both mechanisms may give rise to optimal non-stationary\nradiation fractionation schemes.\n",
        "  In this paper, we analyze some basic features of SPARQL queries coming from\nour practical world in a statistical way. These features include three\nstatistic features such as the occurrence frequency of triple patterns,\nfragments, well-designed patterns and four semantic features such as\nmonotonicity, non-monotonicity, weak monotonicity (old solutions are still\nserved as parts of new solutions when some new triples are added) and\nsatisfiability. All these features contribute to characterize SPARQL queries in\ndifferent dimensions. We hope that this statistical analysis would provide some\nuseful observation for researchers and engineers who are interested in what\npractical SPARQL queries look like, so that they could develop some practical\nheuristics for processing SPARQL queries and build SPARQL query processing\nengines and benchmarks. Besides, they can narrow the scope of their problems by\navoiding those cases that do possibly not happen in our practical world.\n",
        "  We present a novel method for jointly learning compositional and\nnon-compositional phrase embeddings by adaptively weighting both types of\nembeddings using a compositionality scoring function. The scoring function is\nused to quantify the level of compositionality of each phrase, and the\nparameters of the function are jointly optimized with the objective for\nlearning phrase embeddings. In experiments, we apply the adaptive joint\nlearning method to the task of learning embeddings of transitive verb phrases,\nand show that the compositionality scores have strong correlation with human\nratings for verb-object compositionality, substantially outperforming the\nprevious state of the art. Moreover, our embeddings improve upon the previous\nbest model on a transitive verb disambiguation task. We also show that a simple\nensemble technique further improves the results for both tasks.\n",
        "  In this paper we present a methodology to address the problem of brain tissue\ndeformation referred to as 'brain-shift'. This deformation occurs throughout a\nneurosurgery intervention and strongly alters the accuracy of the\nneuronavigation systems used to date in clinical routine which rely solely on\npre-operative patient imaging to locate the surgical target, such as a tumour\nor a functional area. After a general description of the framework of our\nintra-operative image-guided system, we describe a procedure to generate\npatient specific finite element meshes of the brain and propose a biomechanical\nmodel which can take into account tissue deformations and surgical procedures\nthat modify the brain structure, like tumour or tissue resection.\n",
        "  We study the probability distribution function (PDF) of mass surface\ndensities, $\\Sigma$, of infrared dark cloud (IRDC) G028.37+00.07 and its\nsurrounding giant molecular cloud. This PDF constrains the physical processes,\nsuch as turbulence, magnetic fields and self-gravity, that are expected to be\ncontrolling cloud structure and star formation activity. The chosen IRDC is of\nparticular interest since it has almost 100,000 solar masses within a radius of\n8 parsecs, making it one of the most massive, dense molecular structures known\nand is thus a potential site for the formation of a \"super star cluster.\" We\nstudy $\\Sigma$ in two ways. First, we use a combination of NIR and MIR\nextinction maps that are able to probe the bulk of the cloud structure up to\n$\\Sigma\\sim1\\:{\\rm{g\\:cm}^{-2}}\\:$($A_V\\simeq200$ mag). Second, we study the\nFIR and sub-mm dust continuum emission from the cloud utilizing Herschel PACS\nand SPIRE images and paying careful attention to the effects of foreground and\nbackground contamination. We find that the PDFs from both methods, applied over\na $\\sim20^\\prime$(30 pc)-sized region that contains\n$\\simeq1.5\\times10^5\\:M_\\odot$ and encloses a minimum closed contour with\n$\\Sigma\\simeq0.013\\:{\\rm{g\\:cm}^{-2}}\\:$($A_V\\simeq3$ mag), shows a log-normal\nshape with the peak measured at\n$\\Sigma\\simeq0.021\\:{\\rm{g\\:cm}^{-2}}\\:$($A_V\\simeq4.7$ mag). There is\ntentative evidence for the presence of a high-$\\Sigma$ power law tail that\ncontains from $\\sim3\\%$ to 8\\% of the mass of the cloud material. We discuss\nthe implications of these results for the physical processes occurring in this\ncloud.\n",
        "  For the assessment of radiation effects of clinical ion-beams, dosimetry has\nto be complemented by information on particle-energy distribution or related\nquantities. Fluorescence nuclear track detectors made from C,Mg-doped alumina\nsingle crystals allow for the quantification of ion track density and energy\nloss on a single-track basis. In this study, their feasibility and accuracy to\nquantify fluence, linear-energy-transfer (LET) distributions, and eventually\ndose for a spread-out carbon ion Bragg peak was investigated. We found that the\nprimary ions track densities agreed well with the reference data, but the\ndetermination of the individual detector sensitivity represented a major source\nof uncertainty in LET (and dose) assessment. While low-LET fragments in the\nbeam are not contributing to this dose significantly, their number of was\nlargely underestimated by approximately a factor three. The effect was most\npronounced for protons where the measured fluence deviates at least an order of\nmagnitude. We conclude that this is mainly caused by the wide angular\ndistribution of protons in a carbon beam. The use of a dedicated FNTD reader\ndevice and semi-automated workflow improved outcome due to the considerably\nlarger amount of data available as compared to a state-of-the-art multi-purpose\nconfocal laser scanning microscope.\n",
        "  For a recently derived pairwise model of network epidemics with non-Markovian\nrecovery, we prove that under some mild technical conditions on the\ndistribution of the infectious periods, smaller variance in the recovery time\nleads to higher reproduction number, and consequently to a larger epidemic\noutbreak, when the mean infectious period is fixed. We discuss how this result\nis related to various stochastic orderings of the distributions of infectious\nperiods. The results are illustrated by a number of explicit stochastic\nsimulations, suggesting that their validity goes beyond regular networks.\n",
        "  Within crystallization theory, two interesting PL invariants for\n$d$-manifolds have been introduced and studied, namely {\\it gem-complexity} and\n{\\it regular genus}. In the present paper we prove that, for any closed\nconnected PL $4$-manifold $M$, its gem-complexity $\\mathit{k}(M)$ and its\nregular genus $ \\mathcal G(M)$ satisfy: $$\\mathit{k}(M) \\ \\geq \\ 3 \\chi (M) +\n10m -6 \\ \\ \\ \\text{and} \\ \\ \\ \\mathcal G(M) \\ \\geq \\ 2 \\chi (M) + 5m -4,$$\nwhere $rk(\\pi_1(M))=m.$ These lower bounds enable to strictly improve\npreviously known estimations for regular genus and gem-complexity of product\n4-manifolds. Moreover, the class of {\\it semi-simple crystallizations} is\nintroduced, so that the represented PL 4-manifolds attain the above lower\nbounds. The additivity of both gem-complexity and regular genus with respect to\nconnected sum is also proved for such a class of PL 4-manifolds, which\ncomprehends all ones of \"standard type\", involved in existing crystallization\ncatalogues, and their connected sums.\n",
        "  To learn a semantic parser from denotations, a learning algorithm must search\nover a combinatorially large space of logical forms for ones consistent with\nthe annotated denotations. We propose a new online learning algorithm that\nsearches faster as training progresses. The two key ideas are using macro\ngrammars to cache the abstract patterns of useful logical forms found thus far,\nand holistic triggering to efficiently retrieve the most relevant patterns\nbased on sentence similarity. On the WikiTableQuestions dataset, we first\nexpand the search space of an existing model to improve the state-of-the-art\naccuracy from 38.7% to 42.7%, and then use macro grammars and holistic\ntriggering to achieve an 11x speedup and an accuracy of 43.7%.\n",
        "  Neutral macroevolutionary models, such as the Yule model, give rise to a\nprobability distribution on the set of discrete rooted binary trees over a\ngiven leaf set. Such models can provide a signal as to the approximate location\nof the root when only the unrooted phylogenetic tree is known, and this signal\nbecomes relatively more significant as the number of leaves grows. In this\nshort note, we show that among models that treat all taxa equally, and are\nsampling consistent (i.e. the distribution on trees is not affected by taxa yet\nto be included), all such models, except one, convey some information as to the\nlocation of the ancestral root in an unrooted tree.\n",
        "  We give a construction of quandle cocycles from group cocycles, especially,\nfor any integer p \\geq 3, quandle cocycles of the dihedral quandle R_p from\ngroup cocycles of the cyclic group Z/p. We will show that a group 3-cocycle of\nZ/p gives rise to a non-trivial quandle 3-cocycle of R_p. When p is an odd\nprime, since dim_{F_p} H_Q^3(R_p; F_p) = 1, our 3-cocycle is a constant\nmultiple of the Mochizuki 3-cocycle up to coboundary. Dually, we construct a\ngroup cycle represented by a cyclic branched covering branched along a knot K\nfrom the quandle cycle associated with a colored diagram of K.\n",
        "  We present an experimental study of large-scale RDF federations on top of the\nBio2RDF data sources, involving 29 data sets with more than four billion RDF\ntriples deployed in a local federation. Our federation is driven by FedX, a\nhighly optimized federation mediator for Linked Data. We discuss design\ndecisions, technical aspects, and experiences made in setting up and optimizing\nthe Bio2RDF federation, and present an exhaustive experimental evaluation of\nthe federation scenario. In addition to a controlled setting with local\nfederation members, we study implications arising in a hybrid setting, where\nlocal federation members interact with remote federation members exhibiting\nhigher network latency. The outcome demonstrates the feasibility of federated\nsemantic data management in general and indicates remaining bottlenecks and\nresearch opportunities that shall serve as a guideline for future work in the\narea of federated semantic data processing.\n",
        "  We obtain explicit, isometry-invariant integral formulas for twisting,\nwrithing and helicity, and prove the theorem LINK = TWIST + WRITHE on the\n3-sphere and in hyperbolic 3-space. We then use these results to derive upper\nbounds for the helicity of vector fields and lower bounds for the first\neigenvalue of the curl operator on subdomains of these two spaces.\n",
        "  Simple MCBR models of chemical evolution are extended to the limit of\ndominant gas inflow or outflow with respect to gas locked up into long-lived\nstars and remnants. For an assigned empirical differential oxygen abundance\ndistribution, which can be linearly fitted, a family of theoretical curves is\nbuilt up with assigned prescriptions. For curves with increasing cut parameter,\nthe gas mass fraction locked up into long-lived stars and remnants is found to\nattain a maximum and then decrease towards zero as the flow tends to infinity,\nwhile the remaining parameters show a monotonic trend. The theoretical integral\noxygen abundance distribution is also expressed. An application is performed to\nthe empirical distribution deduced from two different samples of disk stars,\nfor both the thin and the thick disk. The constraints on formation and\nevolution are discussed in the light of the model. The evolution is tentatively\nsubdivided into four stages, A, F, C, E. The empirical distribution related to\nany stage is fitted by all curves for a wide range of the cut parameter. The F\nstage may safely be described by a steady inflow regime, implying a flat\ntheoretical distribution, in agreement with the results of hydrodynamical\nsimulations. Finally, (1) the change of fractional mass due to the extension of\nthe linear fit to the empirical distribution, towards both the (undetected)\nlow-metallicity and high-metallicity tail, is evaluated and (2) the idea of a\nthick disk-thin disk collapse is discussed, in the light of the model.\n",
        "  Based on a minimal two-orbital model [Tai {\\it et al.}, Europhys. Lett.\n\\textbf{103}, 67001 (2013)], which captures the canonical electron-hole-doping\nphase diagram of the iron-pnictide BaFe$_{2}$As$_{2}$, we study the evolution\nof quasiparticle states as a function of doping using the Bogoliubov-de Gennes\nequations with and without a single impurity. Analyzing the density of states\nof uniformly doped samples, we are able to identify the origin of the two\nsuperconducting gaps observed in optimally hole- or electron-doped systems. The\nlocal density of states (LDOS) is then examined near a single impurity in\nsamples without antiferromagnetic order. The qualitative features of our\nresults near the single impurity are consistent with a work based on a\nfive-orbital model[K. Toshikaze {\\it et al.}, J. Phys. Soc. Jpn. \\textbf{79},\n083704 (2010)]. This further supports the validity of our two-orbital model in\ndealing with LDOS in the single-impurity problem. Finally, we investigate the\nevolution of the LDOS with doping near a single impurity in the unitary or\nstrong scattering limit, such as Zn replacing Fe. The positions of the ingap\nresonance peaks exhibited in our LDOS may indirectly reflect the evolution of\nthe Fermi surface topology according to the phase diagram. Our prediction of\ningap states and the evolution of the LDOS near a strong scattering single\nimpurity can be validated by experiments probing the local quasiparticle\nspectrum.\n",
        "  We give a formula of the colored Alexander invariant in terms of the\nhomological representation of the braid groups which we call truncated\nLawrence's representation. This formula generalizes the famous Burau\nrepresentation formula of the Alexander polynomial.\n",
        "  Driving a two-dimensional superconductor normal by applying a high magnetic\nfield may lead to Cooper pair localization. In this case, there should be a\nquantum critical point associated with specific scaling laws. Such a transition\nhas been evidenced in a number of low critical temperature superconducting thin\nfilms and has been suggested to occur also in high temperature cuprate\nsuperconductors. Here we show experimental evidence for two distinct quantum\ncritical regimes when applying perpendicular magnetic fields to underdoped\nLa2-xSrxCuO4 thin films. At intermediate values of the magnetic field\n(18T-20T), a \"ghost\" QCP is observed, for which the values of the related\ncritical exponents point towards a fermionic -as opposed to bosonic- scenario.\nAt higher (about 37 T) magnetic field, another QCP is observed, which suggests\nthe existence of either a 2D/3D or a clean/dirty temperature crossover.\n",
        "  Matching dependencies (MDs) were introduced to specify the identification or\nmatching of certain attribute values in pairs of database tuples when some\nsimilarity conditions are satisfied. Their enforcement can be seen as a natural\ngeneralization of entity resolution. In what we call the \"pure case\" of MDs,\nany value from the underlying data domain can be used for the value in common\nthat does the matching. We investigate the semantics and properties of data\ncleaning through the enforcement of matching dependencies for the pure case. We\ncharacterize the intended clean instances and also the \"clean answers\" to\nqueries as those that are invariant under the cleaning process. The complexity\nof computing clean instances and clean answers to queries is investigated.\nTractable and intractable cases depending on the MDs and queries are\nidentified. Finally, we establish connections with database \"repairs\" under\nintegrity constraints.\n",
        "  We define \"fat\" train tracks and use them to give a combinatorial criterion\nfor the Hempel distance of Heegaard splittings for closed orientable\n3-manifolds. We apply this criterion to 3-manifolds obtained from surgery on\nknots in the three sphere.\n",
        "  High critical density molecular lines like HCN(1-0) or HCO+(1-0) represent\nour best tool to study currently star-forming, dense molecular gas at\nextragalactic distances. The optical depth of these lines is a key ingredient\nto estimate the effective density required to excite emission. However,\nconstraints on this quantity are even scarcer in the literature than\nmeasurements of the high density tracers themselves. Here, we combine new\nobservations of HCN, HCO+ and HNC(1-0) and their optically thin isotopologues\nH13CN, H13CO+ and HN13C(1-0) to measure isotopologue line ratios. We use IRAM\n30-m observations from the large program EMPIRE and new ALMA observations,\nwhich together target 6 nearby star-forming galaxies. Using spectral stacking\ntechniques, we calculate or place strong upper limits on the HCN/H13CN,\nHCO+/H13CO+ and HNC/HN13C line ratios in the inner parts of these galaxies.\nUnder simple assumptions, we use these to estimate the optical depths of\nHCN(1-0) and HCO+(1-0) to be \\tau ~2-11 in the active, inner regions of our\ntargets. The critical densities are consequently lowered to values between\n5-20$\\times 10^5$, 1-3$\\times 10^5$ and 9$\\times 10^4$ cm-3 for HCN, HCO+ and\nHNC, respectively. We study the impact of having different beam-filling\nfactors, $\\eta$, on these estimates and find that the effective critical\ndensities decrease by a factor of $\\frac{\\eta_{12}}{\\eta_{13}}\\,\\tau_{12}$. A\ncomparison to existing work in NGC 5194 and NGC 253 shows HCN/H13CN and\nHCO+/H13CO+ ratios in agreement with our measurements within the uncertainties.\nThe same is true for studies in other environments such as the Galactic Centre\nor nuclear regions of AGN-dominated nearby galaxies.\n",
        "  This paper describes a hypernym discovery system for our participation in the\nSemEval-2018 Task 9, which aims to discover the best (set of) candidate\nhypernyms for input concepts or entities, given the search space of a\npre-defined vocabulary. We introduce a neural network architecture for the\nconcerned task and empirically study various neural network models to build the\nrepresentations in latent space for words and phrases. The evaluated models\ninclude convolutional neural network, long-short term memory network, gated\nrecurrent unit and recurrent convolutional neural network. We also explore\ndifferent embedding methods, including word embedding and sense embedding for\nbetter performance.\n",
        "  The paper describes the results of the first shared task on word sense\ninduction (WSI) for the Russian language. While similar shared tasks were\nconducted in the past for some Romance and Germanic languages, we explore the\nperformance of sense induction and disambiguation methods for a Slavic language\nthat shares many features with other Slavic languages, such as rich morphology\nand virtually free word order. The participants were asked to group contexts of\na given word in accordance with its senses that were not provided beforehand.\nFor instance, given a word \"bank\" and a set of contexts for this word, e.g.\n\"bank is a financial institution that accepts deposits\" and \"river bank is a\nslope beside a body of water\", a participant was asked to cluster such contexts\nin the unknown in advance number of clusters corresponding to, in this case,\nthe \"company\" and the \"area\" senses of the word \"bank\". For the purpose of this\nevaluation campaign, we developed three new evaluation datasets based on sense\ninventories that have different sense granularity. The contexts in these\ndatasets were sampled from texts of Wikipedia, the academic corpus of Russian,\nand an explanatory dictionary of Russian. Overall, 18 teams participated in the\ncompetition submitting 383 models. Multiple teams managed to substantially\noutperform competitive state-of-the-art baselines from the previous years based\non sense embeddings.\n",
        "  The automatic ranking of word pairs as per their semantic relatedness and\nability to mimic human notions of semantic relatedness has widespread\napplications. Measures that rely on raw data (distributional measures) and\nthose that use knowledge-rich ontologies both exist. Although extensive studies\nhave been performed to compare ontological measures with human judgment, the\ndistributional measures have primarily been evaluated by indirect means. This\npaper is a detailed study of some of the major distributional measures; it\nlists their respective merits and limitations. New measures that overcome these\ndrawbacks, that are more in line with the human notions of semantic\nrelatedness, are suggested. The paper concludes with an exhaustive comparison\nof the distributional and ontology-based measures. Along the way, significant\nresearch problems are identified. Work on these problems may lead to a better\nunderstanding of how semantic relatedness is to be measured.\n",
        "  We present the OC16-CE80 Chinese-English mixlingual speech database which was\nreleased as a main resource for training, development and test for the\nChinese-English mixlingual speech recognition (MixASR-CHEN) challenge on\nO-COCOSDA 2016. This database consists of 80 hours of speech signals recorded\nfrom more than 1,400 speakers, where the utterances are in Chinese but each\ninvolves one or several English words. Based on the database and another two\nfree data resources (THCHS30 and the CMU dictionary), a speech recognition\n(ASR) baseline was constructed with the deep neural network-hidden Markov model\n(DNN-HMM) hybrid system. We then report the baseline results following the\nMixASR-CHEN evaluation rules and demonstrate that OC16-CE80 is a reasonable\ndata resource for mixlingual research.\n",
        "  We introduce an evolutionary metacommunity of multitrophic food webs on\nseveral habitats coupled by migration. In contrast to previous studies that\nfocus either on evolutionary or on spatial aspects, we include both and\ninvestigate the interplay between them. Locally, the species emerge, interact\nand go extinct according to the rules of the well-known evolutionary food web\nmodel proposed by Loeuille and Loreau in 2005. Additionally, species are able\nto migrate between the habitats. With random migration, we are able to\nreproduce common trends in diversity-dispersal relationships: Regional\ndiversity decreases with increasing migration rates, whereas local diversity\ncan increase in case of a low level of dispersal. Moreover, we find that the\ntotal biomasses in the different patches become similar even when species\ncomposition remains different. With adaptive migration, we observe species\ncompositions that differ considerably between patches and contain species that\nare descendant from ancestors on both patches. This result indicates that the\ncombination of spatial aspects and evolutionary processes affects the structure\nof food webs in different ways than each of them alone.\n",
        "  Recently, attempts have been made to remove Gaussian mixture models (GMM)\nfrom the training process of deep neural network-based hidden Markov models\n(HMM/DNN). For the GMM-free training of a HMM/DNN hybrid we have to solve two\nproblems, namely the initial alignment of the frame-level state labels and the\ncreation of context-dependent states. Although flat-start training via\niteratively realigning and retraining the DNN using a frame-level error\nfunction is viable, it is quite cumbersome. Here, we propose to use a\nsequence-discriminative training criterion for flat start. While\nsequence-discriminative training is routinely applied only in the final phase\nof model training, we show that with proper caution it is also suitable for\ngetting an alignment of context-independent DNN models. For the construction of\ntied states we apply a recently proposed KL-divergence-based state clustering\nmethod, hence our whole training process is GMM-free. In the experimental\nevaluation we found that the sequence-discriminative flat start training method\nis not only significantly faster than the straightforward approach of iterative\nretraining and realignment, but the word error rates attained are slightly\nbetter as well.\n",
        "  Semantic parsing has emerged as a significant and powerful paradigm for\nnatural language interface and question answering systems. Traditional methods\nof building a semantic parser rely on high-quality lexicons, hand-crafted\ngrammars and linguistic features which are limited by applied domain or\nrepresentation. In this paper, we propose a general approach to learn from\ndenotations based on Seq2Seq model augmented with attention mechanism. We\nencode input sequence into vectors and use dynamic programming to infer\ncandidate logical forms. We utilize the fact that similar utterances should\nhave similar logical forms to help reduce the searching space. Under our\nlearning policy, the Seq2Seq model can learn mappings gradually with noises.\nCurriculum learning is adopted to make the learning smoother. We test our\nmethod on the arithmetic domain which shows our model can successfully infer\nthe correct logical forms and learn the word meanings, compositionality and\noperation orders simultaneously.\n",
        "  We present a comprehensive study of the relations between gas kinematics,\nmetallicity, and stellar mass in a sample of 82 GRB-selected galaxies using\nabsorption and emission methods. We find the velocity widths of both emission\nand absorption profiles to be a proxy of stellar mass. We also investigate the\nvelocity-metallicity correlation and its evolution with redshift and find the\ncorrelation derived from emission lines to have a significantly smaller scatter\ncompared to that found using absorption lines. Using 33 GRB hosts with measured\nstellar mass and metallicitiy, we study the mass-metallicity relation for GRB\nhost galaxies in a stellar mass range of $10^{8.2} M_{\\odot}$ to $10^{11.1}\nM_{\\odot}$ and a redshift range of $ z\\sim 0.3-3.4$. The GRB-selected galaxies\nappear to track the mass-metallicity relation of star forming galaxies but with\nan offset of 0.15 towards lower metallicities. This offset is comparable with\nthe average error-bar on the metallicity measurements of the GRB sample and\nalso the scatter on the MZ relation of the general population. It is hard to\ndecide whether this relatively small offset is due to systematic effects or the\nintrinsic nature of GRB hosts. We also investigate the possibility of using\nabsorption-line metallicity measurements of GRB hosts to study the\nmass-metallicity relation at high redshifts. Our analysis shows that the\nmetallicity measurements from absorption methods can significantly differ from\nemission metallicities and assuming identical measurements from the two methods\nmay result in erroneous conclusions.\n",
        "  In this note we explore a connection between finite covers of surfaces and\nthe Teichm\\\"uller polynomial of a fibered face of a hyperbolic 3--manifold. We\nconsider the action of a homological pseudo-Anosov homeomorphism $\\psi$ on the\nhomology groups of a class of finite abelian covers of a surface\n$\\Sigma_{g,n}$. Eigenspaces of the deck group actions on these covers are\nnaturally parametrized by rational points on a torus. We show that away from\nthe trivial eigenspace, the spectrum of the action of $\\psi$ on these\neigenspaces is bounded away from the dilatation of $\\psi$. We show that the\naction $\\psi$ on these eigenspaces is governed by the Teichm\\\"uller polynomial.\n",
        "  Dependency treebank is an important resource in any language. In this paper,\nwe present our work on building BKTreebank, a dependency treebank for\nVietnamese. Important points on designing POS tagset, dependency relations, and\nannotation guidelines are discussed. We describe experiments on POS tagging and\ndependency parsing on the treebank. Experimental results show that the treebank\nis a useful resource for Vietnamese language processing.\n",
        "  We study vaccine control for disease spread on an adaptive network modeling\ndisease avoidance behavior. Control is implemented by adding Poisson\ndistributed vaccination of susceptibles. We show that vaccine control is much\nmore effective in adaptive networks than in static networks due to an\ninteraction between the adaptive network rewiring and the vaccine application.\nDisease extinction rates using vaccination are computed, and orders of\nmagnitude less vaccine application is needed to drive the disease to extinction\nin an adaptive network than in a static one.\n",
        "  We calculate the spin susceptibility of a superconductor without inversion\nsymmetry, both in the clean and disordered cases. The susceptibility has a\nlarge residual value at zero temperature, which is further enhanced in the\npresence of scalar impurities.\n",
        "  We introduce word vectors for the construction domain. Our vectors were\nobtained by running word2vec on an 11M-word corpus that we created from scratch\nby leveraging freely-accessible online sources of construction-related text. We\nfirst explore the embedding space and show that our vectors capture meaningful\nconstruction-specific concepts. We then evaluate the performance of our vectors\nagainst that of ones trained on a 100B-word corpus (Google News) within the\nframework of an injury report classification task. Without any parameter\ntuning, our embeddings give competitive results, and outperform the Google News\nvectors in many cases. Using a keyword-based compression of the reports also\nleads to a significant speed-up with only a limited loss in performance. We\nrelease our corpus and the data set we created for the classification task as\npublicly available, in the hope that they will be used by future studies for\nbenchmarking and building on our work.\n",
        "  The center of our galaxy is home to a massive black hole, SgrA*, and a\nnuclear star cluster containing stellar populations of various ages. While the\nlate type stars may be too old to have retained memory of their initial orbital\nconfiguration, and hence formation mechanism, the kinematics of the early type\nstars should reflect their original distribution. In this contribution we\npresent a new statistic which uses directly-observable kinematical stellar data\nto infer orbital parameters for stellar populations, and is capable of\ndistinguishing between different origin scenarios. We use it on a population of\nB-stars in the Galactic center that extends out to large radii (0.5 pc) from\nthe massive black hole. We find that the high K-magnitude population form an\neccentric distribution, suggestive of a Hills binary-disruption origin.\n",
        "  In this paper we introduce Latent Tree Language Model (LTLM), a novel\napproach to language modeling that encodes syntax and semantics of a given\nsentence as a tree of word roles.\n  The learning phase iteratively updates the trees by moving nodes according to\nGibbs sampling. We introduce two algorithms to infer a tree for a given\nsentence. The first one is based on Gibbs sampling. It is fast, but does not\nguarantee to find the most probable tree. The second one is based on dynamic\nprogramming. It is slower, but guarantees to find the most probable tree. We\nprovide comparison of both algorithms.\n  We combine LTLM with 4-gram Modified Kneser-Ney language model via linear\ninterpolation. Our experiments with English and Czech corpora show significant\nperplexity reductions (up to 46% for English and 49% for Czech) compared with\nstandalone 4-gram Modified Kneser-Ney language model.\n",
        "  We explain that Stegosaurus exhibited exterior chirality and observe that the\nlargest plate in particular of USNM 4394, USNM 4714, DMNS 2818 and NHMUK R36730\nappears to have tilted to the right rather than to the left in each case.\nSeveral instances in which Stegosaurus specimens have been confused with their\ndistinct, hypothetical mirror-image forms are highlighted. We believe our\nfindings to be consistent with the hypothesis that Stegosaurus's plates acted\nprimarily as display structures. A collection of more than one Stegosaurus\nmight be referred to henceforth as a 'handful' of Stegosaurus.\n",
        "  Index tuning, i.e., selecting the indexes appropriate for a workload, is a\ncrucial problem in database system tuning. In this paper, we solve index tuning\nfor large problem instances that are common in practice, e.g., thousands of\nqueries in the workload, thousands of candidate indexes and several hard and\nsoft constraints. Our work is the first to reveal that the index tuning problem\nhas a well structured space of solutions, and this space can be explored\nefficiently with well known techniques from linear optimization. Experimental\nresults demonstrate that our approach outperforms state-of-the-art commercial\nand research techniques by a significant margin (up to an order of magnitude).\n",
        "  We investigate the evolution of the electrical resistivity of BaFe2As2 single\ncrystals with pressure. The samples used were from the same batch grown from\nself flux and showed properties that were highly reproducible. Samples were\npressurised using three different pressure media: pentane-isopentane (in a\npiston cylinder cell), Daphne oil (in an alumina anvil cell) and steatite (in a\nBridgman cell). Each pressure medium has its own intrinsic level of\nhydrostaticity, which dramatically affects the phase diagram. An increasing\nuniaxial pressure component in this system quickly reduces spin density wave\norder and favours the appearance of superconductivity, similar to what is seen\nin SrFe2As2.\n",
        "  We give a survey on Meyer functions, with emphasis on their application to\nthe signatures of fibered 4-manifolds.\n",
        "  Applying natural language processing for mining and intelligent information\naccess to tweets (a form of microblog) is a challenging, emerging research\narea. Unlike carefully authored news text and other longer content, tweets pose\na number of new challenges, due to their short, noisy, context-dependent, and\ndynamic nature. Information extraction from tweets is typically performed in a\npipeline, comprising consecutive stages of language identification,\ntokenisation, part-of-speech tagging, named entity recognition and entity\ndisambiguation (e.g. with respect to DBpedia). In this work, we describe a new\nTwitter entity disambiguation dataset, and conduct an empirical analysis of\nnamed entity recognition and disambiguation, investigating how robust a number\nof state-of-the-art systems are on such noisy texts, what the main sources of\nerror are, and which problems should be further investigated to improve the\nstate of the art.\n",
        "  Semi-analytical models (SAMs) are currently the best way to understand the\nformation of galaxies within the cosmic dark-matter structures. While they\nfairly well reproduce the local stellar mass functions, correlation functions\nand luminosity functions, they fail to match observations at high redshift (z >\n3) in most cases, particularly in the low-mass range. The inconsistency between\nmodels and observations indicates that the history of gas accretion in\ngalaxies, within their host dark-matter halo, and the transformation of gas\ninto stars, are not well followed. Hereafter, we briefly present a new version\nof the GalICS semi-analytical model. We explore the impacts of classical\nmechanisms, such as supernova feedback or photoionization, on the evolution of\nthe stellar mass assembly. Even with a strong efficiency, these two processes\ncannot explain the observed stellar mass function and star formation rate\ndistribution and some other relations. We thus introduce an ad-hoc modification\nof the standard paradigm, based on the presence of a \\textit{no-star-forming}\ngas component, and a concentration of the star-forming gas in galaxy discs. The\nmain idea behind the existence of the no-star-forming gas reservoir is that\nonly a fraction of the total gas mass in a galaxy is available to form stars.\nThe reservoir generates a delay between the accretion of the gas and the star\nformation process. This new model is in much better agreement with the\nobservations of the stellar mass function in the low-mass range than the\nprevious models, and agrees quite well with a large set of observations,\nincluding the redshift evolution of the specific star formation rate. However,\nit predicts a large fraction of no-star-forming baryonic gas, potentially\nlarger than observed, even if its nature has still to be examined in the\ncontext of the missing baryon problem.\n",
        "  \"Green Bean\" Galaxies (GBs) are the most [O III]-luminous type-2 active\ngalactic nuclei (AGN) at z~0.3. However, their infrared luminosities reveal AGN\nin very low activity states, indicating that their gas reservoirs must be\nionized by photons from a recent high activity episode - we are observing\nquasar ionization echoes. We use integral field spectroscopy from the Gemini\nMulti-Object Spectrograph to analyse the 3D kinematics, ionization state,\ntemperature and density of ionized gas in the GB SDSS J224024.1-092748. We\nmodel the emission line spectrum of each spaxel as a superposition of up to\nthree Gaussian components and analyse the physical properties of each component\nindividually. Two narrow components, tracing the velocity fields of the disc\nand an ionized gas cloud, are superimposed over the majority of the galaxy.\nFast shocks produce hot ($T_e$ $\\geq$ 20,000 K), dense ($n_e$ $\\geq$ 100\ncm$^{-3}$), turbulent ($\\sigma$ $\\geq$ 600 km s$^{-1}$), [O III]-bright regions\nwith enhanced [N II]/H$\\alpha$ and [S II]/H$\\alpha$ ratios. The most prominent\nsuch spot is consistent with a radio jet shock-heating the interstellar medium.\nHowever, the AGN is still responsible for $\\geq$ 82 per cent of the galaxy's\ntotal [O III] luminosity, strengthening the case for previous quasar activity.\nThe ionized gas cloud has a strong kinematic link to the central AGN and is\nco-rotating with the main body of the galaxy, suggesting that it may be the\nremnant of a quasar-driven outflow. Our analysis of J224024.1-092748 indicates\nthat GBs provide a unique fossil record of the transformation from the most\nluminous quasars to weak AGN.\n",
        "  The role of large-scale stellar feedback in the formation of molecular clouds\nhas been investigated observationally by examining the relationship between HI\nand 12CO(J=1-0) in supershells. Detailed parsec-resolution case studies of two\nMilky Way supershells demonstrate an enhanced level of molecularisation over\nboth objects, and hence provide the first quantitative observational evidence\nof increased molecular cloud production in volumes of space affected by\nsupershell activity. Recent results on supergiant shells in the LMC suggest\nthat while they do indeed help to organise the ISM into over-dense structures,\ntheir global contribution to molecular cloud formation is of the order of only\n~10%.\n",
        "  Most existing Neural Machine Translation models use groups of characters or\nwhole words as their unit of input and output. We propose a model with a\nhierarchical char2word encoder, that takes individual characters both as input\nand output. We first argue that this hierarchical representation of the\ncharacter encoder reduces computational complexity, and show that it improves\ntranslation performance. Secondly, by qualitatively studying attention plots\nfrom the decoder we find that the model learns to compress common words into a\nsingle embedding whereas rare words, such as names and places, are represented\ncharacter by character.\n",
        "  Computers still have a long way to go before they can interact with users in\na truly natural fashion. From a users perspective, the most natural way to\ninteract with a computer would be through a speech and gesture interface.\nAlthough speech recognition has made significant advances in the past ten\nyears, gesture recognition has been lagging behind. Sign Languages (SL) are the\nmost accomplished forms of gestural communication. Therefore, their automatic\nanalysis is a real challenge, which is interestingly implied to their lexical\nand syntactic organization levels. Statements dealing with sign language occupy\na significant interest in the Automatic Natural Language Processing (ANLP)\ndomain. In this work, we are dealing with sign language recognition, in\nparticular of French Sign Language (FSL). FSL has its own specificities, such\nas the simultaneity of several parameters, the important role of the facial\nexpression or movement and the use of space for the proper utterance\norganization. Unlike speech recognition, Frensh sign language (FSL) events\noccur both sequentially and simultaneously. Thus, the computational processing\nof FSL is too complex than the spoken languages. We present a novel approach\nbased on HMM to reduce the recognition complexity.\n",
        "  This paper describes a unique approach to perform application behavioral\nanalysis for identifying how tables might be related to each other. The\nanalysis techniques are based on the properties of primary and foreign keys and\nalso the data present in their respective columns. We have also implemented the\nidea using JAVA and presented experimental results in Demo Section.This paper\nintroduces a unique approach to predict the possible application level\nrelationships in databases with the help of the application relationship\nanalysis of simple and complex SQL queries. Complex SQL queries are those which\ncontain multiple constraints at different levels of the database. In the\nprocess of deriving relations, we first parse the SQL statements and then\nanalyse the parsed information to extract related information.\n",
        "  X-ray fluorescence computed tomography based on sheet-beam can save a huge\namount of time to obtain a whole set of projections using synchrotron. However,\nit is clearly unpractical for most biomedical research laboratories. In this\npaper, polychromatic X-ray fluorescence computed tomography with sheet-beam\ngeometry is tested by Monte Carlo simulation. First, two phantoms (A and B)\nfilled with PMMA are used to simulate imaging process through GEANT 4. Phantom\nA contains several GNP-loaded regions with the same size (10 mm) in height and\ndiameter but different Au weight concentration ranging from 0.3% to 1.8%.\nPhantom B contains twelve GNP-loaded regions with the same Au weight\nconcentration (1.6%) but different diameter ranging from 1mm to 9mm. Second,\ndiscretized presentation of imaging model is established to reconstruct more\naccurate XFCT images. Third, XFCT images of phantom A and B are reconstructed\nby fliter backprojection (FBP) and maximum likelihood expectation maximization\n(MLEM) with and without correction, respectively. Contrast to noise ratio (CNR)\nis calculated to evaluate all the reconstructed images. Our results show that\nit is feasible for sheet-beam XFCT system based on polychromatic X-ray source\nand the discretized imaging model can be used to reconstruct more accurate\nimages.\n",
        "  A binary supermassive black hole loses energy via ejection of stars in a\ngalactic nucleus, until emission of gravitational waves becomes strong enough\nto induce rapid coalescence. Evolution via the gravitational slingshot requires\nthat stars be continuously supplied to the binary, and it is known that in\nspherical galaxies the reservoir of such stars is quickly depleted, leading to\nstalling of the binary at parsec-scale separations. Recent N-body simulations\nof galaxy mergers and isolated nonspherical galaxies suggest that this stalling\nmay not occur in less idealized systems. However, it remains unclear to what\ndegree these conclusions are affected by collisional relaxation, which is much\nstronger in the numerical simulations than in real galaxies. In this study, we\npresent a novel Monte Carlo method that can efficiently deal with both\ncollisional and collisionless dynamics, and with galaxy models having arbitrary\nshapes. We show that without relaxation, the final-parsec problem may be\novercome only in triaxial galaxies. Axisymmetry is not enough, but even a\nmoderate departure from axisymmetry is sufficient to keep the binary shrinking.\nWe find that the binary hardening rate is always substantially lower than the\nmaximum possible, \"full-loss-cone\" rate, and that it decreases with time, but\nthat stellar-dynamical interactions are nevertheless able to drive the binary\nto coalescence on a timescale <=1 Gyr in any triaxial galaxy.\n",
        "  We use collisionless $N$-body simulations to determine how the growth of a\nsupermassive black hole (SMBH) influences the nuclear kinematics in both barred\nand unbarred galaxies. In the presence of a bar, the increase in the velocity\ndispersion $\\sigma$ (within the effective radius) due to the growth of an SMBH\nis on average $\\lesssim 10%$, whereas the increase is only $\\lesssim 4%$ in an\nunbarred galaxy. In a barred galaxy, the increase results from a combination of\nthree separate factors (a) orientation and inclination effects; (b) angular\nmomentum transport by the bar that results in an increase in the central mass\ndensity; (c) an increase in the vertical and radial velocity anisotropy of\nstars in the vicinity of the SMBH. In contrast the growth of the SMBH in an\nunbarred galaxy causes the velocity distribution in the inner part of the\nnucleus to become less radially anisotropic. The increase in $\\sigma$ following\nthe growth of the SMBH is insensitive to a variation of a factor of 10 in the\nfinal mass of the SMBH, showing that it is the growth process rather than the\nactual SMBH mass that alters bar evolution in a way that increases $\\sigma$. We\nargue that using an axisymmetric stellar dynamical modeling code to measure\nSMBH masses in barred galaxies could result in a slight overestimate of the\nderived $M_{BH}$, especially if a constant M/L ratio is assumed. We conclude\nthat the growth of a black hole in the presence of a bar could result in an\nincrease in $\\sigma$ which is roughly of $4-8%$ larger than the increase that\noccurs in an axisymmetric system. While the increase in $\\sigma$ due to SMBH\ngrowth in a barred galaxy might partially account for the claimed offset of\nbarred galaxies and pseudo bulges from the $M_{BH}-\\sigma$ relation obtained\nfor elliptical galaxies and classical bulges in unbarred galaxies, it is\ninadequate to account for all of the offset.\n",
        "  Hidden Markov Model (HMM) is often regarded as the dynamical model of choice\nin many fields and applications. It is also at the heart of most\nstate-of-the-art speech recognition systems since the 70's. However, from\nGaussian mixture models HMMs (GMM-HMM) to deep neural network HMMs (DNN-HMM),\nthe underlying Markovian chain of state-of-the-art models did not changed much.\nThe \"left-to-right\" topology is mostly always employed because very few other\nalternatives exist. In this paper, we propose that finely-tuned HMM topologies\nare essential for precise temporal modelling and that this approach should be\ninvestigated in state-of-the-art HMM system. As such, we propose a\nproof-of-concept framework for learning efficient topologies by pruning down\ncomplex generic models. Speech recognition experiments that were conducted\nindicate that complex time dependencies can be better learned by this approach\nthan with classical \"left-to-right\" models.\n",
        "  In keyword search, when user cannot get what she wants, query refinement is\nneeded and reason can be various. We first give a thorough categorization of\nthe reason, then focus on solving one category of query refinement problem in\nthe context of XML keyword search, where what user searches for does not exist\nin the data. We refer to it as the MisMatch problem in this paper. Then we\npropose a practical way to detect the MisMatch problem and generate helpful\nsuggestions to users. Our approach can be viewed as a post-processing job of\nquery evaluation, and has three main features: (1) it adopts both the suggested\nqueries and their sample results as the output to user, helping user judge\nwhether the MisMatch problem is solved without consuming all query results; (2)\nit is portable in the sense that it can work with any LCA-based matching\nsemantics and orthogonal to the choice of result retrieval method adopted; (3)\nit is lightweight in the way that it occupies a very small proportion of the\nwhole query evaluation time. Extensive experiments on three real datasets\nverify the effectiveness, efficiency and scalability of our approach. An online\nXML keyword search engine called XClear that embeds the MisMatch problem\ndetector and suggester has been built.\n",
        "  In 1982 Louis Kauffman conjectured that if a knot in the 3-sphere is a slice\nknot then on any Seifert surface for that knot there exists a homologically\nessential simple closed curve of self-linking zero which is itself a slice\nknot, or at least has Arf invariant zero. Since that time, considerable\nevidence has been amassed in support of this conjecture. In particular, many\ninvariants that obstruct a knot from being a slice knot have been explictly\nexpressed in terms of invariants of such curves on the Seifert surface. We give\ncounterexamples to Kauffman's conjecture, that is, we exhibit (smoothly) slice\nknots that admit (unique minimal genus) Seifert surfaces on which every\nhomologically essential simple closed curve of self-linking zero has non-zero\nArf invariant and non-zero signatures.\n",
        "  We formalize a new modular variant of current question answering tasks by\nenforcing complete independence of the document encoder from the question\nencoder. This formulation addresses a key challenge in machine comprehension by\nrequiring a standalone representation of the document discourse. It\nadditionally leads to a significant scalability advantage since the encoding of\nthe answer candidate phrases in the document can be pre-computed and indexed\noffline for efficient retrieval. We experiment with baseline models for the new\ntask, which achieve a reasonable accuracy but significantly underperform\nunconstrained QA models. We invite the QA research community to engage in\nPhrase-Indexed Question Answering (PIQA, pika) for closing the gap. The\nleaderboard is at: nlp.cs.washington.edu/piqa\n",
        "  Currently, gold nanorods can be synthesized in a wide range of sizes.\nHowever, for intended biological applications gold nanorods with approximate\ndimensions 50 nm x 15 nm are used. We investigate by computer simulation the\neffect of particle dimensions on the optical and thermal properties in the\ncontext of the specific applications of photoacoustic imaging. In addition we\ndiscuss the influence of particle size in overcoming the following biophysical\nbarriers when administrated in vivo: extravasation, avoidance of uptake by\norgans of the reticuloendothelial system, penetration through the interstitium,\nbinding capability and uptake by the target cells. Although more complex\nbiological influences can be introduced in future analysis, the present work\nillustrates that larger gold nanorods, designated by us as \"nanobig rods\", may\nperform relatively better at meeting the requirements for successful in vivo\napplications compared to their smaller counterparts which are conventionally\nused.\n",
        "  Though three distinct wounding mechanisms (permanent cavity, temporary\ncavity, and ballistic pressure wave) are described in the wound ballistics\nliterature, they all have their physical origin in the retarding force between\nbullet and tissue as the bullet penetrates. If the bullet path is the same,\nlarger retarding forces produce larger wounding effects and a greater\nprobability of rapid incapacitation. By Newton's third law, the force of the\nbullet on the tissue is equal in magnitude and opposite in direction to the\nforce of the tissue on the bullet. For bullets penetrating with constant mass,\nthe retarding force on the bullet can be determined by frame by frame analysis\nof high speed video of the bullet penetrating a suitable tissue simulant such\nas calibrated 10% ballistic gelatin. Here the technique is demonstrated with\n9mm NATO bullets, 32 cm long blocks of gelatin, and a high speed video camera\noperating at 20,000 frames per second. It is found that different 9mm NATO\nbullets have a wide variety of potential for wounding and rapid incapacitation.\nThis technique also determines the energy transfer in the first 15 cm and/or\nfirst 30 cm of tissue, which are important parameters in estimating the\nprobability of rapid incapacitation in some of the ARL/BRL models. This method\npredicts that some 9mm bullets have a much higher probability of rapid\nincapacitation than others and the rank ordering of bullet effectiveness is in\nagreement with other studies.\n",
        "  The rate of recombination affects the mode of molecular evolution. In\nhigh-recombining sequence, the targets of selection are individual genetic\nloci; under low recombination, selection collectively acts on large,\ngenetically linked genomic segments. Selection under linkage can induce clonal\ninterference, a specific mode of evolution by competition of genetic clades\nwithin a population. This mode is well known in asexually evolving microbes,\nbut has not been traced systematically in an obligate sexual organism. Here we\nshow that the Drosophila genome is partitioned into two modes of evolution: a\nlocal interference regime with limited effects of genetic linkage, and an\ninterference condensate with clonal competition. We map these modes by\ndifferences in mutation frequency spectra, and we show that the transition\nbetween them occurs at a threshold recombination rate that is predictable from\ngenomic summary statistics. We find the interference condensate in segments of\nlow-recombining sequence that are located primarily in chromosomal regions\nflanking the centromeres and cover about 20% of the Drosophila genome.\nCondensate regions have characteristics of asexual evolution that impact gene\nfunction: the efficacy of selection and the speed of evolution are lower and\nthe genetic load is higher than in regions of local interference. Our results\nsuggest that multicellular eukaryotes can harbor heterogeneous modes and tempi\nof evolution within one genome. We argue that this variation generates\nselection on genome architecture.\n",
        "  Data Stream Mining is one of the area gaining lot of practical significance\nand is progressing at a brisk pace with new methods, methodologies and findings\nin various applications related to medicine, computer science, bioinformatics\nand stock market prediction, weather forecast, text, audio and video processing\nto name a few. Data happens to be the key concern in data mining. With the huge\nonline data generated from several sensors, Internet Relay Chats, Twitter, Face\nbook, Online Bank or ATM Transactions, the concept of dynamically changing data\nis becoming a key challenge, what we call as data streams. In this paper, we\ngive the algorithm for finding frequent patterns from data streams with a case\nstudy and identify the research issues in handling data streams.\n",
        "  We propose a new approach for extracting argument structure from natural\nlanguage texts that contain an underlying argument. Our approach comprises of\ntwo phases: Score Assignment and Structure Prediction. The Score Assignment\nphase trains models to classify relations between argument units (Support,\nAttack or Neutral). To that end, different training strategies have been\nexplored. We identify different linguistic and lexical features for training\nthe classifiers. Through ablation study, we observe that our novel use of\nword-embedding features is most effective for this task. The Structure\nPrediction phase makes use of the scores from the Score Assignment phase to\narrive at the optimal structure. We perform experiments on three argumentation\ndatasets, namely, AraucariaDB, Debatepedia and Wikipedia. We also propose two\nbaselines and observe that the proposed approach outperforms baseline systems\nfor the final task of Structure Prediction.\n",
        "  BigDAWG is a polystore system designed to work on complex problems that\nnaturally span across different processing or storage engines. BigDAWG provides\nan architecture that supports diverse database systems working with different\ndata models, support for the competing notions of location transparency and\nsemantic completeness via islands of information and a middleware that provides\na uniform multi-island interface. In this article, we describe the current\narchitecture of BigDAWG, its application on the MIMIC II medical dataset, and\nour plans for the mechanics of cross-system queries. During the presentation,\nwe will also deliver a brief demonstration of the current version of BigDAWG.\n",
        "  Magnetically-anchored surgical devices have recently gained attention in\nabdominal surgery, with the use of magnets to anchor surgical devices onto the\ninsufflated abdominal wall. These anchors have been used to secure passive and\nactive devices, where active device such as robotic manipulators produce\nmotions that would excite the dynamics of the non-rigid abdominal wall. Hence,\nthere is a need to investigate the mechanical dynamics of the abdominal wall\ntissue in insufflated state, combined with magnetic anchoring, specifically its\nresponse to mechanical excitations and the expected disturbances to the\noperation of the anchored devices. In this paper, loading and unloading tests\nare performed on a corresponding porcine specimen for dynamics identification.\nThe experiment setup was constructed to emulate the insufflated state of the\nabdomen with the magnetically anchored mechanism. The tissue responses during\nunloading are captured and approximated with a general numerical model, which\nis in turn used for the dynamic analysis of the tissue using Bode plot. The\nresults showed that in such stretched and compressed state, the steady state\ndisplacement of the tissue is approximately zero. The maximum transient error\nwas found to be 1mm in displacement using a high magnetic anchoring force.\nSignificant attenuation of the disturbances due to the high stiffness and\ndamping of the abdominal wall, was observed from 100rad/s in the frequency\nresponse. If a robotic manipulator was attached to the anchoring device, the\ntypical operating frequency of movements would still produce unattenuated\ndisturbances. It is expected that some error compensation through suitable\ncontrol strategies is required. These outcomes establish the basis for the\ncontroller design and the design specification of the active\nmagnetically-anchored surgical devices for minimal disturbance impact onto the\nabdominal wall tissue.\n",
        "  We compare the isophote shape parameter $a_{4}$ of early-type galaxies (ETGs)\nbetween $z\\sim1$ and 0 as a proxy for dynamics to investigate the epoch at\nwhich the dynamical properties of ETGs are established, using cluster ETG\nsamples with stellar masses of $\\log(M_{*}/M_{\\odot})\\geq10.5$ which have\nspectroscopic redshifts. We have 130 ETGs from the Hubble Space Telescope\nCluster Supernova Survey for $z\\sim1$ and 355 ETGs from the Sloan Digital Sky\nSurvey for $z\\sim0$. We have developed an isophote shape analysis method which\ncan be used for high-redshift galaxies and has been carefully compared with\npublished results. We have applied the same method for both the $z\\sim1$ and\n$0$ samples. We find similar dependence of the $a_{4}$ parameter on the mass\nand size at $z\\sim1$ and 0; the main population of ETGs changes from disky to\nboxy at a critical stellar mass of $\\log(M_{*}/M_{\\odot})\\sim11.5$ with the\nmassive end dominated by boxy. The disky ETG fraction decreases with increasing\nstellar mass both at $z\\sim1$ and $0$, and is consistent between these\nredshifts in all stellar mass bins when the Eddington bias is taken into\naccount. Although uncertainties are large, the results suggest that the\nisophote shapes and probably dynamical properties of ETGs in massive clusters\nare already in place at $z>1$ and do not significantly evolve in $z<1$, despite\nsignificant size evolution in the same galaxy population. The constant disky\nfraction favors less violent processes than mergers as a main cause of the size\nand morphological evolution of intermediate mass ETGs in $z<1$.\n",
        "  Open clusters have long been used to illuminate both stellar evolution and\nGalactic evolution. The oldest clusters, though rather rare, can reveal the\nchemical and nucleosynthetic processes early in the history of the Galaxy. We\nhave studied two turn-off stars in the old, metal-rich open cluster, NGC 6791.\nThe Keck + HIRES spectra have a resolution of 45,000 and signal-to-noise ratios\nof 40 per pixel. We confirm the high value for [Fe/H] finding +0.30 $\\pm$0.08,\nin agreement with earlier results from evolved stars in other parts of the HR\ndiagram. We have also determined abundances for Na, Si, Ca, Ti, Cr, Ni, Y and\nBa. These are compared to a sample of old, metal-rich field stars. With the\nprobable exception of enhanced Ni in the cluster stars, the field and cluster\nstars show similar abundances of the elements. Model predictions show that the\nNi enhancement could result from enrichment of the pre-cluster gas by SN Ia.\nOrbital evidence indicates that NGC 6791 could have originated near the inner\nregions of the Galaxy where the metallicity is generally higher than it is in\nthe disk or halo. Subsequent perturbations and migrations may have resulted in\nits current heliocentric distance of 4 kpc and 1 kpc above the Galactic plane.\n",
        "  In this work we present in-network techniques to improve the efficiency of\nspatial aggregate queries. Such queries are very common in a sensornet setting,\ndemanding more targeted techniques for their handling. Our approach constructs\nand maintains multi-resolution cube hierarchies inside the network, which can\nbe constructed in a distributed fashion. In case of failures, recovery can also\nbe performed with in-network decisions. In this paper we demonstrate how\nin-network cube hierarchies can be used to summarize sensor data, and how they\ncan be exploited to improve the efficiency of spatial aggregate queries. We\nshow that query plans over our cube summaries can be computed in polynomial\ntime, and we present a PTIME algorithm that selects the minimum number of data\nrequests that can compute the answer to a spatial query. We further extend our\nalgorithm to handle optimization over multiple queries, which can also be done\nin polynomial time. We discuss enriching cube hierarchies with extra summary\ninformation, and present an algorithm for distributed cube construction.\nFinally we investigate node and area failures, and algorithms to recover query\nresults.\n",
        "  We describe a new semantic relatedness measure combining the Wikipedia-based\nExplicit Semantic Analysis measure, the WordNet path measure and the mixed\ncollocation index. Our measure achieves the currently highest results on the\nWS-353 test: a Spearman rho coefficient of 0.79 (vs. 0.75 in (Gabrilovich and\nMarkovitch, 2007)) when applying the measure directly, and a value of 0.87 (vs.\n0.78 in (Agirre et al., 2009)) when using the prediction of a polynomial SVM\nclassifier trained on our measure.\n  In the appendix we discuss the adaptation of ESA to 2011 Wikipedia data, as\nwell as various unsuccessful attempts to enhance ESA by filtering at word,\nsentence, and section level.\n",
        "  The point-contact (PC) spectra of the Andreev reflection dV/dI curves of the\nsuperconducting rare-earth nickel borocarbide ErNi2B2C (Tc=11 K) have been\nanalyzed in the \"one-gap\" and \"two-gap\" approximations using the generalized\nBlonder-Tinkham-Klapwijk (GBTK) model and the Beloborod'ko (BB) model allowing\nfor the pair-breaking effect of magnetic impurities. Experimental and\ncalculated curves have been compared not only in shape, but in magnitude as\nwell, which provide more reliable data for determining the temperature\ndependence of the energy gap (or superconducting order parameter) \\Delta(T).\nThe anisotropic effect of antiferromagnetic ordering at T_N =6 K on the\nsuperconducting gap/order parameter has been determined: as the temperature is\nlowered, \\Delta(T) decreases by 25% in the c-direction and only by 4% in the\nab-plane. It is found that the pair-breaking parameter increases in the\nvicinity of the magnetic transitions, the increase being more pronounced in the\nc-direction. The efficiency of the models was tested for providing \\Delta(T)\ndata for ErNi2B2C from Andreev reflection spectra.\n",
        "  A trademark of eusocial insect species is reproductive division of labor, in\nwhich workers forego their own reproduction while the queen produces almost all\noffspring. The presence of the queen is key for maintaining social harmony, but\nthe specific role of the queen in the evolution of eusociality remains unclear.\nA long-discussed scenario is that a queen either behaviorally or chemically\nsterilizes her workers. However, the demographic and ecological conditions that\nenable such manipulation are unknown. Accordingly, we propose a simple model of\nevolutionary dynamics that is based on haplodiploid genetics. We consider a\nmutation that acts in a queen, causing her to control the reproductive behavior\nof her workers. Our mathematical analysis yields precise conditions for the\nevolutionary emergence and stability of queen-induced worker sterility. These\nconditions do not depend on the queen's mating frequency. Moreover, we find\nthat queen control is always established if it increases colony reproductive\nefficiency and can evolve even if it decreases colony efficiency. We further\noutline the conditions under which queen control is evolutionarily stable\nagainst invasion by mutant, reproductive workers.\n",
        "  We introduce a class of links strictly containing quasi-alternating links for\nwhich mod 2 reduced Khovanov homology is always thin. We compute the framed\ninstanton homology for double branched covers of such links. Aligning certain\ndotted markings on a link with bundle data over the branched cover, we also\nprovide many computations of framed instanton homology in the presence of a\nnon-trivial real 3-plane bundle. We discuss evidence for a spectral sequence\nfrom the twisted Khovanov homology of a link with mod 2 coefficients to the\nframed instanton homology of the double branched cover. We also discuss the\nrelevant mod 4 gradings.\n",
        "  The data warehousing is becoming increasingly important in terms of strategic\ndecision making through their capacity to integrate heterogeneous data from\nmultiple information sources in a common storage space, for querying and\nanalysis. So it can evolve into a multi-tier structure where parts of the\norganization take information from the main data warehouse into their own\nsystems. These may include analysis databases or dependent data marts. As the\ndata warehouse evolves and the organization gets better at capturing\ninformation on all interactions with the customer. Data warehouse can track\ncustomer interactions over the whole of the customer's lifetime.\n",
        "  A periodic geodesic on a surface has a natural lift to the unit tangent\nbundle; when the complement of this lift is hyperbolic, its volume typically\ngrows as the geodesic gets longer. We give an upper bound for this volume which\nis linear in the geometric length of the geodesic.\n",
        "  The current work is done to see which artery has more chance of having\ncardiovascular diseases by measuring value of pressure gradient in the common\ncarotid artery (CCA) and ascending aorta according to age and gender. Pressure\ngradient is determined in the CCA and ascending aorta of presumed healthy\nvolunteers, having age between 10 and 60 years. A real 2D model of both aorta\nand common carotid artery is constructed for different age groups using\ncomputational fluid dynamics (CFD). Pressure gradient of both the arteries are\ncalculated and compared for different age groups and gender. It is found that\nwith increase in diameter of common carotid artery and ascending aorta with\nadvancing age pressure gradient decreases. The value of pressure gradient of\naorta is found less than common carotid artery in both cases of age and gender.\n",
        "  We study the spatio-temporal patterns of the proportion of influenza B out of\nlaboratory confirmations of both influenza A and B, with data from 139\ncountries and regions downloaded from the FluNet compiled by the World Health\nOrganization, from January 2006 to October 2015, excluding 2009. We restricted\nour analysis to 34 countries that reported more than 2000 confirmations for\neach of types A and B over the study period. We find that Pearson's correlation\nis 0.669 between effective distance from Mexico and influenza B proportion\namong the countries from January 2006 to October 2015. In the United States,\ninfluenza B proportion in the pre-pandemic period (2003-2008) negatively\ncorrelated with that in the post-pandemic era (2010-2015) at the regional\nlevel. Our study limitations are the country-level variations in both\nsurveillance methods and testing policies. Influenza B proportion displayed\nwide variations over the study period. Our findings suggest that even after\nexcluding 2009's data, the influenza pandemic still has an evident impact on\nthe relative burden of the two influenza types. Future studies could examine\nwhether there are other additional factors. This study has potential\nimplications in prioritizing public health control measures.\n",
        "  Lexical ambiguity makes it difficult to compute various useful statistics of\na corpus. A given word form might represent any of several morphological\nfeature bundles. One can, however, use unsupervised learning (as in EM) to fit\na model that probabilistically disambiguates word forms. We present such an\napproach, which employs a neural network to smoothly model a prior distribution\nover feature bundles (even rare ones). Although this basic model does not\nconsider a token's context, that very property allows it to operate on a simple\nlist of unigram type counts, partitioning each count among different analyses\nof that unigram. We discuss evaluation metrics for this novel task and report\nresults on 5 languages.\n",
        "  H2 formation remains a major issue for the understanding of interstellar\nphysics. We investigate H2 formation in the interstellar medium at the light of\nthe most recent experimental and theoretical data. We implemented detailed H2\nformation mechanisms on grains surface in the Meudon PDR code : i)\nLangmuir-Hinshelwood mechanism taking into account the contribution of the\ndifferent sizes of dust grains in the diffusion processes and ii) the\nEley-Rideal mechanism. We show that, thanks to these processes, H2 can be\nformed even in regions where dust temperature is larger than 25 K. We also show\nthat formation by Eley-Rideal mechanism can be a significant source of heating\nof the gas. We derive line intensities for various astrophysical conditions.\nSuch an approach results in an enhanced H2 formation rate compared to the\nstandard formation determined by observations in absorption in the UV. We\nderive some H2 line intensities for isobaric and isochoric models.\n",
        "  This paper proposes a methodology for generating a stopword list from online\nsocial network (OSN) corpora in Egyptian Dialect(ED). The aim of the paper is\nto investigate the effect of removingED stopwords on the Sentiment Analysis\n(SA) task. The stopwords lists generated before were on Modern Standard Arabic\n(MSA) which is not the common language used in OSN. We have generated a\nstopword list of Egyptian dialect to be used with the OSN corpora. We compare\nthe efficiency of text classification when using the generated list along with\npreviously generated lists of MSA and combining the Egyptian dialect list with\nthe MSA list. The text classification was performed using Na\\\"ive Bayes and\nDecision Tree classifiers and two feature selection approaches, unigram and\nbigram. The experiments show that removing ED stopwords give better performance\nthan using lists of MSA stopwords only.\n",
        "  The set of axes of hyperbolic elements in a Fuchsian group depends on the\ncommensurability class of the group. In fact, it has been conjectured that it\ndetermines the commensurability class and this has been verified in for groups\nof the second kind by G. Mess and for arithemetic groups by by D. Long and A.\nReid. Here we show that the conjecture holds for almost all Fuchsian groups and\nexplain why our method fails for arithemetic groups.\n",
        "  We apply four statistical learning methods to a sample of $7941$ galaxies\n($z<0.06$) from the Galaxy and Mass Assembly (GAMA) survey to test the\nfeasibility of using automated algorithms to classify galaxies. Using $10$\nfeatures measured for each galaxy (sizes, colours, shape parameters \\& stellar\nmass) we apply the techniques of Support Vector Machines (SVM), Classification\nTrees (CT), Classification Trees with Random Forest (CTRF) and Neural Networks\n(NN), returning True Prediction Ratios (TPRs) of $75.8\\%$, $69.0\\%$, $76.2\\%$\nand $76.0\\%$ respectively. Those occasions whereby all four algorithms agree\nwith each other yet disagree with the visual classification (`unanimous\ndisagreement') serves as a potential indicator of human error in\nclassification, occurring in $\\sim9\\%$ of ellipticals, $\\sim9\\%$ of Little Blue\nSpheroids, $\\sim14\\%$ of early-type spirals, $\\sim21\\%$ of intermediate-type\nspirals and $\\sim4\\%$ of late-type spirals \\& irregulars. We observe that the\nchoice of parameters rather than that of algorithms is more crucial in\ndetermining classification accuracy. Due to its simplicity in formulation and\nimplementation, we recommend the CTRF algorithm for classifying future galaxy\ndatasets. Adopting the CTRF algorithm, the TPRs of the 5 galaxy types are : E,\n$70.1\\%$; LBS, $75.6\\%$; S0-Sa, $63.6\\%$; Sab-Scd, $56.4\\%$ and Sd-Irr,\n$88.9\\%$. Further, we train a binary classifier using this CTRF algorithm that\ndivides galaxies into spheroid-dominated (E, LBS \\& S0-Sa) and disk-dominated\n(Sab-Scd \\& Sd-Irr), achieving an overall accuracy of $89.8\\%$. This translates\ninto an accuracy of $84.9\\%$ for spheroid-dominated systems and $92.5\\%$ for\ndisk-dominated systems.\n",
        "  Twelve high schools in Japan (of which six are in Fukushima Prefecture), four\nin France, eight in Poland and two in Belarus cooperated in the measurement and\ncomparison of individual external doses in 2014. In total 216 high-school\nstudents and teachers participated in the study. Each participant wore an\nelectronic personal dosimeter \"D-shuttle\" for two weeks, and kept a journal of\nhis/her whereabouts and activities. The distributions of annual external doses\nestimated for each region overlap with each other, demonstrating that the\npersonal external individual doses in locations where residence is currently\nallowed in Fukushima Prefecture and in Belarus are well within the range of\nestimated annual doses due to the background radiation level of other\nregions/countries.\n",
        "  Learning useful information across long time lags is a critical and difficult\nproblem for temporal neural models in tasks such as language modeling. Existing\narchitectures that address the issue are often complex and costly to train. The\nDifferential State Framework (DSF) is a simple and high-performing design that\nunifies previously introduced gated neural models. DSF models maintain\nlonger-term memory by learning to interpolate between a fast-changing\ndata-driven representation and a slowly changing, implicitly stable state. This\nrequires hardly any more parameters than a classical, simple recurrent network.\nWithin the DSF framework, a new architecture is presented, the Delta-RNN. In\nlanguage modeling at the word and character levels, the Delta-RNN outperforms\npopular complex architectures, such as the Long Short Term Memory (LSTM) and\nthe Gated Recurrent Unit (GRU), and, when regularized, performs comparably to\nseveral state-of-the-art baselines. At the subword level, the Delta-RNN's\nperformance is comparable to that of complex gated architectures.\n",
        "  A computer-aided detection (CAD) system for the identification of lung\ninternal nodules in low-dose multi-detector helical Computed Tomography (CT)\nimages was developed in the framework of the MAGIC-5 project. The three modules\nof our lung CAD system, a segmentation algorithm for lung internal region\nidentification, a multi-scale dot-enhancement filter for nodule candidate\nselection and a multi-scale neural technique for false positive finding\nreduction, are described. The results obtained on a dataset of low-dose and\nthin-slice CT scans are shown in terms of free response receiver operating\ncharacteristic (FROC) curves and discussed.\n",
        "  We classify ground states and normal modes for $n$-component superconductors\nwith frustrated intercomponent Josephson couplings, focusing on $n = 4$. The\nresults should be relevant not only to multiband superconductors, but also to\nJosephson-coupled multilayers and Josephson-junction arrays. It was recently\ndiscussed that three-component superconductors can break time-reversal symmetry\nas a consequence of phase frustration. We discuss how to classify frustrated\nsuperconductors with an arbitrary number of components. Although already for\nthe four-component case there are a large number of different combinations of\nphase-locking and phase-antilocking Josephson couplings, we establish that\nthere are a much smaller number of equivalence classes where properties of\nfrustrated multicomponent superconductors can be mapped to each other. This\nclassification is related to the graph-theoretical concept of Seidel switching.\nNumerically, we calculate ground states, normal modes, and characteristic\nlength scales for the four-component case. We report conditions of appearance\nof new accidental continuous ground-state degeneracies.\n",
        "  This paper studies the problem of identification and extraction of flat and\nnested data records from a given web page. With the explosive growth of\ninformation sources available on the World Wide Web, it has become increasingly\ndifficult to identify the relevant pieces of information, since web pages are\noften cluttered with irrelevant content like advertisements, navigation-panels,\ncopyright notices etc., surrounding the main content of the web page. Hence, it\nis useful to mine such data regions and data records in order to extract\ninformation from such web pages to provide value-added services. Currently\navailable automatic techniques to mine data regions and data records from web\npages are still unsatisfactory because of their poor performance. In this paper\na novel method to identify and extract the flat and nested data records from\nthe web pages automatically is proposed. It comprises of two steps : (1)\nIdentification and Extraction of the data regions based on visual clues\ninformation. (2) Identification and extraction of flat and nested data records\nfrom the data region of a web page automatically. For step1, a novel and more\neffective method is proposed, which finds the data regions formed by all types\nof tags using visual clues. For step2, a more effective and efficient method\nnamely, Visual Clue based Extraction of web Data (VCED), is proposed, which\nextracts each record from the data region and identifies it whether it is a\nflat or nested data record based on visual clue information the area covered by\nand the number of data items present in each record. Our experimental results\nshow that the proposed technique is effective and better than existing\ntechniques.\n",
        "  This paper calculates probability distributions modeling the Luria-Delbr\\\"uck\nexperiment. We show that by thinking purely in terms of generating functions,\nand using a 'backwards in time' paradigm, that formulas describing various\nsituations can be easily obtained. This includes a generating function for\nHaldane's probability distribution due to Ycart. We apply our formulas to both\nsimulated and real data created by looking at yeast cells acquiring an\nimmunization to the antibiotic canavanine.\n  This paper is somewhat incomplete, having been last significantly modified in\nMarch 29, 2014. However the first author feels that this paper has some\nworthwhile ideas, and so is going to make this paper publicly available.\n",
        "  Let $\\{P_1, P_2, P_3, P_4\\}$ be a quadruplet of points in $S^3$ . We define a\n``dual'' quadruplet of it in a conformal geometric way. We show that the dual\nof a dual quadruplet coincides with the original one. We also show that the\ncross ratio of the dual quadruplet is equal to the complex conjugate of that of\nthe original one.\n",
        "  It has been noted that certain surfaces of Weyl semimetals have bound states\nforming open Fermi arcs, which are never seen in typical metallic states. We\nshow that the Fermi arcs enable them to support an even more exotic surface\nstate with crossed flat bands in the superconducting state. We clarify the\ntopological origin of the crossed flat bands and the relevant symmetry that\nstabilizes the cross point. We also discuss their possible experimental\nverification by tunneling spectroscopy.\n",
        "  There are a number of clinically relevant tasks in digital breast\ntomosynthesis (DBT) involving the detection and visual assessment of fiber-like\nstructures such as Cooper's ligaments, blood vessels, and spiculated lesions.\nSuch structures can exhibit orientation dependent variations in conspicuity.\nThis study demonstrates the presence of in-plane orientation-dependent signal\nconspicuity for fiber-like signals in DBT and shows how reconstruction\nalgorithm design can mitigate this phenomenon. We uncover a tradeoff between\nminimizing orientation-dependence and preserving depth resolution that is\ndictated by the regularization strength employed in reconstruction.\n",
        "  The outer Galaxy beyond the Outer Arm provides a good opportunity to study\nstar formation in an environment significantly different from that in the solar\nneighborhood. However, star-forming regions in the outer Galaxy have never been\ncomprehensively studied or cataloged because of the difficulties in detecting\nthem at such large distances. We studied 33 known young star-forming regions\nassociated with 13 molecular clouds at $R_{\\rm G}$ $\\ge$ 13.5 kpc in the outer\nGalaxy with data from the Wide-field Infrared Survey Explorer (WISE)\nmid-infrared all-sky survey. From their color distribution, we developed a\nsimple identification criterion of star-forming regions in the outer Galaxy\nwith the WISE color. We applied the criterion to all the WISE sources in the\nmolecular clouds in the outer Galaxy at $R_{\\rm G}$ $\\ge$ 13.5 kpc detected\nwith the Five College Radio Astronomy Observatory (FCRAO) $^{12}$CO survey of\nthe outer Galaxy, of which the survey region is 102$^\\circ$.49 $\\le$ $l$ $\\le$\n141$^\\circ$.54, $-$3$^\\circ$.03 $\\le$ $b$ $\\le$ 5$^\\circ$.41, and successfully\nidentified 711 new candidate star-forming regions in 240 molecular clouds. The\nlarge number of samples enables us to perform the statistical study of\nstar-formation properties in the outer Galaxy for the first time. This study is\ncrucial to investigate the fundamental star-formation properties, including\nstar-formation rate, star-formation efficiency, and initial mass function, in a\nprimordial environment such as the early phase of the Galaxy formation.\n",
        "  Cobordism of Haken $n$-manifolds is defined by a Haken $(n+1)$-manifold $W$\nwhose boundary has two components, each of which is a closed Haken\n$n$-manifold. In addition, the inclusion map of the fundamental group of each\nboundary component to $\\pi_1(W)$ is injective. In this paper we prove that\nthere are 4-dimensional Haken cobordisms whose boundary consists of any two\nclosed Haken 3-manifolds. In particular, each closed Haken 3-manifold is the\n$\\pi_1$-injective boundary of some Haken 4-manifold.\n",
        "  We study the characteristics of galaxy protoclusters using the latest\nL-galaxies semi-analytic model. Searching for protoclusters on a scale of $\\sim\n10 \\, \\mathrm{cMpc}$ gives an excellent compromise between the completeness and\npurity of their galaxy populations, leads to high distinction from the field in\noverdensity space, and allows accurate determination of the descendant cluster\nmass. This scale is valid over a range of redshifts and selection criteria. We\npresent a procedure for estimating, given a measured galaxy overdensity, the\nprotocluster probability and its descendant cluster mass for a range of\nmodelling assumptions, particularly taking into account the shape of the\nmeasurement aperture. This procedure produces lower protocluster probabilities\ncompared to previous estimates using fixed size apertures. The relationship\nbetween AGN and protoclusters is also investigated, and shows significant\nevolution with redshift; at $z \\sim 2$ the fraction of protoclusters traced by\nAGN is high, but the fraction of all AGN in protoclusters is low, whereas at $z\n\\geqslant 5$ the fraction of protoclusters containing AGN is low, but most AGN\nare in protoclusters. We also find indirect evidence for the emergence of a\npassive sequence in protoclusters at $z \\sim 2$, and note that a significant\nfraction of all galaxies reside in protoclusters at $z \\geqslant 2$,\nparticularly the most massive.\n",
        "  Shape tracking of medical devices using strain sensing properties in optical\nfibers has seen increased attention in recent years. In this paper, we propose\na novel guidance system for intra-arterial procedures using a distributed\nstrain sensing device based on optical frequency domain reflectometry (OFDR) to\ntrack the shape of a catheter. Tracking enhancement is provided by exposing a\nfiber triplet to a focused ultraviolet beam, producing high scattering\nproperties. Contrary to typical quasi-distributed strain sensors, we propose a\ntruly distributed strain sensing approach, which allows to reconstruct a fiber\ntriplet in real-time. A 3D roadmap of the hepatic anatomy integrated with a 4D\nMR imaging sequence allows to navigate the catheter within the\npre-interventional anatomy, and map the blood flow velocities in the arterial\ntree. We employed Riemannian anisotropic heat kernels to map the sensed data to\nthe pre-interventional model. Experiments in synthetic phantoms and an in vivo\nmodel are presented. Results show that the tracking accuracy is suitable for\ninterventional tracking applications, with a mean 3D shape reconstruction\nerrors of 1.6 +/- 0.3 mm. This study demonstrates the promising potential of\nMR-compatible UV-exposed OFDR optical fibers for non-ionizing device guidance\nin intra-arterial procedures.\n",
        "  Aim: To improve treatment plan robustness with respect to small shifts in\npatient position during the VMAT treatment by ensuring a linear ramp-like dose\nprofile in treatment field overlap regions.\n  Background: Craniospinal irradiation (CSI) is considered technically\nchallenging because the target size exceeds the maximal field size, which\nnecessitates using abutted or overlapping treatment fields. Volumetric\nmodulated arc therapy (VMAT) is increasingly being examined for CSI, as it\noffers both better dose homogeneity and better dose conformance while also\noffering a possibility to create field junctions which are more robust towards\nsmall shifts in patient position during the treatment.\n  Materials and Methods: A VMAT treatment plan with three isocenters was made\nfor a test case patient. Three groups of overlapping arc field pairs were used;\none for the cranial and two for the spinal part. In order to assure a ramp-like\ndose profile in the field overlap region, the upper spinal part was optimised\nfirst, with dose prescription explicitely enforcing a ramp-like dose profile.\nThe cranial and lower spinal part were done afterwards, taking into account the\ndose contribution of the upper spinal fields.\n  Results: Using simple geometrical reasoning, we demonstrated that hot- and\ncold spots which arise from small displacement of one treatment field relative\nto the other treatment field can be reduced by taking two precautions: (a)\nwidening the field overlap region, and (b) reducing the field gradient across\nthe overlap region. The function with the smallest maximal gradient is a linear\nramp. We present a treatment planning technique which yields the desired dose\nprofile of the two contributing fields, and minimises dosimetric dependence on\nminor positional errors in patient set-up.\n",
        "  A simple way to model phenotypic evolution is to assume that after splitting,\nthe trait values of the sister species diverge as independent Brownian motions.\nRelying only on a prior distribution for the underlying species tree\n(conditioned on the number, n, of extant species) we study the random vector\n(X_1,...,X_n) of the observed trait values. In this paper we derive compact\nformulae for the variance of the sample mean and the mean of the sample\nvariance for the vector (X_1,...,X_n).\n  The key ingredient of these formulae is the correlation coefficient between\ntwo trait values randomly chosen from (X_1,...,X_n). This interspecies\ncorrelation coefficient takes into account not only variation due to the random\nsampling of two species out of n and the stochastic nature of Brownian motion\nbut also the uncertainty in the phylogenetic tree. The latter is modeled by a\n(supercritical or critical) conditioned branching process. In the critical case\nwe modify the Aldous-Popovic model by assuming a proper prior for the time of\norigin.\n",
        "  The common understanding of protein evolution has been that neutral or\nslightly deleterious mutations are fixed by random drift, and evolutionary rate\nis determined primarily by the proportion of neutral mutations. However, recent\nstudies have revealed that highly expressed genes evolve slowly because of\nfitness costs due to misfolded proteins. Here we study selection maintaining\nprotein stability.\n  Protein fitness is taken to be $s = \\kappa \\exp(\\beta\\Delta G) (1 -\n\\exp(\\beta\\Delta\\Delta G))$, where $s$ and $\\Delta\\Delta G$ are selective\nadvantage and stability change of a mutant protein, $\\Delta G$ is the folding\nfree energy of the wild-type protein, and $\\kappa$ represents protein abundance\nand indispensability. The distribution of $\\Delta\\Delta G$ is approximated to\nbe a bi-Gaussian function, which represents structurally slightly- or\nhighly-constrained sites. Also, the mean of the distribution is negatively\nproportional to $\\Delta G$.\n  The evolution of this gene has an equilibrium ($\\Delta G_e$) of protein\nstability, the range of which is consistent with experimental values. The\nprobability distribution of $K_a/K_s$, the ratio of nonsynonymous to synonymous\nsubstitution rate per site, over fixed mutants in the vicinity of the\nequilibrium shows that nearly neutral selection is predominant only in\nlow-abundant, non-essential proteins of $\\Delta G_e > -2.5$ kcal/mol. In the\nother proteins, positive selection on stabilizing mutations is significant to\nmaintain protein stability at equilibrium as well as random drift on slightly\nnegative mutations, although the average $\\langle K_a/K_s \\rangle$ is less than\n1. Slow evolutionary rates can be caused by high protein\nabundance/indispensability, which produces positive shifts of $\\Delta\\Delta G$\nthrough decreasing $\\Delta G_e$, and by strong structural constraints, which\ndirectly make $\\Delta\\Delta G$ more positive.\n",
        "  We review the statistical properties of the genealogies of a few models of\nevolution. In the asexual case, selection leads to coalescence times which grow\nlogarithmically with the size of the population in contrast with the linear\ngrowth of the neutral case. Moreover for a whole class of models, the\nstatistics of the genealogies are those of the Bolthausen-Sznitman coalescent\nrather than the Kingman coalescent in the neutral case. For sexual\nreproduction, the time to reach the first common ancestors to the whole\npopulation and the time for all individuals to have all their ancestors in\ncommon are also logarithmic in the neutral case, as predicted by Chang []. We\ndiscuss how these times are modified in a simple way of introducing selection.\n",
        "  We present a new dataset and models for comprehending paragraphs about\nprocesses (e.g., photosynthesis), an important genre of text describing a\ndynamic world. The new dataset, ProPara, is the first to contain natural\n(rather than machine-generated) text about a changing world along with a full\nannotation of entity states (location and existence) during those changes (81k\ndatapoints). The end-task, tracking the location and existence of entities\nthrough the text, is challenging because the causal effects of actions are\noften implicit and need to be inferred. We find that previous models that have\nworked well on synthetic data achieve only mediocre performance on ProPara, and\nintroduce two new neural models that exploit alternative mechanisms for state\nprediction, in particular using LSTM input encoding and span prediction. The\nnew models improve accuracy by up to 19%. The dataset and models are available\nto the community at http://data.allenai.org/propara.\n",
        "  Indexing moving objects has been extensively studied in the past decades.\nMoving objects, such as vehicles and mobile device users, usually exhibit some\npatterns on their velocities, which can be utilized for velocity-based\npartitioning to improve performance of the indexes. Existing velocity-based\npartitioning techniques rely on some kinds of heuristics rather than\nanalytically calculate the optimal solution. In this paper, we propose a novel\nspeed partitioning technique based on a formal analysis over speed values of\nthe moving objects. We first show that speed partitioning will significantly\nreduce the search space expansion which has direct impacts on query performance\nof the indexes. Next we formulate the optimal speed partitioning problem based\non search space expansion analysis and then compute the optimal solution using\ndynamic programming. We then build the partitioned indexing system where\nqueries are duplicated and processed in each index partition. Extensive\nexperiments demonstrate that our method dramatically improves the performance\nof indexes for moving objects and outperforms other state-of-the-art\nvelocity-based partitioning approaches.\n",
        "  The phase dynamics of Josephson junctions under external electromagnetic\nradiation is studied through numerical simulations. Current-voltage\ncharacteristics, Lyapunov exponents and Poincare sections are analyzed in\ndetail. It is found that the subharmonic Shapiro steps at certain parameters\nare separated by structured chaotic windows. By performing a linear regression\non the linear part of the data, a fractal dimension of D = 0.868 is obtained,\nwith an uncertainty of +-0.012. The chaotic regions exhibit scaling similarity\nand it is shown that the devil's staircase of the system can form a backbone\nthat unifies and explains the highly correlated and structured chaotic\nbehavior. These features suggest a system possessing multiple complete devil's\nstaircases. The onset of chaos for subharmonic steps occurs through the\nFeigenbaum period doubling scenario. Universality in the sequence of periodic\nwindows is also demonstrated. Finally the influence of the radiation and\nJosephson junction parameters on the structured chaos is investigated and it is\nconcluded that the structured chaos is a stable formation over a wide range of\nparameter values.\n",
        "  We give an alternative presentation of Khovanov homology of links with strict\nfunctoriality result over integers. The construction uses an oriented $sl(2)$\nstate model allowing a natural definition of the boundary operator as twisted\naction of morphisms belonging to a TQFT for trivalent graphs and surfaces.\n",
        "  Marine viruses shape the structure of the microbial community. They are,\nthus, a key determinant of the most important biogeochemical cycles in the\nplanet. Therefore, a correct description of the ecological and evolutionary\nbehavior of these viruses is essential to make reliable predictions about their\nrole in marine ecosystems. The infection cycle, for example, is indistinctly\nmodeled in two very different ways. In one representation, the process is\ndescribed including explicitly a fixed delay between infection and offspring\nrelease. In the other, the offspring are released at exponentially distributed\ntimes according to a fixed release rate. By considering obvious quantitative\ndifferences pointed out in the past, the latter description is widely used as a\nsimplification of the former. However, it is still unclear how the dichotomy\n\"delay versus rate description\" affects long-term predictions of host-virus\ninteraction models. Here, we study the ecological and evolutionary implications\nof using one or the other approaches, applied to marine microbes. To this end,\nwe use mathematical and eco-evolutionary computational analysis. We show that\nthe rate model exhibits improved competitive abilities from both ecological and\nevolutionary perspectives in steady environments. However, rate-based\ndescriptions can fail to describe properly long-term microbe-virus\ninteractions. Moreover, additional information about trade-offs between\nlife-history traits is needed in order to choose the most reliable\nrepresentation for oceanic bacteriophage dynamics. This result affects deeply\nmost of the marine ecosystem models that include viruses, especially when used\nto answer evolutionary questions.\n",
        "  Ticks are important vectors of emerging zoonotic diseases. While adults of\nmany tick species parasitize mammals, immature ticks are often found on wild\nbirds. In the tropics, difficulties in species-level identification of immature\nticks hinder studies of tick ecology and tick-borne disease transmission,\nincluding any potential role for birds. In Panama, we found immature ticks on\n227 out of 3,498 birds representing 93 host species, about 1/8th of the entire\nPanamanian terrestrial avifauna. Tick parasitism rates did not vary with\ntemperature or rainfall, but parasitism rates did vary with host ecological\ntraits: non-migratory residents, forest dwelling birds, bark insectivores,\nterrestrial foragers and lowland species were most likely to be infested with\nticks. Using a molecular library developed from adult ticks specifically for\nthis study, we identified 130 immature ticks obtained from wild birds,\ncorresponding to eleven tick species, indicating that a substantial portion of\nthe Panamanian avifauna is parasitized by a variety of tick species.\nFurthermore, we found evidence that immature ticks show taxonomic or ecological\nspecificity to avian hosts. Finally, our data indicate that Panamanian birds\nare not parasitized regularly by the tick species responsible for most known\ntick-borne diseases. However, they are frequent hosts of other tick species\nknown to carry a variety of rickettsial parasites of unknown pathogenicity.\nGiven the broad interaction between tick and avian biodiversity in the\nNeotropics, future work on emerging tropical tick-borne disease should\nexplicitly consider wild birds as vertebrate hosts.\n",
        "  Neural Machine Translation (NMT) is a new approach for automatic translation\nof text from one human language into another. The basic concept in NMT is to\ntrain a large Neural Network that maximizes the translation performance on a\ngiven parallel corpus. NMT is gaining popularity in the research community\nbecause it outperformed traditional SMT approaches in several translation tasks\nat WMT and other evaluation tasks/benchmarks at least for some language pairs.\nHowever, many of the enhancements in SMT over the years have not been\nincorporated into the NMT framework. In this paper, we focus on one such\nenhancement namely domain adaptation. We propose an approach for adapting a NMT\nsystem to a new domain. The main idea behind domain adaptation is that the\navailability of large out-of-domain training data and a small in-domain\ntraining data. We report significant gains with our proposed method in both\nautomatic metrics and a human subjective evaluation metric on two language\npairs. With our adaptation method, we show large improvement on the new domain\nwhile the performance of our general domain only degrades slightly. In\naddition, our approach is fast enough to adapt an already trained system to a\nnew domain within few hours without the need to retrain the NMT model on the\ncombined data which usually takes several days/weeks depending on the volume of\nthe data.\n",
        "  We address the problem of equivalence of count-distinct aggregate queries,\nprove that the problem is decidable, and can be decided in the third level of\nPolynomial hierarchy. We introduce the notion of core for conjunctive queries\nwith comparisons as an extension of the classical notion for relational\nqueries, and prove that the existence of isomorphism among cores of queries is\na sufficient and necessary condition for equivalence of conjunctive queries\nwith comparisons similar to the classical relational setting. However, it is\nnot a necessary condition for equivalence of count-distinct queries. We\nintroduce a relaxation of this condition based on a new notion, which is a\npotentially new query equivalent to the initial query, introduced to capture\nthe behavior of count-distinct operator.\n",
        "  World wide technological advancement has brought in a widespread change in\nadoption and utilization of open source tools. Since, most of the organizations\nacross the globe deal with a large amount of data to be updated online and\ntransactions are made every second, managing, mining and processing this\ndynamic data is very complex. Successful implementation of the data mining\ntechnique requires a careful assessment of the various tools and algorithms\navailable to mining experts. This paper provides a comparative study of open\nsource data mining tools available to the professionals. Parameters influencing\nthe choice of apt tools in addition to the real time challenges are discussed.\nHowever, it is well proven that agents aid in improving the performance of data\nmining tools. This paper provides information on an agent-based framework for\ndata preprocessing with implementation details for the development of better\ntool in the market. An integration of open source data mining tools with agent\nsimulation enable one to implement an effective data pre processing\narchitecture thereby providing robust capabilities of the application which can\nbe upgraded using a minimum of pre planning requirement from the application\ndeveloper.\n",
        "  X-Ray Phase-Contrast Imaging (PCI) yields absorption, differential phase, and\ndark-field images. Computed Tomography (CT) of grating-based PCI can in\nprinciple provide high-resolution soft-tissue contrast. Recently, grating-based\nPCI took several hurdles towards clinical implementation by addressing, for\nexample, acquisition speed, high X-ray energies, and system vibrations.\nHowever, a critical impediment in all grating-based systems lies in limits that\nconstrain the grating diameter to few centimeters. In this work, we propose a\nsystem and a reconstruction algorithm to circumvent this constraint in a\nclinically compatible way. We propose to perform a phase-sensitive\nRegion-of-Interest (ROI) CT within a full-field absorption CT. The biggest\nadvantage of this approach is that it allows to correct for phase truncation\nartifacts, and to obtain quantitative phase values. Our method is robust, and\nshows high-quality results on simulated data and on a biological mouse sample.\nThis work is a proof of concept showing the potential to use PCI in CT on large\nspecimen, such as humans, in clinical applications.\n",
        "  In this paper we study topological surfaces as gridded surfaces in the\n2-dimensional scaffolding of cubic honeycombs in Euclidean and hyperbolic\nspaces.\n",
        "  This review discusses the heavy-fermion superconductivity in Ce- and U-based\ncompounds crystallizing in the body-centered tetragonal ThCr2Si2 structure.\nSpecial attention will be paid to the theoretical background of these systems\nwhich are located close to a magnetic instability.\n",
        "  AI systems typically make decisions and find patterns in data based on the\ncomputation of aggregate and specifically sum functions, expressed as queries,\non data's attributes. This computation can become costly or even inefficient\nwhen these queries concern the whole or big parts of the data and especially\nwhen we are dealing with big data. New types of intelligent analytics require\nalso the explanation of why something happened. In this paper we present a\nrandomised algorithm that constructs a small summary of the data, called\nAggregate Lineage, which can approximate well and explain all sums with large\nvalues in time that depends only on its size. The size of Aggregate Lineage is\npractically independent on the size of the original data. Our algorithm does\nnot assume any knowledge on the set of sum queries to be approximated.\n",
        "  We use a Minimum Spanning Tree algorithm to characterize the spatial\ndistribution of Galactic Far-IR sources and derive their clustering properties.\nWe aim to reveal the spatial imprint of different types of star forming\nprocesses, e.g. isolated spontaneous fragmentation of dense molecular clouds,\nor events of triggered star formation around HII regions, and highlight global\nproperties of star formation in the Galaxy. We plan to exploit the entire\nHi-GAL survey of the inner Galactic plane to gather significant statistics on\nthe clustering properties of star forming regions, and to look for possible\ncorrelations with source properties such as mass, temperature or evolutionary\nstage. In this paper we present a pilot study based on the two 2x2 square\ndegree fields centered at longitudes l=30 and l=59 obtained during the Science\nDemonstration Phase (SDP) of the Herschel mission. We find that over half of\nthe clustered sources are associated with HII regions and infrared dark clouds.\nOur analysis also reveals a smooth chromatic evolution of the spatial\ndistribution where sources detected at short-wavelengths, likely proto-stars\nsurrounded by warm circumstellar material emitting in the far-infrared, tend to\nbe clustered in dense and compact groups around HII regions while sources\ndetected at long-wavelengths, presumably cold and dusty density enhancements of\nthe ISM emitting in the sub-millimeter, are distributed in larger and looser\ngroups.\n",
        "  x dependences of Tc in YBa2Cu3O6+x and Y1-b(Ca)bBa2Cu3O6+x (b=0.1 and b=0.2)\nhave been calculated assuming that the net doping of CuO2 layers is a sum of\ncontributions from CuO chains and from substitution of Y3+ by Ca2+. We applied\nthe concept of minimal (critical) chain length lcr needed to trigger charge\ntransfer from chains to planes. The model proposed assumes that only a certain\npart, say kappa, of those chain-holes that are created beyond the first lcr-2\nholes in chains of length l>lcr, are able to attract electrons from CuO2\nbilayer. Our analysis points to the conclusion that parameter lcr should be\nequal to 4 (four oxygen atoms in a chain), or very close to it (3, or 5).\nCalculated x dependences of doping, p(x), at constant (room) temperature and\nfor three different substitution levels b=0, 0.1, and 0.2, are found to be in\nexcellent agreement with available experimental data. These p(x) dependences\nare combined with universal (parabolic) phase relation Tc(p) to obtain three\nTc(x) dependences that also remarkably correlate with those reported in\nexperiments. The results obtained indicate that in long chains (x=1) the\nprobability for a chain-hole to capture an electron (expressing hole ability to\nbecome transferred) decreases with the concentration of 3d Cu electrons in CuO2\nlayers, ranking from kappa=40%(42%) in YBa2Cu3O6+x, over kappa=36% in\nY0.9(Ca)0.1Ba2Cu3O6+x to kappa=33% in Y0.8(Ca)0.2Ba2Cu3O6+x. We estimate that\nin these three systems the wavelength of charge corrugations in long chains (at\nx=1) should be ranking around lambda=1.38nm, lambda=1.25nm, lambda=1.20nm,\nrespectively.\n",
        "  The paper describes the Egyptian Arabic-to-English statistical machine\ntranslation (SMT) system that the QCRI-Columbia-NYUAD (QCN) group submitted to\nthe NIST OpenMT'2015 competition. The competition focused on informal dialectal\nArabic, as used in SMS, chat, and speech. Thus, our efforts focused on\nprocessing and standardizing Arabic, e.g., using tools such as 3arrib and\nMADAMIRA. We further trained a phrase-based SMT system using state-of-the-art\nfeatures and components such as operation sequence model, class-based language\nmodel, sparse features, neural network joint model, genre-based\nhierarchically-interpolated language model, unsupervised transliteration\nmining, phrase-table merging, and hypothesis combination. Our system ranked\nsecond on all three genres.\n",
        "  To expedite the commissioning process of the proton therapy system at Samsung\nMedical Center (SMC), we have developed a Monte Carlo simulation model of the\nproton therapy nozzles using TOPAS. At SMC proton therapy center, we have two\ngantry rooms with different types of nozzles; a multi-purpose nozzle and a\ndedicated scanning nozzle. Each nozzle has been modeled in detail following the\ngeometry information provided by the manufacturer, Sumitomo Heavy Industries,\nLtd. For this purpose, novel features of TOPAS, such as the time feature or the\nridge filter class, have been used. And the appropriate physics models for\nproton nozzle simulation were defined. Dosimetric properties, like percent\ndepth dose curve, spread-out Bragg peak (SOBP), beam spot size, have been\nsimulated and verified against measured beam data. Beyond the Monte Carlo\nnozzle modeling, we have developed an interface between TOPAS and the treatment\nplanning system (TPS), RayStation. An exported RT plan data from the TPS has\nbeen interpreted by the interface and then translated into the TOPAS input\ntext. The developed Monte Carlo nozzle model can be used to estimate non-beam\nperformance of the nozzles such as the neutron background. Furthermore, the\nnozzle model can be used to study mechanical optimization in the design of the\nnozzle.\n",
        "  Pseudo-bulges are expected to markedly differ from classical,\nquasi-monolithically forming bulges in their star formation history (SFH) and\nchemical abundance patterns. To test this simple expectation, we carry out a\ncomparative structural and spectral synthesis analysis of 106 red, massive\ngalaxies issued from the SDSS, subdivided into bulgeless, pseudo-bulge and\nclassical bulge galaxies according to their photometric characteristics, and\nfurther obeying a specific selection to minimize uncertainties in the analysis\nand ensure an unbiased derivation and comparison of SFHs. Our 2D photometry\nanalysis suggests that disks underlying pseudo-bulges typically have larger\nexponential scale lengths than bulgeless galaxies, despite similar integral\ndisk luminosities. Spectral synthesis models of the stellar emission within the\n3\" SDSS fiber aperture reveal a clear segregation of bulgeless and pseudo-bulge\ngalaxies from classical bulges on the luminosity-weighted planes of\nage-metallicity and mass-metallicity, though a large dispersion is observed\nwithin the two former classes. The secular growth of pseudo-bulges is also\nreflected upon their cumulative stellar mass as a function of time, which is\nshallower than that for classical bulges. Such results suggest that the centers\nof bulgeless and pseudo-bulge galaxies substantially differ from those of bulgy\ngalaxies with respect to their SFH and chemical enrichment history, which\nlikely points to different formation/assembly mechanisms.\n",
        "  We propose DuoRC, a novel dataset for Reading Comprehension (RC) that\nmotivates several new challenges for neural approaches in language\nunderstanding beyond those offered by existing RC datasets. DuoRC contains\n186,089 unique question-answer pairs created from a collection of 7680 pairs of\nmovie plots where each pair in the collection reflects two versions of the same\nmovie - one from Wikipedia and the other from IMDb - written by two different\nauthors. We asked crowdsourced workers to create questions from one version of\nthe plot and a different set of workers to extract or synthesize answers from\nthe other version. This unique characteristic of DuoRC where questions and\nanswers are created from different versions of a document narrating the same\nunderlying story, ensures by design, that there is very little lexical overlap\nbetween the questions created from one version and the segments containing the\nanswer in the other version. Further, since the two versions have different\nlevels of plot detail, narration style, vocabulary, etc., answering questions\nfrom the second version requires deeper language understanding and\nincorporating external background knowledge. Additionally, the narrative style\nof passages arising from movie plots (as opposed to typical descriptive\npassages in existing datasets) exhibits the need to perform complex reasoning\nover events across multiple sentences. Indeed, we observe that state-of-the-art\nneural RC models which have achieved near human performance on the SQuAD\ndataset, even when coupled with traditional NLP techniques to address the\nchallenges presented in DuoRC exhibit very poor performance (F1 score of 37.42%\non DuoRC v/s 86% on SQuAD dataset). This opens up several interesting research\navenues wherein DuoRC could complement other RC datasets to explore novel\nneural approaches for studying language understanding.\n",
        "  Difficulties in obtaining good phantoms, improvements in technologies of\nvoxel localization, better sequences for water and fat suppression has brought\nus to define a minimal Protocol of home-made quality controls of MRS systems.\nMeasurements, defined in the proposed protocol, have, as main goal, to\nestablish if peaks quantification predicts realistic concentration values,\nmeaning that, the occurrence of this event is a sufficient condition to declare\nthat MRS system works good. Moreover, stability measurements helps in a correct\ndata understanding. It is, indeed, realistic to think that environmental\ncondition can introduce casual errors in the working good system. Discrepancies\nin the working good condition, under stochastic variability (environment), have\nto be related to systematic errors introduced by the set of pre and/or\npost-processing operations and/or by any forms of MRS bad-working tool that\ndiffers from the previous. The quality control minimal protocol has been\nexecuted on a Philips-Achieva MRS system utilizing a phantom supplied by the\nmanufacturer. The minimal protocol consists of two steps: reproducibility and\nperformance tests. The reproducibility of the MRS measurements helps in\nquantifying the stability of the system. The performance test enables to\nestablish if the system is able to reproduce concentrations in a realistic way.\nIn both cases good results have been obtained: fluctuations of measured values\nare below 9% and quantification of concentration is consistent with the known\nvalues.\n",
        "  The bulge dominated galaxy NGC 7814 provides one of the strongest dynamical\ntests possible for Modified Newtonian Dynamics (MOND). Spitzer 3.6 micron\nphotometry fixes the bulge parameterisation and strongly constrains the\nproperties of the sub-dominant stellar disk. Furthermore, the distance is known\nto better than 5 percent, virtually eliminating it as a free parameter. The\nrotation curve is easily measured, since the H I (and stellar) disks are edge\non, and both the receding and approaching sides agree very well. In this paper\nwe explore the agreement between the model and observed rotation curves in MOND\ngiven that the only two free parameters available are the mass-to-light ratios\nof the bulge and disk. We use a grid based MOND Poisson solver that accurately\nsolves for the MOND gravity and produces our model rotation curves from a given\nmass distribution. The input to the Poisson solver is a 3D distribution of N\nparticles which is generated from modelling the observed distribution of stars\nand gas in the galaxy. By ensuring a superior fit to the radial surface\nbrightness profile than previous works, by virtue of a double Sersic fit to the\nbulge, we were able to produce excellent fits to the rotation curve with\ntypical values for both mass-to-light ratios. We conclude that the model\nrotation curve of a mass distribution in MOND is extremely sensitive to the\nbulge-disk decomposition and even slight deviation from the observed mass\ndistribution can produce large differences in the model rotation curve.\n",
        "  Compression can sometimes improve performance by making more of the data\navailable to the processors faster. We consider the compression of integer keys\nin a B+-tree index. For this purpose, systems such as IBM DB2 use variable-byte\ncompression over differentially coded keys. We revisit this problem with\nvarious compression alternatives such as Google's VarIntGB, Binary Packing and\nFrame-of-Reference. In all cases, we describe algorithms that can operate\ndirectly on compressed data. Many of our alternatives exploit the\nsingle-instruction-multiple-data (SIMD) instructions supported by modern CPUs.\nWe evaluate our techniques in a database environment provided by Upscaledb, a\nproduction-quality key-value database. Our best techniques are SIMD\naccelerated: they simultaneously reduce memory usage while improving\nsingle-threaded speeds. In particular, a differentially coded SIMD\nbinary-packing techniques (BP128) can offer a superior query speed (e.g., 40%\nbetter than an uncompressed database) while providing the best compression\n(e.g., by a factor of ten). For analytic workloads, our fast compression\ntechniques offer compelling benefits. Our software is available as open source.\n",
        "  Non-linear models recently receive a lot of attention as people are starting\nto discover the power of statistical and embedding features. However,\ntree-based models are seldom studied in the context of structured learning\ndespite their recent success on various classification and ranking tasks. In\nthis paper, we propose S-MART, a tree-based structured learning framework based\non multiple additive regression trees. S-MART is especially suitable for\nhandling tasks with dense features, and can be used to learn many different\nstructures under various loss functions.\n  We apply S-MART to the task of tweet entity linking --- a core component of\ntweet information extraction, which aims to identify and link name mentions to\nentities in a knowledge base. A novel inference algorithm is proposed to handle\nthe special structure of the task. The experimental results show that S-MART\nsignificantly outperforms state-of-the-art tweet entity linking systems.\n",
        "  Proof nets are a graph theoretical representation of proofs in various\nfragments of type-logical grammar. In spite of this basis in graph theory,\nthere has been relatively little attention to the use of graph theoretic\nalgorithms for type-logical proof search. In this paper we will look at several\nways in which standard graph theoretic algorithms can be used to restrict the\nsearch space. In particular, we will provide an O(n4) algorithm for selecting\nan optimal axiom link at any stage in the proof search as well as a O(kn3)\nalgorithm for selecting the k best proof candidates.\n",
        "  Radiation therapy with carbon ions is a novel technique of cancer\nradiotherapy, applicable in particular to treating radioresistant tumours at\ndifficult localisations. Therapy planning, where the medical physicist,\nfollowing the medical prescription, finds the optimum distribution of cancer\ncells to be inactivated by their irradiation over the tumour volume, is a basic\nprocedure of cancer radiotherapy. The main difficulty encountered in therapy\nplanning for ion radiotherapy is to correctly account for the enhanced\nradiobiological effectiveness of ions in the Spread Out Bragg Peak (SOBP)\nregion over the tumour volume. In this case, unlike in conventional\nradiotherapy with photon beams, achieving a uniform dose distribution over the\ntumour volume does not imply achieving uniform cancer cell inactivation.\n  In this thesis, an algorithm of the basic element (kernel) of a treatment\nplanning system (TPS) for carbon ion therapy is developed. The algorithm\nconsists of a radiobiological part which suitably corrects for the enhanced\nbiological effect of ion irradiation of cancer cells, and of a physical beam\ntransport model. In the radiobiological component, Katz's track structure model\nof cellular survival is applied, after validating its physical assumptions and\nimproving some aspects of this model. The Katz model offers fast and accurate\npredictions of cell survival in mixed fields of the primary carbon ions and of\ntheir secondary fragments. The physical beam model was based on available\ntabularized data, prepared earlier by Monte Carlo simulations. Both components\nof the developed TPS kernel are combined within an optimization tool, allowing\nthe entrance energy-fluence spectra of the carbon ion beam to be selected in\norder to achieve a pre-assumed uniform (flat) depth-survival profile over the\nSOBP region, assuring uniform cancer cell inactivation over the tumour depth.\n",
        "  We construct instanton Floer homology for lens spaces $L(p,q)$. As an\napplication, we prove that $X = \\CP^2 # \\CP^2$ does not admit a decomposition\n$X = X_1 \\cup X_2$. Here $X_1$ and $X_2$ are oriented, simply connected,\nnon-spin 4-manifolds with $b^+ = 1$ and with boundary $L(p, 2)$, and $p$ is a\nprime number of the form $16N+1$.\n",
        "  Nested regular expressions (NREs) have been proposed as a powerful formalism\nfor querying RDFS graphs, but research in a more general graph database context\nhas been scarce, and static analysis results are currently lacking. In this\npaper we investigate the problem of containment of NREs, and show that it can\nbe solved in PSPACE, i.e., the same complexity as the problem of containment of\nregular expressions or regular path queries (RPQs).\n",
        "  Much of the inner Milky Way's (MW) global rotation and velocity dispersion\npatterns can be reproduced by models of secularly-evolved, bar-dominated\nbulges. More sophisticated constraints, including the higher moments of the\nline-of-sight velocity distributions (LOSVDs) and limits on the chemodynamical\nsubstructure, are critical for interpreting observations of the unresolved\ninner regions of extragalactic systems and for placing the MW in context with\nother galaxies. Here, we use SDSS-APOGEE data to develop these constraints, by\npresenting the first maps of the LOSVD skewness and kurtosis of metal-rich and\nmetal-poor inner MW stars (divided at [Fe/H] = -0.4), and comparing the\nobserved patterns to those that are seen both in N-body models and in\nextragalactic bars. Despite closely matching the mean velocity and dispersion,\nthe models do not reproduce the observed LOSVD skewness patterns in different\nways, which demonstrates that our understanding of the detailed orbital\nstructure of the inner MW remains an important regime for improvement. We find\nevidence in the MW of the skewness-velocity correlation that is used as a\ndiagnostic of extragalactic bar/bulges. This correlation appears in metal-rich\nstars only, providing further evidence for different evolutionary histories of\nchemically differentiated populations. We connect these skewness measurements\nto previous work on high-velocity \"peaks\" in the inner Galaxy, confirming the\npresence of that phenomenon, and we quantify the cylindrical rotation of the\ninner Galaxy, finding that the latitude-independent rotation vanishes outside\nof lon ~ 7 deg. Finally, we evaluate the MW data in light of select\nextragalactic bar diagnostics and discuss progress and challenges of using the\nMW as a resolved analog of unresolved stellar populations.\n",
        "  A preliminary iterative 3D meso-scale structural model of the femur was\ndeveloped, in which bar and shell elements were used to represent trabecular\nand cortical bone respectively. The cross-sectional areas of the bar elements\nand the thickness values of the shell elements were adjusted over successive\niterations of the model based on a target strain stimulus, resulting in an\noptimised construct. The predicted trabecular architecture, and cortical\nthickness distribution showed good agreement with clinical observations, based\non the application of a single leg stance load case during gait. The benefit of\nusing a meso-scale structural approach in comparison to micro or macro-scale\ncontinuum approaches to predictive bone modelling was achievement of the\nsymbiotic goals of computational efficiency and structural description of the\nfemur.\n",
        "  The effect of smooth inhomogeneities near a superconductor boundary on the\nmagnetic penetration depth $\\lambda$ is studied with emphasis on the\nproximity-induced spatial dependence of the Cooper pair amplitude. The\ninfluence of surface pair breaking or pair formation on $\\lambda$ is described\nwithin the Ginzburg-Landau theory, with no model assumptions, for both strongly\ntype-II and strongly type-I homogeneous superconductors. Generic values of\n$\\lambda$, which can differ greatly from the London penetration depth, are\nidentified and demonstrated to be induced by large-scale inhomogeneities, when\nsuperconductivity is strongly suppressed on the surface.\n",
        "  This paper presents a method to register a pre-operative Computed-Tomography\n(CT) volume to a sparse set of intra-operative Ultra-Sound (US) slices. In the\ncontext of percutaneous renal puncture, the aim is to transfer planning\ninformation to an intra-operative coordinate system. The spatial position of\nthe US slices is measured by optically localizing a calibrated probe. Assuming\nthe reproducibility of kidney motion during breathing, and no deformation of\nthe organ, the method consists in optimizing a rigid 6 Degree Of Freedom (DOF)\ntransform by evaluating at each step the similarity between the set of US\nimages and the CT volume. The correlation between CT and US images being\nnaturally rather poor, the images have been preprocessed in order to increase\ntheir similarity. Among the similarity measures formerly studied in the context\nof medical image registration, Correlation Ratio (CR) turned out to be one of\nthe most accurate and appropriate, particularly with the chosen non-derivative\nminimization scheme, namely Powell-Brent's. The resulting matching transforms\nare compared to a standard rigid surface registration involving segmentation,\nregarding both accuracy and repeatability. The obtained results are presented\nand discussed.\n",
        "  Metapopulation theory for a long time has assumed dispersal to be symmetric,\ni.e. patches are connected through migrants dispersing bi-directionally without\na preferred direction. However, for natural populations symmetry is often\nbroken, e.g. for species in the marine environment dispersing through the\ntransport of pelagic larvae with ocean currents. The few recent studies of\nasymmetric dispersal concluded, that asymmetry has a distinct negative impact\non the persistence of metapopulations. Detailed analysis however revealed, that\nthese previous studies might have been unable to properly disentangle the\neffect of symmetry from other potentially confounding properties of dispersal\npatterns. We resolve this issue by systematically investigating the symmetry of\ndispersal patterns and its impact on metapopulation persistence. Our main\nanalysis based on a metapopulation model equivalent to previous studies but now\napplied on regular dispersal patterns aims to isolate the effect of dispersal\nsymmetry on metapopulation persistence. Our results suggest, that asymmetry in\nitself does not imply negative effects on metapopulation persistence. For this\nreason we recommend to investigate it in connection with other properties of\ndispersal instead of in isolation.\n",
        "  By evaluating the Burau representation at t=-1, we obtain a symplectic\nrepresentation of the braid group. We define the congruence subgroups of the\nbraid group to be the preimages of the principal congruence subgroups of the\nsymplectic group. Our main result is that the level four congruence subgroup of\nthe braid group is equal to the group generated by squares of Dehn twists and\nis also equal to the group generated by squares of pure braids. We also compute\nthe image of the point pushing subgroup under the symplectic representation.\n",
        "  We develop a novel next generation light-weight highly flexible pediatric\ncoil array, combine it with a high-density pediatric posterior array or\nconventional posterior phased array, and determine feasibility of pediatric\nclinical use. A highly flexible 16 element MRI receiver coil was constructed\nwith low-profile noise controlling preamplifiers that minimized reactive and\nresistive coupling. Element decoupling was assessed in flat and highly flexed\nstates. With IRB approval and informed consent and assent, 24 consecutive\nsubjects undergoing torso or extremity MRI were prospectively recruited. Care\nteam members were surveyed on preference for the coil versus conventional coils\nand diagnostic acceptability of the images was recorded. Confidence interval of\nproportion of diagnostic exams was calculated. The array without cable weighed\n480 grams, demonstrated good flexibility while maintaining element decoupling.\nThe coil was preferred by all nurses and anesthesiologists involved in the care\nof the patients. Technologists preferred the coil in 96 percent of cases, and\n23 of 24 exams were diagnostically adequate with 85 percent confidence interval\nof 90-100 percent. Light-weight highly flexible coil arrays can be constructed\nthat maintain element decoupling. Pediatric clinical image quality is likely to\nbe diagnostic, with acceptance by members of the care team.\n",
        "  Galaxy populations at different cosmic epochs are often linked together by\ncomoving cumulative number density in observational studies. Many theoretical\nworks, however, have shown that the number densities of tracked galaxy\npopulations evolve in bulk and spread out over time. We present a number\ndensity method for linking progenitor and descendant galaxy populations which\ntakes both of these effects into account. We define probability distribution\nfunctions that capture the evolution and dispersion of galaxy populations in\ncomoving number density space, and use these functions to assign galaxies at\none redshift $z_f$ probabilities of being progenitors or descendants of a\ngalaxy population at another redshift $z_0$. These probabilities are then used\nas weights for calculating distributions of physical properties such as stellar\nmass, star formation rate, or velocity dispersion within the\nprogenitor/descendant population. We demonstrate that this probabilistic method\nprovides more accurate predictions for the evolution of physical properties\nthen either the assumption of a constant number density or the assumption of an\nevolving number density in a bin of fixed width by comparing the predictions\nagainst galaxy populations directly tracked through a cosmological simulation.\nWe find that the constant number density method performs most poorly at\nrecovering galaxy properties, the evolving number method density slightly\nbetter, and the probabilistic number density method best of all. The\nimprovement is present for predictions of both stellar mass as well as inferred\nquantities such as star formation rate and velocity dispersion which were not\nincluded in the number density fits. We demonstrate that this method can also\nbe applied robustly and easily to observational data, and provide a code\npackage for doing so.\n",
        "  This study experimentally investigates the temperature dependence of\nsuperheating field, Hsh, of niobium. Accurately determining this field is\nimportant both to test theory and to understand gradient limits in\nsuperconducting cavities for particle accelerators. This paper discusses\ntheories that have been proposed in modeling the field and discriminates\nbetween them. The experimental procedure for measuring the temperature\ndependence of Hsh utilizes high power pulses to drive a niobium cavity\nresonator, ramping up surface magnetic fields extremely quickly. The moment any\npart of the cavity transitions between the superconducting and normal\nconducting state can be determined by measuring the quality factor of the\ncavity as a function of time. Oscillating superleak transducers are used to\ndemonstrate that the transition to the normal conducting state is global in\nnature, showing that a fundamental limit is encountered. Finally, we see that\n110-120 C heat treatment of the cavity--a method commonly used to increase the\nquality factor at high accelerating gradients--may have the deleterious effect\nof reducing the superheating field of the material, which is the fundamental\nlimiting factor in pursuing the maximal achievable accelerating gradient in\nsuperconducting niobium cavities.\n",
        "  GPS enables mobile devices to continuously provide new opportunities to\nimprove our daily lives. For example, the data collected in applications\ncreated by Uber or Public Transport Authorities can be used to plan\ntransportation routes, estimate capacities, and proactively identify low\ncoverage areas. In this paper, we study a new kind of query-Reverse k Nearest\nNeighbor Search over Trajectories (RkNNT), which can be used for route planning\nand capacity estimation. Given a set of existing routes DR, a set of passenger\ntransitions DT, and a query route Q, a RkNNT query returns all transitions that\ntake Q as one of its k nearest travel routes. To solve the problem, we first\ndevelop an index to handle dynamic trajectory updates, so that the most\nup-to-date transition data are available for answering a RkNNT query. Then we\nintroduce a filter refinement framework for processing RkNNT queries using the\nproposed indexes. Next, we show how to use RkNNT to solve the optimal route\nplanning problem MaxRkNNT (MinRkNNT), which is to search for the optimal route\nfrom a start location to an end location that could attract the maximum (or\nminimum) number of passengers based on a pre-defined travel distance threshold.\nExperiments on real datasets demonstrate the efficiency and scalability of our\napproaches. To the best of our best knowledge, this is the first work to study\nthe RkNNT problem for route planning.\n",
        "  Dynamic Contrast-Enhanced Magnetic Resonance Imaging (DCE-MRI) is a valuable\ntool to localize, characterize, and evaluate anomalous prostate tissue.\nUltrafast gradient-echo acquisitions of MRI volumes are generated at regular\ntime intervals while the patient receives a paramagnetic contrast agent. The\nresult is a time series where each voxel exhibits a particular behavior of\ncontrast uptake (wash-in) and posterior wash-out. In this work, a segmentation\nmethod is proposed that groups together neighboring voxels with similar\ncontrast wash-out responses, using a variant of the region growing GrowCut\ncellular automaton algorithm, that evolves iteratively according to the\nrelationship of a cell to its neighboring cells. Initially, seed cells are\ndetermined through morphological filters that identify homogeneous regions in\nthe volume that are representatives of the clinically relevant types of tissues\nin the prostate. Each cell is characterized by three parameters: a label that\nidentifies the type of tissue in the associated voxel, a vector with the values\nof the DCE-MRI time series, and a strength coefficient with values between 0\nand 1 that represents the probability with which the cell belongs to its\nassigned label. Every non-seed cell can modify its state; this occurs when each\ncell is attacked by its neighbors with a strength of attack that is inversely\nproportional to the similarity of the values of the time series between the\ncells. If the strength of the attacked cell is less than the strength of the\nattack of one of its neighbors, its state changes to the one of the attacking\ncell. The attacked cell also updates its strength making it equal to the\nstrength of the attack with which it was conquered. To perform a clinical\nvalidation of the resulting segmentations, we used various cases from the\ndatabase of The Cancer Imaging Archive (TCIA), National Cancer Institute (NCI).\n",
        "  Wireless sensor networks become integral part of our life. These networks can\nbe used for monitoring the data in various domain due to their flexibility and\nfunctionality. Query processing and optimization in the WSN is a very\nchallenging task because of their energy and memory constraint. In this paper,\nfirst our focus is to review the different approaches that have significant\nimpacts on the development of query processing techniques for WSN. Finally, we\naim to illustrate the existing approach in popular query processing engines\nwith future research challenges in query optimization.\n",
        "  Correctness of SQL queries is usually tested by executing the queries on one\nor more datasets. Erroneous queries are often the results of small changes, or\nmutations of the correct query. A mutation Q' of a query Q is killed by a\ndataset D if Q(D) $\\neq$ Q'(D). Earlier work on the XData system showed how to\ngenerate datasets that kill all mutations in a class of mutations that included\njoin type and comparison operation mutations.\n  In this paper, we extend the XData data generation techniques to handle a\nwider variety of SQL queries and a much larger class of mutations. We have also\nbuilt a system for grading SQL queries using the datasets generated by XData.\nWe present a study of the effectiveness of the datasets generated by the\nextended XData approach, using a variety of queries including queries submitted\nby students as part of a database course. We show that the XData datasets\noutperform predefined datasets as well as manual grading done earlier by\nteaching assistants, while also avoiding the drudgery of manual correction.\nThus, we believe that our techniques will be of great value to database course\ninstructors and TAs, particularly to those of MOOCs. It will also be valuable\nto database application developers and testers for testing SQL queries.\n",
        "  Suppose that the 3-manifold M is given by integral surgery along a link L in\nS^3. In the following we construct a stable map from M to the plane, whose\nsingular set is canonically oriented. We obtain upper bounds for the minimal\nnumbers of crossings and non-simple singularities and of connected components\nof fibers of stable maps from M to the plane in terms of properties of L.\n",
        "  This work studies comparatively two typical sentence matching tasks: textual\nentailment (TE) and answer selection (AS), observing that weaker phrase\nalignments are more critical in TE, while stronger phrase alignments deserve\nmore attention in AS. The key to reach this observation lies in phrase\ndetection, phrase representation, phrase alignment, and more importantly how to\nconnect those aligned phrases of different matching degrees with the final\nclassifier. Prior work (i) has limitations in phrase generation and\nrepresentation, or (ii) conducts alignment at word and phrase levels by\nhandcrafted features or (iii) utilizes a single framework of alignment without\nconsidering the characteristics of specific tasks, which limits the framework's\neffectiveness across tasks. We propose an architecture based on Gated Recurrent\nUnit that supports (i) representation learning of phrases of arbitrary\ngranularity and (ii) task-specific attentive pooling of phrase alignments\nbetween two sentences. Experimental results on TE and AS match our observation\nand show the effectiveness of our approach.\n",
        "  The stress caused by vortices in tetragonal superconductors contributes to\nthe intervortex interaction which depends on vortex orientation within the\ncrystal, on elastic moduli, and is attractive within certain angular regions\neven in fields along the $c$ crystal axis. For sufficiently strong stress\ndependence of the critical temperature, this contribution may result in\ndistortions of the hexagonal vortex lattice for $\\bm H||\\bm c$. In small fields\nit leads to formation of a square vortex lattice with a fixed $H$ independent\nspacing. This should be seen in the magnetization $M(H)$ as a discontinuous\njump of magnetization at the transition from the Meissner to mixed states.\n",
        "  Solanum melongena (eggplant) is one of the diversity of the Solanum family\nwhich is grown and widely spread in Indonesia and widely used by the community.\nThis research explored the genetic diversity of four local Indonesian eggplant\nspecies namely leuca, tekokak, gelatik and kopek by using RAPD (Random\nAmplified Polymorphic DNA). The samples were obtained from Agricultural\nTechnology Assessment Institute (BPTP) Bogor, Indonesia. The result of data\nobservation was in the form of Solanum melongena plants DNA profile analyzed\ndescriptively and quantitatively. 30 DNA bands (28 polymorphic and 2\nmonomorphic) were successfully scored by using four primers (OPF01, OPF02,\nOPF03 and OPF04). The Primers were used able to amplify all of the four\neggplant samples. The result of PCR-RAPD visualization produces bands of\n300-1500 bp. The result of cluster analysis showed the existence of three\nclusters (A, B, and C). Cluster A (coefficient of equal to 49%) consisted of a\ngelatik, cluster B (coefficient of 65% equilibrium) consisted of TPU (Kopek)\nand TK (Tekokak), and cluster C (55% equilibrium coefficient) consisted of LC\n(Leunca). These results indicated that the closest proximity is found in\nsamples of TK (Tekokak) and TPU (Kopek)\n",
        "  We performed a systematic analysis of the 4.67 $\\mu$m CO ro-vibrational\nabsorption band toward nearby active galactic nuclei (AGNs) and analyzed the\nabsorption profiles of ten nearby galaxies collected from the AKARI and Spitzer\nspectroscopic observations that show the CO absorption feature by fitting a\nplane-parallel local thermal equilibrium gas model. We found that CO gas is\nwarm (200--500 K) and has a large column density\n($N_\\mathrm{H}\\gtrsim10^{23}~\\mathrm{cm^{-2}}$). The heating of the gas is not\nexplicable by either UV heating or shock heating because these processes cannot\nrepresent the large column densities of the warm gas. Instead, X-ray photons\nfrom the nuclei, which can produce large columns of warm gas with up to\n$N_\\mathrm{H}\\sim10^{24}~\\mathrm{cm^{-2}}$, are the most convincing power\nsource. The hydrogen column density estimated from the CO band is smaller than\nthat inferred from X-ray observations. These results indicate that the region\nprobed by the near-infrared CO absorption is in the vicinity of the nuclei and\nis located outside the X-ray emitting region. Furthermore, the covering factors\nclose to unity required by the observed deep absorption profiles suggest that\nthe probed region is close to the continuum source, which can be designated as\nthe inner rim of the obscuring material around the AGN.\n",
        "  Different types of data skew can result in load imbalance in the context of\nparallel joins under the shared nothing architecture. We study one important\ntype of skew, join product skew (JPS). A static approach based on frequency\nclasses is proposed which takes for granted the data distribution of join\nattribute values. It comes from the observation that the join selectivity can\nbe expressed as a sum of products of frequencies of the join attribute values.\nAs a consequence, an appropriate assignment of join sub-tasks, that takes into\nconsideration the magnitude of the frequency products can alleviate the join\nproduct skew. Motivated by the aforementioned remark, we propose an algorithm,\ncalled Handling Join Product Skew (HJPS), to handle join product skew.\n",
        "  We investigate the question of when distinct branched surfaces in the\ncomplement of a 2-bridge knot support essential surfaces with identical\nboundary slopes. We determine all instances in which this occurs and identify\nan infinite family of knots for which no boundary slopes are repeated.\n",
        "  The properties of dephasing and the resulting relaxation of the magnetization\nare the basic principle on which all magnetic resonance imaging methods are\nbased. The signal obtained from the gyrating spins is essentially determined by\nthe properties of the considered tissue. Especially the susceptibility\ndifferences caused by magnetized materials (for example, deoxygenated blood,\nBOLD-effect) or magnetic nanoparticles are becoming more important for\nbiomedical imaging. In the present work, the influence of such field\ninhomogeneities on the NMR-signal is analyzed.\n",
        "  A century on from its discovery, a complete fundamental understanding of\nsuperconductivity is still missing. Considerable research efforts are currently\ndevoted to elucidating mechanisms by which pairs of electrons can bind together\nthrough the mediation of a boson field different than the one associated to the\nvibrations of a crystal lattice. PuCoGa_5, a 5f-electron heavy-fermion\nsuperconductor with a record critical temperature T_c=18.5 K, is one of the\nmany compounds for which the short-range, isotropic attraction provided by\nsimple electron-phonon coupling does not appear as an adequate glue for\nelectron pairing. Here, we report the results of point-contact spectroscopy\nmeasurements in single crystals of PuCoGa_5. Andreev reflection structures are\nclearly observed in the low-temperature spectra, and unambiguously prove that\nthe paired superconducting electrons have wavefunction with the d-wave symmetry\nof a four-leaf clover. A straightforward analysis of the spectra provide the\namplitude of the gap and its temperature dependence, \\Delta(T). We obtain\n\\Delta(T -> 0) = 5.1 \\pm 0.3 meV and a gap ratio, 2\\Delta/k_B T_c = 6.5 \\pm\n0.3, indicating that the compound is in the regime of strong electron-boson\ncoupling. The gap value and its temperature dependence can be well reproduced\nwithin the Eliashberg theory for superconductivity if the spectral function of\nthe mediating bosons has a spin-fluctuations-like shape, with a peak energy of\n6.5 meV. Electronic structure calculations, combining the local density\napproximation with an exact diagonalization of the Anderson impurity model,\nprovide a hint about the possible origin of the fluctuations.\n",
        "  In this article, we present an approach for non native automatic speech\nrecognition (ASR). We propose two methods to adapt existing ASR systems to the\nnon-native accents. The first method is based on the modification of acoustic\nmodels through integration of acoustic models from the mother tong. The\nphonemes of the target language are pronounced in a similar manner to the\nnative language of speakers. We propose to combine the models of confused\nphonemes so that the ASR system could recognize both concurrent\npronounciations. The second method we propose is a refinment of the\npronounciation error detection through the introduction of graphemic\nconstraints. Indeed, non native speakers may rely on the writing of words in\ntheir uttering. Thus, the pronounctiation errors might depend on the characters\ncomposing the words. The average error rate reduction that we observed is\n(22.5%) relative for the sentence error rate, and 34.5% (relative) in word\nerror rate.\n",
        "  Medical ultrasound scanners are typically calibrated to the soft tissue\naverage of 1540 m s$^{-1}$. In regions of different sound speed, for example,\norgans and tumours, the $B$-scan image then becomes a distortion of the true\ntissue cross-section, due to the misrepresentation of length and refraction. To\nquantify this distortion we develop a general geometric ray model for an object\nwith an atypical speed of sound embedded in an ambient medium. We analyse the\nensuing area distortion for circular and elliptical objects, mapping it out as\na function of the key parameters, including the speed of sound mismatch, the\nobject size and its elongation. We find that the area distortion can become\nsignificant, even for small-scale speed of sound mismatches. Our findings are\nverified by ultrasound imaging of a test object.\n",
        "  We experimentally investigate the temperature dependence of Rabi oscillations\nand Ramsey fringes in superconducting phase qubits driven by microwave pulses.\nIn a wide range of temperatures, we find that both the decay time and the\namplitude of these coherent oscillations remain nearly unaffected by thermal\nfluctuations. The oscillations are observed well above the crossover\ntemperature from thermally activated escape to quantum tunneling for undriven\nqubits. In the two-level limit, coherent qubit response rapidly vanishes as\nsoon as the energy of thermal fluctuations kT becomes larger than the energy\nlevel spacing of the qubit. Our observations shed new light on the origin of\ndecoherence in superconducting qubits. The experimental data suggest that,\nwithout degrading already achieved coherence times, phase qubits can be\noperated at temperatures much higher than those reported till now.\n",
        "  We investigate the physical conditions of the gas, atomic and molecular, in\nthe filaments in the context of Photo-Dissociation Regions (PDRs) using the\nKOSMA-PDR mode of clumpy clouds. We also compare the [CII] vs. [NII] integrated\nintensity predictions in Abel et al. 2005 for HII regions and adjacent PDRs in\nthe Galactic disk, and check for their applicability under the extreme physical\nconditions present in the GC. Our preliminary results show that observed\nintegrated intensities are well reproduced by the PDR model. The gas is exposed\nto a relatively low Far-UV field between 10^2 - 10^3 Draine fields. The total\nvolume hydrogen density is well constrained between 10^4 - 10^5 cm^-3. The\nhydrogen ionization rate due to cosmic-rays varies between 10^-15 and 4x10^-15\ns^-1, with the highest value ~ 10^-14 s^-1 found towards G0.07+0.04. Our\nresults show that the line-of-sight contribution to the total distance of the\nfilaments to the Arches Cluster is not negligible. The spatial distribution of\nthe [CII]/[NII] ratio shows that the integrated intensity ratios are fairly\nhomogeneously distributed for values below 10 in energy units. Calculations\nincluding variation on the [C/N] abundance ratio show that tight constraints on\nthis ratio are needed to reproduce the observations.\n",
        "  The advancement of mobile technologies and the proliferation of map-based\napplications have enabled a user to access a wide variety of services that\nrange from information queries to navigation systems. Due to the popularity of\nmap-based applications among the users, the service provider often requires to\nanswer a large number of simultaneous queries. Thus, processing queries\nefficiently on spatial networks (i.e., road networks) have become an important\nresearch area in recent years. In this paper, we focus on path queries that\nfind the shortest path between a source and a destination of the user. In\nparticular, we address the problem of finding the shortest paths for a large\nnumber of simultaneous path queries in road networks. Traditional systems that\nconsider one query at a time are not suitable for many applications due to high\ncomputational and service costs. These systems cannot guarantee required\nresponse time in high load conditions. We propose an efficient group based\napproach that provides a practical solution with reduced cost. The key concept\nfor our approach is to group queries that share a common travel path and then\ncompute the shortest path for the group. Experimental results show that our\napproach is on an average ten times faster than the traditional approach in\nreturn of sacrificing the accuracy by 0.5% in the worst case, which is\nacceptable for most of the users.\n",
        "  Although Kirby and Siebenmann showed that there are manifolds that do not\nadmit PL structures, the possibility remained that all manifolds could be\ntriangulated. In the late seventies Galewski and Stern and independently\nMatumoto showed that non-triangulable manifolds exist in all dimensions > 4 if\nand only if homology 3-spheres with certain properties do not exist. In 2013\nManolescu showed that, indeed, there were no such homology 3-spheres and hence,\nnon-triangulable manifolds exist in each dimension >4. It follows from work of\nFreedman in 1982 that there are 4-manifolds that cannot be triangulated. In\n1991 Davis and Januszkiewicz applied a hyperbolization procedure to Freedman's\n4-manifolds to get closed aspherical 4-manifolds that cannot be triangulated.\nIn this paper we apply hyperbolization techniques to the Galewski-Stern\nmanifolds to show that there exist closed aspherical n-manifolds that cannot be\ntriangulated for each n> 5. The question remains open in dimension 5.\n",
        "  We present experimental results on the crosstalk between two AC-operated\ndispersive bifurcation detectors, implemented in a circuit for high-fidelity\nreadout of two strongly coupled flux qubits. Both phase-dependent and\nphase-independent contributions to the crosstalk are analyzed. For proper\ntuning of the phase the measured crosstalk is 0.1 % and the correlation between\nthe measurement outcomes is less than 0.05 %. These results show that\nbifurcative readout provides a reliable and generic approach for multi-partite\ncorrelation experiments.\n",
        "  Nanoscale superconducting quantum interference devices (SQUIDs) demonstrate\nrecord sensitivities to small magnetic moments, but are typically sensitive\nonly to the field component that is normal to the plane of the SQUID and\nout-of-plane with respect to the scanned surface. We report on a nanoscale\nthree-junction Pb SQUID which is fabricated on the apex of a sharp tip. Because\nof its three-dimensional structure, it exhibits a unique tunable sensitivity to\nboth in-plane and out-of-plane fields. We analyze the two-dimensional\ninterference pattern from both numerical and experimental points of view. This\ndevice is integrated into a scanning microscope and its ability to\nindependently measure the different components of the magnetic field with\noutstanding spin sensitivity better than $5\\ \\frac{\\mu_B}{\\mathrm{Hz}^{1/2}}$\nis demonstrated. This highlights its potential as a local probe of nanoscale\nmagnetic structures.\n",
        "  When computation is outsourced, the data owner would like to be assured that\nthe desired computation has been performed correctly by the service provider.\nIn theory, proof systems can give the necessary assurance, but prior work is\nnot sufficiently scalable or practical. In this paper, we develop new proof\nprotocols for verifying computations which are streaming in nature: the\nverifier (data owner) needs only logarithmic space and a single pass over the\ninput, and after observing the input follows a simple protocol with a prover\n(service provider) that takes logarithmic communication spread over a\nlogarithmic number of rounds. These ensure that the computation is performed\ncorrectly: that the service provider has not made any errors or missed out some\ndata. The guarantee is very strong: even if the service provider deliberately\ntries to cheat, there is only vanishingly small probability of doing so\nundetected, while a correct computation is always accepted. We first observe\nthat some theoretical results can be modified to work with streaming verifiers,\nshowing that there are efficient protocols for problems in the complexity\nclasses NP and NC. Our main results then seek to bridge the gap between theory\nand practice by developing usable protocols for a variety of problems of\ncentral importance in streaming and database processing. All these problems\nrequire linear space in the traditional streaming model, and therefore our\nprotocols demonstrate that adding a prover can exponentially reduce the effort\nneeded by the verifier. Our experimental results show that our protocols are\npractical and scalable.\n",
        "  The multi relational data mining approach has developed as an alternative way\nfor handling the structured data such that RDBMS. This will provides the mining\nin multiple tables directly. In MRDM the patterns are available in multiple\ntables (relations) from a relational database. As the data are available over\nthe many tables which will affect the many problems in the practice of the data\nmining. To deal with this problem, one either constructs a single table by\nPropositionalisation, or uses a Multi-Relational Data Mining algorithm. MRDM\napproaches have been successfully applied in the area of bioinformatics. Three\npopular pattern finding techniques classification, clustering and association\nare frequently used in MRDM. Multi relational approach has developed as an\nalternative for analyzing the structured data such as relational database. MRDM\nallowing applying directly in the data mining in multiple tables. To avoid the\nexpensive joining operations and semantic losses we used the MRDM technique.\nThis paper focuses some of the application areas of MRDM and feature directions\nas well as the comparison of ILP, GM, SSDM and MRDM\n",
        "  We investigate the superconductivity in two-dimensional electron systems\nformed in SrTiO3 nanostructures. Our the- oretical analysis is based on the\nthree-orbital model, which takes into account t2g orbitals of Ti ions. Because\nof the interfacial breaking of mirror symmetry, a Rashba-type antisymmetric\nspin-orbit coupling arises from the cooperation of intersite and interorbital\nhybridyzation and atomic LS coupling. This model shows a characteristic spin\ntexture and carrier density dependence of Rashba spin-orbit coupling through\nthe orbital degree of freedom. Superconductivity is mainly caused by heavy\nquasiparticles consisting of dyz and dzx orbitals at high carrier densities. We\nfind that the Rashba spin-orbit coupling stabilizes a quasi-one-dimensional\nsuperconducting phase caused by one of the dyz or dzx orbitals at high magnetic\nfields along interfaces. This quasi-one-dimensional superconducting phase is\nprotected against para- magnetic depairing effects by the Rashba spin-orbit\ncoupling and realizes a large upper critical field Hc2 beyond the\nPauli-Clogston-Chandrasekhar limit. This finding is consistent with an\nextraordinarily large upper critical field observed in SrTiO3 /LaAlO3\ninterfaces and its carrier density dependence. The possible coexistence of\nsuperconductivity and fer- romagnetism in SrTiO3 /LaAlO3 interfaces may also be\nattributed to this quasi-one-dimensional superconducting phase.\n",
        "  We present an approach to event coreference resolution by developing a\ngeneral framework for clustering that uses supervised representation learning.\nWe propose a neural network architecture with novel Clustering-Oriented\nRegularization (CORE) terms in the objective function. These terms encourage\nthe model to create embeddings of event mentions that are amenable to\nclustering. We then use agglomerative clustering on these embeddings to build\nevent coreference chains. For both within- and cross-document coreference on\nthe ECB+ corpus, our model obtains better results than models that require\nsignificantly more pre-annotated information. This work provides insight and\nmotivating results for a new general approach to solving coreference and\nclustering problems with representation learning.\n",
        "  We consider a 2-complex in a particular form, called the Quinn model of a\n2-complex. It can be sliced in graphs, where a change from one graph to another\ncan be organized by a sequence of local transitions, which are described in a\nlist of F. Quinn [Q1]. The decomposition of that 2-complex into graphs has to\nbe translated into an algebraic context (for example Topological Quantum field\ntheory (TQFT)) to construct suitable potential invariants under 3-deformations.\nThese invariants are accessible for computation by using a supercomputer and\nthe results may yield a counterexample to the Andrews-Curtis conjecture. To\nachieve invariance under 3-deformations, there are obvious topological\nrelations among the local transitions, for example to deform a bubble out of a\nrectangle. In this paper our main result is that we contribute a complete list\nof such topological relations in a totally geometric fashion. One outcome of\nour considerations is that the corresponding list of F. Quinn [Q1] is extended\nby an additional relation which takes care of locally changing a slicing. We do\nnot know so far whether this relation is a consequence of the remaining ones.\nBut it may be crucial for further work to focus on such subtleties, as\nalgebraic \"simplifications\", where this question is bypassed, so far have been\nunable to distinguish between simple homotopy and 3-deformations at all. In our\nintroduction we summarize some known results on the situation when passing to\nAlgebra; and in {\\S} 8 we calculate an example of an algebraic TQFT in order to\ndemonstrate that our additional relation holds. All considerations are carried\nout for 2-complexes with two generators and two defining relators. But the\nresults also hold in the general case.\n",
        "  The effect of Zn substitution in the CuO2 plane on the superconducting\ntransition temperature, Tc, was studied for the La2-xSrxCu1-yZnyO4 and\nYBa2(Cu1-yZny)3O7-d compounds over a wide range of hole concentration, p, and\nZn content (y). Zn induced rate of suppression of Tc, dTc(p)/dy, was found to\nbe strongly p-dependent and showed a monotonic variation with p, except in the\nvicinity of p ~ 0.125, i.e., near the so-called 1/8th anomaly where the\ncharge/spin stripe correlations are at their strongest in hole doped cuprates.\nThe magnitude of dTc(p)/dy decreased significantly around this hole\nconcentration implying that Zn suddenly became less effective in degrading Tc\nnear the 1/8th anomaly for La2-xSrxCu1-yZnyO4. The same feature, somewhat at a\nreduced scale, was also observed for YBa2(Cu1-yZny)3O7-d compounds. This is\ncounterintuitive since static stripe order itself degrades superconducting\norder. We have discussed the possible scenarios that can give rise to such a\nnon-monotonic dTc(p)/dy near p ~ 0.125. We have also looked at the p-dependent\ncharacteristic pseudogap energy scale, {\\epsilon}g(p), which shows a nearly\nlinear decrease with increasing p without any noticeable feature at p ~ 0.125.\nMoreover, there is no significant effect of Zn content on the value of\n{\\epsilon}g(p). All these observations are indicative of a complex and possibly\ncompeting interplay among the superconducting, pseudogap, and stripe\ncorrelations in the hole doped cuprates.\n",
        "  An Unreliable news is any piece of information which is false or misleading,\ndeliberately spread to promote political, ideological and financial agendas.\nRecently the problem of unreliable news has got a lot of attention as the\nnumber instances of using news and social media outlets for propaganda have\nincreased rapidly. This poses a serious threat to society, which calls for\ntechnology to automatically and reliably identify unreliable news sources. This\npaper is an effort made in this direction to build systems for detecting\nunreliable news articles. In this paper, various NLP algorithms were built and\nevaluated on Unreliable News Data 2017 dataset. Variants of hierarchical\nattention networks (HAN) are presented for encoding and classifying news\narticles which achieve the best results of 0.944 ROC-AUC. Finally, Attention\nlayer weights are visualized to understand and give insight into the decisions\nmade by HANs. The results obtained are very promising and encouraging to deploy\nand use these systems in the real world to mitigate the problem of unreliable\nnews.\n",
        "  We present BOND, a Bayesian code to simultaneously derive oxygen and nitrogen\nabundances in giant H II regions. It compares observed emission lines to a grid\nof photoionization models without assuming any relation between O/H and N/O.\nOur grid spans a wide range in O/H, N/O and ionization parameter U, and covers\ndifferent starburst ages and nebular geometries. Varying starburst ages\naccounts for variations in the ionizing radiation field hardness, which arise\ndue to the ageing of H II regions or the stochastic sampling of the initial\nmass function. All previous approaches assume a strict relation between the\nionizing field and metallicity. The other novelty is extracting information on\nthe nebular physics from semi-strong emission lines. While strong lines ratios\nalone ([O III]/Hbeta, [O II]/Hbeta and [N II]/Hbeta) lead to multiple O/H\nsolutions, the simultaneous use of [Ar III]/[Ne III] allows one to decide\nwhether an H II region is of high or low metallicity. Adding He I/Hbeta pins\ndown the hardness of the radiation field. We apply our method to H II regions\nand blue compact dwarf galaxies, and find that the resulting N/O vs O/H\nrelation is as scattered as the one obtained from the temperature-based method.\nAs in previous strong-line methods calibrated on photoionization models, the\nBOND O/H values are generally higher than temperature-based ones, which might\nindicate the presence of temperature fluctuations or kappa distributions in\nreal nebulae, or a too soft ionizing radiation field in the models.\n",
        "  The large reservoir of antibiotic resistant bacteria in raw and treated water\nsupplies is a matter of public health concern. Currently, the National\nAntimicrobial Resistance Monitoring Systems, a collaborative effort of the\nCenters for Disease Control, the US Department of Agriculture, and the US Food\nand Drug Administration, does not monitor antimicrobial resistance in surface\nwaters. Given the serious nature of antibiotic resistance in clinical settings,\nand the likelihood that antibiotic resistant bacteria can be transmitted to\nhumans from large environmental reservoirs via drinking water, explanations for\nthe distribution of antibiotic resistant bacteria and tools for studying this\ndistribution must be found. Here we focus on mathematical modeling of\ncultivable bacteria in a river, which will be used to study the distribution of\nantibiotic resistant bacteria in the environment. We consider both antibiotic\nresistant and non-antibiotic resistant bacteria in the model, and, taking into\naccount the strong correlation between land use and antibiotic resistant\nbacteria in rivers, we include a function for the influx of bacteria into the\nriver from the shore. We simulate the model for two different time scales and\nshow that if there is too many bacteria from the land entering the river, the\nriver entirely fills with antibiotic resistant bacteria, while less frequent\ninfluxes allows time for the bacteria to lose the antibiotic resistant gene.\nThis mathematically verifies that reduction in antibiotic use near the banks of\nrivers, will reduce the counts of antibiotic resistant bacteria in rivers.\n",
        "  XML keyword search is a user-friendly way to query XML data using only\nkeywords. In XML keyword search, to achieve high precision without sacrificing\nrecall, it is important to remove spurious results not intended by the user.\nEfforts to eliminate spurious results have enjoyed some success by using the\nconcepts of LCA or its variants, SLCA and MLCA. However, existing methods still\ncould find many spurious results. The fundamental cause for the occurrence of\nspurious results is that the existing methods try to eliminate spurious results\nlocally without global examination of all the query results and, accordingly,\nsome spurious results are not consistently eliminated. In this paper, we\npropose a novel keyword search method that removes spurious results\nconsistently by exploiting the new concept of structural consistency.\n",
        "  Entity resolution (ER) is the problem of identifying and merging records that\nrefer to the same real-world entity. In many scenarios, raw records are stored\nunder heterogeneous environment. Specifically, the schemas of records may\ndiffer from each other. To leverage such records better, most existing work\nassume that schema matching and data exchange have been done to convert records\nunder different schemas to those under a predefined schema. However, we observe\nthat schema matching would lose information in some cases, which could be\nuseful or even crucial to ER.\n  To leverage sufficient information from heterogeneous sources, in this paper,\nwe address several challenges of ER on heterogeneous records and show that none\nof existing similarity metrics or their transformations could be applied to\nfind similar records under heterogeneous settings. Motivated by this, we design\nthe similarity function and propose a novel framework to iteratively find\nrecords which refer to the same entity. Regarding efficiency, we build an index\nto generate candidates and accelerate similarity computation. Evaluations on\nreal-world datasets show the effectiveness and efficiency of our methods.\n",
        "  To study the microscopic electronic and magnetic interactions in the\nsubstoichiometric iron chalcogenide FeSe$_{1-x}$ which is observed to\nsuperconduct at x~1/8 up to $T_c$=27 K, we use first principles methods to\nstudy the Se vacancy in this nearly magnetic FeSe system. The vacancy forms a\nferrimagnetic cluster of eight Fe atoms, which for the ordered x=1/8 alloy\nleads to half metallic conduction. Similar magnetic clusters are obtained for\nFeTe$_{1-x}$ and for BaFe$_2$As$_2$ with an As vacancy, although neither of\nthese are half metallic. Based on fixed spin density results, we suggest the\nlow energy excitations in FeSe$_{1-x}$ are antiparamagnon-like with short\ncorrelation length.\n",
        "  Recent developments in deep learning with application to language modeling\nhave led to success in tasks of text processing, summarizing and machine\ntranslation. However, deploying huge language models for mobile device such as\non-device keyboards poses computation as a bottle-neck due to their puny\ncomputation capacities. In this work we propose an embedded deep learning based\nword prediction method that optimizes run-time memory and also provides a real\ntime prediction environment. Our model size is 7.40MB and has average\nprediction time of 6.47 ms. We improve over the existing methods for word\nprediction in terms of key stroke savings and word prediction rate.\n",
        "  Phylogenetic networks model reticulate evolutionary histories. The last two\ndecades have seen an increased interest in establishing mathematical results\nand developing computational methods for inferring and analyzing these\nnetworks. A salient concept underlying a great majority of these developments\nhas been the notion that a network displays a set of trees and those trees can\nbe used to infer, analyze, and study the network.\n  In this paper, we show that in the presence of coalescence effects, the set\nof displayed trees is not sufficient to capture the network. We formally define\nthe set of parental trees of a network and make three contributions based on\nthis definition. First, we extend the notion of anomaly zone to phylogenetic\nnetworks and report on anomaly results for different networks. Second, we\ndemonstrate how coalescence events could negatively affect the ability to infer\na species tree that could be augmented into the correct network. Third, we\ndemonstrate how a phylogenetic network can be viewed as a mixture model that\nlends itself to a novel inference approach via gene tree clustering.\n  Our results demonstrate the limitations of focusing on the set of trees\ndisplayed by a network when analyzing and inferring the network. Our findings\ncan form the basis for achieving higher accuracy when inferring phylogenetic\nnetworks and open up new venues for research in this area, including new\nproblem formulations based on the notion of a network's parental trees.\n",
        "  The gamma-index test has been commonly adopted to quantify the degree of\nagreement between a reference dose distribution and an evaluation dose\ndistribution. Monte Carlo (MC) simulation has been widely used for the\nradiotherapy dose calculation for both clinical and research purposes. The goal\nof this work is to investigate both theoretically and experimentally the impact\nof the MC statistical fluctuation on the gamma-index test when the fluctuation\nexists in the reference, the evaluation, or both dose distributions. To the\nfirst order approximation, we theoretically demonstrated in a simplified model\nthat the statistical fluctuation tends to overestimate gamma-index values when\nexisting in the reference dose distribution and underestimate gamma-index\nvalues when existing in the evaluation dose distribution given the original\ngamma-index is relatively large for the statistical fluctuation. Our numerical\nexperiments using clinical photon radiation therapy cases have shown that 1)\nwhen performing a gamma-index test between an MC reference dose and a non-MC\nevaluation dose, the average gamma-index is overestimated and the passing rate\ndecreases with the increase of the noise level in the reference dose; 2) when\nperforming a gamma-index test between a non-MC reference dose and an MC\nevaluation dose, the average gamma-index is underestimated when they are within\nthe clinically relevant range and the passing rate increases with the increase\nof the noise level in the evaluation dose; 3) when performing a gamma-index\ntest between an MC reference dose and an MC evaluation dose, the passing rate\nis overestimated due to the noise in the evaluation dose and underestimated due\nto the noise in the reference dose. We conclude that the gamma-index test\nshould be used with caution when comparing dose distributions computed with\nMonte Carlo simulation.\n",
        "  Spatial co-location patterns are the subsets of Boolean spatial features\nwhose instances are often located in close geographic proximity. Co-location\nrules can be identified by spatial statistics or data mining approaches. In\ndata mining method, Association rule-based approaches can be used which are\nfurther divided into transaction-based approaches and distance-based\napproaches. Transaction-based approaches focus on defining transactions over\nspace so that an Apriori algorithm can be used. The natural notion of\ntransactions is absent in spatial data sets which are embedded in continuous\ngeographic space. A new distance -based approach is developed to mine\nco-location patterns from spatial data by using the concept of proximity\nneighborhood. A new interest measure, a participation index, is used for\nspatial co-location patterns as it possesses an anti-monotone property. An\nalgorithm to discover co-location patterns are designed which generates\ncandidate locations and their table instances. Finally the co-location rules\nare generated to identify the patterns.\n",
        "  The data mining field is an important source of large-scale applications and\ndatasets which are getting more and more common. In this paper, we present\ngrid-based approaches for two basic data mining applications, and a performance\nevaluation on an experimental grid environment that provides interesting\nmonitoring capabilities and configuration tools. We propose a new distributed\nclustering approach and a distributed frequent itemsets generation well-adapted\nfor grid environments. Performance evaluation is done using the Condor system\nand its workflow manager DAGMan. We also compare this performance analysis to a\nsimple analytical model to evaluate the overheads related to the workflow\nengine and the underlying grid system. This will specifically show that\nrealistic performance expectations are currently difficult to achieve on the\ngrid.\n",
        "  The standard genetic code is well known to be optimized for minimizing the\nphenotypic effects of single nucleotide substitutions, a property that was\nlikely selected for during the emergence of a universal code. Given the fitness\nadvantage afforded by high standing genetic diversity in a population in a\ndynamic environment, it is possible that selection to explore a large fraction\nof the space of functional proteins also occurred. To determine whether\nselection for such a property played a role during the emergence of the nearly\nuniversal genetic code, we investigated the number of functional variants of\nthe Escherichia coli PhoQ protein explored at different time scales under\ntranslation using different genetic codes. We found that the standard genetic\ncode is highly optimal for exploring a large fraction of the space of\nfunctional PhoQ variants at intermediate time scales as compared to random\ncodes. Environmental changes, in response to which genetic diversity in a\npopulation provides a fitness advantage, are likely to have occurred at these\nintermediate time scales. Our results indicate that the ability of the standard\ncode to explore a large fraction of the space of functional sequence variants\narises from a balance between robustness and flexibility and is largely\nindependent of the property of the standard code to minimize the phenotypic\neffects of mutations. We propose that selection to explore a large fraction of\nthe functional sequence space while minimizing the phenotypic effects of\nmutations contributed towards the emergence of the standard code as the\nuniversal genetic code.\n",
        "  We introduce a simple, general strategy to manipulate the behavior of a\nneural decoder that enables it to generate outputs that have specific\nproperties of interest (e.g., sequences of a pre-specified length). The model\ncan be thought of as a simple version of the actor-critic model that uses an\ninterpolation of the actor (the MLE-based token generation policy) and the\ncritic (a value function that estimates the future values of the desired\nproperty) for decision making. We demonstrate that the approach is able to\nincorporate a variety of properties that cannot be handled by standard neural\nsequence decoders, such as sequence length and backward probability\n(probability of sources given targets), in addition to yielding consistent\nimprovements in abstractive summarization and machine translation when the\nproperty to be optimized is BLEU or ROUGE scores.\n",
        "  Two related constructions are studied: (1) The diagonal complex $\\mathcal{D}$\nand its barycentric subdivision $\\mathcal{BD}$ related to a \\textit{punctured}\noriented surface $F$ equipped with a number of labeled marked points. (2) The\nsymmetric diagonal complex $\\mathcal{D}^{inv}$ and its barycentric subdivision\n$\\mathcal{BD}^{inv}$ related to a symmetric (=with an involution) oriented\nsurface $F$ equipped with a number of (symmetrically placed) labeled marked\npoints. Eliminating a puncture gives rise to a bundle whose fibers are\nhomeomorphic to a surgery of the surface $F$. The bundle can be viewed as the\n\"universal curve with holes\". The symmetric complex is shown to be homotopy\nequivalent to the complex of a punctured surface obtained by a surgery of the\ninitial symmetric surface.\n",
        "  The nuclear and magnetic structure of Fe1+y(Te1-x,Sex) (0 < x < 0.20)\ncompounds was analyzed between 2 K and 300 K by means of Rietveld refinement of\nneutron powder diffraction data. Samples with x < 0.075 undergo a tetragonal to\nmonoclinic phase transition at low temperature, whose critical temperature\ndecreases with increasing Se content; this structural transition is strictly\ncoupled to a long range antiferromagnetic ordering at the Fe site. Both the\ntransition to a monoclinic phase and the long range antiferromagnetism are\nsuppressed for 0.10 < x < 0.20. The onset of the structural and of the magnetic\ntransition remains coincident with the increase of Se substitution. The low\ntemperature monoclinic crystal structure has been revised. Superconductivity\narises for x > 0.05, therefore a significant region where superconductivity and\nlong range antiferromagnetism coexist is present in the pseudo-binary FeTe -\nFeSe phase diagram.\n",
        "  In this paper, we study the complexity of answering conjunctive queries (CQ)\nwith inequalities). In particular, we are interested in comparing the\ncomplexity of the query with and without inequalities. The main contribution of\nour work is a novel combinatorial technique that enables us to use any\nSelect-Project-Join query plan for a given CQ without inequalities in answering\nthe CQ with inequalities, with an additional factor in running time that only\ndepends on the query. The key idea is to define a new projection operator,\nwhich keeps a small representation (independent of the size of the database) of\nthe set of input tuples that map to each tuple in the output of the projection;\nthis representation is used to evaluate all the inequalities in the query.\nSecond, we generalize a result by Papadimitriou-Yannakakis [17] and give an\nalternative algorithm based on the color-coding technique [4] to evaluate a CQ\nwith inequalities by using an algorithm for the CQ without inequalities. Third,\nwe investigate the structure of the query graph, inequality graph, and the\naugmented query graph with inequalities, and show that even if the query and\nthe inequality graphs have bounded treewidth, the augmented graph not only can\nhave an unbounded treewidth but can also be NP-hard to evaluate. Further, we\nillustrate classes of queries and inequalities where the augmented graphs have\nunbounded treewidth, but the CQ with inequalities can be evaluated in\npoly-time. Finally, we give necessary properties and sufficient properties that\nallow a class of CQs to have poly-time combined complexity with respect to any\ninequality pattern. We also illustrate classes of queries where our\nquery-plan-based technique outperforms the alternative approaches discussed in\nthe paper.\n",
        "  For each $g \\ge 2$, we prove existence of a computable constant $\\epsilon(g)\n> 0$ such that if $S$ is a strongly irreducible Heegaard surface of genus $g$\nin a complete hyperbolic 3-manifold $M$ and $\\gamma$ is a simple geodesic of\nlength less than $\\epsilon(g)$ in $M$, then $\\gamma$ is isotopic into $S$.\n",
        "  Obscured active galactic nuclei (AGNs) are thought to be very common in the\nUniverse. Observations and surveys have shown that the number of sources\nincreases for near galaxies and at the low-luminosity regime (the so-called\nLLAGNs). Furthermore, many AGNs show changes in their obscuration properties at\nX-rays that may suggest a configuration of clouds very close to the accretion\ndisk. However, these variations could also be due to changes in the intrinsic\ncontinuum of the source. It is therefore important to study nearby AGN to\nbetter understand the locus and distribution of clouds in the neighbourhood of\nthe nucleus. We aim to study the nuclear obscuration of LLAGN NGC835 and its\nextended emission using mid-infrared observations. We present mid-infrared 11.5\nmicrons imaging of the LLAGN galaxy NGC835 obtained with the instrument\nCanariCam in the Gran Telescopio CANARIAS (GTC), archival Spitzer/IRS\nspectroscopy, and archival Chandra data observed in 2000, 2008, and 2013. The\nGTC/CanariCam 11.5 microns image reveals faint extended emission out to ~6\narcsec. We obtained a nuclear flux of F(11.5 microns) ~18 mJy, whereas the\nextended emission accounts for 90% of the total flux within the 6 arcsec. This\nmeans that the low angular resolution (~4 arcsec) IRS spectrum is dominated by\nthis extended emission and not by the AGN, clearly seen in the Spitzer/IRS\nspectrum. Although the extended soft X-ray emission shows some resemblance with\nthat of the mid-infrared, the knots seen at X-rays are mostly located in the\ninner side of this mid-infrared emission. The nuclear X-ray spectrum of the\nsource has undergone a spectral change between 2000/2008 and 2013. We argue\nthat this is most probably due to changes in the hydrogen column density from ~\n8x10E+23 cm-2 to ~ 3x10E+23 cm-2. NGC835 therefore is one of the few LLAGN,\ntogether with NGC1052, in which changes in the absorber can be claimed.\n",
        "  A graph database is a database where the data structures for the schema\nand/or instances are modeled as a (labeled)(directed) graph or generalizations\nof it, and where querying is expressed by graph-oriented operations and type\nconstructors. In this article we present the basic notions of graph databases,\ngive an historical overview of its main development, and study the main current\nsystems that implement them.\n",
        "  X-ray energy spectrum plays an essential role in imaging and related tasks.\nDue to the high photon flux of clinical CT scanners, most of the spectrum\nestimation methods are indirect and are usually suffered from various\nlimitations. The recently proposed indirect transmission measurement-based\nmethod requires at least the segmentation of one material, which is\ninsufficient for CT images of highly noisy and with artifacts. To combat for\nthe bottleneck of spectrum estimation using segmented CT images, in this study,\nwe develop a segmentation-free indirect transmission measurement based energy\nspectrum estimation method using dual-energy material decomposition. The\ngeneral principle of the method is to compare polychromatic forward projection\nwith raw projection to calibrate a set of unknown weights which are used to\nexpress the unknown spectrum together with a set of model spectra. After\napplying dual-energy material decomposition using high- and low-energy raw\nprojection data, polychromatic forward projection is conducted on\nmaterial-specific images. The unknown weights are then iteratively updated to\nminimize the difference between the raw projection and estimated projection.\nBoth numerical simulations and experimental head phantom are used to evaluate\nthe proposed method. The results indicate that the method provides accurate\nestimate of the spectrum and it may be attractive for dose calculations,\nartifacts correction and other clinical applications.\n",
        "  Consistency of knowledge repositories is of prime importance in organization\nmanagement. Integrity constraints are a well-known vehicle for specifying data\nconsistency requirements in knowledge bases; in particular, active integrity\nconstraints go one step further, allowing the specification of preferred ways\nto overcome inconsistent situations in the context of database management. This\npaper describes a tool to validate an SQL database with respect to a given set\nof active integrity constraints, proposing possible repairs in case the\ndatabase is inconsistent. The tool is able to work with the different kinds of\nrepairs proposed in the literature, namely simple, founded, well-founded and\njustified repairs. It also implements strategies for parallelizing the search\nfor them, allowing the user both to compute partitions of independent or\nstratified active integrity constraints, and to apply these partitions to find\nrepairs of inconsistent databases efficiently in parallel.\n",
        "  The Yokonuma-Hecke algebras are quotients of the modular framed braid group\nand they support Markov traces. In this paper, which is sequel to Juyumaya and\nLambropoulou (2007), we explore further the structures of the $p$-adic framed\nbraids and the $p$-adic Yokonuma-Hecke algebras constructed in Juyumaya and\nLambropoulou (2007), by means of dense sub-structures approximating $p$-adic\nelements. We also construct a $p$-adic Markov trace on the $p$-adic\nYokonuma-Hecke algebras and we approximate the values of the $p$-adic trace on\n$p$-adic elements. Surprisingly, the Markov traces do not re-scale directly to\nyield isotopy invariants of framed links. This leads to imposing the\n`$E$-condition' on the trace parameters. For solutions of the `$E$-system' we\nthen define 2-variable isotopy invariants of modular framed links. These lift\nto $p$-adic isotopy invariants of classical framed links. The Yokonuma-Hecke\nalgebras have topological interpretations in the context of framed knots, of\nclassical knots of singular knots and of transverse knots.\n",
        "  This paper reviews the highlights of a 4-years-long research project\nsupported by the Japanese Government to explore new superconducting materials\nand relevant functional materials. The project found several tens of new\nsuperconductors by examining ~1000 materials, each of which was chosen by\nJapanese team member experts with a background in solid state chemistry. This\nreview summarizes the major achievements of the project in newly found\nsuperconducting materials, and the wire and tape fabrication of iron-based\nsuperconductors. It is a unique feature of this review to incorporate a list of\n~700 unsuccessful materials examined for superconductivity in the project. In\naddition, described are new functional materials and functionalities discovered\nduring the project.\n",
        "  Divergence between populations for a given trait can be driven by natural or\nsexual selection, interacting with migration behaviour. Mating preference for\ndifferent phenotypes can lead to the emergence and persistence of\ndifferentiated populations. Dominance between alleles encoding for divergent\nphenotypes can interfere in such processes. Using a diploid model of trait\ndetermining both mating success and migration rate, we explored differentiation\nbetween two connected populations, assuming either co-dominance or strict\ndominance between alleles. The model assumes that individuals prefer mating\nwith partners displaying the same phenotype and therefore tend to move to the\nother population when their own phenotype is rare. We show that the emergence\nof differentiated populations in this diploid model is limited as compared to\nresults obtained with the same model assuming haploidy. When assuming\nco-dominance, differentiation arises only when migration is limited as compared\nto preference. Such differentiation is less dependent on migration when\nassuming strict dominance between haplotypes. Dominant alleles frequently\ninvade populations because their phenotype is more frequently expressed,\nresulting in higher mating success and rapid decrease in migration. However,\ndepending on the initial distribution of alleles, this advantage associated\nwith dominance (i.e. Haldane's sieve) may lead to fixation of the dominant\nallele throughout both populations. Depending on the initial distribution of\nheterozygotes, persistence of polymorphisms within populations can also occur\nbecause heterozygotes displaying the predominant phenotype benefit from mating\npreferences. Altogether, our results highlight that heterozygotes' behaviour\nhas a strong impact on population differentiation and stress out the need of\ndiploid models of differentiation and speciation driven by natural and sexual\nselection.\n",
        "  We give new rational blowdown constructions of exotic CP^2#n(-CP^2) (5\\leq\nn\\leq 9) without using elliptic fibrations. We also show that our 4-manifolds\nadmit handle decompositions without 1- and 3-handles, for 7\\leq n\\leq 9. A\nstrategy for rational blowdown constructions of exotic CP^2#n(-CP^2) (1\\leq\nn\\leq 4) is also proposed.\n",
        "  The evolution of multicellularity was a major transition in the history of\nlife on earth. Conditions under which multicellularity is favored have been\nstudied theoretically and experimentally. But since the construction of a\nmulticellular organism requires multiple rounds of cell division, a natural\nquestion is whether these cell divisions should be synchronous or not. We study\na simple population model in which there compete simple multicellular organisms\nthat grow either by synchronous or asynchronous cell divisions. We demonstrate\nthat natural selection can act differently on synchronous and asynchronous cell\ndivision, and we offer intuition for why these phenotypes are generally not\nneutral variants of each other.\n",
        "  We present the first study of evolution of galaxy groups in the Illustris\nsimulation. We focus on dynamically relaxed and unrelaxed galaxy groups\nrepresenting dynamically evolved and evolving galaxy systems, respectively. The\nevolutionary state of a group is probed from its luminosity gap and separation\nbetween the brightest group galaxy and the center of mass of the group members.\nWe find that the Illustris simulation, over-produces large luminosity gap\ngalaxy systems, known as fossil systems, in comparison to observations and the\nprobed semi-analytical predictions. However, this simulation is equally\nsuccessful in recovering the correlation between luminosity gap and luminosity\ncentroid offset, in comparison to the probed semi-analytic model. We find\nevolutionary tracks based on luminosity gap which indicate that a large\nluminosity gap group is rooted in a small luminosity gap group, regardless of\nthe position of the brightest group galaxy within the halo. This simulation\nhelps, for the first time, to explore the black hole mass and its accretion\nrate in galaxy groups. For a given stellar mass of the brightest group\ngalaxies, the black hole mass is larger in dynamically relaxed groups with a\nlower rate of mass accretion. We find this consistent with the latest\nobservational studies of the radio activities in the brightest group galaxies\nin fossil groups. We also find that the IGM in dynamically evolved groups is\nhotter for a given halo mass than that in evolving groups, again consistent\nwith earlier observational studies.\n",
        "  Purpose: To quantitatively compare the dosimetric and biologic differences in\ntreatment plans from flattened and flattening-filter-free (FFF) beam for three\nanatomic cancer sites. Methods and Materials: Treatment plans with static\nintensity-modulated radiotherapy beams and volumetric modulated arc therapy\nbeams were generated for 13 patients for both the flattened beam and the FFF\nbeam of the TrueBeam system. Beam energies of 6 MV and 10 MV were chosen for\nplanning. A total of 104 treatment plans were generated in 13 patients. In\norder to analyze the biological effectiveness of treatment plans, dose volume\nhistograms (DVH) were utilized. Flattened and FFF beam plans are quantitatively\ncompared. Results: In head and neck cases, for VMAT plans, dose reduction in\nthe FFF beam plans compared to the flattened beam in left cochlea, right\nsubmandibular gland and right parotid gland reached up to 2.36 Gy, 1.21 Gy and\n1.45 Gy, respectively. Similarly, for static IMRT plans, the dose reduction of\nthe FFF beam plans compared to the flattened beam plans for the same organs\nreached up to 0.34 Gy, 1.36 Gy and 1.46 Gy, respectively. Overall, for head and\nneck, the FFF beam plans achieved mean dose reduction of up to 5%, 7% and 9%,\nrespectively for above organs at risk. For lung and prostate cases, the FFF\nbeams provided lower or comparable NTCP values to organ-at-risk (OAR) compared\nto the flattened beam for all plans. Conclusions: In general, we observed\ntreatment plans utilizing FFF beams can improve dose sparing to OARs without\ncompromising the target coverage. Significant dose sparing effect is obtained\nfor head and neck cancer cases, especially for the cases with relatively large\nfield sizes (about 16x20 cm^2). For lung and prostate cases, compared to the\nflattened beam, the FFF beam based treatment plans provide lower or comparable\ndose to most OARs.\n",
        "  We present multi-epoch, total intensity, high-resolution images of 43GHz,\nv=1, J=1-0 SiO maser emission toward the Mira variable R Cas. In total we have\n23 epochs of data for R Cas at approximate monthly intervals over an optical\npulsation phase range from 0.158 to 1.78. These maps show a ring-like\ndistribution of the maser features in a shell, which is assumed to be centred\non the star at a radius of 1.6 to 2.3 times the stellar radii. It is clear from\nthese images that the maser emission is significantly extended around the star.\nAt some epochs a faint outer arc can be seen at 2.2 stellar radii. The\nintensity of the emission waxes and wanes during the stellar phase. Some maser\nfeatures are seen infalling as well as outflowing. We have made initial\ncomparisons of our data with models by Gray et. al. (2009).\n",
        "  We present the structural, electronic and superconducting properties of Li2B2\nunder pressure within the framework of the density functional theory. The\nstructural parameters, electronic band structure, phonon frequency of the E2g\nphonon mode and superconducting critical temperature Tc were calculated for\npressures up to 20 GPa. We predicted that the superconducting critical\ntemperature of Li2B2 is about 11 K and this decreases as pressure increases. We\nfound that even though the lattice dynamics of the E2g phonon mode is similar\nto MgB2, the reduction of the {\\sigma}-band density of states at Fermi level\nand the raising of the E2g phonon frequency with pressure were determinant to\ndecrease {\\lambda} and consequently Tc.\n",
        "  We show the triviality of representations of the mapping class group of a\ngenus $g$ surface in $GL(n,C), Diff(S^2)$ and $Homeo(T^2)$ when appropriate\nrestrictions on the genus $g$ and the size of $n$ hold. For example, if $S_g$\nis a surface of finite type and $\\phi : MCG(S_g) \\to GL(n,C)$ is a\nhomomorphism, then $\\phi$ is trivial provided the genus $g \\ge 3$ and $n < 2g$.\nWe also show that if $S_g$ is a closed surface with genus $g \\ge 7$, then every\nhomomorphism $\\phi: MCG(S_g) \\to Diff(S^2)$ is trivial and that if $g \\ge 3$,\nthen every homomorphism $\\phi: MCG(S_g) \\to Homeo(T^2)$ is trivial.\n",
        "  We report a high resolution neutron diffraction investigation of the coupling\nof structural and magnetic transitions in Ba1xKxFe2As2. The\ntetragonal-orthorhombic and antiferromagnetic transitions are suppressed with\npotassium-doping, falling to zero at x <~ 0.3. However, unlike Ba(Fe1xCox)2As2,\nthe two transitions are first-order and coincident over the entire phase\ndiagram, with a biquadratic coupling of the two order parameters. The phase\ndiagram is refined showing that the onset of superconductivity is at x = 0.133\nwith all three phases coexisting until x >~ 0.24.\n",
        "  With the advance of Web Services technologies and the emergence of Web\nServices into the information space, tremendous opportunities for empowering\nusers and organizations appear in various application domains including\nelectronic commerce, travel, intelligence information gathering and analysis,\nhealth care, digital government, etc. In fact, Web services appear to be s\nsolution for integrating distributed, autonomous and heterogeneous information\nsources. However, as Web services evolve in a dynamic environment which is the\nInternet many changes can occur and affect them. A Web service is affected when\none or more of its associated information sources is affected by schema\nchanges. Changes can alter the information sources contents but also their\nschemas which may render Web services partially or totally undefined. In this\npaper, we propose a solution for integrating information sources into Web\nservices. Then we tackle the Web service synchronization problem by\nsubstituting the affected information sources. Our work is illustrated with a\nhealthcare case study.\n",
        "  Building speech recognizers in multiple languages typically involves\nreplicating a monolingual training recipe for each language, or utilizing a\nmulti-task learning approach where models for different languages have separate\noutput labels but share some internal parameters. In this work, we exploit\nrecent progress in end-to-end speech recognition to create a single\nmultilingual speech recognition system capable of recognizing any of the\nlanguages seen in training. To do so, we propose the use of a universal\ncharacter set that is shared among all languages. We also create a\nlanguage-specific gating mechanism within the network that can modulate the\nnetwork's internal representations in a language-specific way. We evaluate our\nproposed approach on the Microsoft Cortana task across three languages and show\nthat our system outperforms both the individual monolingual systems and systems\nbuilt with a multi-task learning approach. We also show that this model can be\nused to initialize a monolingual speech recognizer, and can be used to create a\nbilingual model for use in code-switching scenarios.\n",
        "  The heart rate variability (HRV) in diabetic human subjects, has been\nanalyzed using lagged Poincar\\'{e} plot, auto-correlation and the detrended\nfluctuation analysis methods. The parameters $SD1$, and $SD12 (= SD1/SD2)$ for\nPoincar\\'{e} plot for diabetic are lower than that for non-diabetic subjects\nand reverse is case for $SD2$ for all lagged number (m). The slope and the\ncurvature of the plot SD12 vs m is much reduced for diabetic subject. The\nscatter plot of two successive interbeat intervals points out smaller\nvariability in diabetic heart. The detrended fluctuation exponent has a higher\nvalue for diabetic group. The auto-correlation function of the deviation of\ninterbeat interval in diabetic group shows highly correlated pattern when\ncompared to that of normal one. The study suggests that the curvature of $SD12$\nand auto-correlation method appear to be better way to assess the alteration of\nregulatory system on heart dynamics in diabetic condition.\n",
        "  We present unresolved single stellar population synthesis models in the\nnear-infrared (NIR) range. The extension to the NIR is important for the study\nof early-type galaxies, since these galaxies are predominantly old and\ntherefore emit most of their light in this wavelength range. The models are\nbased on a library of empirical stellar spectra, the NASA infrared telescope\nfacility (IRTF) spectral library. Integrating these spectra along theoretical\nisochrones, while assuming an initial mass function (IMF), we have produced\nmodel spectra of single age-metallicity stellar populations at a resolution\nR~2000. These models can be used to fit observed spectral of globular clusters\nand galaxies, to derive their age distribution, chemical abundances and IMF.\nThe models have been tested by comparing them to observed colours of elliptical\ngalaxies and clusters in the Magellanic Clouds. Predicted absorption line\nindices have been compared to published indices of other elliptical galaxies.\nThe comparisons show that our models are well suited for studying stellar\npopulations in unresolved galaxies. They are particularly useful for studying\nthe old and intermediate-age stellar populations in galaxies, relatively free\nfrom contamination of young stars and extinction by dust. These models will be\nindispensable for the study of the upcoming data from JWST and extremely large\ntelescopes, such as the E-ELT.\n",
        "  Taylor's Law (TL) relates the variance to the mean of a random variable via\npower law. In ecology it applies to populationsand it is a common empirical\npattern shared among different ecosystems. Measurements give power law exponent\nto be between 1 and 2, and more often to cluster around 2, whereas theoretical\nmodels predict TL exponent can assume any real value. In this paper, adopting\nthe framework of multiplicative growth models in a Markovian environment, we\ninvestigate the possibility of evolutionary strategies to be responsible for TL\nexponent to be in a finite range. We implement three different strategies the\nindividuals can follow and for each strategy set two different optimization\ninvestment objectives. In all the studied cases we find TL exponent can assume\nany real value due to the existence of regions of the model parameters in which\nthe exponent can diverge. Furthermore, under natural hypothesis on the dynamics\nof the environment, the shapes of these regions do not depend on different\nstrategies adopted and nor on the optimization objective. Thus the introduction\nof strategies dose not affect the range of TL exponent in the model. In our\ntheoretical framework rare events are shaping the value of the TL exponent,\nsuggesting, as hinted by previous works, that empirical values may be a\nstatistical artifact following from under sampling.\n",
        "  Isothermal magnetic field dependence of magnetization and the magnetic\nrelaxation measurements were performed for $H$$\\parallel$c axis on single\ncrystal of Ba(Fe$_{0.935}$Co$_{0.065}$)$_2$As$_2$ pnictide superconductor\nhaving $T_c$ = 21.7 K. The second magnetization peak (SMP) for each isothermal\n$M(H)$ was observed in a wide temperature range from $T_c$ to the lowest\ntemperature of measurement (2 K). Magnetic field dependence of relaxation rate\n$R(H)$, shows a peak (H$_{spt}$) between H$_{on}$ (onset of SMP in $M(H)$) and\nH$_p$ (peak field of SMP in $M(H)$), which is likely to be related with a\nvortex-lattice structural phase transition, as suggested in literature for\nsimilar sample. In addition, the magnetic relaxation measured for magnetic\nfields near H$_{spt}$ show some noise which might be the signature of the\nstructural phase transition of the vortex lattice. Analysis of the magnetic\nrelaxation data using Maley's criterion and the collective pinning theory\nsuggests that the second magnetization peak (SMP) in the sample is due to the\ncollective (elastic) to plastic creep crossover, which is also accompanied with\na rhombic to square vortex lattice phase transition. Analysis of the pinning\nforce density suggests single dominating pinning mechanism in the sample and is\nnot showing the usual $\\delta$l and $\\delta T_c$ nature of pinning. The\ncritical current density ($J_c$) estimated using the Bean's critical state\nmodel is found to be 5 $\\times$ 10$^5$ A/cm$^2$ at 2 K in the zero magnetic\nfield limit. Surprisingly, the maximum in the pinning force density is not\nresponsible for the maximum value of the critical current density in the\nsample.\n",
        "  We demonstrate that in a d-wave superconductor the bulk nonlinear Meissner\neffect is dominated by a surface effect due to Andreev bound states at low\ntemperatures. The contribution of this surface effect to the nonlinear response\ncoefficient follows a 1/T^3 law with opposite sign compared to the bulk 1/T\nbehavior. The cross-over from bulk dominated behavior to surface dominated\nbehavior occurs at a temperature of T/T_c ~ 1/sqrt(kappa). We present an\napproximate analytical calculation, which supports our numerical calculations\nand provides a qualitative understanding of the effect. The effect can be\nprobed by intermodulation distortion experiments.\n",
        "  Interactions are ubiquitous across biological systems. These interactions can\nbe abstracted as patterns of connections among distinct units such as genes,\nproteins, individual organisms, or species which form a hierarchy of biological\norganisation. Connections in this hierarchy are arranged in a nested structure:\ngene and protein networks shape phenotypic traits and together constitute\nindividuals, individuals are embedded within populations, populations within\ncommunities, and communities within ecosystems. This pervasive \"nestedness\" of\nnetworks can result in propagation of the effects from within-level\ninteractions at one level to units at higher or lower levels of organization.\nThe concept of nested biological networks is implicit in a variety of\ndisciplines ranging from the study of genetic circuits regulating phenotypic\ntrait expression (Babu et al. 2004), to the study of predator-prey interactions\ninfluencing community composition (Poisot et al. 2016). However, studies\ntypically only address interactions within and among directly neighbouring\nhierarchical levels, such as genotypes and phenotypic traits, or populations\nand communities. Here, we formalise nested networks as having nodes that can\ncontain, or be embedded in, other nodes, and where edges can bridge connections\nbetween sets of embedded nodes. We then argue that explicitly accounting for\nnetwork nestedness across levels of organization will encourage integrative\nthinking on new interdisciplinary research fronts. We focus on two phenomena in\nparticular: (i) indirect connections among units can arise from the structure\nof connections at higher or lower levels of organisation, (ii) the propagation\nof effects across neighbouring hierarchical levels of organization. This\nframework of nested interaction networks provides a tool for researchers across\ndisciplines to conceptualize their work as elements on a common scaffold.\n",
        "  We show that the only closed 4-manifolds admitting genus two trisections are\n$S^2 \\times S^2$ and connected sums of $S^1 \\times S^3$, $\\mathbb{CP}^2$, and\n$\\overline{\\mathbb{CP}}^2$ with two summands. Moreover, each of these manifolds\nadmits a unique genus two trisection up to diffeomorphism. The proof relies\nheavily on the combinatorics of genus two Heegaard diagrams of $S^3$. As a\ncorollary, we classify two-component links contained in a genus two Heegaard\nsurface for $S^3$ with a surface-sloped cosmetic Dehn surgery.\n",
        "  The effect of the local environment on the evolution of dwarf spheroidal\ngalaxies is poorly understood. We have undertaken a suite of simulations to\ninvestigate the tidal impact of the Milky Way on the chemodynamical evolution\nof dwarf spheroidals that resemble present day classical dwarfs using the SPH\ncode GEAR. After simulating the models through a large parameter space of\npotential orbits the resulting properties are compared with observations from\nboth a dynamical point of view, but also from the, often neglected, chemical\npoint of view. In general, we find that tidal effects quench the star formation\neven inside gas-endowed dwarfs. Such quenching, may produce the radial\ndistribution of dwarf spheroidals from the orbits seen within large\ncosmological simulations. We also find that the metallicity gradient within a\ndwarf is gradually erased through tidal interactions as stellar orbits move to\nhigher radii. The model dwarfs also shift to higher $\\langle$[Fe/H]$\\rangle$/L\nratios, but only when losing $>$$20\\%$ of stellar mass.\n",
        "  Conversations in social media often contain the use of irony or sarcasm, when\nthe users say the opposite of what they really mean. Irony markers are the\nmeta-communicative clues that inform the reader that an utterance is ironic. We\npropose a thorough analysis of theoretically grounded irony markers in two\nsocial media platforms: $Twitter$ and $Reddit$. Classification and frequency\nanalysis show that for $Twitter$, typographic markers such as emoticons and\nemojis are the most discriminative markers to recognize ironic utterances,\nwhile for $Reddit$ the morphological markers (e.g., interjections, tag\nquestions) are the most discriminative.\n",
        "  Superconducting (Li1-xFex)OHFe1-ySe films are attractive for both the basic\nresearch and practical application. However, the conventional vapor deposition\ntechniques are not applicable in synthesizing the films of such a complex\nsystem. So no intrinsic charge transport measurements on the films are\navailable so far to reveal the nature of charge carriers, which is fundamental\nto understanding the iron-based superconductivity mechanism. Herein we report a\nsoft chemical film technique (matrix-assisted hydrothermal epitaxial growth),\nby which we have succeeded in growing a series of (Li1-xFex)OHFe1-ySe films\ncovering the whole superconducting regime, with the superconducting transition\ntemperature (Tc) from 4 K up to 42 K. This film technique opens up a new way\nfor fabricating other complex functional materials as well. Furthermore, our\nsystematic transport investigation on the film samples indicates that both the\nelectron and hole carriers contribute to the charge transport, with the\nscattering rates deviating from the Fermi liquid. We find that the\nsuperconductivity occurs upon the electron and hole mobility becoming\ndivergent. And in the high Tc samples, the electron carriers are found much\nmore mobile than the holes, a feature distinct from the low Tc samples. Hence,\nour transport results provide key insights into the underlying physics for\niron-based high-Tc superconductivity.\n",
        "  Starting from the (apparently) elementary problem of deciding how many\ndifferent topological spaces can be obtained by gluing together in pairs the\nfaces of an octahedron, we will describe the central role played by hyperbolic\ngeometry within three-dimensional topology. We will also point out the striking\ndifference with the two-dimensional case, and we will review some of the\nresults of the combinatorial and computational approach to three-manifolds\ndeveloped by different mathematicians over the last several years.\n",
        "  An extended Kleinian group whose orientation-preserving half is a Schottky\ngroup is called an extended Schottky group. These groups correspond to the real\npoints in the Schottky space. Their geometric structures is well known and it\npermits to provide information on the locus of fixed points of symmetries of\nhandlebodies. A group generated by two different extended Schottky groups, both\nwith the same orientation-preserving half, is called a dihedral extended\nSchottky group. We provide a structural description of these type of groups\nand, as a consequence, we obtain sharp upper bounds for the sum of the\ncardinalities of the connected components of the locus of fixed points of two\nor three different symmetries of a handlebody.\n",
        "  Twenty cases of different brain pathology have been studied via MRI using an\nopen resistive magnet with magnetic field strength of 0.2 Tesla. The relative\nsignal intensity with respect to the repetition time (TR) at fixed echo time\n(TE) 0.117 sec. has been studied. It was found that the signal intensity\nsaturates for most lesions beyond a certain TR~6 sec in the T2 - weighted\nimage. The signal intensity differs with respect to the inversion time (TI) for\nfat and cerebrospinal fluid (CSF). It was found that the intensity is nulled\nfor CSF at TI ~1.5 sec. and for Fat at TI~0.10 sec in the FLAIR imaging\nsequence. Thus the intensity of the lesions is qualitatively different for the\ntwo sequences. From the radiological diagnostic point of view, it was concluded\nthat the FLAIR sequence is more useful for the detection of lesions compared to\nT2 sequences.\n",
        "  We present a Submillimeter Array study in the 1.3 mm waveband of the NGC 7538\nIRS 1--3 massive star-forming region. The brightest core in the mm continuum\nmap, MM1, harbors the IRS 1 young O star. The core has a gas temperature of\nabout 245 K and shows spatially unresolved emission in complex organic\nmolecules, all typical of a hot molecular core. Toward MM1, redshifted\nabsorption is seen in molecular lines with different energies above the ground\nstate. This absorption probes inward motion of the dense gas toward the central\nyoung O star, and the estimated mass accretion rate reaches 10^{-3} Msun/yr.\nMultiple outflows are seen in the CO and 13CO maps. The gas mass of 50 Msun and\nmass outflow rate of 2.5 by 10^{-3} Msun/yr measured in CO line wings are\ndominated by the MM1 outflow, which is most likely driven by a fast wide-angle\nwind. Apart from MM1, we discover eight new dusty cores, MM2--9, within a\nprojected distance of 0.35 pc from MM1. These cores show no counterpart in\ninfrared or radio continuum emission, while seven of them appear to be forming\nintermediate- to high-mass stars. This manifests a deeply embedded star-forming\ncomponent of the parent cloud of IRS 1--3. Apparently we are observing a\nTrapezium system in formation, and the system is presumably surrounded by a\ncluster of lower mass stars.\n",
        "  Nuclear rings at centers of barred galaxies exhibit strong star formation\nactivities. They are thought to undergo gravitational instability when\nsufficiently massive. We approximate them as rigidly-rotating isothermal\nobjects and investigate their gravitational instability. Using a\nself-consistent field method, we first construct their equilibrium sequences\nspecified by two parameters: alpha corresponding to the thermal energy relative\nto gravitational potential energy, and R_B measuring the ellipticity or ring\nthickness. Unlike in the incompressible case, not all values of R_B yield an\nisothermal equilibrium, and the range of R_B for such equilibria shrinks with\ndecreasing alpha. The density distributions in the meridional plane are steeper\nfor smaller alpha, and well approximated by those of infinite cylinders for\nslender rings. We also calculate the dispersion relations of nonaxisymmetric\nmodes in rigidly-rotating slender rings with angular frequency Omega_0 and\ncentral density rho_max. Rings with smaller alpha are found more unstable with\na larger unstable range of the azimuthal mode number. The instability is\ncompletely suppressed by rotation when Omega_0 exceeds the critical value. The\ncritical angular frequency is found to be almost constant at ~ 0.7\nsqrt(G*rho_c) for alpha > 0.01 and increases rapidly for smaller alpha. We\napply our results to a sample of observed star-forming rings and confirm that\nrings without a noticeable azimuthal age gradient of young star clusters are\nindeed gravitationally unstable.\n",
        "  We investigate the fishtail effect, critical current density ($J_c$) and\nvortex dynamics in LiFeAs single crystals. The sample exhibits a second peak\n(SP) in the magnetization loop only with the field $||$ c-axis. We calculate a\nreasonably high $J_c$, however, values are lower than in 'Ba-122' and\n'1111'-type FeAs-compounds. Magnetic relaxation data imply a strong pinning\nwhich appears not to be due to conventional defects. Instead, its behavior is\nsimilar to that of the triplet superconductor Sr$_2$RuO$_4$. Our data suggest\nthat the origin of the SP may be related to a vortex lattice phase transition.\nWe have constructed the vortex phase diagram for LiFeAs on the\nfield-temperature plane.\n",
        "  The largest amplitude light curves for both RR Lyrae (RRL) variables and\nclassical Cepheids with periods less than 10 days and greater than 20 days\noccur at the blue edge of the respective instability strips. It is shown that\nthe equation for the decrease in amplitude with penetration into the strip from\nthe blue edge, and hence the amplitude fine structure within the strip, is the\nsame for RRL and the Cepheids despite their metallicity differences. However,\nthe manifestation of this identity is different between the two classes of\nvariables because the sampling of the RRL strip is restricted by the discrete\nstrip positions of the horizontal branch, a restriction that is absent for the\nCepheids in stellar aggregates with a variety of ages.\n  To show the similarity of the strip amplitude fine structure for RRL and\nCepheids we make a grid of lines of constant amplitude in the HR diagram of the\nstrip using amplitude data for classical Cepheids in the Galaxy, LMC, and SMC.\nThe model implicit in the grid, that also contains lines of constant period, is\nused to predict the correlations between period, amplitude, and color for the\ntwo Oosterhoff RRL groups in globular clusters. The good agreement of the\npredictions with the observations using the classical Cepheid amplitude fine\nstructure also for the RRL shows one aspect of the unity of the pulsation\nprocesses between the two classes of variables.\n",
        "  Surface Electromyography (sEMG) is a technology to measure the bio-potentials\nacross the muscles. The true prospective of this technology is yet to be\nexplored. In this paper, a simple and economic construction of a sEMG sensor is\nproposed. These sensors are used to determine the differences in the\nElectromyography (EMG) signal patterns of different individuals. Signals of\nseveral volunteers from different age groups, gender and individual having\nparalysis have been obtained. The sEMG data acquisition is done using the\nsoundcard of a computer, hence reducing the need of additional hardware.\nFinally, the data is used to analyse the relationship between electromyography\nand factors like age, gender and health condition i.e. paralysis.\n",
        "  Cloud computing allows for vast computational resources to be leveraged\nquickly and easily in bursts as and when required. Using the Amazon Elastic\nCompute Cloud and the Amazon Simple Storage Solution, we describe a technique\nthat allows for Monte Carlo radiotherapy dose calculations to be performed\nusing GEANT4 and executed in the cloud. Simulation cost and completion time was\nevaluated as a function of instance count using compute instances acquired via\nbiding on the Elastic Compute Cloud spot market. Bidding for instances on the\ninstance spot market was found to be 35-60% of the cost of on-demand instances\nof the same type. Using the technique, we demonstrate the potential usefulness\nof cloud computing as a solution for rapid Monte Carlo simulation for\nradiotherapy dose calculation.\n",
        "  Using the colored Kauffman skein relation, we study the highest and lowest\n$4n$ coefficients of the $n^{th}$ unreduced colored Jones polynomial of\nalternating links. This gives a natural extension of a result by Kauffman in\nregard with the Jones polynomial of alternating links and its highest and\nlowest coefficients. We also use our techniques to give a new and natural proof\nfor the existence of the tail of the colored Jones polynomial for alternating\nlinks.\n",
        "  We calcuate electronic spin susceptibility and spin-lattice relaxation rate\nin singlet superconductor near a pairbreaking surface, or in a domain wall of\nthe order parameter. We directly link presence of high-density Andreev bound\nstates in the inhomogeneous region, combined with coherence factors, to\nenhancement of the susceptibility above the normal state's value for certain\n$\\bf q$ vectors. Beside the dominant peak at ferromagnetic vector $q=0$, we\nfind significant enhancement of antiferromagnetic correlations at vectors\n$q\\lesssim 2 k_f$, with $\\bf q$ $along$ the domain wall in $S$-wave\nsuperconductor, and $across$ domain wall in $D$-wave (nodes along the wall).\nThese features are destroyed by applying moderate Zeeman field that splits the\nzero-energy peak. We solve Bogoliubov-de Gennes equations in momentum space and\nour results deviate from the lattice models investigated previously. Large\nenhancement of the spin-lattice relaxation rate $T_1^{-1}$ at the domain wall\nprovides clear signature of the quasiparticle bound states, and is in good\nagreement with recent experiment in organic superconductor\n$\\kappa$-(BEDT-TTF)$_2$Cu(NCS)$_2$.\n",
        "  Explaining why an answer is in the result of a query or why it is missing\nfrom the result is important for many applications including auditing,\ndebugging data and queries, and answering hypothetical questions about data.\nBoth types of questions, i.e., why and why-not provenance, have been studied\nextensively. In this work, we present the first practical approach for\nanswering such questions for queries with negation (first-order queries). Our\napproach is based on a rewriting of Datalog rules (called firing rules) that\ncaptures successful rule derivations within the context of a Datalog query. We\nextend this rewriting to support negation and to capture failed derivations\nthat explain missing answers. Given a (why or why-not) provenance question, we\ncompute an explanation, i.e., the part of the provenance that is relevant to\nanswer the question. We introduce optimizations that prune parts of a\nprovenance graph early on if we can determine that they will not be part of the\nexplanation for a given question. We present an implementation that runs on top\nof a relational database using SQL to compute explanations. Our experiments\ndemonstrate that our approach scales to large instances and significantly\noutperforms an earlier approach which instantiates the full provenance to\ncompute explanations.\n",
        "  With the wide development of databases in general and data warehouses in\nparticular, it is important to reduce the tasks that a database administrator\nmust perform manually. The aim of auto-administrative systems is to\nadministrate and adapt themselves automatically without loss (or even with a\ngain) in performance. The idea of using data mining techniques to extract\nuseful knowledge for administration from the data themselves has existed for\nsome years. However, little research has been achieved. This idea nevertheless\nremains a very promising approach, notably in the field of data warehousing,\nwhere queries are very heterogeneous and cannot be interpreted easily. The aim\nof this study is to search for a way of extracting useful knowledge from stored\ndata themselves to automatically apply performance optimization techniques, and\nmore particularly indexing techniques. We have designed a tool that extracts\nfrequent itemsets from a given workload to compute an index configuration that\nhelps optimizing data access time. The experiments we performed showed that the\nindex configurations generated by our tool allowed performance gains of 15% to\n25% on a test database and a test data warehouse.\n",
        "  The existing solutions to privacy preserving publication can be classified\ninto the theoretical and heuristic categories. The former guarantees provably\nlow information loss, whereas the latter incurs gigantic loss in the worst\ncase, but is shown empirically to perform well on many real inputs. While\nnumerous heuristic algorithms have been developed to satisfy advanced privacy\nprinciples such as l-diversity, t-closeness, etc., the theoretical category is\ncurrently limited to k-anonymity which is the earliest principle known to have\nsevere vulnerability to privacy attacks. Motivated by this, we present the\nfirst theoretical study on l-diversity, a popular principle that is widely\nadopted in the literature. First, we show that optimal l-diverse generalization\nis NP-hard even when there are only 3 distinct sensitive values in the\nmicrodata. Then, an (l*d)-approximation algorithm is developed, where d is the\ndimensionality of the underlying dataset. This is the first known algorithm\nwith a non-trivial bound on information loss. Extensive experiments with real\ndatasets validate the effectiveness and efficiency of proposed solution.\n",
        "  Recently, word representation has been increasingly focused on for its\nexcellent properties in representing the word semantics. Previous works mainly\nsuffer from the problem of polysemy phenomenon. To address this problem, most\nof previous models represent words as multiple distributed vectors. However, it\ncannot reflect the rich relations between words by representing words as points\nin the embedded space. In this paper, we propose the Gaussian mixture skip-gram\n(GMSG) model to learn the Gaussian mixture embeddings for words based on\nskip-gram framework. Each word can be regarded as a gaussian mixture\ndistribution in the embedded space, and each gaussian component represents a\nword sense. Since the number of senses varies from word to word, we further\npropose the Dynamic GMSG (D-GMSG) model by adaptively increasing the sense\nnumber of words during training. Experiments on four benchmarks show the\neffectiveness of our proposed model.\n",
        "  The widespread adoption of on-board volumetric imaging in cancer radiotherapy\nhas stimulated research efforts to develop online adaptive radiotherapy\ntechniques to handle the inter-fraction variation of the patient's geometry.\nSuch efforts face major technical challenges to perform treatment planning in\nreal-time. To overcome this challenge, we are developing a supercomputing\nonline re-planning environment (SCORE) at the University of California San\nDiego (UCSD). As part of the SCORE project, this paper presents our work on the\nimplementation of an intensity modulated radiation therapy (IMRT) optimization\nalgorithm on graphics processing units (GPUs). We adopt a penalty-based\nquadratic optimization model, which is solved by using a gradient projection\nmethod with Armijo's line search rule. Our optimization algorithm has been\nimplemented in CUDA for parallel GPU computing as well as in C for serial CPU\ncomputing for comparison purpose. A prostate IMRT case with various beamlet and\nvoxel sizes was used to evaluate our implementation. On an NVIDIA Tesla C1060\nGPU card, we have achieved speedup factors of 20-40 without losing accuracy,\ncompared to the results from an Intel Xeon 2.27 GHz CPU. For a specific 9-field\nprostate IMRT case with 5x5 mm^2 beamlet size and 2.5x2.5x2.5 mm^3 voxel size,\nour GPU implementation takes only 2.8 seconds to generate an optimal IMRT plan.\nOur work has therefore solved a major problem in developing online re-planning\ntechnologies for adaptive radiotherapy.\n",
        "  In this paper we investigate the unlinking numbers of 10-crossing links. We\nmake use of various link invariants and explore their behaviour when crossings\nare changed. The methods we describe have been used previously to compute\nunlinking numbers of links with crossing number at most 9. Ultimately, we find\nthe unlinking numbers of all but 2 of the 287 prime, non-split links with\ncrossing number 10.\n",
        "  The symmetry and structure of the superconducting gap in the Fe-based\nsuperconductor are the central issue for understanding these novel materials.\nSo far the experimental data and theoretical models have been highly\ncontroversial. Some experiments favor two or more constant or nearly-constant\ngaps, others indicate strong anisotropy and yet others suggest gap zeros\n(\"nodes\"). Theoretical models also vary, suggesting that the absence or\npresence of the nodes depends quantitatively on the model parameters. An\nopinion that has gained substantial currency is that the gap structure, unlike\nall other known superconductors, including cuprates, may be different in\ndifferent compounds within the same family. A unique method for addressing this\nissue, one of the very few methods that are bulk and angle-resolved, calls for\nmeasuring the electronic specific heat in a rotating magnetic field, as a\nfunction of field orientation with respect to the crystallographic axes. In\nthis Communication we present the first such measurement for an Fe-based\nhigh-Tc superconductor (FeBSC). We observed a fourfold oscillation of the\nspecific heat as a function of the in-plane magnetic field direction, which\nallowed us to identify the locations of the gap minima (or nodes) on the Fermi\nsurface. Our results place severe restrictions on the gap structure and on the\nexisting theoretical models.\n",
        "  Querying uncertain data sets (represented as probability distributions)\npresents many challenges due to the large amount of data involved and the\ndifficulties comparing uncertainty between distributions. The Earth Mover's\nDistance (EMD) has increasingly been employed to compare uncertain data due to\nits ability to effectively capture the differences between two distributions.\nComputing the EMD entails finding a solution to the transportation problem,\nwhich is computationally intensive. In this paper, we propose a new lower bound\nto the EMD and an index structure to significantly improve the performance of\nEMD based K-nearest neighbor (K-NN) queries on uncertain databases. We propose\na new lower bound to the EMD that approximates the EMD on a projection vector.\nEach distribution is projected onto a vector and approximated by a normal\ndistribution, as well as an accompanying error term. We then represent each\nnormal as a point in a Hough transformed space. We then use the concept of\nstochastic dominance to implement an efficient index structure in the\ntransformed space. We show that our method significantly decreases K-NN query\ntime on uncertain databases. The index structure also scales well with database\ncardinality. It is well suited for heterogeneous data sets, helping to keep EMD\nbased queries tractable as uncertain data sets become larger and more complex.\n",
        "  Trigonometric parallaxes measured with ground-based telescopes of the RECONS\nconsortium as part of the CTIOPI program are used to search for stars that have\neither had an encounter with the solar system in the past or will have such an\nencounter in the future, at distances of less than a few parsecs. These are\nmainly low-mass dwarfs and subdwarfs of types M, L, and T currently at\ndistances of less than 30 pc from the Sun. Six stars for which encounters with\nthe solar orbit at distances of less than 1 pc are possible have been\nidentified for the first time. For example, the minimum distance for the star\n**SOZ 3A will 0.72+/- 0.11 pc at an epoch of 103+/-44 thousand years in the\nfuture.\n",
        "  The Yule-Harding-Kingman (YHK) model and the proportional to distinguishable\narrangements (PDA) model are two binary tree generating models that are widely\nused in evolutionary biology. Understanding the distributions of clade sizes\nunder these two models provides valuable insights into macro-evolutionary\nprocesses, and is important in hypothesis testing and Bayesian analyses in\nphylogenetics. Here we show that these distributions are log-convex, which\nimplies that very large clades or very small clades are more likely to occur\nunder these two models. Moreover, we prove that there exists a critical value\n$\\kappa(n)$ for each $n\\geqslant 4$ such that for a given clade with size $k$,\nthe probability that this clade is contained in a random tree with $n$ leaves\ngenerated under the YHK model is higher than that under the PDA model if\n$1<k<\\kappa(n)$, and lower if $\\kappa(n)<k<n$. Finally, we extend our results\nto binary unrooted trees, and obtain similar results for the distributions of\nclan sizes.\n",
        "  Until recently, much of the microbial world was hidden from view. A global\nresearch effort has changed this, unveiling and quantifying microbial diversity\nacross enormous range of critically-important contexts, from the human\nmicrobiome, to plant-soil interactions, to marine life. Yet what has remained\nlargely hidden is the interplay of ecological and evolutionary processes that\nled to the diversity we observe in the present day. We introduce a theoretical\nframework to quantify the effect of ecological innovations in microbial\nevolutionary history, using a new, coarse-grained approach that is robust to\nthe incompleteness and ambiguities in microbial community data. Applying this\nmethodology, we identify a balance of gradual, ongoing diversification and\nrapid bursts across a vast range of microbial habitats. Moreover, we find\nuniversal quantitative similarities in the tempo of diversification,\nindependent of habitat type.\n",
        "  Sarcoidosis is a rare, multi-systemic, inflammatory disease, primarily\naffecting the lungs. High-resolution computed tomography (CT) scans are used to\nclinically characterize pulmonary sarcoidosis. In the medical imaging field,\nthere is growing recognition to switch from visual examination of CT images to\nmore rapid, objective assessments of the abnormalities. In this work, we\nexplore the usefulness of various objective measures of spatial\nheterogeneity---fractal dimension, Moran's I, and Geary's C ---for\ndistinguishing between abnormal sarcoidosis and normal lung parenchyma.\n  CT data for N=58 sarcoidosis subjects enrolled at National Jewish Health were\nobtained from the GRADS study. CT data for N=101 control patients were obtained\nfrom the COPDGene study. Radiomic measures were computed for each\ntwo-dimensional slice of a given scan, in the axial, coronal, and sagittal\nplanes. Functional regression was applied to identify lung regions where CT\nnodules tend to proliferate.\n  Moran's I, Geary's C and fractal dimension significantly differentiate\nbetween subjects with and without sarcoidosis throughout the majority of the\nlung, with disease abnormalities most apparent in the top axial, middle\ncoronal, and outer sagittal regions. A trend appeared across Scadding stages,\nwith CT scans from patients with Scadding stages I and III appearing the\nhealthiest, and Scadding stage IV appearing the least healthy.\n  The radiomic measures and techniques presented herein successfully\ncharacterize CT images in sarcoidosis by objectively and efficiently\napproximating what we know about the pathology of sarcoidosis.\n",
        "  We demonstrate full frequency conversion in the microwave domain using a\nJosephson three-wave mixing device pumped at the difference between the\nfrequencies of its fundamental eigenmodes. By measuring the signal output as a\nfunction of the intensity and phase of the three input signal, idler and pump\ntones, we show that the device functions as a controllable three-wave\nbeam-splitter/combiner for propagating microwave modes, in accordance with\ntheory. Losses at the full conversion point are found to be less than 10^-2.\nPotential applications of the device include quantum information transduction\nand realization of an ultra-sensitive interferometer with controllable\nfeedback.\n",
        "  We present a multi-wavelength study to analyse the star formation process\nassociated with the mid-infrared bubble CN 148 (H II region G10.3-0.1), which\nharbors an O5V-O6V star. The arc-shaped distribution of molecular CO(2-1)\nemission, the cold dust emission, and the polycyclic aromatic hydrocarbon\nfeatures trace a photodissociation region (PDR) around the H II region. We have\nidentified 371 young stellar objects (YSOs) in the selected region and,\ninterestingly, their spatial distribution correlates well with the PDR. 41% of\nthese YSOs are present in 13 clusters, each having visual extinction larger\nthan 16 mag. The clusters at the edges of the bubble (both northeast and\nsouthwest) are found to be relatively younger than the clusters located further\naway from the bubble. We also find that four 6.7 GHz methanol masers, two\nExtended Green Objects, an ultra-compact H II region, and a massive protostar\ncandidate (as previously reported) are spatially positioned at the edges of the\nbubble. The existence of an apparent age gradient in YSO clusters and different\nearly evolutionary stages of massive star formation around the bubble suggest\ntheir origin to be influenced by an H II region expanding into the surrounding\ninterstellar medium. The data sets are suggestive of triggered star formation.\n",
        "  We investigate the behavior of a mesoscopic one-dimensional ring in an\nexternal magnetic field by simulating the time dependent Ginzburg-Landau\nequations with periodic boundary conditions. We analyze the stability and the\ndifferent possible evolutions for the phase slip phenomena starting from a\nmetastable state. We find a stability condition relating the winding number of\nthe initial solution and the number of flux quanta penetrating the ring. The\nanalysis of multiple phase slips solutions is based on analytical results and\nsimulations. The role of the ratio of two characteristic times u is studied for\nthe case of a multiple phase slips transition. We found out that if u>>1,\nconsecutive multiple phase slips will be more favorable than simultaneous ones.\nIf u<<1 the opposite is true and we confirm that u>>1 is often a necessary\ncondition to reach the ground state. The influence of the Langevin noise on the\nkinetics of the phase transition is discussed.\n",
        "  After an initial transient period, the conduction regime in a 1D\nsuperconducting wire that carries a fixed current is either normal or periodic\nor stationary. The phase diagram for these possibilities was studied in Phys.\nRev. Lett. {\\bf 99}, 167003 (2007) for particular values of the length and the\nmaterial parameters. We have extended this study to arbitrary length and to a\nrange of material parameters that includes realistic values. Variation of the\nlength leads to scaling laws for the phase diagram. Variation of the material\nparameters leads to new qualitative features and new phases, including a\nparameter region in which all three regimes are possible.\n",
        "  Neglected tropical diseases (NTD), particularly vector-borne diseases (VBD),\naccount for a large proportion of the global disease burden, and their control\nfaces several challenges including diminishing human and financial resources\nfor those distressed from such diseases. Visceral Leishmaniasis (VL), the\nsecond-largest parasitic killer in the world (after malaria) affects poor\npopulations in endemic countries and causes considerable cost to the affected\nindividuals and their society. Mathematical models can serve as a critical tool\nfor understanding the driving mechanisms of a NTD such as VL. The WHO promotes\nintegrated control programs for VL but this policy is not well supported by\nsystematic quantitative and dynamic evidence and so potential benefits of the\npolicy are limited. Moreover, mathematical models can be readily developed and\nused to understand the functioning of the VL system cheaply and systematically.\nThe focus of this research is three-fold: (i) to identify non-traditional but\ncritical mechanisms for ongoing VL transmission in resource limited regions,\n(ii) to review mathematical models used for other infectious diseases that have\nthe potential to capture identified factors of VL, and (iii) to suggest novel\nquantitative models for understanding VL dynamics and for evaluating control\nprograms in such frameworks for achieveing VL elimination goals.\n",
        "  We report the results of high-field, low-temperature MuSR measurements of the\nquasi two-dimensional organic superconductors \\k{appa}-(ET)2Cu(NCS)2 and\n\\k{appa}-(ET)2Cu[N(CN)2]Br. The MuSR lineshapes for these compounds indicate\nthe existence of partially-ordered vortex lattice phases in the high magnetic\nfield regime, up to 2.5 T for the former compound and 4 T for the latter\ncompound. The observed sharp loss of order is found to be consistent with a\nvortex-lattice melting transition that is predicted by numerical simulations of\nweakly coupled layers of pancake vortices. It is argued that the robustness of\nthe partially-ordered vortex lattice phases could be due to strong flux-line\npinning by a dilute ensemble of defects.\n",
        "  The air dose rate in an environment contaminated with 134Cs and 137Cs depends\non the amount, depth profile and horizontal distribution of these contaminants\nwithin the ground. This paper introduces and verifies a tool that models these\nvariables and calculates ambient dose equivalent rates at 1 m above the ground.\nGood correlation is found between predicted dose rates and dose rates measured\nwith survey meters in Fukushima Prefecture in areas contaminated with\nradiocesium from the Fukushima Dai-ichi Nuclear Power Plant accident. This\nfinding is insensitive to the choice for modelling the activity depth\ndistribution in the ground using activity measurements of collected soil\nlayers, or by using exponential and hyperbolic secant fits to the measurement\ndata. Better predictions are obtained by modelling the horizontal distribution\nof radioactive cesium across an area if multiple soil samples are available, as\nopposed to assuming a spatially homogeneous contamination distribution.\nReductions seen in air dose rates above flat, undisturbed fields in Fukushima\nPrefecture are consistent with decrement by radioactive decay and downward\nmigration of cesium into soil. Analysis of remediation strategies for farmland\nsoils confirmed that topsoil removal and interchanging a topsoil layer with a\nsubsoil layer result in similar reductions in the air dose rate. These two\nstrategies are more effective than reverse tillage to invert and mix the\ntopsoil.\n",
        "  We present a pilot study of the z=2.923 radio galaxy MRC0943-242, where we\nfor the first time combine information from ALMA and MUSE data cubes. Even with\nmodest integration times, we disentangle an AGN and a starburst dominated set\nof components. These data reveal a highly complex morphology, as the AGN,\nstarburst, and molecular gas components show up as widely separated sources in\ndust continuum, optical continuum and CO line emission observations. CO(1-0)\nand CO(8-7) line emission suggest that there is a molecular gas reservoir\noffset from both the dust and the optical continuum that is located ~90kpc from\nthe AGN. The UV line emission has a complex structure in emission and\nabsorption. The line emission is mostly due to i) a large scale ionisation cone\nenergised by the AGN, ii) a Ly-alpha emitting bridge of gas between the radio\ngalaxy and a heavily star-forming set of components. Strangely, the ionisation\ncone has no Ly-alpha emission. We find this is due to an optically thick layer\nof neutral gas with unity covering fraction spread out over a region of at\nleast ~100kpc from the AGN. Other, less thick absorption components are\nassociated with Ly-alpha emitting gas within a few tens of kpc from the radio\ngalaxy and are connected by a bridge of emission. We speculate that this linear\nstructure of dust, Ly-alpha and CO emission, and the redshifted absorption seen\nin the circum-nuclear region may represent an accretion flow feeding gas into\nthis massive AGN host galaxy.\n",
        "  Predicting the spread of vector-borne diseases in response to incursions\nrequires knowledge of both host and vector demographics in advance of an\noutbreak. Whereas host population data is typically available, for novel\ndisease introductions there is a high chance of the pathogen utilising a vector\nfor which data is unavailable. This presents a barrier to estimating the\nparameters of dynamical models representing host-vector-pathogen interaction,\nand hence limits their ability to provide quantitative risk forecasts. The\nTheileria orientalis (Ikeda) outbreak in New Zealand cattle demonstrates this\nproblem: even though the vector has received extensive laboratory study, a high\ndegree of uncertainty persists over its national demographic distribution.\nAddressing this, we develop a Bayesian data assimilation approach whereby\nindirect observations of vector activity inform a seasonal spatio-temporal risk\nsurface within a stochastic epidemic model. We provide quantitative predictions\nfor the future spread of the epidemic, quantifying uncertainty in the model\nparameters, case infection times, and the disease status of undetected\ninfections. Importantly, we demonstrate how our model learns sequentially as\nthe epidemic unfolds, and provides evidence for changing epidemic dynamics\nthrough time. Our approach therefore provides a significant advance in rapid\ndecision support for novel vector-borne disease outbreaks.\n",
        "  This paper presents a novel prototype for biomedical term normalization of\nelectronic health record excerpts with the Unified Medical Language System\n(UMLS) Metathesaurus. Despite being multilingual and cross-lingual by design,\nwe first focus on processing clinical text in Spanish because there is no\nexisting tool for this language and for this specific purpose. The tool is\nbased on Apache Lucene to index the Metathesaurus and generate mapping\ncandidates from input text. It uses the IXA pipeline for basic language\nprocessing and resolves ambiguities with the UKB toolkit. It has been evaluated\nby measuring its agreement with MetaMap in two English-Spanish parallel\ncorpora. In addition, we present a web-based interface for the tool.\n",
        "  Understanding unstructured text is a major goal within natural language\nprocessing. Comprehension tests pose questions based on short text passages to\nevaluate such understanding. In this work, we investigate machine comprehension\non the challenging {\\it MCTest} benchmark. Partly because of its limited size,\nprior work on {\\it MCTest} has focused mainly on engineering better features.\nWe tackle the dataset with a neural approach, harnessing simple neural networks\narranged in a parallel hierarchy. The parallel hierarchy enables our model to\ncompare the passage, question, and answer from a variety of trainable\nperspectives, as opposed to using a manually designed, rigid feature set.\nPerspectives range from the word level to sentence fragments to sequences of\nsentences; the networks operate only on word-embedding representations of text.\nWhen trained with a methodology designed to help cope with limited training\ndata, our Parallel-Hierarchical model sets a new state of the art for {\\it\nMCTest}, outperforming previous feature-engineered approaches slightly and\nprevious neural approaches by a significant margin (over 15\\% absolute).\n",
        "  Purpose: The analysis of optimized spin ensemble trajectories for relaxometry\nin the hybrid state.\n  Methods: First, we constructed visual representations to elucidate the\ndifferential equation that governs spin dynamics in hybrid state. Subsequently,\nnumerical optimizations were performed to find spin ensemble trajectories that\nminimize the Cram\\'er-Rao bound for $T_1$-encoding, $T_2$-encoding, and their\nweighted sum, respectively, followed by a comparison of the Cram\\'er-Rao bounds\nobtained with our optimized spin-trajectories, as well as Look-Locker and\nmulti-spin-echo methods. Finally, we experimentally tested our optimized spin\ntrajectories with in vivo scans of the human brain.\n  Results: After a nonrecurring inversion segment on the southern hemisphere of\nthe Bloch sphere, all optimized spin trajectories pursue repetitive loops on\nthe northern half of the sphere in which the beginning of the first and the end\nof the last loop deviate from the others. The numerical results obtained in\nthis work align well with intuitive insights gleaned directly from the\ngoverning equation. Our results suggest that hybrid-state sequences outperform\ntraditional methods. Moreover, hybrid-state sequences that balance $T_1$- and\n$T_2$-encoding still result in near optimal signal-to-noise efficiency. Thus,\nthe second parameter can be encoded at virtually no extra cost.\n  Conclusion: We provide insights regarding the optimal encoding processes of\nspin relaxation times in order to guide the design of robust and efficient\npulse sequences. We find that joint acquisitions of $T_1$ and $T_2$ in the\nhybrid state are substantially more efficient than sequential encoding\ntechniques.\n",
        "  In the scatter correction for x-ray Cone Beam (CB) CT, the single-scan scheme\nwith moving Beam Stop Array (BSA) offers reliable scatter measurement with low\ndose, and by using Projection Correlation based View Interpolation (PC-VI), the\nprimary fluence shaded by the moving BSA (during scatter measurement) could be\nrecovered with high accuracy. However, the moving BSA may increase the\nmechanical burden in real applications. For better practicability, in this\npaper we proposed a PC-VI based single-scan scheme with a ring-shaped\nstationary BSA, which serves as a virtual moving BSA during CB scan, so the\nshaded primary fluence by this stationary BSA can be also well recovered by\nPC-VI. The principle in designing the whole system is deduced and evaluated.\nThe proposed scheme greatly enhances the practicability of the single-scan\nscatter correction scheme.\n",
        "  The rise of Web 2.0 is signaled by sites such as Flickr, del.icio.us, and\nYouTube, and social tagging is essential to their success. A typical tagging\naction involves three components, user, item (e.g., photos in Flickr), and tags\n(i.e., words or phrases). Analyzing how tags are assigned by certain users to\ncertain items has important implications in helping users search for desired\ninformation. In this paper, we explore common analysis tasks and propose a dual\nmining framework for social tagging behavior mining. This framework is centered\naround two opposing measures, similarity and diversity, being applied to one or\nmore tagging components, and therefore enables a wide range of analysis\nscenarios such as characterizing similar users tagging diverse items with\nsimilar tags, or diverse users tagging similar items with diverse tags, etc. By\nadopting different concrete measures for similarity and diversity in the\nframework, we show that a wide range of concrete analysis problems can be\ndefined and they are NP-Complete in general. We design efficient algorithms for\nsolving many of those problems and demonstrate, through comprehensive\nexperiments over real data, that our algorithms significantly out-perform the\nexact brute-force approach without compromising analysis result quality.\n",
        "  We give sharp, effective bounds on the distance between tori of fixed\ninjectivity radius inside a Margulis tube in a hyperbolic 3-manifold.\n",
        "  The mining of frequent subgraphs from labeled graph data has been studied\nextensively. Furthermore, much attention has recently been paid to frequent\npattern mining from graph sequences. A method, called GTRACE, has been proposed\nto mine frequent patterns from graph sequences under the assumption that\nchanges in graphs are gradual. Although GTRACE mines the frequent patterns\nefficiently, it still needs substantial computation time to mine the patterns\nfrom graph sequences containing large graphs and long sequences. In this paper,\nwe propose a new version of GTRACE that enables efficient mining of frequent\npatterns based on the principle of a reverse search. The underlying concept of\nthe reverse search is a general scheme for designing efficient algorithms for\nhard enumeration problems. Our performance study shows that the proposed method\nis efficient and scalable for mining both long and large graph sequence\npatterns and is several orders of magnitude faster than the original GTRACE.\n",
        "  Our PACS and SPIRE images of the Aquila Rift and part of the Polaris Flare\nregions, taken during the science demonstration phase of Herschel discovered\nfascinating, omnipresent filamentary structures that appear to be physically\nrelated to compact cores. We briefly describe a new multi-scale,\nmulti-wavelength source extraction method used to detect objects and measure\ntheir parameters in our Herschel images. All of the extracted starless cores\n(541 in Aquila and 302 in Polaris) appear to form in the long and very narrow\nfilaments. With its combination of the far-IR resolution and sensitivity,\nHerschel directly reveals the filaments in which the dense cores are embedded;\nthe filaments are resolved and have deconvolved widths of 35 arcsec in Aquila\nand 59 arcsec in Polaris (9000 AU in both regions). Our first results of\nobservations with Herschel enable us to suggest that in general dense cores may\noriginate in a process of fragmentation of complex networks of long, thin\nfilaments, likely formed as a result of an interplay between gravity,\ninterstellar turbulence, and magnetic fields. To unravel the roles of the\nprocesses, one has to obtain additional kinematic and polarization information;\nthese follow-up observations are planned.\n",
        "  In previous papers, the author realized the following principle for many knot\ntheories: if a knot diagram is complicated enough then it reproduces itself,\ni.e., is a subdiagram of any other diagram equivalent to it. This principle is\nrealized by diagram-valued invariants [ ] of knots such that [K]=K. It turns\nout that in the case of free braids, the same principle can be realized an\nunexpectedly easy way by a one-term invariant formula.\n",
        "  Due to its wide field of view, cone-beam computed tomography (CBCT) is\nplagued by large amounts of scatter, where attenuated photons hit the detector,\nand corrupt the linear models used for reconstruction. Given that one can\ngenerate a good estimate of scatter however, then image accuracy can be\nretained. In the context of adaptive radiotherapy, one usually has a\nlow-scatter planning CT image of the same patient at an earlier time.\nCorrecting for scatter in the subsequent CBCT scan can either be self\nconsistent with the new measurements or exploit the prior image, and there are\nseveral recent methods that report high accuracy with the latter. In this\nstudy, we will look at the accuracy of various scatter estimation methods, how\nthey can be effectively incorporated into a statistical reconstruction\nalgorithm, along with introducing a method for matching off-line Monte-Carlo\n(MC) prior estimates to the new measurements. Conclusions we draw from testing\non a neck cancer patient are: statistical reconstruction that incorporates the\nscatter estimate significantly outperforms analytic and iterative methods with\npre-correction; and although the most accurate scatter estimates can be made\nfrom the MC on planning image, they only offer a slight advantage over the\nmeasurement based scatter kernel superposition (SKS) in reconstruction error.\n",
        "  Data-management-as-a-service systems are increasingly being used in\ncollaborative settings, where multiple users access common datasets. Cloud\nproviders have the choice to implement various optimizations, such as indexing\nor materialized views, to accelerate queries over these datasets. Each\noptimization carries a cost and may benefit multiple users. This creates a\nmajor challenge: how to select which optimizations to perform and how to share\ntheir cost among users. The problem is especially challenging when users are\nselfish and will only report their true values for different optimizations if\ndoing so maximizes their utility. In this paper, we present a new approach for\nselecting and pricing shared optimizations by using Mechanism Design. We first\nshow how to apply the Shapley Value Mechanism to the simple case of selecting\nand pricing additive optimizations, assuming an offline game where all users\naccess the service for the same time-period. Second, we extend the approach to\nonline scenarios where users come and go. Finally, we consider the case of\nsubstitutive optimizations. We show analytically that our mechanisms induce\ntruth- fulness and recover the optimization costs. We also show experimentally\nthat our mechanisms yield higher utility than the state-of-the-art approach\nbased on regret accumulation.\n",
        "  Much has been written about introduced rainbow trout interbreeding and\noutcompeting native cutthroat trout. However, specific mechanisms have not been\nthoroughly explored, and most data is limited to lotic ecosystems. Samples of\nSnake River cutthroat trout (Oncorhynchus clarkii bouvieri), the\nrainbow-cutthroat hybrid, the cutbow trout (Onchorhynchus mykiss x clarkii),\nand rainbow trout (Oncorhynchus mykiss), were obtained from a lentic ecosystem\n(Eleven Mile Reservoir, Colorado) by creel surveys conducted from May to\nOctober, 2012. The total length and weight of each fish was measured and the\nrelative condition factor of each fish was computed using expected weight from\nweight-length relationships from the Colorado Division of Parks and Wildlife\n(CDPW). Data from the CDPW collected from 2003-2010 in the same lentic\necosystem were used to compute relative condition factors for additional\ncomparison, as was independent creel survey data from 2011. Cutthroat trout\nwere plump: the mean relative condition factor of the cutthroat trout was\n112.0% (+/- 1.0%). Cutbow hybrid trout were close to the expected weights with\na mean relative condition factor of 99.8% (+/- 0.6%). Rainbow trout were\nthinner with a mean relative condition factor of 96.4% (+/- 1.4%). Comparing\nmean relative condition factors of CDPW data from earlier years and plotting\nthe 2012 data relative to percentile curves also shows the same trend of\ncutthroat trout being plumper than expected and rainbow trout being thinner\nthan the cutthroat trout, with the hybrid cutbow trout in between. This data\nsupports the hypothesis that rainbow trout do not outcompete cutthroat trout in\nlentic ecosystems. Comparison with data from three other Colorado reservoirs\nalso shows that cutthroat trout tend to be more plump than rainbow trout and\ntheir hybrids in sympatric lentic ecosystems.\n",
        "  For $n \\geq 2$ we describe an $O(l^3n)$-time algorithm that determines if a\nlength $l$ virtual braid word in the standard presentation of the virtual braid\ngroup ${\\mathcal VB}_n$ represents the trivial virtual braid.\n",
        "  With the continued evolution of modern radiation therapy towards high\nprecision delivery of high therapeutic doses to the tumour while optimally\nsparing surrounding healthy tissue, imaging becomes a crucial component to\nidentify the intended target, properly position it at the treatment site and,\nin more advanced research applications, visualize the treatment delivery. This\ncontribution reviews the main role of imaging in modern external beam radiation\ntherapy, with special emphasis on emerging ion beam therapy techniques, which\naim at exploiting the favourable properties of ion interaction in matter for\nunprecedented ballistic accuracy in dose delivery\n",
        "  This paper explores methodologies, advantages and challenges related to the\nuse of the Information Centric Network technology for developing NoSQL\ndistributed databases, which are expected to play a central role in the\nforthcoming IoT and BigData era. ICN services make possible to simplify the\ndevelopment of the database software, improve performance, and provide\ndata-level access control. We use our findings for devising a NoSQL\nspatio-temporal database, named OpenGeoBase, and evaluate its performance with\na real data set related to Intelligent Transport System applications.\n",
        "  Statistical language models (LM) play a key role in Automatic Speech\nRecognition (ASR) systems used by conversational agents. These ASR systems\nshould provide a high accuracy under a variety of speaking styles, domains,\nvocabulary and argots. In this paper, we present a DNN-based method to adapt\nthe LM to each user-agent interaction based on generalized contextual\ninformation, by predicting an optimal, context-dependent set of LM\ninterpolation weights. We show that this framework for contextual adaptation\nprovides accuracy improvements under different possible mixture LM partitions\nthat are relevant for both (1) Goal-oriented conversational agents where it's\nnatural to partition the data by the requested application and for (2) Non-goal\noriented conversational agents where the data can be partitioned using topic\nlabels that come from predictions of a topic classifier. We obtain a relative\nWER improvement of 3% with a 1-pass decoding strategy and 6% in a 2-pass\ndecoding framework, over an unadapted model. We also show up to a 15% relative\nimprovement in recognizing named entities which is of significant value for\nconversational ASR systems.\n",
        "  We show that biaxial strain induces alternating tetragonal superconducting\nand orthorhombic nematic domains in Co substituted CaFe2As2. We use Atomic\nForce, Magnetic Force and Scanning Tunneling Microscopy (AFM, MFM and STM) to\nidentify the domains and characterize their properties, finding in particular\nthat tetragonal superconducting domains are very elongated, more than several\ntens of micron long and about 30 nm wide, have the same Tc than unstrained\nsamples and hold vortices in a magnetic field. Thus, biaxial strain produces a\nphase separated state, where each phase is equivalent to what is found at\neither side of the first order phase transition between antiferromagnetic\northorhombic and superconducting tetragonal phases found in unstrained samples\nwhen changing Co concentration. Having such alternating superconducting domains\nseparated by normal conducting domains with sizes of order of the coherence\nlength opens opportunities to build Josephson junction networks or vortex\npinning arrays and suggests that first order quantum phase transitions lead to\nnanometric size phase separation under the influence of strain.\n",
        "  How self-incompatibility systems are maintained in plant populations is still\na debated issue. Theoretical models predict that self-incompatibility systems\nbreak down according to the intensity of inbreeding depression and number of\nS-alleles. Other studies have explored the function of asexual reproduction in\nthe maintenance of self-incompatibility. However, the population genetics of\npartially asexual, self-incompatible populations are poorly understood and\nprevious studies have failed to consider all possible effects of asexual\nreproduction or could only speculate on those effects. In this study, we\ninvestigated how partial asexuality may affect genetic diversity at the S-locus\nand fitness in small self-incompatible populations. A genetic model including\nan S-locus and a viability locus was developed to perform forward simulations\nof the evolution of populations of various sizes. Drift combined with partial\nasexuality produced a decrease in the number of alleles at the S-locus. In\naddition, an excess of heterozygotes was present in the population, causing an\nincrease in mutation load. This heterozygote excess was enhanced by the\nself-incompatibility system in small populations. In addition, in highly\nasexual populations, individuals produced asexually had some fitness advantages\nover individuals produced sexually, because sexual reproduction produces\nhomozygotes of the deleterious allele, contrary to asexual reproduction. Our\nresults suggest that future research on the function of asexuality for the\nmaintenance of self-incompatibility will need to (1) account for whole-genome\nfitness (mutation load generated by asexuality, self-incompatibility and drift)\nand (2) acknowledge that the maintenance of self-incompatibility may not be\nindependent of the maintenance of sex itself.\n",
        "  We have conducted 22 GHz 1\" JVLA imaging of 70 radio-quiet AGN from the\nSwift-BAT survey. We find radio cores in all but three objects. The radio\nmorphologies of the sample fall into three groups: compact and core-dominated,\nextended, and jet-like. We spatially decompose each image into core flux and\nextended flux, and compare the extended radio emission to that predicted from\nprevious Herschel observations using the canonical FIR-radio relation. After\nremoving the AGN contribution to the FIR and radio flux densities, we find that\nthe relation holds remarkably well despite the potentially different star\nformation physics in the circumnuclear environment. We also compare our core\nradio flux densities with predictions of coronal models and scale-invariant jet\nmodels for the origin of radio emission in radio-quiet AGN, and find general\nconsistency with both models. However, we find that the $L_{\\mathrm{R}} /\nL_{\\mathrm{X}}$ relation does not distinguish between star formation and\nnon-relativistic AGN-driven outflows as the origin of radio emission in\nradio-quiet AGN. Finally, we examine where objects with different radio\nmorphologies fall in relation to the main sequence of star formation, and\nconclude that those AGN that fall below the main sequence, as X-ray selected\nAGN have been found to do, have core-dominated or jet-like 22 GHz morphologies.\n",
        "  Biomedical information extraction (BioIE) is important to many applications,\nincluding clinical decision support, integrative biology, and\npharmacovigilance, and therefore it has been an active research. Unlike\nexisting reviews covering a holistic view on BioIE, this review focuses on\nmainly recent advances in learning based approaches, by systematically\nsummarizing them into different aspects of methodological development. In\naddition, we dive into open information extraction and deep learning, two\nemerging and influential techniques and envision next generation of BioIE.\n",
        "  We investigate applying repurposed generic QA data and models to a recently\nproposed relation extraction task. We find that training on SQuAD produces\nbetter zero-shot performance and more robust generalisation compared to the\ntask specific training set. We also show that standard QA architectures (e.g.\nFastQA or BiDAF) can be applied to the slot filling queries without the need\nfor model modification.\n",
        "  This document is part of a series of near real-time weekly influenza\nforecasts made during the 2012-2013 influenza season. Here we present results\nof a forecast initiated following assimilation of observations for Week 51\n(i.e. the forecast begins December 23, 2012) for municipalities in the United\nStates. The forecast was made on December 28, 2012. Results from forecasts\ninitiated the four previous weeks (Weeks 47-50) are also presented. Predictions\ngenerated with an alternate SIRS model, run without absolute humidity forcing\n(no AH), are also presented.\n",
        "  The asymmetric emission of gravitational waves produced during the\ncoalescence of a massive black hole (MBH) binary imparts a velocity \"kick\" to\nthe system that can displace the hole from the center of its host. Here we\nstudy the trajectories and observability of MBHs recoiling in three (one major,\ntwo minor) gas-rich galaxy merger remnants that were previously simulated at\nhigh resolution, and in which the pairing of the MBHs had been shown to be\nsuccessful. We run new simulations of MBHs recoiling in the major merger\nremnant with Mach numbers in the range 1<M<6, and use simulation data to\nconstruct a semi-analytical model for the orbital evolution of MBHs in gas-rich\nsystems. We show that: 1) in major merger remnants the energy deposited by the\nmoving hole into the rotationally supported, turbulent medium makes a\nnegligible contribution to the thermodynamics of the gas. This contribution\nbecomes significant in minor merger remnants, potentially allowing for an\nelectromagnetic signature of MBH recoil; 2) in major merger remnants, the\ncombination of both deeper central potential well and drag from high-density\ngas confines even MBHs with kick velocities as high as 1200 km/s within 1 kpc\nfrom the host's center; 3) kinematically offset nuclei may be observable for\ntimescales of a few Myr in major merger remnants in the case of recoil\nvelocities in the range 700-1,000 km/s; 4) in minor mergers remnants the effect\nof gas drag is weaker, and MBHs with recoil speeds in the range 300-600 km/s\nwill wander through the host halo for longer timescales. When accounting for\nthe probability distribution of kick velocities, however, we find that the\nlikelihood of observing recoiling MBHs in gas-rich galaxy mergers is very low,\ntypically below 10^-5 - 10^-6.\n",
        "  We prove the following: Let $2p + 1$ be no less than 5 and $p$ be a natural\nnumber. Let $K$ and $J$ be closed, oriented, $(2p+1)$-dimensional connected,\n$(p-1)$-connected, simple submanifolds of the standard $(2p+3)$-sphere. Then\n$K$ is equivalent to $J$ if and only if a Seifert matrix associated with a\nsimple Seifert hypersurface for $K$ is $(-1)^p$-$S$-equivalent to that for $J$.\n  We also discuss the $2p+1=3$ case. This result implies one of our main\nresults: Let $\\mu$ be a natural number. A 1-link $A$ is pass-move equivalent to\na 1-link $B$ if and only if the knot product of $A$ and $\\mu$ copies of the\nHopf link is $(2\\mu+1, 2\\mu+1)$-pass-move equivalent to that of B and $\\mu$\ncopies of the Hopf link.\n  It also implies the other of them: Two-fold cyclic suspension commutes with\nthe performance of the twist move for spherical $(2k+1)$-knots ($2k+1>4$).\n  Furthemroe we prove the following: Let $2p+1$ be no less than 5 and p be a\nnatural number. Let $K$ be a closed oriented $(2p+1)$-dimensionalsubmanifold of\nthe standard $(2p+3)$-sphere. Then $K$ is a Brieskorn submanifold if and only\nif $K$ is connected, $(p-1)$-connected, simple and has a $(p+1)$-Seifert matrix\nassociated with a simple Seifert hypersurface that is $(-1)^p$-$S$-equivalent\nto a Kauffman-Neumann-type, or a KN-type (See the body of the paper for a\ndefinition.) We also discuss the $2p+1=3$ case.\n",
        "  We have carried out an extensive multi-wavelength study to investigate the\nstar formation process in the S235 complex. The S235 complex has a sphere-like\nshell appearance at wavelengths longer than 2 $\\mu$m and harbors an O9.5V type\nstar approximately at its center. Near-infrared extinction map traces eight\nsubregions (having A$_{V}$ $>$ 8 mag), and five of them appear to be\ndistributed in an almost regularly spaced manner along the sphere-like shell\nsurrounding the ionized emission. This picture is also supported by the\nintegrated $^{12}$CO and $^{13}$CO intensity maps and by Bolocam 1.1 mm\ncontinuum emission. The position-velocity analysis of CO reveals an almost\nsemi-ring like structure, suggesting an expanding H\\,{\\sc ii} region. We find\nthat the Bolocam clump masses increase as we move away from the location of the\nionizing star. This correlation is seen only for those clumps which are\ndistributed near the edges of the shell. Photometric analysis reveals 435 young\nstellar objects (YSOs), 59\\% of which are found in clusters. Six subregions\n(including five located near the edges of the shell) are very well correlated\nwith the dust clumps, CO gas, and YSOs. The average values of Mach numbers\nderived using NH$_{3}$ data for three (East~1, East~2, and Central~E) out of\nthese six subregions are 2.9, 2.3, and 2.9, indicating these subregions are\nsupersonic. The molecular outflows are detected in these three subregions,\nfurther confirming the on-going star formation activity. Together, all these\nresults are interpreted as observational evidence of positive feedback of a\nmassive star.\n",
        "  Corpus Pattern Analysis (CPA) has been the topic of Semeval 2015 Task 15,\naimed at producing a system that can aid lexicographers in their efforts to\nbuild a dictionary of meanings for English verbs using the CPA annotation\nprocess. CPA parsing is one of the subtasks which this annotation process is\nmade of and it is the focus of this report. A supervised machine-learning\napproach has been implemented, in which syntactic features derived from parse\ntrees and semantic features derived from WordNet and word embeddings are used.\nIt is shown that this approach performs well, even with the data sparsity\nissues that characterize the dataset, and can obtain better results than other\nsystem by a margin of about 4% f-score.\n",
        "  Observations of the Galactic Centre show evidence of one or two disc-like\nstructures of very young stars orbiting the central super-massive black hole\nwithin a distance of a few 0.1 pc. A number of analyses have been carried out\nto investigate the dynamical behaviour and consequences of these discs,\nincluding disc thickness and eccentricity growth as well as mutual interaction\nand warping. However, most of these studies have neglected the influence of the\nstellar cusp surrounding the black hole, which is believed to be 1-2 orders of\nmagnitude more massive than the disc(s).\n  By means of N-body integrations using our bhint code, we study the impact of\nstellar cusps of different compositions. We find that although the presence of\na cusp does have an important effect on the evolution of an otherwise isolated\nflat disc, its influence on the evolution of disc thickness and warping is\nrather mild in a two-disc configuration. However, we show that the creation of\nhighly eccentric orbits strongly depends on the graininess of the cusp (i.e.\nthe mean and maximum stellar masses): While Chang (2009) recently found that\nfull cycles of Kozai resonance are prevented by the presence of an analytic\ncusp, we show that relaxation processes play an important role in such highly\ndense regions and support short-term resonances. We thus find that young disc\nstars on initially circular orbits can achieve high eccentricities by resonant\neffects also in the presence of a cusp of stellar remnants, yielding a\nmechanism to create S-stars and hyper-velocity stars.\n  Furthermore, we discuss the underlying initial mass function (IMF) of the\nyoung stellar discs and find no definite evidence for a non-canonical IMF.\n",
        "  The evolution of complex molecular traits such as disulphide bridges often\nrequires multiple mutations. The intermediate steps in such evolutionary\ntrajectories are likely to be selectively neutral or deleterious. Therefore,\nlarge populations and long times may be required to evolve such traits. We\npropose that errors in transcription and translation may allow selection for\nthe intermediate mutations if the final trait provides a large enough selective\nadvantage. We test this hypothesis using a population based model of protein\nevolution. If an individual acquires one of two mutations needed for a novel\ntrait, the second mutation can be introduced into the phenotype due to\ntranscription and translation errors. If the novel trait is advantageous\nenough, the allele with only one mutation will spread through the population,\neven though the gene sequence does not yet code for the complete trait. The\nfirst mutation then has a higher frequency than expected without phenotypic\nmutations giving the second mutation a higher probability of fixation. Thus,\nerrors allow protein sequences to ''look-ahead'' for a more direct path to a\ncomplex trait.\n",
        "  We present near-infrared spectroscopy and 1 mm line and continuum\nobservations of a recently identified site of high mass star formation likely\nto be located in the Central Molecular Zone near Sgr C. Located on the\noutskirts of the massive evolved HII region associated with Sgr C, the area is\ncharacterized by an Extended Green Object measuring ~10\" in size (0.4 pc),\nwhose observational characteristics suggest the presence of an embedded massive\nprotostar driving an outflow. Our data confirm that early-stage star formation\nis taking place on the periphery of the Sgr C HII region, with detections of\ntwo protostellar cores and several knots of H2 and Brackett gamma emission\nalongside a previously detected compact radio source. We calculate the cores'\njoint mass to be ~10^3 Msun, with column densities of 1-2 x 10^24 cm-2. We show\nthe host molecular cloud to hold ~10^5 Msun of gas and dust with temperatures\nand column densities favourable for massive star formation to occur, however,\nthere is no evidence of star formation outside of the EGO, indicating that the\ncloud is predominantly quiescent. Given its mass, density, and temperature, the\ncloud is comparable to other remarkable non-star-forming clouds such as G0.253\nin the Eastern CMZ.\n",
        "  In the last decade, many business applications have moved into the cloud. In\nparticular, the \"database-as-a-service\" paradigm has become mainstream. While\nexisting multi-tenant data management systems focus on single-tenant query\nprocessing, we believe that it is time to rethink how queries can be processed\nacross multiple tenants in such a way that we do not only gain more valuable\ninsights, but also at minimal cost. As we will argue in this paper, standard\nSQL semantics are insufficient to process cross-tenant queries in an\nunambiguous way, which is why existing systems use other, expensive means like\nETL or data integration. We first propose MTSQL, a set of extensions to\nstandard SQL, which fixes the ambiguity problem. Next, we present MTBase, a\nquery processing middleware that efficiently processes MTSQL on top of SQL. As\nwe will see, there is a canonical, provably correct, rewrite algorithm from\nMTSQL to SQL, which may however result in poor query execution performance,\neven on high-performance database products. We further show that with\ncarefully-designed optimizations, execution times can be reduced in such ways\nthat the difference to single-tenant queries becomes marginal.\n",
        "  We present preliminary results of the CIDA Equatorial Variability Survey\n(CEVS), looking for quasar (hereafter QSO) candidates near the Galactic plane.\nThe CEVS contains photometric data from extended and adjacent regions of the\nMilky Way disk ($\\sim$ 500 sq. deg.). In this work 2.5 square degrees with\nmoderately high temporal sampling in the CEVS were analyzed. The selection of\nQSO candidates was based on the study of intrinsic optical photometric\nvariability of 14,719 light curves. We studied samples defined by cuts in the\nvariability index (Vindex $>$ 66.5), periodicity index (Q $>$ 2), and the\ndistribution of these sources in the plane (AT , ${\\gamma}$), using a slight\nmodification of the first-order of the structure function for the temporal\nsampling of the survey. Finally, 288 sources were selected as QSO candidates.\nThe results shown in this work are a first attempt to develop a robust method\nto detect QSO towards the Galactic plane in the era of massive surveys such as\nVISTA and Gaia.\n",
        "  The UV spectral range (1100 - 3000 A) contains the strongest resonance lines\nobserved in active galactic nuclei (AGN). Analysis of UV line intensity ratios\nand profile shapes in quasar spectra provide diagnostics of physical and\ndynamical conditions in the broad line emitting region. This paper discusses\nproperties of UV lines in type-1 AGN spectra, and how they lead an estimate of\nionizing photon flux, chemical abundances, radius of the broad line emitting\nregion and central black hole mass. These estimates are meaningfully\ncontextualised through the 4D \"eigenvector-1\" (4DE1) formalism.\n",
        "  In recent years, an increasing amount of data is collected in different and\noften, not cooperative, databases. The problem of privacy-preserving,\ndistributed calculations over separated databases and, a relative to it, issue\nof private data release were intensively investigated. However, despite a\nconsiderable progress, computational complexity, due to an increasing size of\ndata, remains a limiting factor in real-world deployments, especially in case\nof privacy-preserving computations.\n  In this paper, we present a general method for trade off between performance\nand accuracy of distributed calculations by performing data sampling. Sampling\nwas a topic of extensive research that recently received a boost of interest.\nWe provide a sampling method targeted at separate, non-collaborating,\nvertically partitioned datasets. The method is exemplified and tested on\napproximation of intersection set both without and with privacy-preserving\nmechanism. An analysis of the bound on error as a function of the sample size\nis discussed and heuristic algorithm is suggested to further improve the\nperformance. The algorithms were implemented and experimental results confirm\nthe validity of the approach.\n",
        "  Study design Are there neuro-anatomical abnormalities associated with\nidiopathic scoliosis (IS)? Posterior Basicranium (PBA) reflects cerebellum\ngrowth and contains vestibular organs, two structures suspected to be involved\nin scoliosis. Objective The aim of this study was to compare posterior\nbasicranium asymmetry (PBA) in Idiopathic scoliosis (IS) and normal subjects.\n  Method: To measure the shape of PBA in 3D, we defined an intra-cranial frame\nof reference based on CNS and guided by embryology of the neural tube.\nMeasurements concerned three directions of space referred to a specific intra\ncranial referential. Data acquisition was performed with T2 MRI (G.E. Excite\n1.5T, mode Fiesta). We explored a scoliosis group of 76 women and 20 men with a\nmean age of 17, 2 and a control group of 26 women and 16 men, with a mean age\nof 27, 7.\n  Results: IS revealed a significant asymmetry of PBA (Pr>|t|<.0001) in 3\ndirections of space compared to the control group. This asymmetry was more\npronounced in antero-posterior (AP) and lateral direction, forming a torque of\nthe posterior base shape associated with identical cerebellar torque.\n",
        "  Jim Shapiro synthesizes a great many observations about the mechanisms of\nevolution to reach the remarkable conclusion that large-scale modification,\nexchange, and rearrangement of the genome are common and should be viewed as\nfundamental features of life. In other words, the genome should be viewed not\nas mostly read-only with a few rare mutations, but rather as a fully-fledged\nread-write library of genetic functions under continuous revision. Revision of\nthe genome occurs during cellular replication, during multicellular\ndevelopment, and during evolution of a population of individuals. DNA\nformatting controls the timing and location of genetic rearrangements, gene\nexpression, and genetic repair. Each of these events is under the control of\nprecise cellular circuits. Shapiro reviews the toolbox of natural genetic\nengineering that provides the functionalities necessary for efficient long-term\ngenome restructuring.\n",
        "  Online writing lacks the non-verbal cues present in face-to-face\ncommunication, which provide additional contextual information about the\nutterance, such as the speaker's intention or affective state. To fill this\nvoid, a number of orthographic features, such as emoticons, expressive\nlengthening, and non-standard punctuation, have become popular in social media\nservices including Twitter and Instagram. Recently, emojis have been introduced\nto social media, and are increasingly popular. This raises the question of\nwhether these predefined pictographic characters will come to replace earlier\northographic methods of paralinguistic communication. In this abstract, we\nattempt to shed light on this question, using a matching approach from causal\ninference to test whether the adoption of emojis causes individual users to\nemploy fewer emoticons in their text on Twitter.\n",
        "  New models for evolutionary processes of mutation accumulation allow\nhypotheses about the age-specificity of mutational effects to be translated\ninto predictions of heterogeneous population hazard functions. We apply these\nmodels to questions in the biodemography of longevity, including proposed\nexplanations of Gompertz hazards and mortality plateaus, and use them to\nexplore the possibility of melding evolutionary and functional models of aging.\n",
        "  Biochemical and regulatory interactions central to biological networks are\nexpected to cause extensive genetic interactions or epistasis affecting the\nheritability of complex traits and the distribution of genotypes in\npopulations. However, the inference of epistasis from the observed\nphenotype-genotype correlation is impeded by statistical difficulties, while\nthe theoretical understanding of the effects of epistasis remains limited, in\nturn limiting our ability to interpret data. Of particular interest is the\nbiologically relevant situation of numerous interacting genetic loci with small\nindividual contributions to fitness. Here, we present a computational model of\nselection dynamics involving many epistatic loci in a recombining population.\nWe demonstrate that a large number of polymorphic interacting loci can, despite\nfrequent recombination, exhibit cooperative behavior that locks alleles into\nfavorable genotypes leading to a population consisting of a set of competing\nclones. When the recombination rate exceeds a certain critical value that\ndepends on the strength of epistasis, this \"genotype selection\" regime\ndisappears in an abrupt transition, giving way to \"allele selection\"-the regime\nwhere different loci are only weakly correlated as expected in sexually\nreproducing populations. We show that large populations attain highest fitness\nat a recombination rate just below critical. Clustering of interacting sets of\ngenes on a chromosome leads to the emergence of an intermediate regime, where\nblocks of cooperating alleles lock into genetic modules. These haplotype blocks\ndisappear in a second transition to pure allele selection. Our results\ndemonstrate that the collective effect of many weak epistatic interactions can\nhave dramatic effects on the population structure.\n",
        "  Carbon ion beam radiotherapy enables a very localised dose deposition.\nHowever, already small changes in the patient geometry or positioning errors\ncan significantly distort the dose distribution. A live monitoring system of\nthe beam delivery within the patient is therefore highly desirable and could\nimprove patient treatment. We present a novel three-dimensional imaging method\nof the beam in the irradiated object, exploiting the measured tracks of single\nsecondary ions emerging under irradiation. The secondary particle tracks are\ndetected with a TimePix stack, a set of parallel pixelated semiconductor\ndetectors. We developed a three-dimensional reconstruction algorithm based on\nmaximum likelihood expectation maximisation. We demonstrate the applicability\nof the new method in an irradiation of a cylindrical PMMA phantom of human head\nsize with a carbon ion pencil beam of 226MeV/u. The beam image in the phantom\nis reconstructed from a set of 9 discrete detector positions between -80 and 50\ndegrees from the beam axis. Furthermore, we demonstrate the potential to\nvisualise inhomogeneities by irradiating a PMMA phantom with an air gap as well\nas bone and adipose tissue surrogate inserts. We successfully reconstructed a\n3D image of the treatment beam in the phantom from single secondary ion tracks.\nThe beam image corresponds well to the distribution expected from the beam\ndirection and energy. In addition, cylindrical inhomogeneities with a diameter\nof 2.85cm and density differences down to 0.3g/cm$^3$ to the surrounding\nmaterial are clearly visualised. This novel 3D method to image a therapeutic\ncarbon ion beam in the irradiated object does not interfere with the treatment\nand requires knowledge only of single secondary ion tracks. Even with detectors\nwith only a small angular coverage, the 3D reconstruction of the fragmentation\npoints presented in this work was found to be feasible.\n",
        "  We present a series of novel techniques and algorithms for transaction\ncommit, logging, recovery, and propagation control. In combination, they\nprovide a recovery component that maintains the persistent state of the\ndatabase (both log and data pages) always in a committed state. Recovery from\nsystem and media failures only requires only REDO operations, which can happen\nconcurrently with the processing of new transactions. The mechanism supports\nfine-granularity locking, partial rollbacks, and snapshot isolation for reader\ntransactions. Our design does not assume a specific hardware configuration such\nas non-volatile RAM or flash---it is designed for traditional disk\nenvironments. Nevertheless, it can exploit modern I/O devices for higher\ntransaction throughput and reduced recovery time with a high degree of\nflexibility.\n",
        "  We present an analysis of the orientation effects in SDSS quasar composite\nspectra. In a previous work we have shown that the equivalent width EW of the\n[OIII] {\\lambda}5008{\\AA} line is a reliable indicator of the inclination of\nthe accretion disk. Here, we have selected a sample of ~15,000 quasars from the\nSDSS 7th Data Release and divided it in sub-samples with different values of\nEW([OIII]). We find inclination effects both on broad and narrow quasars\nemission lines, among which an increasing broadening from low to high EW for\nthe broad lines and a decreasing importance of the blue component for the\nnarrow lines. These effects are naturally explained with a variation of source\ninclination from nearly face-on to edge-on, confirming the goodness of\nEW([OIII]) as an orientation indicator. Moreover, we suggest that orientation\neffects could explain, at least partially, the origin of the anticorrelation\nbetween [OIII] and FeII intensities, i.e. the well known Eigenvector 1.\n",
        "  The requirements for OLTP database systems are becoming ever more demanding.\nDomains such as finance and computer games increasingly mandate that developers\nbe able to encode complex application logic and control transaction latencies\nin in-memory databases. At the same time, infrastructure engineers in these\ndomains need to experiment with and deploy OLTP database architectures that\nensure application scalability and maximize resource utilization in modern\nmachines. In this paper, we propose a relational actor programming model for\nin-memory databases as a novel, holistic approach towards fulfilling these\nchallenging requirements. Conceptually, relational actors, or reactors for\nshort, are application-defined, isolated logical actors that encapsulate\nrelations and process function calls asynchronously. Reactors ease reasoning\nabout correctness by guaranteeing serializability of application-level function\ncalls. In contrast to classic transactional models, however, reactors allow\ndevelopers to take advantage of intra-transaction parallelism and state\nencapsulation in their applications to reduce latency and improve locality.\nMoreover, reactors enable a new degree of flexibility in database deployment.\nWe present ReactDB, a system design exposing reactors that allows for flexible\nvirtualization of database architecture between the extremes of shared-nothing\nand shared-everything without changes to application code. Our experiments\nillustrate latency control, low overhead, and asynchronicity trade-offs with\nReactDB in OLTP benchmarks.\n",
        "  In large but finite populations, weak demographic stochasticity due to random\nbirth and death events can lead to population extinction. The process is\nanalogous to the escaping problem of trapped particles under random forces.\nMethods widely used in studying such physical systems, for instance,\nWentzel-Kramers-Brillouin (WKB) and Fokker-Planck methods, can be applied to\nsolve similar biological problems. In this article, we comparatively analyse\napplications of WKB and Fokker-Planck methods to some typical stochastic\npopulation dynamical models, including the logistic growth, endemic SIR,\npredator-prey, and competitive Lotka-Volterra models. The mean extinction time\nstrongly depends on the nature of the corresponding deterministic fixed\npoint(s). For different types of fixed points, the extinction can be driven\neither by rare events or typical Gaussian fluctuations. In the former case, the\nlarge deviation function that governs the distribution of rare events can be\nwell-approximated by the WKB method in the weak noise limit. In the later case,\nthe simpler Fokker-Planck approximation approach is also appropriate.\n",
        "  Optimal sequence alignments depend heavily on alignment scoring parameters.\nGiven input sequences, {\\em parametric alignment} is the well-studied problem\nthat asks for all possible optimal alignment summaries as parameters vary, as\nwell as the {\\em optimality region} of alignment scoring parameters which yield\neach optimal alignment. But biologically correct alignments might be {\\em\nsuboptimal} for all parameter choices. Thus we extend parametric alignment to\n{\\em parametric $k$-best alignment}, which asks for all possible $k$-tuples of\n$k$-best alignment summaries $(s_1, s_2, ..., s_k)$, as well as the {\\em\n$k$-best optimality region} of scoring parameters which make $s_1, s_2, ...,\ns_k$ the top $k$ summaries. By exploiting the integer-structure of alignment\nsummaries, we show that, astonishingly, the complexity of parametric $k$-best\nalignment is only polynomial in $k$. Thus parametric $k$-best alignment is\ntractable, and can be applied at the whole-genome scale like parametric\nalignment.\n",
        "  We present a thermodynamic theory for a generic population of $M$ individuals\ndistributed into $N$ groups (clusters). We construct the ensemble of all\ndistributions with fixed $M$ and $N$, introduce a selection functional that\nembodies the physics that governs the population, and obtain the distribution\nthat emerges in the scaling limit as the most probable among all distributions\nconsistent with the given physics. We develop the thermodynamics of the\nensemble and establish a rigorous mapping to thermodynamics. We treat the\nemergence of a so-called \"giant component\" as a formal phase transition and\nshow that the criteria for its emergence are entirely analogous to the\nequilibrium conditions in molecular systems. We demonstrate the theory by an\nanalytic model and confirm the predictions by Monte Carlo simulation.\n",
        "  Humans and chimpanzees are believed to have shared a common ancestor about 6\nmillion years ago. Here using a new distance measure called the Jump distance,\nwe calculate the number of base substitutions that might have occurred in the\nmitochondrial DNA during these 6 million years.\n",
        "  Consider the cyclic group C_2 of order two acting by complex-conjugation on\nthe unit circle S^1. The main result is that a finitely dominated manifold W of\ndimension > 4 admits a cocompact, free, discontinuous action by the infinite\ndihedral group D_\\infty if and only if W is the infinite cyclic cover of a free\nC_2-manifold M such that M admits a C_2-equivariant manifold approximate\nfibration to S^1. The novelty in this setting is the existence of\ncodimension-one, invariant submanifolds of M and W. Along the way, we develop\nan equivariant sucking principle for certain orthogonal actions of finite\ngroups on Euclidean space.\n",
        "  Purpose: To make the planning of volumetric modulated arc therapy (VMAT)\nfaster and to explore the tradeoffs between planning objectives and delivery\nefficiency.\n  Methods: A convex multicriteria dose optimization problem is solved for an\nangular grid of 180 equi-spaced beams. This allows the planner to navigate the\nideal dose distribution Pareto surface and select a plan of desired target\ncoverage versus organ at risk sparing. The selected plan is then made VMAT\ndeliverable by a fluence map merging and sequencing algorithm, which combines\nneighboring fluence maps based on a similarity score and then delivers the\nmerged maps together, simplifying delivery. Successive merges are made as long\nas the dose distribution quality is maintained. The complete algorithm is\ncalled VMERGE.\n  Results: VMERGE is applied to three cases: a prostate, a pancreas, and a\nbrain. In each case, the selected Pareto-optimal plan is matched almost exactly\nwith the VMAT merging routine, resulting in a high quality plan delivered with\na single arc in less than five minutes on average.\n  VMERGE offers significant improvements over existing VMAT algorithms. The\nfirst is the multicriteria planning aspect, which greatly speeds up planning\ntime and allows the user to select the plan which represents the most desirable\ncompromise between target coverage and organ at risk sparing. The second is the\nuser-chosen epsilon-optimality guarantee of the final VMAT plan. Finally, the\nuser can explore the tradeoff between delivery time and plan quality, which is\na fundamental aspect of VMAT that cannot be easily investigated with current\ncommercial planning systems.\n",
        "  Rational design of superconductivity from Periodic Table properties is one of\nthe grand challenges of superconductivity. We recently showed (Arxiv:\n1208.0071) that high-Tc superconductivity exists in the Z = 5.667 with\nNe=2.333. Here we propose and show that materials with Z = 6.0 and Ne =2.0 and\n2.22 also meet the conditions for high-Tc superconductivity. We predict that\nthe Ne=2.67 variety will not be superconducting but the ternary and quaternary\nsystems of the Z =6.0 family with Ne=2.0 and 2.22 would have 12.5\\leqFw/Z\\leq25\nand Tcs that fall in the range 60K - 100K. We provide material specific\nexamples of such potential low-Z, Low-Ne high-Tc superconductors.\n",
        "  If an $m+2$-manifold $M$ is locally modeled on $\\RR^{m+2}$ with coordinate\nchanges lying in the subgroup $G=\\RR^{m+2}\\rtimes ({\\rO}(m+1,1)\\times \\RR^+)$\nof the affine group ${\\rA}(m+2)$, then $M$ is said to be a \\emph{Lorentzian\nsimilarity manifold}. A Lorentzian similarity manifold is also a conformally\nflat Lorentzian manifold because $G$ is isomorphic to the stabilizer of the\nLorentz group ${\\rPO}(m+2,2)$ which is the full Lorentzian group of the Lorentz\nmodel $S^{2n+1,1}$. It contains a class of Lorentzian flat space forms. We\nshall discuss the properties of compact Lorentzian similarity manifolds using\ndeveloping maps and holonomy representations.\n",
        "  Most phylogenetic models assume that the evolutionary process is stationary\nand reversible. As a result, the root of the tree cannot be inferred as part of\nthe analysis because the likelihood of the data does not depend on the position\nof the root. Yet defining the root of a phylogenetic tree is a key component of\nphylogenetic inference because it provides a point of reference for polarising\nancestor/descendant relationships and therefore interpreting the tree. In this\npaper we investigate the effect of relaxing the reversibility assumption and\nallowing the position of the root to be another unknown in the model. We\npropose two hierarchical models that are centred on a reversible model but\nperturbed to allow non-reversibility. The models differ in the degree of\nstructure imposed on the perturbations. The analysis is performed in the\nBayesian framework using Markov chain Monte Carlo methods. We illustrate the\nperformance of the two non-reversible models in analyses of simulated data sets\nusing two types of topological priors. We then apply the models to a real\nbiological data set, the radiation of polyploid yeasts, for which there is a\nrobust biological opinion about the root position. Finally we apply the models\nto a second biological data set for which the rooted tree is controversial: the\nribosomal tree of life. We compare the two non-reversible models and conclude\nthat both are useful in inferring the position of the root from real biological\ndata sets.\n",
        "  In recent years, numerous studies have inferred personality and other traits\nfrom people's online writing. While these studies are encouraging, more\ninformation is needed in order to use these techniques with confidence. How do\nlinguistic features vary across different online media, and how much text is\nrequired to have a representative sample for a person? In this paper, we\nexamine several large sets of online, user-generated text, drawn from Twitter,\nemail, blogs, and online discussion forums. We examine and compare\npopulation-wide results for the linguistic measure LIWC, and the inferred\ntraits of Big5 Personality and Basic Human Values. We also empirically measure\nthe stability of these traits across different sized samples for each\nindividual. Our results highlight the importance of tuning models to each\nonline medium, and include guidelines for the minimum amount of text required\nfor a representative result.\n",
        "  We study principal curvatures of fibers and Heegaard surfaces smoothly\nembedded in hyperbolic 3-manifolds. It is well known that a fiber or a Heegaard\nsurface in a hyperbolic 3-manifold cannot have principal curvatures everywhere\nless than one in absolute value. We show that given an upper bound on the genus\nof a minimally embedded fiber or Heegaard surface and a lower bound on the\ninjectivity radius of the hyperbolic 3-manifold, there exists a $\\delta > 0$\nsuch that the fiber or Heegaard surface must contain a point at which one of\nthe principal curvatures is greater than $1 + \\delta$ in absolute value.\n",
        "  We investigate the electronic properties and the superconducting gap\ncharacteristics of a single crystal of hole-doped 122 Fe-pnictide\nBa$_{0.65}$Na$_{0.35}$Fe$_2$As$_2$ by means of specific heat measurements. The\nspecific heat exhibits a pronounced anomaly around the superconducting\ntransition temperature $T_c$ = 29.4 K, and a small residual part at low\ntemperature. In a magnetic field of 90 kOe, the transition is broadened and\n$T_c$ is lowered insignificantly by an amount $\\sim$ 1.5 K. We estimate a high\nelectronic coefficient in the normal state with a value 57.5 mJ mol$^{-1}$\nK$^2$, being consistent with hole-doped 122 compounds. The\ntemperature-dependent superconducting electronic specific heat cannot be\ndescribed with single-gap BCS theory under weak coupling approach. Instead, our\nanalysis implies a presence of two s-wave like gaps with magnitudes\n$\\Delta_1(0)/k_BT_c$ = 1.06 and $\\Delta_2(0)/k_BT_c$ = 2.08 with their\nrespective weights of 48% and 52%. While our results have qualitative\nsimilarities with other hole-doped 122 materials, the gap's magnitude and their\nratio are quite different.\n",
        "  We introduce diagrams and Reidemeister moves for links in FxS^{1}, where F is\nan orientable surface. Using these diagrams we compute (in a new way) the\nKauffman Bracket Skein Modules (KBSM) for D^{2}xS^{1} and AxS^{1}, where D^{2}\nis a disk and A is an annulus. Moreover, we also find the KBSM for the\nF_{0,3}xS^{1}, where F_{0,3} denotes a disk with two holes, and thus show that\nthe module is free.\n",
        "  A looming challenge for agriculture is sustainable intensification of food\nproduction to feed the growing human population. Current chemical and genetic\ntechnologies used to manage plant diseases are highly vulnerable to pathogen\nevolution and are not sustainable. Pathogen evolution is facilitated by the\ngenetic uniformity underlying modern agroecosystems, suggesting that one path\nto sustainable disease control lies through increasing genetic diversity at the\nfield scale by using genetically diverse host mixtures. We investigate how host\nmixtures can improve disease control using a population dynamics model. We find\nthat when a population of crop plants is exposed to host-specialized pathogen\nspecies or strains, the overall disease severity is smaller in the mixture of\ntwo host varieties than in each of the corresponding pure stands. The disease\nseverity can be minimized over a range of mixing ratios. These findings may\nhelp in designing host mixtures that efficiently control diseases of crops. We\nthen generalize the model to describe host mixtures with many components. We\nfind that when pathogens exhibit host specialization, the overall disease\nseverity decreases with the number of components in the mixture. As the degree\nof specialization increases, the decrease in disease severity becomes larger.\nUsing these model outcomes, we propose ways to optimize the use of host\nmixtures to decrease disease in agroecosystems.\n",
        "  In this work, it is analyzed the ability of split-ring metamaterial slabs\nwith zero/high permeability to reject/confine the radiofrequency magnetic field\nin magnetic resonance imaging systems. Using an homogenization procedure,\nsplit-ring slabs have been designed and fabricated to work in a 1.5T system.\nActive elements consisting of pairs of crossed diodes are inserted in the\nsplit-rings. With these elements, the permeability of the slabs can be\nautomatically switched between a unity value when interacting with the strong\nexcitation field of the transmitting body coil, and zero or high values when\ninteracting with the weak field produced by protons in tissue. Experiments are\nshown for different configurations where these slabs can help to locally\nincrease the signal-to-noise-ratio.\n",
        "  We present a theoretical study of restoration of superconductivity in the\nform of the triplet reentrant superconducting phase in a quasi-one-dimensional\n(Q1D) conductor. Substitution of known band and superconducting parameters of\nthe presumably triplet Q1D superconductor Li$_{0.9}$Mo$_6$O$_{17}$ into our\ntheoretical equations shows that such restoration can happen in feasibly high\nnon-destructive pulsed magnetic field of the order of $H \\simeq 100 \\ T$. We\ninvestigate in detail how small inclinations of a direction of a magnetic field\nfrom its best experimental geometry decrease superconducting transition\ntemperature of the reentrant phase, which is important for its possible\nexperimental discovery.\n",
        "  For certain classes of knots we define geometric invariants called\nhigher-order genera. Each of these invariants is a refinement of the slice\ngenus of a knot. We find lower bounds for the higher-order genera in terms of\ncertain von Neumann $\\rho$-invariants, which we call higher-order signatures.\nThe higher-order genera offer a refinement of the Grope filtration of the knot\nconcordance group.\n",
        "  Object This study proposes a scale space based algorithm for automated\nsegmentation of single-shot tagged images of modest SNR. Furthermore the\nalgorithm was designed for analysis of discontinuous or shearing types of\nmotion, i.e. segmentation of broken tag patterns.\n  Materials and methods The proposed algorithm utilizes non-linear scale space\nfor automatic segmentation of single-shot tagged images. The algorithm's\nability to automatically segment tagged shearing motion was evaluated in a\nnumerical simulation and in vivo. A typical shearing deformation was simulated\nin a Shepp-Logan phantom allowing for quantitative evaluation of the\nalgorithm's success rate as a function of both SNR and the amount of\ndeformation. For a qualitative in vivo evaluation tagged images showing\ndeformations in the calf muscles and eye movement in a healthy volunteer were\nacquired.\n  Results Both the numerical simulation and the in vivo tagged data\ndemonstrated the algorithm's ability for automated segmentation of single-shot\ntagged MR provided that SNR of the images is above 10 and the amount of\ndeformation does not exceed the tag spacing. The latter constraint can be met\nby adjusting the tag delay or the tag spacing.\n  Conclusion The scale space based algorithm for automatic segmentation of\nsingle-shot tagged MR enables the application of tagged MR to complex\n(shearing) deformation and the processing of datasets with relatively low SNR.\n",
        "  Optimising queries in real-world situations under imperfect conditions is\nstill a problem that has not been fully solved. We consider finding the optimal\norder in which to execute a given set of selection operators under partial\nignorance of their selectivities. The selectivities are modelled as intervals\nrather than exact values and we apply a concept from decision theory, the\nminimisation of the maximum regret, as a measure of optimality. We show that\nthe associated decision problem is NP-hard, which renders a brute-force\napproach to solving it impractical. Nevertheless, by investigating properties\nof the problem and identifying special cases which can be solved in polynomial\ntime, we gain insight that we use to develop a novel heuristic for solving the\ngeneral problem. We also evaluate minmax regret query optimisation\nexperimentally, showing that it outperforms a currently employed strategy of\noptimisers that uses mean values for uncertain parameters.\n",
        "  Generalizing Howie and Greene's characterization of alternating knots, we\ngive a topological characterization of almost alternating knots.\n",
        "  The increasing growth of databases raises an urgent need for more accurate\nmethods to better understand the stored data. In this scope, association rules\nwere extensively used for the analysis and the comprehension of huge amounts of\ndata. However, the number of generated rules is too large to be efficiently\nanalyzed and explored in any further process. Association rules selection is a\nclassical topic to address this issue, yet, new innovated approaches are\nrequired in order to provide help to decision makers. Hence, many interesting-\nness measures have been defined to statistically evaluate and filter the\nassociation rules. However, these measures present two major problems. On the\none hand, they do not allow eliminating irrelevant rules, on the other hand,\ntheir abun- dance leads to the heterogeneity of the evaluation results which\nleads to confusion in decision making. In this paper, we propose a two-winged\napproach to select statistically in- teresting and semantically incomparable\nrules. Our statis- tical selection helps discovering interesting association\nrules without favoring or excluding any measure. The semantic comparability\nhelps to decide if the considered association rules are semantically related\ni.e comparable. The outcomes of our experiments on real datasets show promising\nresults in terms of reduction in the number of rules.\n",
        "  We analyze interstellar absorption features in the full UV spectrum of the\nnearby (d = 24 pc) B8 IVn star alpha Leo (Regulus) obtained at high resolution\nand high S/N by the HST ASTRAL Treasury program. We derive column densities for\nmany key atomic species and interpret their partial ionizations. The gas in\nfront of alpha Leo exhibits two absorption components, one of which coincides\nin velocity with the local interstellar cloud (LIC) that surrounds the Sun. The\nsecond, smaller, component is shifted by +5.6 km/s relative to the main\ncomponent, in agreement with results for other lines of sight in this region of\nthe sky. The excitation of the C II fine-structure levels and the ratio of Mg I\nto Mg II reveal a temperature T = 6500 (+750,-600)K and electron density n(e) =\n0.11 (+0.025,-0.03) cm^-3. Our investigation of the ionization balance of all\nthe available species indicates that about 1/3 of the hydrogen atoms are\nionized and that metals are significantly depleted onto grains. We infer that\nN(H I) = 1.9 (+0.9,-0.6) X 10^{18} cm^-2, which indicates that this partly\nneutral gas occupies only 2 to 8 pc (about 13%) of the space toward the star,\nwith the remaining volume presumably being filled with a hot gas that emits\nsoft X-rays. We do not detect any absorption features from the highly ionized\nspecies that could be produced in an interface between the warm medium and the\nsurrounding hot gas. Finally, the radial velocity of the LIC agrees with that\nof the Local Leo Cold Cloud, indicating that they may be physically related.\n",
        "  A lattice of Abrikosov vortices in type II superconductors is characterized\nby a periodic modulation of the magnetic induction perpendicular to the applied\nmagnetic field. For a coherent vortex motion under the action of a transport\ncurrent, the magnetic induction at a given point of the sample varies in time\nwith a washboard frequency f_WB = v/d, where v is the vortex velocity and d is\nthe distance between the vortices in the direction of motion. Here, by using a\nspectrum analyzer connected to a 50 nm-wide Au nanowire meander near the\nsurface of a superconducting Nb film we detect an ac voltage induced by\ncoherently moving fluxons. The voltage is peaked at the washboard frequency,\nf_WB, and its subharmonics, f_TOF = f_WB/5, determined by the antenna width. By\nsweeping the dc current value, we reveal that f_WB can be tuned from 100 MHz to\n1.5 GHz, thereby demonstrating that patterned normal metal/superconductor\nnanostructures can be used as dc-tunable generators operating in the\nradiofrequency range.\n",
        "  1. Theoretical models pertaining to feedbacks between ecological and\nevolutionary processes are prevalent in multiple biological fields. An\nintegrative overview is currently lacking, due to little crosstalk between the\nfields and the use of different methodological approaches.\n  2. Here we review a wide range of models of eco-evolutionary feedbacks and\nhighlight their underlying assumptions. We discuss models where feedbacks occur\nboth within and between hierarchical levels of ecosystems, including\npopulations, communities, and abiotic environments, and consider feedbacks\nacross spatial scales.\n  3. Identifying the commonalities among feedback models, and the underlying\nassumptions, helps us better understand the mechanistic basis of\neco-evolutionary feedbacks. Eco-evolutionary feedbacks can be readily modelled\nby coupling demographic and evolutionary formalisms. We provide an overview of\nthese approaches and suggest future integrative modelling avenues.\n  4. Our overview highlights that eco-evolutionary feedbacks have been\nincorporated in theoretical work for nearly a century. Yet, this work does not\nalways include the notion of rapid evolution or concurrent ecological and\nevolutionary time scales. We discuss the importance of density- and\nfrequency-dependent selection for feedbacks, as well as the importance of\ndispersal as a central linking trait between ecology and evolution in a spatial\ncontext.\n",
        "  The Lambek-Grishin calculus is a symmetric extension of the Lambek calculus:\nin addition to the residuated family of product, left and right division\noperations of Lambek's original calculus, one also considers a family of\ncoproduct, right and left difference operations, related to the former by an\narrow-reversing duality. Communication between the two families is implemented\nin terms of linear distributivity principles. The aim of this paper is to\ncomplement the symmetry between (dual) residuated type-forming operations with\nan orthogonal opposition that contrasts residuated and Galois connected\noperations. Whereas the (dual) residuated operations are monotone, the Galois\nconnected operations (and their duals) are antitone. We discuss the algebraic\nproperties of the (dual) Galois connected operations, and generalize the\n(co)product distributivity principles to include the negative operations. We\ngive a continuation-passing-style translation for the new type-forming\noperations, and discuss some linguistic applications.\n",
        "  We study knots in $S^3$ with infinitely many $SU(2)$-cyclic surgeries, which\nare Dehn surgeries such that every representation of the resulting fundamental\ngroup into $SU(2)$ has cyclic image. We show that for every such nontrivial\nknot $K$, its set of $SU(2)$-cyclic slopes is bounded and has a unique limit\npoint, which is both a rational number and a boundary slope for $K$. We also\nshow that such knots are prime and have infinitely many instanton L-space\nsurgeries. Our methods include the application of holonomy perturbation\ntechniques to instanton knot homology, using a strengthening of recent work by\nthe second author.\n",
        "  The pressure dependence of the structural, magnetic and superconducting\ntransitions and of the superconducting upper critical field were studied in\nsulfur-substituted Fe(Se$_{1-x}$S$_{x}$). Resistance measurements were\nperformed on single crystals with three substitution levels ($x$=0.043, 0.096,\n0.12) under hydrostatic pressures up to 1.8 GPa and in magnetic fields up to 9\nT, and compared to data on pure FeSe. Our results illustrate the effects of\nchemical and physical pressure on Fe(Se$_{1-x}$S$_{x}$). On increasing sulfur\ncontent, magnetic order in the low-pressure range is strongly suppressed to a\nsmall dome-like region in the phase diagrams. However, $T_s$ is much less\nsuppressed by sulfur substitution and $T_c$ of Fe(Se$_{1-x}$S$_{x}$) exhibits\nsimilar non-monotonic pressure dependence with a local maximum and a local\nminimum present in the low pressure range for all $x$. The local maximum in\n$T_c$ coincides with the emergence of the magnetic order above $T_c$. At this\npressure the slope of the upper critical field decreases abruptly. The minimum\nof $T_c$ correlates with a broad maximum of the upper critical field slope\nnormalized by $T_c$.\n",
        "  Current inverse treatment planning methods that optimize both catheter\npositions and dwell times in prostate HDR brachytherapy use surrogate linear or\nquadratic objective functions that have no direct interpretation in terms of\ndose-volume histogram (DVH) criteria, do not result in an optimum or have long\nsolution times.\n  We decrease the solution time of existing linear and quadratic dose-based\nprogramming models (LP and QP, respectively) to allow optimizing over potential\ncatheter positions using mixed integer programming. An additional average\nspeed-up of 75% can be obtained by stopping the solver at an early stage,\nwithout deterioration of the plan quality. For a fixed catheter configuration,\nthe dwell time optimization model LP solves to optimality in less than 15\nseconds, which confirms earlier results. We propose an iterative procedure for\nQP that allows to prescribe the target dose as an interval, while retaining\nindependence between the solution time and the number of dose calculation\npoints. This iterative procedure is comparable in speed to the LP model, and\nproduces better plans than the non-iterative QP.\n  We formulate a new dose-volume based model that maximizes $V_{100\\%}$ while\nsatisfying pre-set DVH-criteria. This model optimizes both catheter positions\nand dwell times within a few minutes depending on prostate volume and number of\ncatheters, optimizes dwell times within 35 seconds, and gives better DVH\nstatistics than dose-based models. The solutions suggest that the correlation\nbetween objective value and clinical plan quality is weak in existing\ndose-based models.\n",
        "  We report on a community effort between industry and academia to shape the\nfuture of graph query languages. We argue that existing graph database\nmanagement systems should consider supporting a query language with two key\ncharacteristics. First, it should be composable, meaning, that graphs are the\ninput and the output of queries. Second, the graph query language should treat\npaths as first-class citizens. Our result is G-CORE, a powerful graph query\nlanguage design that fulfills these goals, and strikes a careful balance\nbetween path query expressivity and evaluation complexity.\n",
        "  The possibility of exploiting multiple resources is usually regarded as\npositive from both the economic and the environmental point of view. However,\nresource switching may also lead to unsustainable growth and, ultimately, to an\nequilibrium condition which is worse than the one that could have been achieved\nwith a single resource. We applied a system dynamics model where users exploit\nmultiple resources and have different levels of preference among them. In this\nsetting, exploiting multiple resources leads to worse outcomes than the\nsingle-resource case under a wide range of parameter configurations. Our\narguments are illustrated using two empirical situations, namely oil drilling\nin the North Sea and whale hunting in the Antarctic.\n",
        "  We investigate a decentralised approach to committing transactions in a\nreplicated database, under partial replication. Previous protocols either\nre-execute transactions entirely and/or compute a total order of transactions.\nIn contrast, ours applies update values, and orders only conflicting\ntransactions. It results that transactions execute faster, and distributed\ndatabases commit in small committees. Both effects contribute to preserve\nscalability as the number of databases and transactions increase. Our algorithm\nensures serializability, and is live and safe in spite of faults.\n",
        "  In these days, 3D-printer is on the rise in various fields including\nradiation therapy. This preliminary study aimed to estimate the dose\ncharacteristics of the 3D-printer materials which could be used as the\ncompensator or immobilizer in radiation treatment. The cubes which have 5cm\nlength and different densities as 50%, 75% and 100% were printed by 3D-printer.\nA planning CT scans for cubes were performed using a CT simulator (Brilliance\nCT, Philips Medical System, Netherlands). Dose distributions behind the cube\nwere calculated when 6MV photon beam passed through cube. The dose response for\n3D-printed cube, air and water were measured by using EBT3 film and 2D array\ndetector. When results of air case were normalized to 100, dose calculated by\nTPS and measured dose of 50% and 75% cube were 96~99. Measured and calculated\ndoses of water and 100% cube were 82~84. HU values of 50%, 75% and 100% were\n-910, -860 and -10, respectively. From these results, 3D-printer in\nradiotherapy could be used for medical purpose accurately.\n",
        "  Seed banks are a common characteristics to many plant species, which allow\nstorage of genetic diversity in the soil as dormant seeds for various periods\nof time. We investigate an above-ground population following a Fisher-Wright\nmodel with selection coupled with a deterministic seed bank assuming the length\nof the seed bank is kept constant and the number of seeds is large. To assess\nthe combined impact of seed banks and selection on genetic diversity, we derive\na general diffusion model. The applied techniques outline a path of\napproximating a stochastic delay differential equation by an appropriately\nrescaled stochastic differential equation, which is a common issue in\nstatistical physics. We compute the equilibrium solution of the site-frequency\nspectrum and derive the times to fixation of an allele with and without\nselection. Finally, it is demonstrated that seed banks enhance the effect of\nselection onto the site-frequency spectrum while slowing down the time until\nthe mutation-selection equilibrium is reached.\n",
        "  Providing machine learning (ML) over relational data is a mainstream\nrequirement for data analytics systems. While almost all the ML tools require\nthe input data to be presented as a single table, many datasets are\nmulti-table, which forces data scientists to join those tables first, leading\nto data redundancy and runtime waste. Recent works on \"factorized\" ML mitigate\nthis issue for a few specific ML algorithms by pushing ML through joins. But\ntheir approaches require a manual rewrite of ML implementations. Such piecemeal\nmethods create a massive development overhead when extending such ideas to\nother ML algorithms. In this paper, we show that it is possible to mitigate\nthis overhead by leveraging a popular formal algebra to represent the\ncomputations of many ML algorithms: linear algebra. We introduce a new logical\ndata type to represent normalized data and devise a framework of algebraic\nrewrite rules to convert a large set of linear algebra operations over\ndenormalized data into operations over normalized data. We show how this\nenables us to automatically \"factorize\" several popular ML algorithms, thus\nunifying and generalizing several prior works. We prototype our framework in\nthe popular ML environment R and an industrial R-over-RDBMS tool. Experiments\nwith both synthetic and real normalized data show that our framework also\nyields significant speed-ups, up to 36x on real data.\n",
        "  We prove that there is an algorithm which determines whether or not a given\n2-polyhedron can be embedded into some integral homology 3-sphere.\n  This is a corollary of the following main result. Let $M$ be a compact\nconnected orientable 3-manifold with boundary. Denote $G=\\Z$, $G=\\Z/p\\Z$ or\n$G=\\Q$. If $H_1(M;G)\\cong G^k$ and $\\bd M$ is a surface of genus $g$, then the\nminimal group $H_1(Q;G)$ for closed 3-manifolds $Q$ containing $M$ is\nisomorphic to $G^{k-g}$.\n  Another corollary is that for a graph $L$ the minimal number $\\rk H_1(Q;\\Z)$\nfor closed orientable 3-manifolds $Q$ containing $L\\times S^1$ is twice the\norientable genus of the graph.\n",
        "  Living at high-altitude is one of the most difficult challenges that humans\nhad to cope with during their evolution. Whereas several genomic studies have\nrevealed some of the genetic bases of adaptations in Tibetan, Andean and\nEthiopian populations, relatively little evidence of convergent evolution to\naltitude in different continents has accumulated. This lack of evidence can be\ndue to truly different evolutionary responses, but it can be also due to the\nlow power of former studies that have mainly focused on populations from a\nsingle geographical region or performed separate analyses on multiple pairs of\npopulations to avoid problems linked to shared histories between some\npopulations. We introduce here a hierarchical Bayesian method to detect local\nadaptation that can deal with complex demographic histories. Our method can\nidentify selection occurring at different scales, as well as convergent\nadaptation in different regions. We apply our approach to the analysis of a\nlarge SNP dataset from low- and high-altitude human populations from America\nand Asia. The simultaneous analysis of these two geographic areas allows us to\nidentify several candidate genome regions for altitudinal selection, and we\nshow that convergent evolution among continents has been quite common. In\naddition to identifying several genes and biological processes involved in high\naltitude adaptation, we identify two specific biological pathways that could\nhave evolved in both continents to counter toxic effects induced by hypoxia.\n",
        "  We propose a new segmentation evaluation metric, called segmentation\nsimilarity (S), that quantifies the similarity between two segmentations as the\nproportion of boundaries that are not transformed when comparing them using\nedit distance, essentially using edit distance as a penalty function and\nscaling penalties by segmentation size. We propose several adapted\ninter-annotator agreement coefficients which use S that are suitable for\nsegmentation. We show that S is configurable enough to suit a wide variety of\nsegmentation evaluations, and is an improvement upon the state of the art. We\nalso propose using inter-annotator agreement coefficients to evaluate automatic\nsegmenters in terms of human performance.\n",
        "  We describe a new method that allows us to quantitatively characterize\ngalactic satellites from analysis of disturbances in outer gas disks, without\nrequiring knowledge of their optical light. We have demonstrated the validity\nof this method, which we call Tidal Analysis, by applying it to local spirals\nwith known optical companions, including M51 and NGC 1512. These galaxies span\nthe range from having a low mass companion (~ one-hundredth the mass of the\nprimary galaxy) to a fairly massive companion (~ one-third the mass of the\nprimary galaxy). This approach has broad implications for many areas of\nastrophysics - for the indirect detection of dark matter (or dark-matter\ndominated dwarf galaxies), and for galaxy evolution in its use as a decipher of\nthe dynamical impact of satellites on galactic disks. Here, we present some\npreliminary results on the emergent SEDs and images, calculated along the time\nsequence of these dynamical simulations using the 3-D self-consistent Monte\nCarlo radiative transfer code RADISHE. We explore star formation prescriptions\nand how they affect the emergent SEDs and images. Our goal is to identify SED\ncolors that are primarily affected by the galaxy's interaction history, and not\nsignificantly affected by the choice of star formation prescription. If\nsuccessful, we may be able to utilize the emergent UV-IR SED of the primary\ngalaxy to understand its recent interaction history.\n",
        "  The predominantly ancient stellar populations observed in the lowest-mass\ngalaxies (i.e. ultra-faint dwarfs) suggest that their star formation was\nsuppressed by reionization. Most of the well-studied ultra-faint dwarfs,\nhowever, are within the central half of the Milky Way dark matter halo, such\nthat they are consistent with a population that was accreted at early times and\nthus potentially quenched via environmental processes. To study the potential\nrole of environment in suppressing star formation on the smallest scales, we\nutilize the Exploring the Local Volume in Simulations (ELVIS) suite of $N$-body\nsimulations to constrain the distribution of infall times for low-mass subhalos\nlikely to host the ultra-faint population. For the ultra-faint satellites of\nthe Milky Way with star-formation histories inferred from\n$Hubble~Space~Telescope$ imaging, we find that environment is highly unlikely\nto play a dominant role in quenching their star formation. Even when including\nthe potential effects of pre-processing, there is a $\\lesssim 0.1\\%$\nprobability that environmental processes quenched all of the known ultra-faint\ndwarfs early enough to explain their observed star-formation histories.\nInstead, we argue for a mass floor in the effectiveness of satellite quenching\nat roughly $M_{\\star} \\sim 10^{5}~M_{\\odot}$, below which star formation in\nsurviving galaxies is globally suppressed by reionization. We predict a large\npopulation of quenched ultra-faint dwarfs in the Local Field ($1 <\nR/R_{\\rm{vir}} < 2$), with as many as $\\sim250$ to be discovered by future\nwide-field imaging surveys.\n",
        "  In the present paper we report an in-situ high-energy X-ray diffraction\nanalysis of MgB2 tapes during the preparation process. The experiment was\nperformed in a specifically designed furnace working in reducing atmosphere,\ncompatible with the Laue diffraction condition. The MgB2 synthesis was realized\nstarting from MgH2 and amorphous B in powder form as precursors, varying\nreaction temperature and testing different cooling processes. We analyzed both\nthe MgB2 synthesis and the sintering process of tapes prepared with these\npowders. Phase evolution, micro and crystallographic structure were monitored\nduring the different thermal treatments. Among the main results we observed the\nformation of MgB2 at an extraordinary low temperature (300C), probably as a\nresult of a solid-state reaction between MgH2 and B. Furthermore, we studied\nthe dependence of the micro-structure upon the thermal treatment and its effect\non the critical current performance of the superconducting tapes.\n",
        "  This paper presents architecture for health care data warehouse specific to\ncancer diseases which could be used by executive managers, doctors, physicians\nand other health professionals to support the healthcare process. The data\ntoday existing in multi-sources with different formats makes it necessary to\nhave some techniques for data integration. Executive managers need access to\nInformation so that decision makers can react in real time to changing needs.\nInformation is one of the most factors to an organization success that\nexecutive managers or physicians would need to base their decisions on, during\ndecision making. A health care data warehouse is therefore necessary to\nintegrate the different data sources into a central data repository and\nanalysis this data.\n",
        "  In this work, automatic analysis of themes contained in a large corpora of\njudgments from public procurement domain is performed. The employed technique\nis unsupervised latent Dirichlet allocation (LDA). In addition, it is proposed,\nto use LDA in conjunction with recently developed method of unsupervised\nkeyword extraction. Such an approach improves the interpretability of the\nautomatically obtained topics and allows for better computational performance.\nThe described analysis illustrates a potential of the method in detecting\nrecurring themes and discovering temporal trends in lodged contract appeals.\nThese results may be in future applied to improve information retrieval from\nrepositories of legal texts or as auxiliary material for legal analyses carried\nout by human experts.\n",
        "  We examine the stellar velocity dispersions (sigma) of a sample of 48\ngalaxies, 35 of which are spirals, from the Palomar nearby galaxy survey. It is\nknown that for ultra-luminous infrared galaxies (ULIRGs) and merger remnants\nthesigma derived from the near-infrared CO band-heads is smaller than that\nmeasured from optical lines, while no discrepancy between these measurements is\nfound for early-type galaxies. No such studies are available for spiral\ngalaxies - the subject of this paper. We used cross-dispersed spectroscopic\ndata obtained with the Gemini Near-Infrared Spectrograph (GNIRS), with spectral\ncoverage from 0.85 to 2.5um, to obtain sigma measurements from the 2.29 $\\mu$m\nCO band-heads (sigma_{CO}), and the 0.85 um calcium triplet (sigma_{CaT}). For\nthe spiral galaxies in the sample, we found that sigma_{CO} is smaller than\nsigma_{CaT}, with a mean fractional difference of 14.3%. The best fit to the\ndata is given by sigma_{opt} = (46.0+/-18.1) + (0.85+/-0.12)sigma_{CO}. This\n\"sigma discrepancy\" may be related to the presence of warm dust, as suggested\nby a slight correlation between the discrepancy and the infrared luminosity.\nThis is consistent with studies that have found no sigma-discrepancy in\ndust-poor early-type galaxies, and a much larger discrepancy in dusty merger\nremnants and ULIRGs. That sigma_{CO}$ is lower than sigma_{opt} may also\nindicate the presence of a dynamically cold stellar population component. This\nwould agree with the spatial correspondence between low sigma_{CO} and\nyoung/intermediate-age stellar populations that has been observed in\nspatially-resolved spectroscopy of a handful of galaxies.\n",
        "  We have studied the temperature induced $0 -\\pi $ thermodynamic transition in\nNb/PdNi/Nb Superconductor/Ferromagnetic/Superconductor (SFS) heterostructures\nby microwave measurements of the superfluid density. We have observed a shift\nin the transition temperature with the ageing of the heterostructures,\nsuggesting that structural and/or chemical changes took place. Motivated by the\nelectrodynamics findings, we have extensively studied the local structural\nproperties of the samples by means of X-ray Absorption Spectroscopy (XAS)\ntechnique, and the compositional profile by Time-of-Flight Secondary Ion Mass\nSpectrometry (ToF-SIMS). We found that the samples have indeed changed their\nproperties, in particular for what concerns the interfaces and the composition\nof the ferromagnetic alloy layer. The structural and compositional data are\nconsistent with the shift of the $0-\\pi$ transition toward the behaviour of\nheterostructures with different F layers. An important emerging indication to\nthe physics of SFS is the weak relevance of the ideality of the interfaces:\neven in aged samples, with less-than-ideal interfaces, the temperature-induced\n$0-\\pi$ transition is still detectable albeit at a different critical F\nthickness.\n",
        "  The eco-evolutionary dynamics of species are fundamentally linked to the\nenergetic constraints of its constituent individuals. Of particular importance\nis the interplay between reproduction and the dynamics of starvation and\nrecovery. To elucidate this interplay, we introduce a nutritional\nstate-structured model that incorporates two classes of consumer: nutritionally\nreplete, reproducing consumers, and undernourished, non-reproducing consumers.\nWe obtain strong constraints on starvation and recovery rates by deriving\nallometric scaling relationships and find that population dynamics are\ntypically driven to a steady state. Moreover, these rates fall within a\n'refuge' in parameter space, where the probability of population extinction is\nminimized. We also show that our model provides a natural framework to predict\nmaximum mammalian body size by determining the relative stability of an\notherwise homogeneous population to a competing population with altered percent\nbody fat. This framework provides a principled mechanism for a selective driver\nof Cope's rule.\n",
        "  It is known that there are 21 ribbon knots with 10 crossings or fewer. We\nshow that for every ribbon knot, there exists a tangle that satisfies two\nproperties associated with the knot. First, under a specific closure, the\nclosed tangle is equivalent to its corresponding knot. Second, under a\ndifferent closure, the closed tangle is equivalent to the unlink. For each of\nthese 21 ribbon knots, we present a 4-strand tangle that satisfies these\nproperties. We provide diagrams of these tangles and also express them in\nplanar diagram notation.\n",
        "  We prove that a fundamental group of codimension one nonnegative Ricci\ncurvature C2-foliation of a closed Riemannian manifold is finitely generated\nand almost abelian, i.e. it contains abelian subgroup of finite index. In\nparticular, we confirm the Milnor conjecture for manifolds which are leaves of\ncodimension one nonnegative Ricci curvature foliation of closed manifold.\n",
        "  Motivated by Sr$_2$RuO$_4$, edge quasiparticle states are analyzed based on\nthe self-consistent solution of the Bogolyubov-de Gennes equations for a\ntopological chiral $p$-wave superconductor. Using a tight-binding model of a\nsquare lattice for the dominant $\\gamma$-band we explore the non-trivial\ngeometry and band structure dependence of the edge states and currents. As a\npeculiar finding we show that for high band fillings currents flow in reversed\ndirection comparing straight and zigzag edges. We give a simple explanation in\nterms of the positions of the zero-energy bound states using a quasi-classical\npicture. We also show that a Ginzburg-Landau approach can reproduce these\nresults. Moreover, the band filling dependence of the most stable domain wall\nstructure is discussed.\n",
        "  We establish a correspondence between trisections of smooth, compact,\noriented $4$--manifolds with connected boundary and diagrams describing these\ntrisected $4$--manifolds. Such a diagram comes in the form of a compact,\noriented surface with boundary together with three tuples of simple closed\ncurves, with possibly fewer curves than the genus of the surface, satisfying a\npairwise standardness condition. This should be thought of as the\n$4$--dimensional analog of a sutured Heegaard diagram for a sutured\n$3$--manifold. We also give many foundational examples.\n",
        "  A brief description of the magnetic resonance imaging and related advanced\ntechniques like diffusion, perfusion and spectroscopy in the human brain\nexaminations is given.\n",
        "  Individual resource intake rates are known to depend on both individual body\nsize and resource availability. Here, we have developed a model to integrate\nthese two drivers, accounting explicitly for the scaling of perceived resource\navailability with individual body size. The model merges a Kleiber-like scaling\nlaw with Holling functional responses into a single mathematical framework,\ninvolving both body-size the density of resources.\n  When the availability of resources is held constant the model predicts a\nrelationship between resource intake rates and body sizes whose log-log graph\nis a concave curve. The significant deviation from a power law accounts for the\nbody size dependency of resource limitations. The model results are consistent\nwith data from both a laboratory experiment on benthic macro-invertebrates and\nthe available literature.\n",
        "  When predicting the fate and consequences of recurring deleterious mutations\nin self-fertilising populations most models developed make the assumption that\npopulations have discrete non-overlapping generations. This makes them\nbiologically irrelevant when considering perennial species with over-lapping\ngenerations and where mating occurs independently of the age group. Previous\nmodels studying the effect of perennial life-histories on the genetic\nproperties of populations in the presence of self-fertilisation have done so\nconsidering age-dependent selection and have found that, contrary to empirical\nobservations, perennial populations should exhibit lower levels of inbreeding\ndepression. Here we propose a simple deterministic model in continuous time\nwith selection at different fitness traits and feedback between population\nfitness and size. We find that a perennial life-history can result in high\nlevels of inbreeding depression in spite of inbreeding, due to higher\nfrequencies of heterozygous individuals at the adult stage. We also propose\nthat there may be demographic advantages for self-fertilisation that are\nindependent of reproductive success.\n",
        "  We show that there are links whose individual components are concordant to\nthe unknot, but which are not concordant to any link with unknotted components.\nWe give examples in the topological category, and examples in the smooth\ncategory which are topologically slice. We also give generalizations regarding\ncomponents of prescribed Alexander polynomials. The main tools are covering\nlink calculus, algebraic invariants of rational knot concordance theory, and\nthe correction term of Heegaard Floer homology.\n",
        "  Let R be a compact oriented surface of genus g with one boundary component.\nHomology cylinders over R form a monoid IC into which the Torelli group I of R\nembeds by the mapping cylinder construction. Two homology cylinders M and M'\nare said to be Y_k-equivalent if M' is obtained from M by \"twisting\" an\narbitrary surface S in M with a homeomorphim belonging to the k-th term of the\nlower central series of the Torelli group of S. The J_k-equivalence relation on\nIC is defined in a similar way using the k-th term of the Johnson filtration.\nIn this paper, we characterize the Y_3-equivalence with three classical\ninvariants: (1) the action on the third nilpotent quotient of the fundamental\ngroup of R, (2) the quadratic part of the relative Alexander polynomial, and\n(3) a by-product of the Casson invariant. Similarly, we show that the\nJ_3-equivalence is classified by (1) and (2). We also prove that the core of\nthe Casson invariant (originally defined by Morita on the second term of the\nJohnson filtration of I) has a unique extension (to the corresponding submonoid\nof IC) that is preserved by Y_3-equivalence and the mapping class group action.\n",
        "  Outsourcing data into the cloud becomes popular thanks to the pay-as-you-go\nparadigm. However, such practice raises privacy concerns. The conventional way\nto achieve data privacy is to encrypt sensitive data before outsourcing. When\ndata are encrypted, a trade-off must be achieved between security and efficient\nquery processing. Existing solutions that adopt multiple encryption schemes\ninduce a heavy overhead in terms of data storage and query performance, and are\nnot suited for cloud data warehouses. In this paper, we propose an efficient\nadditive encryption scheme (S4) based on Shamir's secret sharing for securing\ndata warehouses in the cloud. S4 addresses the shortcomings of existing\napproaches by reducing overhead while still enforcing good data privacy.\nExperimental results show the efficiency of S4 in terms of computation and\nstorage overhead with respect to existing solutions.\n",
        "  This paper proposes to tackle open- domain question answering using Wikipedia\nas the unique knowledge source: the answer to any factoid question is a text\nspan in a Wikipedia article. This task of machine reading at scale combines the\nchallenges of document retrieval (finding the relevant articles) with that of\nmachine comprehension of text (identifying the answer spans from those\narticles). Our approach combines a search component based on bigram hashing and\nTF-IDF matching with a multi-layer recurrent neural network model trained to\ndetect answers in Wikipedia paragraphs. Our experiments on multiple existing QA\ndatasets indicate that (1) both modules are highly competitive with respect to\nexisting counterparts and (2) multitask learning using distant supervision on\ntheir combination is an effective complete system on this challenging task.\n",
        "  We consider Conway polynomials of two-bridge links as Euler continuant\npolynomials. As a consequence, we obtain new and elementary proofs of classical\nMurasugi's 1958 alternating theorem and Hartley's 1979 trapezoidal theorem. We\ngive a modulo 2 congruence for links, which implies the classical Murasugi's\n1971 congruence for knots. We also give sharp bounds for the coefficients of\nEuler continuants and deduce bounds for the Alexander polynomials of two-bridge\nlinks. These bounds improve and generalize those of Nakanishi Suketa'96. We\neasily obtain some bounds for the roots of the Alexander polynomials of\ntwo-bridge links. This is a partial answer to Hoste's conjecture on the roots\nof Alexander polynomials of alternating knots.\n",
        "  This paper proves that convex Brunnian links exist for every dimension $n\n\\geq 3$ by constructing explicit examples. These examples are three-component\nlinks which are higher-dimensional generalizations of the Borromean rings.\n",
        "  We present estimates of intrinsic scatter in the Star Formation Rate (SFR) -\nStellar Mass (M*) correlation in the redshift range 0.5 < z < 3.0 and in the\nmass range 10^7 < M* < 10^11 Msun. We utilize photometry in the Hubble\nUltradeep Field (HUDF12), Ultraviolet Ultra Deep Field (UVUDF) campaigns and\nCANDELS/GOODS-S. We estimate SFR, M* from broadband Spectral Energy\nDistributions (SEDs) and the best available redshifts. The maximum depth of the\nHUDF photometry (F160W 29.9 AB, 5 sigma depth) probes the SFR-M* correlation\ndown to M* ~ 10 ^7 Msun, a factor of 10-100X lower in M* than previous studies,\nand comparable to dwarf galaxies in the local universe. We find the slope of\nthe SFR-M* relationship to be near unity at all redshifts and the normalization\nto decrease with cosmic time. We find a moderate increase in intrinsic scatter\nwith cosmic time from 0.2 to 0.4 dex across the epoch of peak cosmic star\nformation. None of our redshift bins show a statistically significant increase\nin intrinsic scatter at low mass. However, it remains possible that intrinsic\nscatter increases at low mass on timescales shorter than ~ 100 Myr. Our results\nare consistent with a picture of gradual and self-similar assembly of galaxies\nacross more than three orders of magnitude in stellar mass from as low as 10^7\nMsun.\n",
        "  The electronic structure near the Fermi level (EF) of Ba1-xKxFe2As2 (BaK122 ;\nx = 0.2 - 0.7) is studied using laser ultrahigh-resolution angle-resolved\nphotoemission spectroscopy(ARPES). For the optimally doped case of x = 0.4, we\nclearly observe two peaks below Tc in the ARPES spectra at a binding energies\n(BE) of 5 meV and 13meV. The former is assigned to a superconducting (SC)\ncoherence peak since it appears and evolves below the bulk SC transition at Tc\n(= 36 K), accompanying a gap opening centered at EF. In contrast, the latter\npeak, which appears below ~ 90 K without any gap formation, is interpreted to\nbe not directly related to a SC coherence peak. This high-BE peak is observed\nfrom x = 0.2 to 0.6, reduces in energy with overdoping (x > 0.4) and is absent\nfor x = 0.7. The temperature(T)- and doping-dependent ARPES results suggest\nthat the high-BE peak originates from coupling to a bosonic mode of energy ~ 8\nmeV.\n",
        "  The GLEU metric was proposed for evaluating grammatical error corrections\nusing n-gram overlap with a set of reference sentences, as opposed to\nprecision/recall of specific annotated errors (Napoles et al., 2015). This\npaper describes improvements made to the GLEU metric that address problems that\narise when using an increasing number of reference sets. Unlike the originally\npresented metric, the modified metric does not require tuning. We recommend\nthat this version be used instead of the original version.\n",
        "  We show that tubes of melt cast Bi-2212 used as current leads for LTS magnets\ncan also act as efficient magnetic shields. The magnetic screening properties\nunder an axial DC magnetic field are characterized at several temperatures\nbelow the liquid nitrogen temperature (77 K). Two main shielding properties are\nstudied and compared with those of Bi-2223, a material that has been considered\nin the past for bulk magnetic shields. The first property is related to the\nmaximum magnetic flux density that can be screened, Blim; it is defined as the\napplied magnetic flux density below which the field attenuation measured at the\ncentre of the shield exceeds 1000. For a cylinder of Bi-2212 with a wall\nthickness of 5 mm and a large ratio of length over radius, Blim is evaluated to\n1 T at T = 10 K. This value largely exceeds the Blim value measured at the same\ntemperature on similar tubes of Bi-2223. The second shielding property that is\ncharacterized is the dependence of Blim with respect to variations of the sweep\nrate of the applied field, dBapp/dt. This dependence is interpreted in terms of\nthe power law E = Ec(J/Jc)^n and allows us to determine the exponent n of this\nE(J) characteristics for Bi-2212. The characterization of the magnetic field\nrelaxation involves very small values of the electric field. This gives us the\nopportunity to experimentally determine the E(J) law in an unexplored region of\nsmall electric fields. Combining these results with transport and AC shielding\nmeasurements, we construct a piecewise E(J) law that spans over 8 orders of\nmagnitude of the electric field.\n",
        "  Biomechanical motion simulation and dynamic analysis of human joint moments\nwill provide insights into Musculoskeletal Disorders. As one of the mainstream\nsimulation tools, OpenSim uses proportional scaling to specify model segment\nmasses to the simulated subject, which may bring about errors. This study aims\nat estimating the errors caused by the specifying method used in OpenSim as\nwell as the influence of these errors on dynamic analysis. A 3D scan is used to\nconstruct subject's 3D geometric model, according to which segment masses are\ndetermined. The determined segment masses data is taken as the yardstick to\nassess the errors of OpenSim scaled model. Then influence of these errors on\nthe dynamic calculation is evaluated in the simulation of a motion in which the\nsubject walks in an ordinary gait. Result shows that the mass error in one\nsegment can be as large as 5.31\\% of overall body weight. The mean influence on\ncalculated joint moment varies from 0.68\\% to 12.68\\% in 18 joints. In\nconclusion, a careful specification of segment masses will increase the\naccuracy of the dynamic simulation. As far as estimating human segment masses,\nthe use of segment volume and density data can be an economical choice apart\nfrom referring to population mass distribution data.\n",
        "  Distributed RDF systems partition data across multiple computer nodes\n(workers). Some systems perform cheap hash partitioning, which may result in\nexpensive query evaluation, while others apply heuristics aiming at minimizing\ninter-node communication during query evaluation. This requires an expensive\ndata preprocessing phase, leading to high startup costs for very large RDF\nknowledge bases. Apriori knowledge of the query workload has also been used to\ncreate partitions, which however are static and do not adapt to workload\nchanges; hence, inter-node communication cannot be consistently avoided for\nqueries that are not favored by the initial data partitioning.\n  In this paper, we propose AdHash, a distributed RDF system, which addresses\nthe shortcomings of previous work. First, AdHash applies lightweight\npartitioning on the initial data, that distributes triples by hashing on their\nsubjects; this renders its startup overhead low. At the same time, the\nlocality-aware query optimizer of AdHash takes full advantage of the\npartitioning to (i)support the fully parallel processing of join patterns on\nsubjects and (ii) minimize data communication for general queries by applying\nhash distribution of intermediate results instead of broadcasting, wherever\npossible. Second, AdHash monitors the data access patterns and dynamically\nredistributes and replicates the instances of the most frequent ones among\nworkers. As a result, the communication cost for future queries is drastically\nreduced or even eliminated. To control replication, AdHash implements an\neviction policy for the redistributed patterns. Our experiments with synthetic\nand real data verify that AdHash (i) starts faster than all existing systems,\n(ii) processes thousands of queries before other systems become online, and\n(iii) gracefully adapts to the query load, being able to evaluate queries on\nbillion-scale RDF data in sub-seconds.\n",
        "  Various commercial coated conductors were irradiated with fast neutrons in\norder to introduce randomly distributed, uncorrelated defects which increase\nthe critical current density, Jc, in a wide temperature and field range. The\nJc-anisotropy is significantly reduced and the angular dependence of Jc does\nnot obey the anisotropic scaling approach. These defects enhance the\nirreversibility line in not fully optimized tapes, but they do not in\nstate-of-the-art conductors. Neutron irradiation provides a clear distinction\nbetween the low field region, where Jc is limited by the grain boundaries, and\nthe high field region, where depinning leads to dissipation.\n",
        "  We measure the star formation quenching efficiency and timescale in cluster\nenvironments. Our method uses N-body simulations to estimate the probability\ndistribution of possible orbits for a sample of observed SDSS galaxies in and\naround clusters based on their position and velocity offsets from their host\ncluster. We study the relationship between their star formation rates and their\nlikely orbital histories via a simple model in which star formation is quenched\nonce a delay time after infall has elapsed. Our orbit library method is\ndesigned to isolate the environmental effect on the star formation rate due to\na galaxy's present-day host cluster from `pre-processing' in previous group\nhosts. We find that quenching of satellite galaxies of all stellar masses in\nour sample ($10^{9}-10^{11.5}\\,{\\rm M}_\\odot$) by massive ($> 10^{13}\\,{\\rm\nM}_\\odot$) clusters is essentially $100$ per cent efficient. Our fits show that\nall galaxies quench on their first infall, approximately at or within a Gyr of\ntheir first pericentric passage. There is little variation in the onset of\nquenching from galaxy-to-galaxy: the spread in this time is at most $\\sim 2$\nGyr at fixed $M_*$. Higher mass satellites quench earlier, with very little\ndependence on host cluster mass in the range probed by our sample.\n",
        "  Media failures usually leave database systems unavailable for several hours\nuntil recovery is complete, especially in applications with large devices and\nhigh transaction volume. Previous work introduced a technique called\nsingle-pass restore, which increases restore bandwidth and thus substantially\ndecreases time to repair. Instant restore goes further as it permits read/write\naccess to any data on a device undergoing restore--even data not yet\nrestored--by restoring individual data segments on demand. Thus, the restore\nprocess is guided primarily by the needs of applications, and the observed mean\ntime to repair is effectively reduced from several hours to a few seconds.\n  This paper presents an implementation and evaluation of instant restore. The\ntechnique is incrementally implemented on a system starting with the\ntraditional ARIES design for logging and recovery. Experiments show that the\ntransaction latency perceived after a media failure can be cut down to less\nthan a second and that the overhead imposed by the technique on normal\nprocessing is minimal. The net effect is that a few \"nines\" of availability are\nadded to the system using simple and low-overhead software techniques.\n",
        "  It is a natural consequence of fundamental properties of the Casson invariant\nthat the Rokhlin invariant of an amphichiral integral homology 3-sphere M\nvanishes. In this paper, we give a new direct proof of this vanishing property.\nFor such an M, we construct a manifold pair (Y,Q) of dimensions 6 and 3\nequipped with some additional structure (6-dimensional spin e-manifold), such\nthat Q = M \\cup M \\cup (-M) and (Y,Q) \\cong (-Y,-Q). We prove that (Y,Q) bounds\na 7-dimensional spin e-manifold (Z,X) by studying the cobordism group of\n6-dimensional spin e-manifolds and the Z/2-actions on the two--point\nconfiguration space of M minus one point. For any such (Z,X), the signature of\nX vanishes, and this implies the vanishing of the Rokhlin invariant. The idea\nof the construction of (Y,Q) comes from the definition of the\nKontsevich-Kuperberg-Thurston invariant for rational homology 3-spheres.\n",
        "  Plant-pollinator mutualistic networks are asymmetric in their interactions:\nspecialist plants are pollinated by generalist animals, while generalist plants\nare pollinated by a broad involving specialists and generalists. It has been\nsuggested that this asymmetric ---or disassortative--- assemblage could play an\nimportant role in determining the equal susceptibility of specialist and\ngeneralist plants under habitat destruction. At the core of the argument lies\nthe observation that specialist plants, otherwise candidates to extinction,\ncould cope with the disruption thanks to their interaction with generalist\npollinators. We present a theoretical framework that supports this thesis. We\nanalyze a dynamical model of a system of mutualistic plants and pollinators,\nsubject to the destruction of their habitat. We analyze and compare two\nfamilies of interaction topologies, ranging from highly assortative to highly\ndisassortative ones, as well as real pollination networks. We found that\nseveral features observed in natural systems are predicted by the mathematical\nmodel. First, there is a tendency to increase the asymmetry of the network as a\nresult of the extinctions. Second, an entropy measure of the differential\nsusceptibility to extinction of specialist and generalist species show that\nthey tend to balance when the network is disassortative. Finally, the\ndisappearance of links in the network, as a result of extinctions, shows that\nspecialist plants preserve more connections than the corresponding plants in an\nassortative system, enabling them to resist the disruption.\n",
        "  We show that any open subset of a contact manifold of dimension greater than\nthree contains a certain non-convex hypersurface violating the\nThurston-Bennequin inequality.\n",
        "  We present a study of the spatial and color distributions of four early-type\ngalaxies and their globular cluster (GC) systems observed as part of our\nongoing wide-field imaging survey. We use $BVR$ KPNO-4m+MOSAIC imaging data to\ncharacterize the galaxies' GC populations, perform surface photometry of the\ngalaxies, and compare the projected two-dimensional shape of the host galaxy\nlight to that of the GC population. The GC systems of the ellipticals NGC 4406\nand NGC 5813 both show an elliptical distribution consistent with that of the\nhost galaxy light. Our analysis suggests a similar result for the giant\nelliptical NGC 4472, but a smaller GC candidate sample precludes a definite\nconclusion. For the S0 galaxy NGC 4594, the GCs have a circular projected\ndistribution, in contrast to the host galaxy light which is flattened in the\ninner regions. For NGC 4406 and NGC 5813 we also examine the projected shapes\nof the metal-poor and metal-rich GC subpopulations and find that both\nsubpopulations have elliptical shapes that are consistent with those of the\nhost galaxy light. Lastly, we use integrated colors and color profiles to\ncompare the stellar populations of the galaxies to their GC systems. For each\ngalaxy, we explore the possibility of color gradients in the individual\nmetal-rich and metal-poor GC subpopulations. We find statistically significant\ncolor gradients in both GC subpopulations of NGC 4594 over the inner $\\sim 5$\neffective radii ($\\sim 20$ kpc). We compare our results to scenarios for the\nformation and evolution of giant galaxies and their GC systems.\n",
        "  We explore the processes that trigger local AGN and the role of these AGN in\nregulating star formation, using ~350 nearby galaxies observed by the mJy\nImaging VLBA Exploration at 20cm (mJIVE) survey. The >10^7 K brightness\ntemperature required for an mJIVE detection cannot be achieved via star\nformation alone, allowing us to unambiguously detect nearby radio AGN and study\ntheir role in galaxy evolution. Radio AGN are an order of magnitude more common\nin early-type galaxies (ETGs) than in their late-type counterparts. The\nVLBI-detected ETGs in this study have a similar stellar mass distribution to\ntheir undetected counterparts, are typically not the central galaxies of\nclusters and exhibit merger fractions that are significantly higher than in the\naverage ETG. This suggests that these radio AGN (which have VLBI luminosities\n>10^22 W Hz^-1) are primarily fuelled by mergers, and not by internal stellar\nmass loss or cooling flows. Our radio AGN are a factor of ~3 times more likely\nto reside in the UV-optical red sequence than the average ETG. Furthermore,\ntypical AGN lifetimes (a few 10^7 yr) are much shorter than the transit times\nfrom blue cloud to red sequence (~1.5 Gyr). This indicates that the AGN are not\ntriggered promptly and appear several dynamical timescales into the associated\nstar formation episode, implying that they typically couple only to residual\ngas, at a point where star formation has already declined significantly. While\nevidence for AGN feedback is strong in systems where the black hole is fed by\nthe cooling of hot gas, AGN triggered by mergers appear not to strongly\nregulate the associated star formation. The inability of the AGN to rapidly\nquench merger-driven star formation is likely to make merging the dominant mode\nof star formation in nearby ETGs, in line with evidence for minor mergers being\nthe primary driver of stellar mass growth in these systems.\n",
        "  Background: While many infectious disease epidemics are initially\ncharacterized by an exponential growth in time, we show that district-level\nEbola virus disease (EVD) outbreaks in West Africa follow slower\npolynomial-based growth kinetics over several generations of the disease.\nMethods: We analyzed epidemic growth patterns at three different spatial scales\n(regional, national, and subnational) of the Ebola virus disease epidemic in\nGuinea, Sierra Leone and Liberia by compiling publicly available weekly time\nseries of reported EVD case numbers from the patient database available from\nthe World Health Organization website for the period 05-Jan to 17-Dec 2014.\nResults: We found significant differences in the growth patterns of EVD cases\nat the scale of the country, district, and other subnational administrative\ndivisions. The national cumulative curves of EVD cases in Guinea, Sierra Leone,\nand Liberia show periods of approximate exponential growth. In contrast, local\nepidemics are asynchronous and exhibit slow growth patterns during 3 or more\nEVD generations, which can be better approximated by a polynomial than an\nexponential. Conclusions: The slower than expected growth pattern of local EVD\noutbreaks could result from a variety of factors, including behavior changes,\nsuccess of control interventions, or intrinsic features of the disease such as\na high level of clustering. Quantifying the contribution of each of these\nfactors could help refine estimates of final epidemic size and the relative\nimpact of different mitigation efforts in current and future EVD outbreaks.\n",
        "  In the past few years, the number of OLAP applications increased quickly.\nThese applications use two significantly different DB structures:\nmultidimensional (MD) and table-based. One can show that the traditional model\nof relational databases cannot make difference between these two structures.\nAnother model is necessary to make the differences visible. One of these is the\nspeed of the system. It can be proven that the multidimensional DB organization\nresults in shorter response times. And it is crucial, since a manager may\nbecome impatient, if he or she has to wait say more than 20 seconds for the\nnext screen. On the other hand, we have to pay for the speed with a bigger DB\nsize. Why does the size of MD databases grow so quickly? The reason is the\nsparsity of data: The MD matrix contains many empty cells. Efficient handling\nof sparse matrices is indispensable in an OLAP application. One way to handle\nsparsity is to take the structure closer to the table-based one. Thus the DB\nsize decreases, while the application gets slower. Therefore, other methods are\nneeded. This paper deals with the comparison of the two DB structures and the\nlimits of their usage. The new results of the paper: (1) It gives a\nconstructive proof that all relations can be represented in MD arrays. (2) It\nalso shows when the MD array representation is quicker than the table-based\none. (3) The MD representation results in smaller DB size under some\nconditions. One such sufficient condition is proved in the paper. (4) A\nvariation of the single count header compression scheme is described with an\nalgorithm, which creates the compressed array from the ordered table without\nmaterializing the uncompressed array. (5) The speed of the two different\ndatabase organizations is tested with experiments, as well. The tests are done\non benchmark as well as real life data. The experiments support the theoretical\nresults.\n",
        "  This work studies comparatively two typical sentence pair classification\ntasks: textual entailment (TE) and answer selection (AS), observing that phrase\nalignments of different intensities contribute differently in these tasks. We\naddress the problems of identifying phrase alignments of flexible granularity\nand pooling alignments of different intensities for these tasks. Examples for\nflexible granularity are alignments between two single words, between a single\nword and a phrase and between a short phrase and a long phrase. By intensity we\nroughly mean the degree of match, it ranges from identity over surface-form\nco-occurrence, rephrasing and other semantic relatedness to unrelated words as\nin lots of parenthesis text. Prior work (i) has limitations in phrase\ngeneration and representation, or (ii) conducts alignment at word and phrase\nlevels by handcrafted features or (iii) utilizes a single attention mechanism\nover alignment intensities without considering the characteristics of specific\ntasks, which limits the system's effectiveness across tasks. We propose an\narchitecture based on Gated Recurrent Unit that supports (i) representation\nlearning of phrases of arbitrary granularity and (ii) task-specific focusing of\nphrase alignments between two sentences by attention pooling. Experimental\nresults on TE and AS match our observation and are state-of-the-art.\n",
        "  Superconductivity in the cuprates emerges from an enigmatic metallic state.\nThere remain profound open questions regarding the universality of observed\nphenomena and the character of precursor fluctuations above the superconducting\n(SC) transition temperature (T_c). For single-CuO_2-layer La_{2-x}Sr_xCuO_4\n(LSCO) and Bi_2(Sr,La)_2CuO_{6+\\delta} (Bi2201), some experiments seem to\nindicate an onset of SC fluctuations at very high temperatures (2-3 times\nT_c^{max}, the T_c value at optimal hole concentration p), whereas other\nmeasurements suggest that fluctuations are confined to the immediate vicinity\nof T_c(p). Here we use torque magnetization to resolve this conundrum by\nsystematically studying LSCO, Bi2201 and HgBa_2CuO_{4+\\delta} (Hg1201). The\nlatter is a more ideal single-layer compound, featuring high structural\nsymmetry, minimal disorder, and T_c^{max} = 97 K, a value more than twice those\nof LSCO and Bi2201. We find in all three cases that SC diamagnetism vanishes in\nan unusual exponential fashion above T_c, and at a rapid rate that is\nuniversal. Furthermore, the high characteristic fluctuation temperatures of\nLSCO and Bi2201 closely track T_c(p) of Hg1201. These observations suggest\nthat, rather than being indicative of SC diamagnetism, the fluctuations at high\ntemperatures in the low-T_c^{max} compounds are associated with a competing\norder. This picture is further supported by an analysis of available results\nfor double-layer cuprates.\n",
        "  We examined the proximity effect in granular films made of Pb, a\nsuperconductor, and Ni, a ferromagnet, with various compositions. Slow decay of\nthe critical temperature as a function of the relative volume concentration of\nNi per sample was demonstrated by our measurements, followed by a saturation of\nT$_c$. Using an approximate theoretical description of our granular system in\nterms of a layered one, we show that our data can only be reasonably fitted by\na multilayer model. This indicates the importance of the interplay between\ndifferent ferromagnetic grains; when non-collinearly magnetized they should\nlead to triplet Cooper pairing.\n",
        "  Milnor fibrations have been studied since 1960's. In this paper, we study\nsingular points of differentiable maps, called Milnor fibration product maps,\nobtained by several Milnor fibrations. We give a characterization of singular\npoints of such product maps, and for the case of certain weighted homogeneous\npolynomials, a criterion for a fold point together with its index.\n",
        "  The annotation of the results of database transformations was shown to be\nvery effective for various applications. Until recently, most works in this\ncontext focused on positive query languages. The provenance semirings is a\nparticular approach that was proven effective for these languages, and it was\nshown that when propagating provenance with semirings, the expected equivalence\naxioms of the corresponding query languages are satisfied. There have been\nseveral attempts to extend the framework to account for relational algebra\nqueries with difference. We show here that these suggestions fail to satisfy\nsome expected equivalence axioms (that in particular hold for queries on\n\"standard\" set and bag databases). Interestingly, we show that this is not a\npitfall of these particular attempts, but rather every such attempt is bound to\nfail in satisfying these axioms, for some semirings. Finally, we show\nparticular semirings for which an extension for supporting difference is\n(im)possible.\n",
        "  Purpose: To develop a simple and robust tool for the estimation of gradient\ndelays from highly undersampled radial k-space data. Theory: In radial imaging\ngradient delays induce parallel and orthogonal trajectory shifts, which can be\ndescribed using an ellipse model. The intersection points of the radial spokes,\nwhich can be estimated by spoke-by-spoke comparison of k-space samples,\ndistinctly determine the parameters of the ellipse. Using the proposed method\n(RING), these parameters can be obtained using a least-squares fit and utilized\nfor the correction of gradient delays. Methods: The functionality and accuracy\nof the proposed RING method is validated and compared to correlation-based\ngradient-delay estimation from opposing spokes using numerical simulations,\nphantom and in vivo heart measurements. Results: In all experiments, RING\nrobustly provides accurate gradient delay estimations even for as few as three\nradial spokes. Conclusion: The simple and straightforward to implement RING\nmethod provides accurate gradient delay estimation for highly undersampled\nradial imaging. Keywords: trajectory correction, radial imaging, gradient\ndelay, artifacts, system imperfections, RING\n",
        "  In this paper, we first give a new simple proof to the elimination theorem of\ndefinite fold by homotopy for generic smooth maps of manifolds of dimension\nstrictly greater than $2$ into the $2$--sphere or into the real projective\nplane. Our new proof has the advantage that it is not only constructive, but is\nalso algorithmic: the procedures enable us to construct various explicit\nexamples. We also study simple stable maps of $3$--manifolds into the\n$2$--sphere without definite fold. Furthermore, we prove the non-existence of\nsingular Legendre fibrations on $3$--manifolds, answering negatively to a\nquestion posed in our previous paper.\n",
        "  We present VLT/FORS2 spectroscopy of candidate blue horizontal branch (BHB)\nstars in the vicinity of the Hercules ultrafaint dwarf galaxy. We identify\neight convincing Hercules BHB members, and a further five stars with similar\nsystemic velocities to that of Hercules, but ~ 0.5 kpc from the centre of the\ngalaxy along its major axis. It is likely that these stars once belonged to\nHercules, but have been tidally stripped and are now unbound. We emphasise the\nusefulness of looking for any gradient in the systemic velocity of this\nstretched system, which would further support our interpretation of the origin\nof its elongated and distended morphology.\n",
        "  Analogy completion has been a popular task in recent years for evaluating the\nsemantic properties of word embeddings, but the standard methodology makes a\nnumber of assumptions about analogies that do not always hold, either in recent\nbenchmark datasets or when expanding into other domains. Through an analysis of\nanalogies in the biomedical domain, we identify three assumptions: that of a\nSingle Answer for any given analogy, that the pairs involved describe the Same\nRelationship, and that each pair is Informative with respect to the other. We\npropose modifying the standard methodology to relax these assumptions by\nallowing for multiple correct answers, reporting MAP and MRR in addition to\naccuracy, and using multiple example pairs. We further present BMASS, a novel\ndataset for evaluating linguistic regularities in biomedical embeddings, and\ndemonstrate that the relationships described in the dataset pose significant\nsemantic challenges to current word embedding methods.\n",
        "  We develop a natural and geometric way to realize the hyperbolic plane as the\nmoduli space of marked genus 1 Riemann surfaces. To do so, a metric is defined\non the Teichm\\\"uller space of the torus, inspired by Thurston's Lipschitz\nmetric for the case of hyperbolic surfaces. Based on extremal Lipschitz maps,\nthe Teichm\\\"uller space of the torus with this new metric is shown to be\nisometric to the hyperbolic plane under the usual identification. This also\ngives a new way to recover the complex-analytic Teichm\\\"uller metric via metric\ngeometry on the underlying surfaces. Along the way, we prove a few results\nabout this metric analogous to Thurston's Lipschitz metric in the case of\nhyperbolic surfaces, and analogous to the Teichm\\\"uller metric.\n",
        "  The quick and pervasive infiltration of decision support systems, artificial\nintelligence, and data mining in consumer electronics and everyday life in\ngeneral has been significant in recent years. Fields such as UX have been\nfacilitating the integration of such technologies into software and hardware,\nbut the back-end processing is still based on binary foundations. This article\ndescribes an approach to mining for imprecise temporal associations among\nevents in data streams, taking into account the very natural concept of\napproximation. This type of association analysis is likely to lead to more\nmeaningful and actionable decision support systems.\n",
        "  We use the deep CANDELS observations in the GOODS North and South fields to\nrevisit the correlations between stellar mass ($M_*$), star--formation rate\n(SFR) and morphology, and to introduce a fourth dimension, the mass-weighted\nstellar age, in galaxies at $1.2<z<4$. We do this by making new measures of\n$M_*$, $SFR$, and stellar age thanks to an improved SED fitting procedure that\nallows various star formation history for each galaxy. Like others, we find\nthat the slope of the Main Sequence (MS) of star formation in the $(M_*;SFR)$\nplane bends at high mass. We observe clear morphological differences among\ngalaxies across the MS, which also correlate with stellar age. At all\nredshifts, galaxies that are quenching or quenched, and thus old, have high\n$\\Sigma_1$ (the projected density within the central 1 kpc), while younger,\nstar-forming galaxies span a much broader range of $\\Sigma_1$, which includes\nthe high values observed for quenched galaxies, but also extends to much lower\nvalues. As galaxies age and quench, the stellar age and the dispersion of\n$\\Sigma_1$ for fixed values of $M_{*}$ shows two different regimes, one, at the\nlow--mass end, where quenching might be driven by causes external to the\ngalaxies; the other, at the high--mass end, where quenching is driven by\ninternal causes, very likely the mass given the low scatter of $\\Sigma_1$ (mass\nquenching). We suggest that the monotonic increase of central density as\ngalaxies grow is one manifestation of a more general phenomenon of structural\ntransformation that galaxies undergo as they evolve.\n",
        "  In this contribution we give a brief overview of the Panoramic Radio\nAstronomy (PRA) conference held on 2-5 June 2009 in Groningen, the Netherlands.\nThe conference was motivated by the on-going development of a large number of\nnew radio telescopes and instruments which, within a few years, will bring a\nmajor improvement over current facilities. Interferometers such as the EVLA,\nASKAP, ATA, MeerKAT, and APERTIF will provide a combination of larger field of\nview and increased simultaneous bandwidth, while maintaining good collecting\narea and angular resolution. They will achieve a survey speed 10-50 times\nlarger at 1-2 GHz than the current possibilities, allowing for the first time\noptical-like all-sky extra-galactic surveys at these frequencies.\n  Significant progress will be made in many fields of radio astronomy. In this\nconference we focused on research into the evolution of galaxies over the past\nfew Gyr. In particular, wide-field observations at 1-2 GHz will provide an\nunprecedented panoramic view of the gas properties and star formation in\ngalaxies, embedded in their environment, from z~0.2-0.5 to the present. Within\nthe framework of our current knowledge of the galaxy population at z<0.5, we\ndiscussed: the key science questions that the new telescopes will permit us to\nanswer in combination with complimentary work at other wavelengths; the\nobserving modes and analysis strategies which will allow us to most efficiently\nexploit the data; and the techniques for most effectively coping with the huge\nvolume of survey products, so far unusual for the radio community. Emphasis was\nplaced on the complementarity of the upcoming facilities and on their role in\npaving the way for the technological development and science goals of the\nSquare Kilometre Array.\n",
        "  Recent technological advances and long-term data studies provide interaction\ndata that can be modelled through dynamic networks, i.e a sequence of different\nsnapshots of an evolving ecological network. Most often time is the parameter\nalong which these networks evolve but any other one-dimensional gradient\n(temperature, altitude, depth, humidity, . . . ) could be considered.Here we\npropose a statistical tool to analyse the underlying structure of these\nnetworks and follow its evolution dynamics (either in time or any other\none-dimensional factor). It consists in extracting the main features of these\nnetworks and summarise them into a high-level view.We analyse a dynamic animal\ncontact network and a seasonal food web and in both cases we show that our\napproach allows for the identification of a backbone organisation as well as\ninteresting temporal variations at the individual level.Our method, implemented\ninto the R package dynsbm, can handle the largest ecological datasets and is a\nversatile and promising tool for ecologists that study dynamic interactions.\n",
        "  The entity relationship modelling using the original ER notation has been\napplauded providing a natural view of data in conceptual modelling of\ninformation systems. However, the current ER to relational model transformation\nalgorithm is known to be insufficient in providing a complete and accurate\nrepresentation of the ER model undertaken for transformation. In an effort to\nderive better transformations from ER models, we have understood that\nmodifications should be introduced to both of the existing transformation\nalgorithm as well as to the ER notation. Introducing some new concepts, we have\nadapted the original ER notation and developed a new transformation algorithm\nbased on the existing one. This paper presents the modified ER notation with an\nER diagram drawn based on the new notation.\n",
        "  A data tree is a finite tree whose every node carries a label from a finite\nalphabet and a datum from some infinite domain. We introduce a new model of\nautomata over unranked data trees with a decidable emptiness problem. It is\nessentially a bottom-up alternating automaton with one register that can store\none data value and can be used to perform equality tests with the data values\noccurring within the subtree of the current node. We show that it captures the\nexpressive power of the vertical fragment of XPath - containing the child,\ndescendant, parent and ancestor axes - obtaining thus a decision procedure for\nits satisfiability problem.\n",
        "  Despite its radical assumption of ecological equivalence between species,\nneutral biodiversity theory can often provide good fits to species abundance\ndistributions observed in nature. Major criticisms of neutral theory have\nfocused on interspecific differences, which are in conflict with ecological\nequivalence. However, neutrality in nature is also broken by differences\nbetween conspecific individuals at different life stages, which in many\ncommunities may vastly exceed interspecific differences between individuals at\nsimilar stages. These within-species asymmetries have not been fully explored\nin species-neutral models, and it is not known whether demographic stage\nstructure affects macroecological patterns in neutral theory. Here we present a\ntwo-stage neutral model where fecundity and mortality change as an individual\ntransitions from one stage to the other. We explore several qualitatively\ndifferent scenarios, and compare numerically obtained species abundance\ndistributions to the predictions of unstructured neutral theory. We find that\nabundance distributions are generally robust to this kind of stage structure,\nbut significant departures from unstructured predictions occur if adults have\nsufficiently low fecundity and mortality. In addition, we show that the\ncumulative number of births per species, which is distributed as a power law\nwith a 3/2 exponent, is invariant even when the abundance distribution departs\nfrom unstructured model predictions. Our findings potentially explain power\nlaw-like abundance distributions in organisms with strong demographic\nstructure, such as eusocial insects and humans, and partially rehabilitate\nspecies abundance distributions from past criticisms as to their inability to\ndistinguish between biological mechanisms.\n",
        "  Resolving the interplay between magnetic interactions and structural\nproperties in strongly correlated materials through a quantitatively accurate\napproach has been a major challenge in condensed matter physics. Here we apply\nhighly accurate first principles quantum Monte Carlo (QMC) techniques to obtain\nstructural and magnetic properties of the iron selenide (FeSe) superconductor\nunder pressure. Where comparable, the computed properties are very close to the\nexperimental values. Of potential ordered magnetic configurations, collinear\nspin configurations are the most energetically favorable over the explored\npressure range. They become nearly degenerate in energy with bicollinear spin\norderings at around 7 GPa, when the experimental critical temperature $T_c$ is\nthe highest. On the other hand, ferromagnetic, checkerboard, and staggered\ndimer configurations become relatively higher in energy as the pressure\nincreases. The behavior under pressure is explained by an accurate analysis of\nthe charge compressibility and the orbital occupation as described by the QMC\nmany-body wave function, which reveals how spin, charge and orbital degrees of\nfreedom are strongly coupled in this compound. This remarkable pressure\nevolution suggests that stripe-like magnetic fluctuations may be responsible\nfor the enhanced $T_c$ in FeSe and that higher T$_c$ is associated with\nnearness to a crossover between collinear and bicollinear ordering.\n",
        "  In considering evolution of transcribed regions, regulatory modules, and\nother genomic loci of interest, we are often faced with a situation in which\nthe number of allelic states greatly exceeds the population size. In this\nlimit, the population eventually adopts a steady state characterized by\nmutation-selection-drift balance. Although new alleles continue to be explored\nthrough mutation, the statistics of the population, and in particular the\nprobabilities of seeing specific allelic configurations in samples taken from a\npopulation, do not change with time. In the absence of selection, probabilities\nof allelic configurations are given by the Ewens sampling formula, widely used\nin population genetics to detect deviations from neutrality. Here we develop an\nextension of this formula to arbitrary, possibly epistatic, fitness landscapes.\nAlthough our approach is general, we focus on the class of landscapes in which\nalleles are grouped into two, three, or several fitness states. This class of\nlandscapes yields sampling probabilities that are computationally more\ntractable, and can form a basis for the inference of selection signatures from\nsequence data. We demonstrate that, for a sizeable range of mutation rates and\nselection coefficients, the steady-state allelic diversity is not neutral.\nTherefore, it may be used to infer selection coefficients, as well as other key\nevolutionary parameters, using high-throughput sequencing of evolving\npopulations to collect data on locus polymorphisms. We also find that our\ntheory remains sufficiently accurate even if the assumptions such as the\ninfinite allele limit and the \"full connectivity\" assumption in which each\nallele can mutate into any other allele are relaxed. Thus, our framework\nestablishes a theoretical foundation for inferring selection signatures from\nsamples of sequences produced by evolution on epistatic fitness landscapes.\n",
        "  A dialog state tracker is an important component in modern spoken dialog\nsystems. We present an incremental dialog state tracker, based on LSTM\nnetworks. It directly uses automatic speech recognition hypotheses to track the\nstate. We also present the key non-standard aspects of the model that bring its\nperformance close to the state-of-the-art and experimentally analyze their\ncontribution: including the ASR confidence scores, abstracting scarcely\nrepresented values, including transcriptions in the training data, and model\naveraging.\n",
        "  Controlled natural languages (CNL) with a direct mapping to formal logic have\nbeen proposed to improve the usability of knowledge representation systems,\nquery interfaces, and formal specifications. Predictive editors are a popular\napproach to solve the problem that CNLs are easy to read but hard to write.\nSuch predictive editors need to be able to \"look ahead\" in order to show all\npossible continuations of a given unfinished sentence. Such lookahead features,\nhowever, are difficult to implement in a satisfying way with existing grammar\nframeworks, especially if the CNL supports complex nonlocal structures such as\nanaphoric references. Here, methods and algorithms are presented for a new\ngrammar notation called Codeco, which is specifically designed for controlled\nnatural languages and predictive editors. A parsing approach for Codeco based\non an extended chart parsing algorithm is presented. A large subset of Attempto\nControlled English (ACE) has been represented in Codeco. Evaluation of this\ngrammar and the parser implementation shows that the approach is practical,\nadequate and efficient.\n",
        "  Approximate results based on samples often provide the only way in which\nadvanced analytical applications on very massive data sets can satisfy their\ntime and resource constraints. Unfortunately, methods and tools for the\ncomputation of accurate early results are currently not supported in\nMapReduce-oriented systems although these are intended for `big data'.\nTherefore, we proposed and implemented a non-parametric extension of Hadoop\nwhich allows the incremental computation of early results for arbitrary\nwork-flows, along with reliable on-line estimates of the degree of accuracy\nachieved so far in the computation. These estimates are based on a technique\ncalled bootstrapping that has been widely employed in statistics and can be\napplied to arbitrary functions and data distributions. In this paper, we\ndescribe our Early Accurate Result Library (EARL) for Hadoop that was designed\nto minimize the changes required to the MapReduce framework. Various tests of\nEARL of Hadoop are presented to characterize the frequent situations where EARL\ncan provide major speed-ups over the current version of Hadoop.\n",
        "  It is the first time, when genetic diversity and the common kilka population\nstructure were investigated throughout the areal. Data about the species\ncondition at the Upper Volga basins were updated accordingly to modern\ncondition. Physiological and ecological adaptations to northern water basins\nwere evaluated. Significance of interaction between some loci and the most\nimportant abiotic environmental factors at the selection was demonstrated. The\nresults suggest that the common kilka Clupeonella cultriventris (Nordmann,\n1840) is presented by uniform set of population throughout the areal. The\nsuccessive expansion of the species through the cascade of the Volga's water\nreservoirs can be explained by complex of genetic and biochemical adaptation\nfor sweet water habitat. It was supposed that the sweet water populations\noriginated from Saratov's backwaters sweet water population. Seasonal\nfluctuations of abiotic and biotic environmental factors have significant\ninfluence on genotype distribution at the newly formed population. This book is\nintended for use by ichthyologists, ecologists, environmental protection and\nmanagement of natural resources specialists. In Russian.\n",
        "  We study the problem of domain search where a domain is a set of distinct\nvalues from an unspecified universe. We use Jaccard set containment, defined as\n$|Q \\cap X|/|Q|$, as the relevance measure of a domain $X$ to a query domain\n$Q$. Our choice of Jaccard set containment over Jaccard similarity makes our\nwork particularly suitable for searching Open Data and data on the web, as\nJaccard similarity is known to have poor performance over sets with large\ndifferences in their domain sizes. We demonstrate that the domains found in\nseveral real-life Open Data and web data repositories show a power-law\ndistribution over their domain sizes.\n  We present a new index structure, Locality Sensitive Hashing (LSH) Ensemble,\nthat solves the domain search problem using set containment at Internet scale.\nOur index structure and search algorithm cope with the data volume and skew by\nmeans of data sketches (MinHash) and domain partitioning. Our index structure\ndoes not assume a prescribed set of values. We construct a cost model that\ndescribes the accuracy of LSH Ensemble with any given partitioning. This allows\nus to formulate the partitioning for LSH Ensemble as an optimization problem.\nWe prove that there exists an optimal partitioning for any distribution.\nFurthermore, for datasets following a power-law distribution, as observed in\nOpen Data and Web data corpora, we show that the optimal partitioning can be\napproximated using equi-depth, making it efficient to use in practice.\n  We evaluate our algorithm using real data (Canadian Open Data and WDC Web\nTables) containing up over 262 M domains. The experiments demonstrate that our\nindex consistently outperforms other leading alternatives in accuracy and\nperformance. The improvements are most dramatic for data with large skew in the\ndomain sizes. Even at 262 M domains, our index sustains query performance with\nunder 3 seconds response time.\n",
        "  Due to the increasing complexity of radiotherapy delivery, accurate dose\nverification has become an essential part of the clinical treatment process.\nThe purpose of this work was to develop an electronic portal image (EPI) based\npre-treatment verification technique capable of quickly reconstructing 3D dose\ndistributions from both coplanar and non-coplanar treatments. The dose\nreconstruction is performed in a spherical water phantom by modulating, based\non EPID measurements, pre-calculated Monte Carlo (MC) doselets defined on a\nspherical coordinate system. This is called the spherical doselet modulation\n(SDM) method. This technique essentially eliminates the statistical uncertainty\nof the MC dose calculations by exploiting both azimuthal symmetry in a\npatient-independent phase-space and symmetry of a virtual spherical water\nphantom. The symmetry also allows the number of doselets necessary for dose\nreconstruction to be reduced by a factor of about 250. In this work, 51\ndoselets were used. The SDM method mitigates the most computationally intensive\npart of this type of dose reconstruction - reading, weighting and summing dose\nmatrices. The accuracy of the system was tested against MC calculations as well\nas our previously reported phase-space modulation (PSM) method using a series\nof open field and IMRT cases. The mean chi- and gamma-test 3% / 3 mm success\nrates of the SDM method were 98.6% and 99.5%, respectively, when compared to\nfull MC simulation. The total calculation time was 96 seconds per treatment\nfield on a single processor core.\n",
        "  Big data applications have fast arriving data that must be quickly ingested.\nAt the same time, they have specific needs to preprocess and transform the data\nbefore it could be put to use. The current practice is to do these preparatory\ntransformations once the data is already ingested, however, this is expensive\nto run and cumbersome to manage. As a result, there is a need to push data\npreprocessing down to the ingestion itself. In this paper, we present a\ndeclarative data ingestion system, called INGESTBASE, to allow application\ndevelopers to plan and specify their data ingestion logic in a more systematic\nmanner. We introduce the notion of ingestions plans, analogous to query plans,\nand present a declarative ingestion language to help developers easily build\nsophisticated ingestion plans. INGESTBASE provides an extensible ingestion\noptimizer to rewrite and optimize ingestion plans by applying rules such as\noperator reordering and pipelining. Finally, the INGESTBASE runtime engine runs\nthe optimized ingestion plan in a distributed and fault-tolerant manner. Later,\nat query processing time, INGESTBASE supports ingestion-aware data access and\ninterfaces with upstream query processors, such as Hadoop MapReduce and Spark,\nto post- process the ingested data. We demonstrate through a number of\nexperiments that INGESTBASE: (i) is flexible enough to express a variety of\ningestion techniques, (ii) incurs a low ingestion overhead, (iii) provides\nefficient access to the ingested data, and (iv) has much better performance, up\nto 6 times, than preparing data as an afterthought, via a query processor.\n",
        "  Social media user geolocation is vital to many applications such as event\ndetection. In this paper, we propose GCN, a multiview geolocation model based\non Graph Convolutional Networks, that uses both text and network context. We\ncompare GCN to the state-of-the-art, and to two baselines we propose, and show\nthat our model achieves or is competitive with the state- of-the-art over three\nbenchmark geolocation datasets when sufficient supervision is available. We\nalso evaluate GCN under a minimal supervision scenario, and show it outperforms\nbaselines. We find that highway network gates are essential for controlling the\namount of useful neighbourhood expansion in GCN.\n",
        "  Neural Machine Translation (NMT) has drawn much attention due to its\npromising translation performance recently. However, several studies indicate\nthat NMT often generates fluent but unfaithful translations. In this paper, we\npropose a method to alleviate this problem by using a phrase table as\nrecommendation memory. The main idea is to add bonus to words worthy of\nrecommendation, so that NMT can make correct predictions. Specifically, we\nfirst derive a prefix tree to accommodate all the candidate target phrases by\nsearching the phrase translation table according to the source sentence. Then,\nwe construct a recommendation word set by matching between candidate target\nphrases and previously translated target words by NMT. After that, we determine\nthe specific bonus value for each recommendable word by using the attention\nvector and phrase translation probability. Finally, we integrate this bonus\nvalue into NMT to improve the translation results. The extensive experiments\ndemonstrate that the proposed methods obtain remarkable improvements over the\nstrong attentionbased NMT.\n",
        "  Purpose: For Monte Carlo simulation of radiotherapy, x-ray CT number of every\nsystem needs to be calibrated and converted to mass density and elemental\ncomposition. This study aims to formulate material properties of body tissues\nfor practical two-step conversion from CT number. Methods: We used the latest\ncompilation on body tissues that constitute reference adult male and female. We\nformulated the relations among mass, electron, and elemental densities into\npolylines to connect representative tissues, for which we took mass-weighted\nmean for the tissues in limited density regions. We compared the polyline\nfunctions of mass density with a bi-line for electron density and broken lines\nfor elemental densities, which were derived from preceding studies. Results:\nThere was generally high correlation between mass density and the other\ndensities except of C, N, and O for light spongiosa tissues occupying 1% of\nbody mass. The polylines fitted to the dominant tissues and were generally\nconsistent with the bi-line and the broken lines. Conclusions: We have\nformulated the invariant relations between mass and electron densities and from\nmass to elemental densities for body tissues. The formulation enables Monte\nCarlo simulation in treatment planning practice without additional burden with\nCT-number calibration.\n",
        "  Nowadays, graph databases are employed when relationships between entities\nare in the scope of database queries to avoid performance-critical join\noperations of relational databases. Graph queries are used to query and modify\ngraphs stored in graph databases. Graph queries employ graph pattern matching\nthat is NP-complete for subgraph isomorphism. Graph database views can be\nemployed that keep ready answers in terms of precalculated graph pattern\nmatches for often stated and complex graph queries to increase query\nperformance. However, such graph database views must be kept consistent with\nthe graphs stored in the graph database.\n  In this paper, we describe how to use incremental graph pattern matching as\ntechnique for maintaining graph database views. We present an incremental\nmaintenance algorithm for graph database views, which works for imperatively\nand declaratively specified graph queries. The evaluation shows that our\nmaintenance algorithm scales when the number of nodes and edges stored in the\ngraph database increases. Furthermore, our evaluation shows that our approach\ncan outperform existing approaches for the incremental maintenance of graph\nquery results.\n",
        "  Many nearby AGNs display a significant short-term variability. In this work\nwe re-analyze photometric data of four active galactic nuclei observed by\nKepler in order to study the flickering activity, having as main goal that of\nsearching for multiple components in the power density spectra. We find that\nall four objects have similar characteristics, with two break frequencies at\napproximately log(f/Hz)=-5.2 and -4.7. We consider some physical phenomena\nwhose characteristic time-scales are consistent with those observed, in\nparticular mass accretion fluctuations in the inner geometrically thick disc\n(hot X-ray corona) and unstable relativistic Rayleigh-Taylor modes. The former\nis supported by detection of the same break frequencies in the Swift X-ray data\nof ZW229-15. We also discuss rms-flux relations, and we detect a possible\ntypical linear trend at lower flux levels. Our findings support the hypothesis\nof a multiplicative character of variability, in agreement with the propagating\naccretion fluctuation model.\n",
        "  This paper suggests a full protocol of Computer Aided Surgery as previously\nrecommended in literature addressing the challenging task of primary or\nsecondary reconstruction of orbito-zygomatic dislocation. First, on a\nspecifically developed planning software, the best zygoma reduction and orbital\nboundaries reconstruction to achieve skeletal symmetry are determined. This\ntreatment plan is then transferred to the 3D Navigation Systems within the\noperating room. After patient's anatomy registration to his preoperative CT\nscan data, the navigation system allows zygomatic guiding to its planned\nreduced location and bone orbital volume restoration control. The feasibility\nof this technique was checked in 3 patients with major orbito-zygomatic\ndeformities. Preliminary clinical results are presented.\n",
        "  The present study analyzes the changes in acceleration produced by swimmers\nbefore and after fatiguing effort. The subjects (n=15) performed a 25-meter\ncrawl series at maximum speed without fatigue, and a second series with\nfatigue. The data were registered with a synchronized system that consisted in\na position transducer (1 kHz) and a video photogrametry (50Hz). The\nacceleration (ms-2) was obtained by the derivative analysis of the variation of\nthe position with time. The amplitude in the time domain was calculated with\nthe root mean square (RMS); while the peak power (PP), the peak power frequency\n(PPF) and the spectrum area (SA) was calculated in the frequency domain with\nFourier analysis. On one hand, the results of the temporal domain show that the\nRMS change percentage between series was 67.5% (p<0.001). On the other hand,\nPP, PPF, and SA show significant changes (p<0.001). PP and SA were reduced by\n63.1% and 59.5%, respectively. Our results show that the acceleration analysis\nof the swimmer with Fourier analysis permits a more precise understanding of\nwhich propulsive forces contribute to the swimmer performance before and after\nfatigue appears.\n",
        "  We have started automatized photometric monitoring of active galactic nuclei\nusing the 46 cm telescope of the Wise Observatory in Israel. The telescope is\nspecially equipped with narrow-band filters to perform high-fidelity\nphotometric reverberation mapping of the accretion disk in V < 17 mag sources\nup to z ~ 0.1. Here, we describe the capability and accuracy of the experiment,\nand present the first science verification data obtained for the Seyfert 1\ngalaxy Mrk 279. With sub-diurnal sampling over more than two months, and\ntypical flux measurement uncertainties of $1\\%$, we are able to measure\ninter-band time-delays of up to ~2 days across the optical range.\n",
        "  Emerging applications in Internet of Things (IoT) and Cyber-Physical Systems\n(CPS) present novel challenges to Big Data platforms for performing online\nanalytics. Ubiquitous sensors from IoT deployments are able to generate data\nstreams at high velocity, that include information from a variety of domains,\nand accumulate to large volumes on disk. Complex Event Processing (CEP) is\nrecognized as an important real-time computing paradigm for analyzing\ncontinuous data streams. However, existing work on CEP is largely limited to\nrelational query processing, exposing two distinctive gaps for query\nspecification and execution: (1) infusing the relational query model with\nhigher level knowledge semantics, and (2) seamless query evaluation across\ntemporal spaces that span past, present and future events. These allow\naccessible analytics over data streams having properties from different\ndisciplines, and help span the velocity (real-time) and volume (persistent)\ndimensions. In this article, we introduce a Knowledge-infused CEP (X-CEP)\nframework that provides domain-aware knowledge query constructs along with\ntemporal operators that allow end-to-end queries to span across real-time and\npersistent streams. We translate this query model to efficient query execution\nover online and offline data streams, proposing several optimizations to\nmitigate the overheads introduced by evaluating semantic predicates and in\naccessing high-volume historic data streams. The proposed X-CEP query model and\nexecution approaches are implemented in our prototype semantic CEP engine,\nSCEPter. We validate our query model using domain-aware CEP queries from a\nreal-world Smart Power Grid application, and experimentally analyze the\nbenefits of our optimizations for executing these queries, using event streams\nfrom a campus-microgrid IoT deployment.\n",
        "  Workflow technology is rapidly evolving and, rather than being limited to\nmodeling the control flow in business processes, is becoming a key mechanism to\nperform advanced data management, such as big data analytics. This survey\nfocuses on data-centric workflows (or workflows for data analytics or data\nflows), where a key aspect is data passing through and getting manipulated by a\nsequence of steps. The large volume and variety of data, the complexity of\noperations performed, and the long time such workflows take to compute give\nrise to the need for optimization. In general, data-centric workflow\noptimization is a technology in evolution. This survey focuses on techniques\napplicable to workflows comprising arbitrary types of data manipulation steps\nand semantic inter-dependencies between such steps. Further, it serves a\ntwofold purpose. Firstly, to present the main dimensions of the relevant\noptimization problems and the types of optimizations that occur before flow\nexecution. Secondly, to provide a concise overview of the existing approaches\nwith a view to highlighting key observations and areas deserving more attention\nfrom the community.\n",
        "  A manifold which admits a reducible genus-$2$ Heegaard splitting is one of\nthe $3$-sphere, $S^2 \\times S^1$, lens spaces or their connected sums. For each\nof those splittings, the complex of Haken spheres is defined. When the manifold\nis the $3$-sphere, $S^2 \\times S^1$ or the connected sum whose summands are\nlens spaces or $S^2 \\times S^1$, the combinatorial structure of the complex has\nbeen studied by several authors. In particular, it was shown that those\ncomplexes are all contractible. In this work, we study the remaining cases,\nthat is, when the manifolds are lens spaces. We give a precise description of\neach of the complexes for the genus-$2$ Heegaard splittings of lens spaces. A\nremarkable fact is that the complexes for most lens spaces are not contractible\nand even not connected.\n",
        "  It is well known from numerous experiments that nuclear multifragmentation is\na dominating mechanism for production of intermediate-mass fragments in\nnucleus-nucleus collisions at energies above 100 A MeV. In this paper we\ninvestigate the validity and performance of the Fermi break-up model and the\nstatistical multifragmentation model implemented as parts of the Geant4\ntoolkit. We study the impact of violent nuclear disintegration reactions on the\ndepth-dose profiles and yields of secondary fragments for beams of light and\nmedium-weight nuclei propagating in extended media. Implications for ion-beam\ncancer therapy and shielding from cosmic radiation are discussed.\n",
        "  Traditional Chinese Medicine (TCM) is an influential form of medical\ntreatment in China and surrounding areas. In this paper, we propose a TCM\nprescription generation task that aims to automatically generate a herbal\nmedicine prescription based on textual symptom descriptions.\nSequence-to-sequence (seq2seq) model has been successful in dealing with\nsequence generation tasks. We explore a potential end-to-end solution to the\nTCM prescription generation task using seq2seq models. However, experiments\nshow that directly applying seq2seq model leads to unfruitful results due to\nthe repetition problem. To solve the problem, we propose a novel decoder with\ncoverage mechanism and a novel soft loss function. The experimental results\ndemonstrate the effectiveness of the proposed approach. Judged by professors\nwho excel in TCM, the generated prescriptions are rated 7.3 out of 10. It shows\nthat the model can indeed help with the prescribing procedure in real life.\n",
        "  This paper contains general formulae for the reduced relative Tutte, Kauffman\nbracket and Jones polynomials of families of virtual knots and links given in\nConway notation and discussion of a counterexample to the Z-move conjecture of\nFenn, Kauffman and Manturov.\n",
        "  We study a class of graph analytics SQL queries, which we call relationship\nqueries. Relationship queries are a wide superset of fixed-length graph\nreachability queries and of tree pattern queries. Intuitively, it discovers\ntarget entities that are reachable from source entities specified by the query.\nIt usually also finds aggregated scores, which correspond to the target\nentities and are calculated by applying aggregation functions on measure\nattributes, which are found on the target entities, the source entities and the\npaths from the sources to the targets. We present real-world OLAP scenarios,\nwhere efficient relationship queries are needed. However, row stores, column\nstores and graph databases are unacceptably slow in such OLAP scenarios. We\nbriefly comment on the straightforward extension of relationship queries that\nallows accessing arbitrary schemas.\n  The GQ-Fast in-memory analytics engine utilizes a bottom-up fully pipelined\nquery execution model running on a novel data organization that combines\nsalient features of column-based organization, indexing and compression.\nFurthermore, GQ-Fast compiles its query plans into executable C++ source codes.\nBesides achieving runtime efficiency, GQ-Fast also reduces main memory\nrequirements because, unlike column databases, GQ-Fast selectively allows more\ndense forms of compression including heavy-weighted compressions, which do not\nsupport random access.\n  We used GQ-Fast to accelerate queries for two OLAP dashboards in the\nbiomedical field. It outperforms Postgres by 2-4 orders of magnitude and\noutperforms MonetDB and Neo4j by 1-3 orders of magnitude when all of them are\nrunning on RAM. In addition, it generally saves space due to the appropriate\nuse of compression methods.\n",
        "  Big data integration could involve a large number of sources with\nunpredictable redundancy information between them. The approach of building a\ncentral warehousing to integrate big data from all sources then becomes\ninfeasible because of so large number of sources and continuous updates\nhappening. A practical approach is to apply online query scheduling that\ninquires data from sources at runtime upon receiving a query. In this paper, we\naddress the Time-Cost Minimization Problem for online query scheduling, and\ntackle the challenges of source permutation and statistics estimation to\nminimize the time cost of retrieving answers for the real-time receiving query.\nWe propose the online scheduling strategy that enables the improvement of\nstatistics, the construction of source permutation and the execution of query\nworking in parallel. Experimental results show high efficiency and scalability\nof our scheduling strategy.\n",
        "  Species accumulation curves (SAC), i.e. the relationship between species\nrichness and the number of sampling units in a given community, can be used to\ndescribe diversity patterns while accounting for the well-known\nscale-dependence of species richness. Despite their value, the functional form\nand the parameters of SAC, as well as their determinants, have barely been\ninvestigated in plant communities, particularly in drylands. We characterized\nthe SAC of perennial plant communities from 233 dryland ecosystems from six\ncontinents by comparing the fit of major functions (power-law, logarithmic and\nMichaelis-Menten). We tested the theoretical prediction that the effects of\naridity and soil pH on SAC are mediated by vegetation attributes such as\nevenness, cover, and spatial aggregation. We found that the logarithmic\nrelationship was the most common functional form, followed by Michaelis-Menten\nand power-law. Functional form was mainly determined by evenness while the SAC\nparameters (intercept and slope) were largely determined by spatial\naggregation. In addition, aridity decreased small scale richness (intercept of\nSAC) but did not affect accumulation rate (slope of the SAC). Our results\nhighlight the role that attributes such as spatial aggregation and evenness\nplay as main mediators of the SAC of vegetation in drylands, the Earth's\nlargest biome.\n",
        "  We simulate an individual-based model that represents both the phenotype and\ngenome of digital organisms with predator-prey interactions. We show how\nopen-ended growth of complexity arises from the invariance of genetic evolution\noperators with respect to changes in the complexity, and that the dynamics\nwhich emerges is controlled by a non-equilibrium critical point. The mechanism\nis analogous to the development of the cascade in fluid turbulence.\n",
        "  For a knot $K$ the cube number is a knot invariant defined to be the smallest\n$n$ for which there is a cube diagram of size $n$ for $K$. There is also a\nLegendrian version of this invariant called the \\emph{Legendrian cube number}.\nWe will show that the Legendrian cube number distinguishes the Legendrian left\nhand torus knots with maximal Thurston-Bennequin number and maximal rotation\nnumber from the Legendrian left hand torus knots with maximal\nThurston-Bennequin number and minimal rotation number.\n",
        "  We study spin-fluctuation-mediated superconductivity in the one-band Hubbard\nmodel. Higher order effective interactions in $U$ give rise to a\nsuperconducting instability which is very sensitive to changes in the Fermi\nsurface topology arising as a function of doping and changes in the band\nstructure. We show the superconducting phase diagram as a function of doping\nand next-nearest neighbor hopping in the limit of very small Coulomb\ninteraction strength and discuss peculiarities arising at the phase boundaries\nseparating different superconducting domains.\n",
        "  Hidden Markov model (HMM) has been well studied and extensively used. In this\npaper, we present DPHMM ({Differentially Private Hidden Markov Model}), an HMM\nembedded with a private data release mechanism, in which the privacy of the\ndata is protected through a graph. Specifically, we treat every state in Markov\nmodel as a node, and use a graph to represent the privacy policy, in which\n\"indistinguishability\" between states is denoted by edges between nodes. Due to\nthe temporal correlations in Markov model, we show that the graph may be\nreduced to a subgraph with disconnected nodes, which become unprotected and\nmight be exposed. To detect such privacy risk, we define sensitivity hull and\ndegree of protection based on the graph to capture the condition of information\nexposure. Then to tackle the detected exposure, we study how to build an\noptimal graph based on the existing graph. We also implement and evaluate the\nDPHMM on real-world datasets, showing that privacy and utility can be better\ntuned with customized policy graph.\n",
        "  A two-patch mathematical model of Dengue virus type 2 (DENV-2) that accounts\nfor vectors' vertical transmission and between patches human dispersal is\nintroduced. Dispersal is modeled via a Lagrangian approach. A host-patch\nresidence-times basic reproduction number is derived and conditions under which\nthe disease dies out or persists are established. Analytical and numerical\nresults highlight the role of hosts' dispersal in mitigating or exacerbating\ndisease dynamics. The framework is used to explore dengue dynamics using, as a\nstarting point, the 2002 outbreak in the state of Colima, Mexico.\n",
        "  The warping sum $e(K)$ of a knot $K$ is the minimal value of the sum of the\nwarping degrees of a minimal diagram of $K$ with both orientations. In this\npaper, knots $K$ with $e(K) \\le 3$ are characterized, and some knots $K$ with\n$e(K)=4$ are given.\n",
        "  We study proximity effects in clean nanoscale superconductor-normal\nmetal-superconductor (S$\\mid$N$\\mid$S) graphene heterostructures using a\nself-consistent numerical solution to the continuum Dirac Bogoliubov-de Gennes\n(DBdG) equations. We obtain results for the pair amplitude and the local\ndensity of states (DOS), as a function of doping and of the geometrical\nparameters determining the width of the structures. The superconducting\ncorrelations are found to penetrate the normal graphene layers even when there\nis extreme mismatch in the normal and superconducting doping levels, where\nspecular Andreev reflection dominates. The local DOS exhibits peculiar\nfeatures, which we discuss, arising from the Dirac cone dispersion relation and\nfrom the interplay between the superconducting and Thouless energy scales. The\ncorresponding characteristic energies emerge in the form of resonant peaks in\nthe local DOS, that depend strongly on the doping level, as does the energy\ngap, which declines sharply as the relative difference in doping between the S\nand N regions is reduced. We also linearize the DBdG equations and develop an\nessentially analytical method that determines the critical temperature $T_c$ of\nan \\sns nanostructure self-consistently. We find that for S regions that occupy\na fraction of the coherence length, $T_c$ can undergo substantial variations as\na function of the relative doping. At finite temperatures and by manipulating\nthe doping levels, the self consistent pair amplitudes reveal dramatic\ntransitions between a superconducting and resistive normal state of the\nstructure. Such behavior suggests the possibility of using the proposed system\nas a carbon-based superconducting switch, turning superconductivity on or off\nby tuning the relative doping levels.\n",
        "  Differentiating intrinsic language words from transliterable words is a key\nstep aiding text processing tasks involving different natural languages. We\nconsider the problem of unsupervised separation of transliterable words from\nnative words for text in Malayalam language. Outlining a key observation on the\ndiversity of characters beyond the word stem, we develop an optimization method\nto score words based on their nativeness. Our method relies on the usage of\nprobability distributions over character n-grams that are refined in step with\nthe nativeness scorings in an iterative optimization formulation. Using an\nempirical evaluation, we illustrate that our method, DTIM, provides significant\nimprovements in nativeness scoring for Malayalam, establishing DTIM as the\npreferred method for the task.\n",
        "  We report the discovery of a ultraluminous X-ray source (ULX; CXO\nJ133815.6+043255) in NGC 5252. This ULX is an off-nuclear point-source, which\nis 22$^{\\prime\\prime}$ away from the center of NGC 5252, and has an X-ray\nluminosity of 1.5 $\\times$ $10^{40}$erg s$^{-1}$. It is one of the rare\nexamples of ULX, which exhibits clear counterparts in radio, optical, UV bands.\nFollow-up optical spectrum of the ULX shows strong emission lines. The redshift\nof [O III] emission line coincides with the systematic velocity of NGC 5252,\nsuggesting the ULX is gravitationally bound to NGC 5252. The flux of [O III]\nappears to be correlated with both X-ray and radio luminosity in the same\nmanner as ordinary AGNs, indicating that the [O III] emission is intrinsically\nassociated with the ULX. Based on the multiwavelength data, we argue that the\nULX is unlikely to be a background AGN. A more likely option is an accreting BH\nwith a black hole mass of $\\geq 10^4M_\\odot$, which might be a stripped remnant\nof a merging dwarf galaxy.\n",
        "  The research works on the biological effects of electromagnetic (EM)\nradiation have increased globally. Computer simulation models were developed to\nassess exposure of square, rectangular, pyramidal, and cylindrical water\ncontainers to microwave radiation at 300, 900, and 2,400 MHz. The development\nof the models included determination of EM field distribution and the resulting\nspecific absorption rate (SAR) in the stored water. These models employed CST\nSTUDIO SUITE 2014 package to solve EM field equations by applying the\nfinite-difference time-domain (FDTD) method. The effect of frequency, packaging\nshape, and polarization on SAR induced in water was determined. High electric\nfield and point SAR were obtained over the whole azimuth and elevation angles\nrange in the pyramidal container model. The highest values of SAR were induced\nin water at the sharp edges of the four water container models. The order of\nthe effect on total SAR and maximum point SAR is cylindrical < square <\nrectangular < pyramidal model at 300, 900, and 2,400 MHz for both vertical and\nhorizontal polarizations. It can be concluded that the variation in the\npackaging shape of the containers, polarization, irradiation geometry, and\nfrequency. In addition, the sharp edges of the container models significantly\naffect the calculation and distribution of electric and magnetic fields and SAR\nvalues induced in the stored water, which in turn could cause variations in the\nnon-thermal effects of the electromagnetic fields in the stored water.\n",
        "  A vortex crossing a thin-film superconducting strip from one edge to the\nother, perpendicular to the bias current, is the dominant mechanism of\ndissipation for films of thickness d on the order of the coherence length XI;\nand of width w much narrower than the Pearl length LAMBDA >> w >> XI. At high\nbias currents, I* < I < Ic, the heat released by the crossing of a single\nvortex suffices to create a belt-like normal-state region across the strip,\nresulting in a detectable voltage pulse. Here Ic is the critical current at\nwhich the energy barrier vanishes for a single vortex crossing. The belt forms\nalong the vortex path and causes a transition of the entire strip into the\nnormal state. We estimate I* to be roughly Ic/3. Further, we argue that such\n\"hot\" vortex crossings are the origin of dark counts in photon detectors, which\noperate in the regime of metastable superconductivity at currents between I*\nand Ic. We estimate the rate of vortex crossings and compare it with recent\nexperimental data for dark counts. For currents below I*, i.e., in the stable\nsuperconducting but resistive regime, we estimate the amplitude and duration of\nvoltage pulses induced by a single vortex crossing.\n",
        "  There has been much recent interest in modelling epidemics on networks,\nparticularly in the presence of substantial clustering. Here, we develop\npairwise methods to answer questions that are often addressed using epidemic\nmodels, in particular: on the basis of potential observations early in an\noutbreak, what can be predicted about the epidemic outcomes and the levels of\nintervention necessary to control the epidemic? We find that while some results\nare independent of the level of clustering (early growth predicts the level of\n`leaky' vaccine needed for control and peak time, while the basic reproductive\nratio predicts the random vaccination threshold) the relationship between other\nquantities is very sensitive to clustering.\n",
        "  This survey article describes the algorithmic approaches successfully used\nover the time to construct hyperbolic structures on 3-dimensional topological\n\"objects\" of various types, and to classify several classes of such objects\nusing such structures.\n",
        "  While the introduction of differential privacy has been a major breakthrough\nin the study of privacy preserving data publication, some recent work has\npointed out a number of cases where it is not possible to limit inference about\nindividuals. The dilemma that is intrinsic in the problem is the simultaneous\nrequirement of data utility in the published data. Differential privacy does\nnot aim to protect information about an individual that can be uncovered even\nwithout the participation of the individual. However, this lack of coverage may\nviolate the principle of individual privacy. Here we propose a solution by\nproviding protection to sensitive information, by which we refer to the answers\nfor aggregate queries with small counts. Previous works based on\n$\\ell$-diversity can be seen as providing a special form of this kind of\nprotection. Our method is developed with another goal which is to provide\ndifferential privacy guarantee, and for that we introduce a more refined form\nof differential privacy to deal with certain practical issues. Our empirical\nstudies show that our method can preserve better utilities than a number of\nstate-of-the-art methods although these methods do not provide the protections\nthat we provide.\n",
        "  Background: Real-time (RT) assessment of ventricular volumes and function\nenables data acquisition during free-breathing. However, in children the\nrequirement for high spatiotemporal resolution requires accelerated imaging\ntechniques. In this study, we implemented a novel RT bSSFP spiral sequence\nreconstructed using Compressed Sensing (CS) and validated it against the\nbreath-hold (BH) reference standard for assessment of ventricular volumes in\nchildren with heart disease.\n  Methods: Data was acquired in 60 children. Qualitative image scoring and\nevaluation of ventricular volumes was performed by 3 clinical cardiac MR\nspecialists. 30 cases were reassessed for intra-observer variability, and the\nother 30 cases for inter-observer variability.\n  Results: Spiral RT images were of good quality, however qualitative scores\nreflected more residual artefact than standard BH images and slightly lower\nedge definition. Quantification of Left Ventricular (LV) and Right Ventricular\n(RV) metrics showed excellent correlation between the techniques with narrow\nlimits of agreement. However, we observed small but statistically significant\noverestimation of LV end-diastolic volume, underestimation of LV end-systolic\nvolume, as well as a small overestimation of RV stroke volume and ejection\nfraction using the RT imaging technique. No difference in inter-observer or\nintra-observer variability were observed between the BH and RT sequences.\n  Conclusions: Real-time bSSFP imaging using spiral trajectories combined with\na compressed sensing reconstruction is feasible. The main benefit is that it\ncan be acquired during free breathing. However, another important secondary\nbenefit is that a whole ventricular stack can be acquired in ~20 seconds, as\nopposed to ~6 minutes for standard BH imaging. Thus, this technique holds the\npotential to significantly shorten MR scan times in children.\n",
        "  Traditional approaches to ecosystem modelling have relied on spatially\nhomogeneous approximations to interaction, growth and death. More recently,\nspatial interaction and dispersal have also been considered. While these leads\nto certain changes in community dynamics, their effect is sometimes fairly\nminimal, and demographic scenarios in which this difference is important have\nnot been systematically investigated.\n  We take a simple mean-field model which simulates birth, growth and death\nprocesses, and rewrite it with spatially distributed discrete individuals. Each\nindividual's growth and mortality is determined by a competition measure which\ncaptures the effects of neighbours in a way which retains the conceptual\nsimplicity of a generic, analytically-solvable model. Although the model is\ngeneric, we here parameterise it using data from Caledonian Scots Pine stands.\nThe dynamics of simulated populations, starting from a plantation lattice\nconfiguration, mirror those of well-established qualitative descriptions of\nnatural forest stand behaviour; an analogy which assists in understanding the\ntransition from artificial to old-growth structure.\n  When parameterised for Scots Pine populations, the signature of spatial\nprocesses is evident, but they do not have a large effect on first-order\nstatistics such as density and biomass. The sensitivity of this result to\nvariation in each individual rate parameter is investigated; distinct\ndifferences between spatial and mean-field models are seen only upon alteration\nof the interaction strength parameters, and in low density populations. Under\nthe Scots Pine parameterisation, dispersal also has an effect of spatial\nstructure, but not first-order properties. Only in more intense competitive\nscenarios does altering the relative scales of dispersal and interaction lead\nto a clear signal in first order behaviour.\n",
        "  Coded aperture imaging transcends planar imaging with conventional\ncollimators in efficiency and Field of View (FoV). We present experimental\nresults for the detection of 141keV and 122keV {\\gamma}-photons emitted by\nuniformly extended 99mTc and 57Co hot-spots along with simulations of uniformly\nand normally extended 99mTc hot-spots. These results prove that the method can\nbe used for intra-operative imaging of radio-traced sentinel nodes and thyroid\nremnants. The study is performed using a setup of two gamma cameras, each\nconsisting of a coded-aperture (or mask) of Modified Uniformly Redundant Array\n(MURA) of rank 19 positioned on top of a CdTe detector. The detector pixel\npitch is 350 {\\mu}m and its active area is 4.4x4.4 cm2, while the mask element\nsize is 1.7mm. The detectable photon energy ranges from 15 keV up to 200 keV\nwith an energy resolution of 3-4 keV FWHM. Triangulation is exploited to\nestimate the 3D spatial coordinates of the radioactive spots within the system\nFoV. Two extended sources, with uniform distributed activity (11 and 24 mm in\ndiameter, respectively), positioned at 16cm from the system and with 3cm\ndistance between their centers, can be resolved and localized with accuracy\nbetter than 5%. The results indicate that the estimated positions of spatially\nextended sources lay within their volume size and that neighboring sources,\neven with a low level of radioactivity, such as 30 MBq, can be clearly\ndistinguished with counting time about 3 seconds\n",
        "  To introduce selection into a model of coalescence, I explore the use of\nmodified integer partitions that allow the identification of a preferred\nlineage. I show that a partition-partition transition matrix, along with Monte\nCarlo discrete time kinetics, treats both the neutral case and a wide range of\npositive and negative selection pressures for small population sizes. Selection\npressure causes multiple collisions per generation, short coalescence times,\nincreased lengths of terminal branches, increased tree asymmetry, and\ndependence of coalescence times on the logarithm of population size. These\nfeatures are consistent with higher order coalescences that permit multiple\ncollisions per generation. While the treatment is exact in terms of the\nsimplified Wright-Fisher model used, it is not easily extended to large\npopulation size. Keywords: Selection, Coalescence, Integer Partitions, Multiple\nCollisions, Tree Asymmetry.\n",
        "  We extend the Neumann's methods and give the explicit formulae for the volume\nand the Chern-Simons invariant for hyperbolic alternating knot orbifolds.\n",
        "  Generative Adversarial Networks (GANs) are a promising approach to language\ngeneration. The latest works introducing novel GAN models for language\ngeneration use n-gram based metrics for evaluation and only report single\nscores of the best run. In this paper, we argue that this often misrepresents\nthe true picture and does not tell the full story, as GAN models can be\nextremely sensitive to the random initialization and small deviations from the\nbest hyperparameter choice. In particular, we demonstrate that the previously\nused BLEU score is not sensitive to semantic deterioration of generated texts\nand propose alternative metrics that better capture the quality and diversity\nof the generated samples. We also conduct a set of experiments comparing a\nnumber of GAN models for text with a conventional Language Model (LM) and find\nthat neither of the considered models performs convincingly better than the LM.\n",
        "  In recent years, the increased need to house and process large volumes of\ndata has prompted the need for distributed storage and querying systems. The\ngrowth of machine-readable RDF triples has prompted both industry and academia\nto develop new database systems, called NoSQL, with characteristics that differ\nfrom classical databases. Many of these systems compromise ACID properties for\nincreased horizontal scalability and data availability. This thesis concerns\nthe development and evaluation of a NoSQL triplestore. Triplestores are\ndatabase management systems central to emerging technologies such as the\nSemantic Web and linked data. The evaluation spans several benchmarks,\nincluding the two most commonly used in triplestore evaluation, the Berlin\nSPARQL Benchmark, and the DBpedia benchmark, a query workload that operates an\nRDF representation of Wikipedia. Results reveal that the join algorithm used by\nthe system plays a critical role in dictating query runtimes. Distributed graph\ndatabases must carefully optimize queries before generating MapReduce query\nplans as network traffic for large datasets can become prohibitive if the query\nis executed naively.\n",
        "  We show that many graphs naturally associated to a connected, compact,\norientable surface are hierarchically hyperbolic spaces in the sense of\nBehrstock, Hagen and Sisto. They also automatically have the coarse median\nproperty defined by Bowditch. Consequences for such graphs include a distance\nformula analogous to Masur and Minsky's distance formula for the mapping class\ngroup, an upper bound on the maximal dimension of quasiflats, and the existence\nof a quadratic isoperimetric inequality. The hierarchically hyperbolic\nstructure also gives rise to a simple criterion for when such graphs are Gromov\nhyperbolic.\n",
        "  There are two contrasting explanations of sleep: as a proximate, essential\nphysiological function or as an adaptive state of inactivity and these\nhypotheses remain widely debated. To investigate the adaptive significance of\nsleep, we develop an evolutionary argument formulated as a tractable partial\ndifferential equation model. We allow demographic parameters such as birth and\nmortality rates to vary through time in both safe and vulnerable sleeping\nenvironments. From this model we analytically calculate population growth rate\n(fitness) for sleeping and non-sleeping strategies. We find that, in a\ntemporally heterogeneous environment, sleeping always achieves a higher fitness\nthan not sleeping. As organisms do not exist in constant environments, we\nconclude that the evolution of sleep is inevitable.\n",
        "  We used narrowband interference filters with the CCD imaging camera on the\nNickel 1.0 meter telescope at Lick Observatory to observe 31 nearby (z < 0.03)\nSeyfert galaxies in the 12 {\\mu}m Active Galaxy Sample. We obtained pure\nemission line images of each galaxy in order to separate H{\\alpha} emission\nfrom the nucleus from that of the host galaxy. The extended H{\\alpha} emission\nis expected to be powered by newly formed hot stars, and correlates well with\nother indicators of current star formation in these galaxies: 7.7 {\\mu}m PAH,\nfar-infrared, and radio luminosity. Relative to what would be expected from\nrecent star formation, there is a 0.8 dex excess of radio emission in our\nSeyfert galaxies. The nuclear H{\\alpha} luminosity is dominated by the AGN, and\nis correlated with the hard X-ray luminosity. There is an upward offset of 1\ndex in this correlation for the Seyfert 1s due to a strong contribution from\nthe Broad Line Region. We found a correlation between star formation rate and\nAGN luminosity. In spite of selection effects, we concluded that the absence of\nbright Seyfert nuclei in galaxies with low SFRs is real, albeit only weakly\nsignificant. We used our measured spatial distributions of H{\\alpha} emission\nto determine what these Seyfert galaxies would look like when observed through\nfixed apertures at high redshifts. Although all would be detectable emission\nline galaxies at any redshift, most would appear dominated by HII region\nemission. Only the most luminous AGN would still be identified at z~0.3.\n",
        "  Nowadays there is a growing interest in Particle Therapy treatments\nexploiting light ion beams against tumors due to their enhanced Relative\nBiological Effectiveness and high space selectivity. In particular promising\nresults are obtained by the use of $^4$He projectiles. Unlike the treatments\nperformed using protons, the beam ions can undergo a fragmentation process when\ninteracting with the atomic nuclei in the patient body. In this paper the\nresults of measurements performed at the Heidelberg Ion-Beam Therapy center are\nreported. For the first time the absolute fluxes and the energy spectra of the\nfragments - protons, deuterons, and tritons - produced by $^4$He ion beams of\n102, 125 and 145 MeV/u energies on a poly-methyl methacrylate target were\nevaluated at different angles. The obtained results are particularly relevant\nin view of the necessary optimization and review of the Treatment Planning\nSoftware being developed for clinical use of $^4$He beams in clinical routine\nand the relative benchmarking of Monte Carlo algorithm predictions.\n",
        "  The Pan-STARRS1 survey has obtained multi-epoch imaging in five bands\n(Pan-STARRS1 gps, rps, ips, zps, and yps) on twelve \"Medium Deep Fields\", each\nof which spans a 3.3 degree circle. For the period between Apr 2009 and Apr\n2011 these fields were observed 50-200 times. Using a reduced proper motion\ndiagram, we have extracted a list of 47 white dwarf (WD) candidates whose\nPan-STARRS1 astrometry indicates a non-zero proper motion at the 6-sigma level,\nwith a typical 1-sigma proper motion uncertainty of 10 mas/yr. We also used\nastrometry from SDSS (when available) and USNO-B to assess our proper motion\nfits. None of the WD candidates exhibits evidence of statistically significant\nparallaxes, with a typical 1-sigma uncertainty of 8 mas. Twelve of these\ncandidates are known WDs, including the high proper motion (1.7\"/yr) WD LHS\n291. We confirm three more objects as WDs through optical spectroscopy. Based\non the Pan-STARRS1 colors, ten of the stars are likely to be cool WDs with 4170\nK Teff 5000 K and cooling ages <9 Gyr. We classify these objects as likely\nthick disk WDs based on their kinematics. Our current sample represents only a\nsmall fraction of the Pan-STARRS1 data. With continued coverage from the Medium\nDeep Field Survey and the 3pi survey, Pan-STARRS1 should find many more high\nproper motion WDs that are part of the old thick disk and halo.\n",
        "  In this review, we discuss modularity and hierarchy in biological systems. We\nreview examples from protein structure, genetics, and biological networks of\nmodular partitioning of the geometry of biological space. We review theories to\nexplain modular organization of biology, with a focus on explaining how biology\nmay spontaneously organize to a structured form. That is, we seek to explain\nhow biology nucleated from among the many possibilities in chemistry. The\nemergence of modular organization of biological structure will be described as\na symmetry-breaking phase transition, with modularity as the order parameter.\nExperimental support for this description will be reviewed. Examples will be\npresented from pathogen structure, metabolic networks, gene networks, and\nprotein-protein interaction networks. Additional examples will be presented\nfrom ecological food networks, developmental pathways, physiology, and social\nnetworks.\n",
        "  Kin selection theory is a kind of causal analysis. The initial form of kin\nselection ascribed cause to costs, benefits, and genetic relatedness. The\ntheory then slowly developed a deeper and more sophisticated approach to\npartitioning the causes of social evolution. Controversy followed because\ncausal analysis inevitably attracts opposing views. It is always possible to\nseparate total effects into different component causes. Alternative causal\nschemes emphasize different aspects of a problem, reflecting the distinct\ngoals, interests, and biases of different perspectives. For example, group\nselection is a particular causal scheme with certain advantages and significant\nlimitations. Ultimately, to use kin selection theory to analyze natural\npatterns and to understand the history of debates over different approaches,\none must follow the underlying history of causal analysis. This article\ndescribes the history of kin selection theory, with emphasis on how the causal\nperspective improved through the study of key patterns of natural history, such\nas dispersal and sex ratio, and through a unified approach to demographic and\nsocial processes. Independent historical developments in the multivariate\nanalysis of quantitative traits merged with the causal analysis of social\nevolution by kin selection.\n",
        "  Sentence is a basic linguistic unit, however, little is known about how\ninformation content is distributed across different positions of a sentence.\nBased on authentic language data of English, the present study calculated the\nentropy and other entropy-related statistics for different sentence positions.\nThe statistics indicate a three-step staircase-shaped distribution pattern,\nwith entropy in the initial position lower than the medial positions (positions\nother than the initial and final), the medial positions lower than the final\nposition and the medial positions showing no significant difference. The\nresults suggest that: (1) the hypotheses of Constant Entropy Rate and Uniform\nInformation Density do not hold for the sentence-medial positions; (2) the\ncontext of a word in a sentence should not be simply defined as all the words\npreceding it in the same sentence; and (3) the contextual information content\nin a sentence does not accumulate incrementally but follows a pattern of \"the\nwhole is greater than the sum of parts\".\n",
        "  Motivated by recent progress in development of cryogenic memory compatible\nwith single flux quantum (SFQ) circuits we have performed a theoretical study\nof magnetic SIsFS Josephson junctions, where 'S' is a bulk superconductor, 's'\nis a thin superconducting film, 'F' is a metallic ferromagnet and 'I' is an\ninsulator. We calculate the Josephson current as a function of s and F layers\nthickness, temperature and exchange energy of F film. We outline several modes\nof operation of these junctions and demonstrate their unique ability to have\nlarge product of a critical current $I_{C}$ and a normal-state resistance\n$R_{N}$ in the $\\pi$ state, comparable to that in SIS tunnel junctions commonly\nused in SFQ circuits. We develop a model describing switching of the Josephson\ncritical current in these devices by external magnetic field. The results are\nin good agreement with the experimental data for\nNb-Al/AlO${_x}$-Nb-Pd$_{0.99}$Fe$_{0.01}$-Nb junctions.\n",
        "  Divide knots and links, defined by A'Campo in the singularity theory of\ncomplex curves, is a method to present knots or links by real plane curves. The\npresent paper is a continuation of the author's previous result that every knot\nin the major subfamilies of Berge's lens space surgery (i.e., knots yielding a\nlens space by Dehn surgery) is presented by an L-shaped curve as a divide knot.\nIn the present paper, L-shaped curves are generalized and it is shown that\nevery knot in the minor subfamilies, called sporadic examples, of Berge's lens\nspace surgery is presented by a generalized L-shaped curve as a divide knot. A\nformula on the surgery coefficients and the presentation is also generalized.\n",
        "  Is cancer a disease that can be cured or a degenerative disease which comes\npredominantly with old age? We give an answer based on a two-dimensional\nrepresentation of diseases. These two dimensions are defined as follows. In\nmortality curves there is an age, namely a_c = 10 years, which plays a crucial\nrole in the sense that the mortality rate decreases in the interval I1=(a<a_c)\nand increases in the interval I2=(a>a_c). The respective trends in I1 and I2\nare the two parameters used in our classification of diseases. Within the\nframework of reliability analysis, I1 and I2 would be referred to as the\n\"burn-in\" and \"wear-out\" phases. This leads to define three broad groups of\ndiseases. (AS1) Asymmetry with prevalence of I1. (AS2) Asymmetry with\nprevalence of I2. (S) Symmetry, with I1 and I2 both playing roles of comparable\nimportance. Not surprisingly, among AS1-cases one finds all diseases due to\ncongenital malformations. In the AS2-class one finds degenerative diseases,\ne.g. Alzheimer's disease. Among S-cases one finds most diseases due to external\npathogens or to wear-out processes. Cancer is one of those mixed cases and it\nis closer to (AS2) than to (AS1). This representation also explains what we\ncall an OVERKILL EFFECT in old age. It tells us that even an effective cancer\ntherapy would have no influence whatsoever on the extent of human life unless\nALL other diseases are cured simultaneously. We conclude that whilst one might\ndevelop a cure for many of the known diseases of old age, new diseases will\narise and the quest for the ultimate cure will be never ending. Death before\nthe age of 120 seems firmly written in our genetic code and the best we shall\nbe able to do is to help people cope with the inevitability of death as and\nwhen it arises.\n",
        "  Given a gene tree and a species tree, ancestral configurations represent the\ncombinatorially distinct sets of gene lineages that can reach a given node of\nthe species tree. They have been introduced as a data structure for use in the\nrecursive computation of the conditional probability under the multispecies\ncoalescent model of a gene tree topology given a species tree, the cost of this\ncomputation being affected by the number of ancestral configurations of the\ngene tree in the species tree. For matching gene trees and species trees, we\nobtain enumerative results on ancestral configurations. We study ancestral\nconfigurations in balanced and unbalanced families of trees determined by a\ngiven seed tree, showing that for seed trees with more than one taxon, the\nnumber of ancestral configurations increases for both families exponentially in\nthe number of taxa $n$. For fixed $n$, the maximal number of ancestral\nconfigurations tabulated at the species tree root node and the largest number\nof labeled histories possible for a labeled topology occur for trees with\nprecisely the same unlabeled shape. For ancestral configurations at the root,\nthe maximum increases with $k_0^n$, where $k_0 \\approx 1.5028$ is a quadratic\nrecurrence constant. Under a uniform distribution over the set of labeled trees\nof given size, the mean number of root ancestral configurations grows with\n$\\sqrt{3/2}(4/3)^n$ and the variance with approximately $1.4048(1.8215)^n$. The\nresults provide a contribution to the combinatorial study of gene trees and\nspecies trees.\n",
        "  Long wavelength sonic oscillations are observed or inferred in many of the\nsmall, dark molecular clouds, the starless cores, that are the precursors to\nprotostars. The oscillations provide significant internal energy and the time\nscale for their dissipation may control the rate of star formation in the\nstarless cores. Despite their importance, their origin is unknown. We explore\none hypothesis that the oscillations develop as a starless core forms from a\nfilament. We model this process with a numerical hydrodynamic simulation and\ngenerate synthetic molecular line observations with a radiative transfer\nsimulation. Comparison with actual observations shows general agreement\nsuggesting the proposed mechanism is viable.\n",
        "  We investigate the optical conductivity as a function of temperature with\nlight polarized along the in-plane orthorhombic $a$- and $b$-axes of\nBa(Fe$_{1-x}$Co$_x$)$_2$As$_2$ for $x$=0 and 2.5$\\%$ under uniaxial pressure.\nThe charge dynamics at low frequencies on these detwinned, single domain\ncompounds tracks the anisotropic $dc$ transport properties across their\nstructural and magnetic phase transitions. Our findings allow us to estimate\nthe dichroism, which extends to relatively high frequencies. These results are\nconsistent with a scenario in which orbital order plays a significant role in\nthe tetragonal-to-orthorhombic structural transition.\n",
        "  Mitochondrial DNA from fourteen archaeological samples at the Ural State\nUniversity in Yekaterinburg, Russia was extracted to test the feasibility of\nancient DNA work on their collection. These samples come from a number of sites\nthat fall into two groupings. Seven samples are from three sites that belong to\na northern group of what are thought to be Ugrians dating to the 8th-12th\ncentury AD, who lived along the Ural Mountains in northwestern Siberia. The\nremaining seven samples are from two sites that belong to a southern group\nrepresenting the Sargat culture, dating between roughly the 5th century BC and\nthe 5th century AD, from southwestern Siberia near the Ural Mountains and the\npresent-day Kazakhstan border. The samples derived from several burial types,\nincluding kurgan burials. They also represented a number of different skeletal\nelements, as well as a range of observed preservation. The northern sites\nrepeatedly failed to amplify after multiple extraction and amplification\nattempts, but the samples from the southern sites were successfully extracted\nand amplified. The sequences obtained from the southern sites support the\nhypothesis that the Sargat culture was a potential zone of intermixture between\nnative Ugrian and/or Siberian populations and steppe peoples from the South,\npossibly early Iranian or Indo-Iranian, which has been previously suggested by\narchaeological analysis.\n",
        "  We consider 3-manifolds admitting the action of an involution such that its\nspace of orbits is homeomorphic to $S^3.$ Such involutions are called\nhyperelliptic as the manifolds admitting such an action. We consider finite\ngroups acting on 3-manifolds and containing hyperelliptic involutions whose\nfixed-point set has $r>2$ components. In particular we prove that a simple\ngroup containing such an involution is isomorphic to $PSL(2,q)$ for some odd\nprime power $q$, or to one of four other small simple groups.\n",
        "  Online adaptive radiation therapy (ART) promises the ability to deliver an\noptimal treatment in response to daily patient anatomic variation. A major\ntechnical barrier for the clinical implementation of online ART is the\nrequirement of rapid image segmentation. Deformable image registration (DIR)\nhas been used as an automated segmentation method to transfer tumor/organ\ncontours from the planning image to daily images. However, the current\ncomputational time of DIR is insufficient for online ART. In this work, this\nissue is addressed by using computer graphics processing units (GPUs). A\ngrey-scale based DIR algorithm called demons and five of its variants were\nimplemented on GPUs using the Compute Unified Device Architecture (CUDA)\nprogramming environment. The spatial accuracy of these algorithms was evaluated\nover five sets of pulmonary 4DCT images with an average size of 256x256x100 and\nmore than 1,100 expert-determined landmark point pairs each. For all the\ntesting scenarios presented in this paper, the GPU-based DIR computation\nrequired around 7 to 11 seconds to yield an average 3D error ranging from 1.5\nto 1.8 mm. It is interesting to find out that the original passive force demons\nalgorithms outperform subsequently proposed variants based on the combination\nof accuracy, efficiency, and ease of implementation.\n",
        "  We investigate the bulk orbital angular momentum (AM) in a two-dimensional\nhole-doped topological superconductor (SC) which is composed of a hole-doped\nsemiconductor thin film, a magnetic insulator, and an $s$-wave SC and is\ncharacterized by the Chern number $C = -3$. In the topological phase, $L_z/N$\nis strongly reduced from the intrinsic value by the non-particle-hole-symmetric\nedge states as in the corresponding chiral $f$-wave SCs when the spin-orbit\ninteractions (SOIs) are small, while this reduction of $L_z/N$ does not work\nfor the large SOIs. Here $L_z$ and $N$ are the bulk orbital AM and the total\nnumber of particles at zero temperature, respectively. As a result, $L_z/N$ is\ndiscontinuous or continuous at the topological phase transition depending on\nthe strengths of the SOIs. We also discuss the effects of the edge states by\ncalculating the radial distributions of the orbital AM.\n",
        "  By considering negative surgeries on a knot $K$ in $S^3$, we derive a lower\nbound to the non-orientable slice genus $\\gamma_4(K)$ in terms of the signature\n$\\sigma(K)$ and the concordance invariants $V_i(\\overline{K})$, which\nstrengthens a previous bound given by Batson, and which coincides with\nOzsv\\'ath-Stipsicz-Szab\\'o's bound in terms of their $\\upsilon$ invariant for\nL-space knots and quasi-alternating knots. A curious feature of our bound is\nsuperadditivity, implying, for instance, that the bound on the stable\nnon-orientable genus is sometimes better than the one on $\\gamma_4(K)$.\n",
        "  We study the interplay of fluctuations and superconductivity in\nBaFe$_2$As$_2$ (Ba-122) compounds with Ba and Fe substituted by K ($p$ doping)\nand Co ($n$ doping), respectively. To this end we measured electronic Raman\nspectra as a function of polarisation and temperature. We observe gap\nexcitations and fluctuations for all doping levels studied. The response from\nfluctuations is much stronger for Co substitution and, according to the\nselection rules and the temperature dependence, originates from the exchange of\ntwo critical spin fluctuations with characteristic wave vectors $(\\pm\\pi, 0)$\nand $(0,\\pm\\pi)$. At 22\\% K doping ($p=0.22$), we find the same selection rules\nand spectral shape for the fluctuations but the intensity is smaller by a\nfactor of 5. Since there exists no nematic region above the orthorhombic\nspin-density-wave (SDW) phase the identification of the fluctuations via the\ntemperature dependence is not possible. The gap excitations in the\nsuperconducting state indicate strongly anisotropic near-nodal gaps for Co\nsubstitution which make the observation of collective modes difficult. The\nvariation with doping of the spectral weights of the $A_{1g}$ and $B_{1g}$ gap\nfeatures does not support the influence of fluctuations on Cooper pairing.\nTherefore, the observation of Bardasis-Schrieffer modes inside the nearly clean\ngaps on the K-doped side remains the only experimental evidence for the\nrelevance of fluctuations for pairing.\n",
        "  Implementation of real-time, continuous, and three-dimensional imaging (4D\nintervention guidance) would be a quantum leap for minimally-invasive medicine.\nIt allows guidance during interventions by assessing the spatial position of\ninstruments continuously in respect to their surroundings. Recent research\nshowed that it is feasible using X-ray and novel tomographic reconstruction\napproaches. Radiation dose stays within reasonable limits. This article\nprovides abstractions and background information together with an outlook on\nthese prospects. There are explanations of how situational awareness during\ninterventions is generated today and how they will be in future. The\ndifferences between fluoroscopically and CT-guided interventions are eluted to\nwithin the context of these developments. The exploration of uncharted terrain\nbetween these current methods is worth pursuing. Necessary image quality of 4D\nintervention guidance varies relevantly from that of standard computed\ntomography. Means to analyze the risk-benefit ratio of 4D intervention guidance\nare provided and arguments for gantry-based setups vs C-arm based setups are\ngiven. Due to the lack of moving organs, neuro-interventions might be the first\nfield in which 4D intervention guidance might become available, however,\nregistration and fusion algorithms might make it applicable in complex\nwhole-body interventions such as aortic valve replacement soon thereafter.\n",
        "  State of the art speech recognition systems use data-intensive\ncontext-dependent phonemes as acoustic units. However, these approaches do not\ntranslate well to low resourced languages where large amounts of training data\nis not available. For such languages, automatic discovery of acoustic units is\ncritical. In this paper, we demonstrate the application of nonparametric\nBayesian models to acoustic unit discovery. We show that the discovered units\nare correlated with phonemes and therefore are linguistically meaningful. We\nalso present a spoken term detection (STD) by example query algorithm based on\nthese automatically learned units. We show that our proposed system produces a\nP@N of 61.2% and an EER of 13.95% on the TIMIT dataset. The improvement in the\nEER is 5% while P@N is only slightly lower than the best reported system in the\nliterature.\n",
        "  We have carried out multiwavelength observations of the near-by ($z=0.046$)\nrich, merging galaxy cluster Abell 3376 with the Murchison Widefield Array\n(MWA). As a part of the GaLactic and Extragalactic All-sky MWA survey (GLEAM),\nthis cluster was observed at 88, 118, 154, 188 and 215 MHz. The known radio\nrelics, towards the eastern and western peripheries of the cluster, were\ndetected at all the frequencies. The relics, with a linear extent of $\\sim$ 1\nMpc each, are separated by $\\sim$ 2 Mpc. Combining the current observations\nwith those in the literature, we have obtained the spectra of these relics over\nthe frequency range 80 -- 1400 MHz. The spectra follow power laws, with\n$\\alpha$ = $-1.17\\pm0.06$ and $-1.37\\pm0.08$ for the west and east relics,\nrespectively ($S \\propto \\nu^{\\alpha}$). Assuming the break frequency to be\nnear the lower end of the spectrum we estimate the age of the relics to be\n$\\sim$ 0.4 Gyr. No diffuse radio emission from the central regions of the\ncluster (halo) was detected. The upper limit on the radio power of any possible\nhalo that might be present in the cluster is a factor of 35 lower than that\nexpected from the radio power and X-ray luminosity correlation for cluster\nhalos. From this we conclude that the cluster halo is very extended ($>$ 500\nkpc) and/or most of the radio emission from the halo has decayed. The current\nlimit on the halo radio power is a factor of ten lower than the existing upper\nlimits with possible implications for models of halo formation.\n",
        "  We present spatial correlations of galaxies and IGM HI in the\nCOSMOS/UltraVISTA 1.62 deg$^2$ field. Our data consist of 13,415 photo-$z$\ngalaxies at $z\\sim2-3$ with $K_s<23.4$ and the Ly$\\alpha$ forest absorptions in\nthe background quasar spectra selected from SDSS data with no signature of\ndamped Ly$\\alpha$ system contamination. We estimate a galaxy overdensity\n$\\delta_{gal}$ in an impact parameter of 2.5 pMpc, and calculate the Ly$\\alpha$\nforest fluctuations $\\delta_{\\langle F\\rangle}$ whose negative values\ncorrespond to the strong Ly$\\alpha$ forest absorptions. We identify weak\nevidence of an anti-correlation between $\\delta_{gal}$ and $\\delta_{\\langle\nF\\rangle}$ with a Spearman's rank correlation coefficient of $-0.39$ suggesting\nthat the galaxy overdensities and the Ly$\\alpha$ forest absorptions positively\ncorrelate in space at the $\\sim90\\%$ confidence level. This positive\ncorrelation indicates that high-$z$ galaxies exist around an excess of HI gas\nin the Ly$\\alpha$ forest. We find four cosmic volumes, dubbed\n$A_{obs}$-$D_{obs}$, that have extremely large (small) values of $\\delta_{gal}\n\\simeq0.8$ ($-1$) and $\\delta_{\\langle F\\rangle}$ $\\simeq0.1$ ($-0.4$), three\nout of which, $B_{obs}$-$D_{obs}$, significantly depart from the correlation,\nand weaken the correlation signal. We perform cosmological hydrodynamical\nsimulations, and compare with our observational results. Our simulations\nreproduce the correlation, agreeing with the observational results. Moreover,\nour simulations have model counterparts of $A_{obs}$-$D_{obs}$, and suggest\nthat the observations pinpoint, by chance, a galaxy overdensity like a\nproto-cluster, gas filaments lying on the sightline, a large void, and\northogonal low-density filaments. Our simulations indicate that the significant\ndepartures of $B_{obs}$-$D_{obs}$ are produced by the filamentary large-scale\nstructures and the observation sightline effects.\n",
        "  It has been conjectured that for knots $K$ and $K'$ in $S^3$, $w(K#K')=\nw(K)+w(K')-2$. Scharlemann and Thompson have proposed potential counterexamples\nto this conjecture. For every $n$, they proposed a family of knots ${K^n_i}$\nfor which they conjectured that $w(B^n#K^n_i)=w(K^n_i)$ where $B^n$ is a bridge\nnumber $n$ knot. We show that for $n>2$ none of the knots in ${K^n_i}$ produces\nsuch counterexamples.\n",
        "  Two full rotating gantry with different nozzles (Multipurpose nozzle with\nMLC, Scanning Dedicated nozzle) with conventional cyclotron system is installed\nand under commissioning for various proton treatment options at Samsung Medical\nCenter in Korea. The purpose of this study is to investigate neutron dose\nequivalent per therapeutic dose, H/D, to x-ray imaging equipment under various\ntreatment conditions with monte carlo simulation. At first, we investigated H/D\nwith the various modifications of the beam line devices (Scattering, Scanning,\nMulti-leaf collimator, Aperture, Compensator) at isocenter, 20, 40, 60 cm\ndistance from isocenter and compared with other research groups. Next, we\ninvestigated the neutron dose at x-ray equipments used for real time imaging\nwith various treatment conditions. Our investigation showed the 0.07 ~ 0.19\nmSv/Gy at x-ray imaging equipments according to various treatment options and\nintestingly 50% neutron dose reduction effect of flat panel detector was\nobserved due to multi- leaf collimator during proton scanning treatment with\nmultipurpose nozzle. In future studies, we plan to investigate experimental\nmeasurement of neutron dose and validation of simulation data for x-ray imaging\nequipment with additional neutron dose reduction method.\n",
        "  In addition to their unusually long life cycle, periodical cicadas, {\\it\nMagicicada} spp., provide an exceptional example of spatially synchronized life\nstage phenology in nature. Within regions (\"broods\") spanning 50,000 to 500,000\nkm$^2$, adults emerge synchronously every 13 or 17 years. While satiation of\navian predators is believed to be a key component of the ability of these\npopulations to reach high densities, it is not clear why populations at a\nsingle location remain entirely synchronized. We develop nonlinear Leslie\nmatrix-type models of periodical cicadas that include predation-driven Allee\neffects and competition in addition to reproduction and survival. Using both\nanalytical and numerical techniques, we demonstrate the observed presence of a\nsingle brood critically depends on the relationship between fecundity,\ncompetition, and predation. We analyze the single-brood, two-brood and\nall-brood equilibria in the large life-span limit using a tractable hybrid\napproximation to the Leslie matrix model with continuous time competition in\nbetween discrete reproduction events. Within the hybrid model we prove that the\nsingle-brood equilibrium is the only stable equilibrium. This hybrid model\nallows us to quantitatively predict population sizes and the range of\nparameters for which the stable single-brood and unstable two-brood and\nall-brood equilibria exist. The hybrid model yields a good approximation to the\nnumerical results for the Leslie matrix model for the biologically relevant\ncase of a 17-year lifespan.\n",
        "  We investigate the geometry of hyperbolic knots and links whose diagrams have\na high amount of twisting of multiple strands. We find information on volume\nand certain isotopy classes of geodesics for the complements of these links,\nbased only on a diagram. The results are obtained by finding geometric\ninformation on generalized augmentations of these links.\n",
        "  Interest in modulated paired states, long sought since the first proposals by\nFulde and Ferrell and by Larkin and Ovchinnikov, has grown recently in the\ncontext of strongly coupled superconductors under the name of pair density\nwave. However, there is little theoretical understanding of how such a state\nmight arise out of strong coupling physics in simple models. Although density\nmatrix renormalization group has been a powerful tool for exploring strong\ncoupling modulation phenomena of spin and charge stripe in the Hubbard model\nand the t-J model, there has been no numerical evidence of PDW within these\nmodels using DMRG. Here we note that a system with inversion breaking, C3v\npoint group symmetry may host a PDW-like state. Motivated by the fact that\nspin-valley locked band structure of hole-doped group VI transition metal\ndichalcogenides materializes such a setting, we use DMRG to study the\nsuperconducting tendencies in spin-valley locked systems with strong\nshort-ranged repulsion. Remarkably we find robust evidence for a PDW and the\nfirst of such evidence within DMRG studies of a simple fermionic model.\n",
        "  One essential ingredient of evolutionary theory is the concept of fitness as\na measure for a species' success in its living conditions. Here, we quantify\nthe effect of environmental fluctuations onto fitness by analytical\ncalculations on a general evolutionary model and by studying corresponding\nindividual-based microscopic models. We demonstrate that not only larger growth\nrates and viabilities, but also reduced sensitivity to environmental\nvariability substantially increases the fitness. Even for neutral evolution,\nvariability in the growth rates plays the crucial role of strongly reducing the\nexpected fixation times. Thereby, environmental fluctuations constitute a\nmechanism to account for the effective population sizes inferred from genetic\ndata that often are much smaller than the census population size.\n",
        "  The thermal field theory is applied to fermionic superfluids by doubling the\ndegrees of freedom of the BCS theory. We construct the two-mode states and the\ncorresponding Bogoliubov transformation to obtain the BCS thermal vacuum. The\nexpectation values with respect to the BCS thermal vacuum produce the\nstatistical average of the thermodynamic quantities. The BCS thermal vacuum\nallows a quantum-mechanical perturbation theory with the BCS theory serving as\nthe unperturbed state. We evaluate the leading-order corrections to the order\nparameter and other physical quantities from the perturbation theory. A direct\nevaluation of the pairing correlation as a function of temperature shows the\npseudogap phenomenon results from the perturbation theory. The BCS thermal\nvacuum is shown to be a generalized coherent and squeezed state. The\ncorrespondence between the thermal vacuum and purification of the density\nmatrix allows a unitary transformation, and we found the geometric phase in the\nparameter space associated with the transformation.\n",
        "  In practical applications of tomographic imaging, there are often challenges\nfor image reconstruction due to under-sampling and insufficient data. In\ncomputed tomography (CT), for example, image reconstruction from few views\nwould enable rapid scanning with a reduced x-ray dose delivered to the patient.\nLimited-angle problems are also of practical significance in CT. In this work,\nwe develop and investigate an iterative image reconstruction algorithm based on\nthe minimization of the image total variation (TV) that applies to\ndivergent-beam CT. Numerical demonstrations of our TV algorithm are performed\nwith various insufficient data problems in fan-beam CT. The TV algorithm can be\ngeneralized to cone-beam CT as well as other tomographic imaging modalities.\n",
        "  We extend Hoste-Shanahan's calculations for the A-polynomial of twist knots,\nto give an explicit formula.\n",
        "  We have probed a section (l ~ 150, b ~ -60) of the trailing tidal arm of the\nSagittarius dwarf spheroidal galaxy by identifying a sample of Red Clump stream\nstars. Red Clump stars are not generally found in the halo field, but are found\nin significant numbers in both the Sagittarius galaxy and its tidal streams,\nmaking them excellent probes of stream characteristics. Our target sample was\nselected using photometric data from the Sloan Digital Sky Survey, Data Release\n6, which was constrained in color to match the Sagittarius Red Clump stars.\nSpectroscopic observations of the target stars were conducted at Kitt Peak\nNational Observatory using the WIYN telescope. The resulting spectroscopic\nsample is magnitude limited and contains both main sequence disk stars and\nevolved Red Clump stars. We have developed a method to systematically separate\nthese two stellar classes using kinematic information and a Bayesian approach\nfor surface gravity determination. The resulting Red Clump sample allows us to\ndetermine an absolute stellar density of {\\rho} = 2.7 +/- 0.5 RC stars kpc-3 at\nthis location in the stream. Future measurements of stellar densities for a\nvariety of populations and at various locations along the streams will lead to\na much improved understanding of the original nature of the Sagittarius galaxy\nand the physical processes controlling its disruption and subsequent stream\ngeneration.\n",
        "  Recently we have introduced a simplified model of ecosystem assembly (Capitan\net al., 2009) for which we are able to map out all assembly pathways generated\nby external invasions in an exact manner. In this paper we provide a deeper\nanalysis of the model, obtaining analytical results and introducing some\napproximations which allow us to reconstruct the results of our previous work.\nIn particular, we show that the population dynamics equations of a very general\nclass of trophic-level structured food-web have an unique interior equilibrium\npoint which is globally stable. We show analytically that communities found as\nend states of the assembly process are pyramidal and we find that the\nequilibrium abundance of any species at any trophic level is approximately\ninversely proportional to the number of species in that level. We also find\nthat the per capita growth rate of a top predator invading a resident community\nis key to understand the appearance of complex end states reported in our\nprevious work. The sign of these rates allows us to separate regions in the\nspace of parameters where the end state is either a single community or a\ncomplex set containing more than one community. We have also built up\nanalytical approximations to the time evolution of species abundances that\nallow us to determine, with high accuracy, the sequence of extinctions that an\ninvasion may cause. Finally we apply this analysis to obtain the communities in\nthe end states. To test the accuracy of the transition probability matrix\ngenerated by this analytical procedure for the end states, we have compared\naverages over those sets with those obtained from the graph derived by\nnumerical integration of the Lotka-Volterra equations. The agreement is\nexcellent.\n",
        "  According to the work of Laitinen, Morimoto, Oliver and Pawa\\l{}owski, a\nfinite group $G$ has a smooth effective one fixed point action on some sphere\nif and only if $G$ is an Oliver group. For some finite Oliver groups $G$ of\norder up to $216$, and for $G=A_5\\times C_n$ for $n=3,5,7$, we present a\nstrategy of excluding of smooth effective one fixed point $G$-actions on\nlow-dimensional spheres.\n",
        "  Despite successful applications across a broad range of NLP tasks,\nconditional random fields (\"CRFs\"), in particular the linear-chain variant, are\nonly able to model local features. While this has important benefits in terms\nof inference tractability, it limits the ability of the model to capture\nlong-range dependencies between items. Attempts to extend CRFs to capture\nlong-range dependencies have largely come at the cost of computational\ncomplexity and approximate inference. In this work, we propose an extension to\nCRFs by integrating external memory, taking inspiration from memory networks,\nthereby allowing CRFs to incorporate information far beyond neighbouring steps.\nExperiments across two tasks show substantial improvements over strong CRF and\nLSTM baselines.\n",
        "  In the second paper of the series, we have modeled low frequency carbon radio\nrecombination lines (CRRL) from the interstellar medium. Anticipating the LOw\nFrequency ARray (LOFAR) survey of Galactic CRRLs, we focus our study on the\nphysical conditions of the diffuse cold neutral medium (CNM). We have used the\nimproved departure coefficients computed in the first paper of the series to\ncalculate line-to-continuum ratios. The results show that the line width and\nintegrated optical depths of CRRL are sensitive probes of the electron density,\ngas temperature, and the emission measure of the cloud. Furthermore, the ratio\nof CRRL to the [CII] at 158 $\\mu$m line is a strong function of the temperature\nand density of diffuse clouds. Guided by our calculations, we analyze CRRL\nobservations and illustrate their use with data from the literature.\n",
        "  The classic imaging geometry for computed tomography is for collection of\nun-truncated projections and reconstruction of a global image, with the Fourier\ntransform as the theoretical foundation that is intrinsically non-local.\nRecently, interior tomography research has led to theoretically exact\nrelationships between localities in the projection and image spaces and\npractically promising reconstruction algorithms. Initially, interior tomography\nwas developed for x-ray computed tomography. Then, it has been elevated as a\ngeneral imaging principle. Finally, a novel framework known as omni-tomography\nis being developed for grand fusion of multiple imaging modalities, allowing\ntomographic synchrony of diversified features.\n",
        "  Josephson junctions containing ferromagnetic layers are of considerable\ninterest for the development of practical cryogenic memory and superconducting\nqubits. Such junctions exhibit a phase shift of $\\pi$ for certain ranges of\nferromagnetic layer thickness. We present studies of Nb based micron-scale\nelliptically-shaped Josephson junctions containing ferromagnetic barriers of\nNi$_{81}$Fe$_{19}$ or Ni$_{65}$Co$_{20}$Fe$_{15}$. By applying an external\nmagnetic field, the critical current of the junctions are found to follow\ncharacteristic Fraunhofer patterns, and display sharp switching behavior\nsuggestive of single-domain magnets. The high quality of the Fraunhofer\npatterns enables us to extract the maximum value of the critical current even\nwhen the peak is shifted significantly outside the range of the data due to the\nmagnetic moment of the ferromagnetic layer. The maximum value of the critical\ncurrent oscillates as a function of the ferromagnetic barrier thickness,\nindicating transitions in the phase difference across the junction between\nvalues of zero and $\\pi$. We compare the data to previous work and to models of\nthe 0-$\\pi$ transitions based on existing theories.\n",
        "  There has been a large number of reported cases of the occurrence of Zika in\ndifferent countries in 2016 and it is necessary to develop an early warning\nsystem to initiate preventive campaigns against the disease. A potential early\nwarning system based on the rise in ocean temperature of the Pacific Ni\\~no\nIndex is proposed. The efficacy is verified using data for the outbreak in\nColombia as obtained from Google Trends.\n",
        "  We present a study of the X-ray properties of a sample of six nearby\nlate-type spiral galaxies based on XMM-Newton observations. Since our primary\nfocus is on the linkage between X-ray emission and star formation in extended,\nextranuclear galactic disks, we have selected galaxies with near face-on aspect\nand sufficient angular extent so as to be readily amenable to investigation\nwith the moderate spatial resolution afforded by XMM-Newton. After excluding\nregions in each galaxy dominated by bright point sources, we study both the\nmorphology and spectral properties of the residual X-ray emission, comprised of\nboth diffuse emission and the integrated signal of the fainter discrete source\npopulations. The soft X-ray morphology generally traces the inner spiral arms\nand shows a strong correlation with the distribution of UV light, indicative of\na close connection between the X-ray emission and recent star formation. The\nsoft (0.3-2 keV) X-ray luminosity to star formation rate (SFR) ratio varies\nfrom 1-5 x 10^39 erg/s(/Msun/yr), with an indication that the lower range of\nthis ratio relates to regions of lower SFR density. The X-ray spectra are well\nmatched by a two-temperature thermal model with derived temperatures of\ntypically ~0.2 keV and ~0.65 keV, in line with published results for other\nnormal and star-forming galaxies. The hot component contributes a higher\nfraction of the soft luminosity in the galaxies with highest X-ray/SFR ratio,\nsuggesting a link between plasma temperature and X-ray production efficiency.\nThe physical properties of the gas present in the galactic disks are consistent\nwith a clumpy thin-disk distribution, presumably composed of diffuse structures\nsuch as superbubbles together with the integrated emission of unresolved\ndiscrete sources including young supernova remnants.\n",
        "  We have re-examined the temperature-doping (T-p) phase diagram of YBa2Cu3Oy\n(YBCO) to address several issues therein by using the temperature derivative of\nin-plane resistivity, d\\{rho}ab/dT. For p less than about 0.15, a temperature\nTf has been defined to mark the onset of an upturn in d\\{rho}ab/dT at T>Tc,\nwhich, in light of prior studies on another cuprate La2-xSrxCuO4, is attributed\nto the onset of superconducting fluctuations in normal state. The Tf exhibits a\ndoping dependence similar to Tc, and the interval between Tf and Tc is about\n5-30 K across the underdoped regime, showing agreement with a variety of other\nmeasurements to probe a restricted temperature range of superconducting\nfluctuations. Above Tf, the d\\{rho}ab/dT increases linearly as T increases up\nto a value denoted as T2, which is about half the T* and falls roughly in\nparallel with T* as p rises up to about 0.13, indicating a prominent\nT^2-dependent \\{rho}ab in this T-p region. The d\\{rho}ab/dT helps reveal that,\nat Tf<T<T2, the \\{rho}ab also involves an insulating-like component for p<0.08,\nor a T-linear component for p>0.10-0.11, thereby leaving a narrow window of\ndoping for \\{rho}ab to show a pure T^2 dependence. As T increases further, the\nd\\{rho}ab/dT reaches a local maximum at a temperature T_R, signifying the known\ncurvature change (inflection point) in \\{rho}ab. With the derivatives, it is\nillustrated that, in the vicinity of T_R, the in-plane Hall coefficient R_H and\nthermopower Sab of YBCO display curvature changes as well, suggesting a\ncorrelation of the three transport properties. It is also found that the\ndSab/dT shows the onset of an upturn at a temperature coinciding with the Tf,\nwhich, backing the identification of Tf with the onset of superconducting\nfluctuations, demonstrates further the virtue of using the temperature\nderivative to unveil information helpful for the study of high-Tc cuprates.\n",
        "  We exhibit an infinite family of knots in the Poincare homology sphere with\ntunnel number 2 that have a lens space surgery. Notably, these knots are not\ndoubly primitive and provide counterexamples to a few conjectures. In the\nappendix, it is shown that hyperbolic knots in the Poincare homology sphere\nwith a lens space surgery has either no symmetries or just a single strong\ninvolution.\n",
        "  Vector space models have become popular in distributional semantics, despite\nthe challenges they face in capturing various semantic phenomena. We propose a\nnovel probabilistic framework which draws on both formal semantics and recent\nadvances in machine learning. In particular, we separate predicates from the\nentities they refer to, allowing us to perform Bayesian inference based on\nlogical forms. We describe an implementation of this framework using a\ncombination of Restricted Boltzmann Machines and feedforward neural networks.\nFinally, we demonstrate the feasibility of this approach by training it on a\nparsed corpus and evaluating it on established similarity datasets.\n",
        "  A well-known diagnostic imaging modality, termed ultrasound tomography, was\nquickly developed for the detection of very small tumors whose sizes are\nsmaller than the wavelength of the incident pressure wave without ionizing\nradiation, compared to the current gold-standard X-ray mammography. Based on\ninverse scattering technique, ultrasound tomography uses some material\nproperties such as sound contrast or attenuation to detect small targets. The\nDistorted Born Iterative Method (DBIM) based on first-order Born approximation\nis an efficient diffraction tomography approach. Compressed Sensing (CS)\ntechnique was applied to the detection geometry configuration of ultrasound\ntomography as a powerful tool for improved image reconstruction quality.\nHowever, this configuration is very difficult to implement in practice.\nInspired of easier hardware implementation of deterministic CS, in this paper,\nwe propose the chaos measurements in the detection geometry configuration and\nthe image reconstruction process is implemented using L1 regularization. The\nsimulation results of the proposed method have demonstrated the high\nperformance of the proposed approach, the normalized error is approximately 90%\nreduced, compared to the conventional approach. Furthermore, with the same\nquality, we can save half of number of measurements and only use two iterations\nwhen using the proposed method.\n",
        "  We report a study of the magnetic and electronic properties of the\nnon-centrosymmetric half-Heusler antiferromagnet HoPdBi ($T_N = 2.0$ K).\nMagnetotransport measurements show HoPdBi has a semimetallic behaviour with a\ncarrier concentration $n=3.7 \\times 10^{18}$ cm$^{-3}$ extracted from the\nShubnikov-de Haas effect. The magnetic phase diagram in the field-temperature\nplane has been determined by transport, magnetization and thermal expansion\nmeasurements: magnetic order is suppressed at $B_M\\sim 3.6$ T for $T\n\\rightarrow 0$. Superconductivity with $T_c \\sim 1.9$ K is found in the\nantiferromagnetic phase. Ac-susceptibility measurements provide solid evidence\nfor bulk superconductivity below $T_c = 0.75$ K with a screening signal close\nto a volume fraction of 100 %. The upper critical field shows an unusual linear\ntemperature variation with $B_{c2}(T \\rightarrow 0) = 1.1$ T. We also report\nelectronic structure calculations that classify HoPdBi as a new topological\nsemimetal, with a non-trivial band inversion of 0.25 eV.\n",
        "  In this paper, we propose a novel neural approach for paraphrase generation.\nConventional para- phrase generation methods either leverage hand-written rules\nand thesauri-based alignments, or use statistical machine learning principles.\nTo the best of our knowledge, this work is the first to explore deep learning\nmodels for paraphrase generation. Our primary contribution is a stacked\nresidual LSTM network, where we add residual connections between LSTM layers.\nThis allows for efficient training of deep LSTMs. We evaluate our model and\nother state-of-the-art deep learning models on three different datasets: PPDB,\nWikiAnswers and MSCOCO. Evaluation results demonstrate that our model\noutperforms sequence to sequence, attention-based and bi- directional LSTM\nmodels on BLEU, METEOR, TER and an embedding-based sentence similarity metric.\n",
        "  The Kinoshita graph is the most famous example of a Brunnian theta graph, a\nnontrivial spatial theta graph with the property that removing any edge yields\nan unknot. We produce a new family of diagrams of spatial theta graphs with the\nproperty that removing any edge results in the unknot. The family is\nparameterized by a certain subgroup of the pure braid group on four strands. We\nprove that infinitely many of these diagrams give rise to non-isotopic Brunnian\ntheta graphs.\n",
        "  Thin film junctions and bilayers of the doped topological insulator $\\rm\nBi_2Se_3$ and the s-wave superconductor NbN were found to exhibit conductance\nspectra with prominent zero bias and coherence peaks. Various tunneling models\nwith different pair potentials have failed to fit our data, except for the\ntriplet $p_x+ip_y$ pair potential, which breaks time reversal symmetry, but\nyielded reasonably good fits. This provides evidence for proximity induced\ntriplet superconductivity in the $\\rm Bi_2Se_3$ layer near the interface with\nthe NbN film.\n",
        "  This is the documentation of the tomographic X-ray data of emoji phantom made\navailable at https://zenodo.org/record/1183532#.WpA35Y5rIy1. The data can be\nfreely used for scientific purposes with appropriate references to the data and\nto this document in arxiv.org. The data set consists of (1) the X-ray sinogram\nof a single 2D slice of 33 emoji faces (contains 15 different emoji faces) made\nby small squared ceramic stones and (2) the corresponding static and dynamic\nmeasurement matrices modeling the linear operation of the X-ray transform. Each\nof these sinograms was obtained from a measured 60-projection fan-beam sinogram\nby down-sampling and taking logarithms. The original (measured) sinogram is\nalso provided in its original form and resolution. The original (measured)\nsinogram is also provided in its original form and resolution.\n",
        "  Although the concepts of age, survival and transit time have been widely used\nin many fields, including population dynamics, chemical engineering, and\nhydrology, a comprehensive mathematical framework is still missing. Here we\ndiscuss several relationships among these quantities by starting from the\nevolution equation for the joint distribution of age and survival, from which\nthe equations for age and survival time readily follow. It also becomes\napparent how the statistical dependence between age and survival is directly\nrelated to either the age-dependence of the loss function or the survival-time\ndependence of the input function. The solution of the joint distribution\nequation also allows us to obtain the relationships between the age at exit (or\ndeath) and the survival time at input (or birth), as well as to stress the\nsymmetries of the various distributions under time reversal. The transit time\nis then obtained as a sum of the age and survival time, and its properties are\ndiscussed along with the general relationships between their mean values. The\nspecial case of steady state case is analyzed in detail. Some examples,\ninspired by hydrologic applications, are presented to illustrate the theory\nwith the specific results.\n",
        "  Magneto-optical imaging studies on a high-quality Bi2Sr2CaCu2O8 single\ncrystal partially patterned with a triangular array of holes reveal enhanced\nflux shielding in the patterned region of the sample. By mapping local magnetic\nfield and shielding current density distributions at different applied magnetic\nfields and temperatures we determine the regime where pinning from the\npatterned holes dominates over the intrinsic pinning in the sample. In this\nregime, the flux density near the center of the patterned region is observed to\nincrease when the applied field is varied from below the matching field to just\nabove it, while significant magnetic field gradients are sustained in the\npatterned region. Our measurements indicate heterogeneous pinning properties of\nthe vortex population, exhibiting signatures of both weak and strong pinning,\nin the nanopatterned region of the superconductor.\n",
        "  Recently, we investigated the geographical origins of Ashkenazic Jews (AJs)\nand their native language Yiddish by applying a biogeographical tool, the\nGeographic Population Structure (GPS), to a cohort of 367 exclusively\nYiddish-speaking and multilingual AJs genotyped on the Genochip microarray. GPS\nlocalized most AJs along major ancient trade routes in northeastern Turkey\nadjacent to primeval villages with names that may be derived from the word\n\"Ashkenaz.\" These findings were compatible with the hypothesis of an\nIrano-Turko-Slavic origin for AJs and a Slavic origin for Yiddish and at odds\nwith the Rhineland hypothesis advocating a German origin of both. Our approach\nhas been recently adopted by Flegontov et al. (2016a) to trace the origin of\nthe Siberian Ket people and their language. Recently, Flegontov et al. (2016b)\nhave raised several questions concerning the accuracy of the Genochip\nmicroarray and GPS, specifically in relation to AJs and Yiddish. Although many\nof these issues have been addressed in our previous papers, we take this\nopportunity to clarify the principles of the GPS approach, review the recent\nbiogeographical and ancient DNA findings regarding AJs, and comment on the\norigin of Yiddish.\n",
        "  It is believed that turbulence may have a significant impact on star\nformation and the dynamics and evolution of the molecular clouds in which this\noccurs. It is also known that non-ideal magnetohydrodynamic effects influence\nthe nature of this turbulence. We present the results of a numerical study of\n4-fluid MHD turbulence in which the dynamics of electrons, ions, charged dust\ngrains and neutrals and their interactions are followed. The parameters\ndescribing the fluid being simulated are based directly on observations of\nmolecular clouds. We find that the velocity and magnetic field power spectra\nare strongly influenced by multifluid effects on length-scales at least as\nlarge as 0.05 pc. The PDFs of the various species in the system are all found\nto be close to log-normal, with charged species having a slightly less\nplatykurtic (flattened) distribution than the neutrals. We find that the\nintroduction of multifluid effects does not significantly alter the structure\nfunctions of the centroid velocity increment.\n",
        "  Behavioural differences may arise in the absence of genetic or environmental\nvariation. Chaotic dynamics may influence behavioural development, and so this\namong-individual variation. We discuss methods and experimental designs to test\nthis idea. Ultimately, nonlinear and chaotic behavioural development may\nexplain much of natural variation.\n",
        "  First-order factoid question answering assumes that the question can be\nanswered by a single fact in a knowledge base (KB). While this does not seem\nlike a challenging task, many recent attempts that apply either complex\nlinguistic reasoning or deep neural networks achieve 65%-76% accuracy on\nbenchmark sets. Our approach formulates the task as two machine learning\nproblems: detecting the entities in the question, and classifying the question\nas one of the relation types in the KB. We train a recurrent neural network to\nsolve each problem. On the SimpleQuestions dataset, our approach yields\nsubstantial improvements over previously published results --- even neural\nnetworks based on much more complex architectures. The simplicity of our\napproach also has practical advantages, such as efficiency and modularity, that\nare valuable especially in an industry setting. In fact, we present a\npreliminary analysis of the performance of our model on real queries from\nComcast's X1 entertainment platform with millions of users every day.\n",
        "  Recently, neural network approaches for parsing have largely automated the\ncombination of individual features, but still rely on (often a larger number\nof) atomic features created from human linguistic intuition, and potentially\nomitting important global context. To further reduce feature engineering to the\nbare minimum, we use bi-directional LSTM sentence representations to model a\nparser state with only three sentence positions, which automatically identifies\nimportant aspects of the entire sentence. This model achieves state-of-the-art\nresults among greedy dependency parsers for English. We also introduce a novel\ntransition system for constituency parsing which does not require binarization,\nand together with the above architecture, achieves state-of-the-art results\namong greedy parsers for both English and Chinese.\n",
        "  In order to establish the doping dependence of the critical current\nproperties in the iron-based superconductors, the in-plane critical current\ndensity (Jc) of BaFe2As2-based superconductors, Ba1-xKxFe2As2 (K-Ba122),\nBa(Fe1-xCox)2As2 (Co-Ba122), and BaFe2(As1-xPx)2 (P-Ba122) in a wide range of\ndoping concentration (x) was investigated by means of magnetization hysteresis\nloop (MHL) measurements on single-crystal samples. Depending on the dopant\nelements and their concentration, Jc exhibits a variety of magnetic-field (H)\nand temperature (T) dependences. (1) In the case of K-Ba122, MHL of the\nunderdoped samples (x < 0.33) exhibits the second magnetization peak (SMP),\nwhich sustains high Jc at high H and high T, exceeding 10^5 A/cm2 at T = 25 K\nand H = 6 T for x = 0.30. On the other hand, SMP is missing in the optimally (x\n~ 0.36-0.40) and overdoped (x ~ 0.50) samples, and consequently Jc rapidly\ndecreases by more than one order of magnitude, although the change in Tc is\nwithin a few K. (2) For Co-Ba122, SMP is always present over the entire\nsuperconducting (SC) dome from the under (x ~ 0.05) to the overdoped (x ~ 0.12)\nregion. However, the magnitude of Jc significantly changes with x, exhibiting a\nsharp maximum at x ~ 0.057, which is a slightly underdoped composition for\nCo-Ba122. (3) For P-Ba122, the highest Jc is attained at x = 0.30 corresponding\nto the highest Tc composition. For the overdoped samples, MHL is characterized\nby SMP located close to the irreversibility field. Common to the three doping\nvariations, Jc becomes highest at the under-doping side of SC dome near the\nphase boundary between SC phase and the antiferromagnetic/orthorhombic phase.\nAlso, the peak appears in a narrow range of doping, distinct from the Tc dome\nwith broad maximum. These similarities in the three cases indicate that the\nobserved doping dependence of Jc is intrinsic to the BaFe2As2-based\nsuperconductors.\n",
        "  This paper presents the current achievements of a long term project aiming at\npredicting and assessing the impact of tongue and mouth floor surgery on tongue\nmobility. The ultimate objective of this project is the design of a software\nwith which surgeons should be able (1) to design a 3D biomechanical model of\nthe tongue and of the mouth floor that matches the anatomical characteristics\nof each patient specific oral cavity, (2) to simulate the anatomical changes\ninduced by the surgery and the possible reconstruction, and (3) to\nquantitatively predict and assess the consequences of these anatomical changes\non tongue mobility and speech production after surgery.\n",
        "  Superconductor/ferromagnet/superconductor heterostructures exhibit a\nso-called long-range proximity effect provided some layers of conical magnet\nHolmium are included in the respective interface regions. The Ho layers lead to\na spin-flip process at the interface generating equal-spin spin-triplet pairing\ncorrelations in the ferromagnet. These equal-spin spin-triplet pairing\ncorrelations penetrate much further into the heterostructure compared to the\nspin-singlet and unequal-spin spin-triplet correlations which occur in the\nabsence of Ho. Here we present calculations of this effect based on the\nspin-dependent microscopic Bogoliubov-de Gennes equations solved within a\ntight-binding model in the clean limit. The influence of the ferromagnet and\nconical magnet layer thickness on the induced equal-spin spin-triplet pairing\ncorrelations is obtained and compared to available experimental data. It is\nshown that, in agreement with experiment, a critical minimum thickness of\nconical magnet layers has to be present in order to observe a sizeable amount\nof equal-spin spin-triplet pairing correlations.\n",
        "  This review discusses the structure and evolution of the Milky Way, in the\ncontext of opportunities provided by asteroseismology of red giants. The review\nis structured according to the main Galactic components: the thin disk, thick\ndisk, stellar halo, and the Galactic bar/bulge. The review concludes with an\noverview of Galactic archaeology and chemical tagging, and a brief account of\nthe upcoming HERMES survey with the AAT.\n",
        "  Optoacoustic tomography (OAT), also known as photoacoustic tomography, is a\nrapidly emerging hybrid imaging technique that possesses great potential for a\nwide range of biomedical imaging applications. In OAT, a laser is employed to\nilluminate the tissue of interest and acoustic signals are produced via the\nphotoacoustic effect. From these data, an estimate of the distribution of the\nabsorbed optical energy density within the tissue is reconstructed, referred to\nas the object function. This quantity is defined, in part, by the distribution\nof light fluence within the tissue that is established by the laser source.\nWhen performing three-dimensional imaging of large objects, such as a female\nhuman breast, it can be difficult to achieve a relatively uniform coverage of\nlight fluence within the volume of interest when the position of the laser\nsource is fixed. To circumvent this, researchers have proposed illumination\nschemes in which the relative position of the laser source and ultrasound probe\nis fixed, and both are rotated together to acquire a tomographic data set. A\nproblem with this rotating-illumination scheme is that the tomographic data are\ninconsistent; namely, the acoustic data recorded at each tomographic view angle\n(i.e., probe position) are produced by a distinct object function. In this\nwork, the impact of this data inconsistency on image reconstruction accuracy is\ninvestigated systematically. This is accomplished by use of computer-simulation\nstudies and application of mathematical results from the theory of microlocal\nanalysis. These studies specify the set of image discontinuities that can be\nstably reconstructed with a non-stationary optical illumination set-up. The\nstudy also includes a comparison of the ability of iterative and analytic image\nreconstruction methods to mitigate artifacts attributable to the data\ninconsistency.\n",
        "  Resistivity, Hall effect and magnetization have been investigated on the new\nsuperconductor Bi4O4S3. A weak insulating behavior has been induced in the\nnormal state when the superconductivity is suppressed. Hall effect measurements\nillustrate clearly a multiband feature dominated by electron charge carriers,\nwhich is further supported by the magnetoresistance data. Interestingly, a kink\nappears on the temperature dependence of resistivity at about 4 K at all high\nmagnetic fields when the bulk superconductivity is completely suppressed. This\nkink can be well traced back to the upper critical field Hc2(T) in the low\nfield region, and is explained as the possible evidence of residual Cooper\npairs on the one dimensional chains.\n",
        "  In a treatment plan optimization problem for radiotherapy, a clinically\nacceptable plan is usually generated by an optimization process with weighting\nfactors or reference doses adjusted for organs. Recent discoveries indicate\nthat adjusting parameters associated with each voxel may lead to better plan\nquality. However, it is still unclear regarding the mathematical reasons behind\nit. To answer questions related to this problem, we establish in this work a\nnew mathematical framework equipped with two theorems. The new framework\nclarifies the different consequences of adjusting organ-dependent and\nvoxel-dependent parameters for the treatment plan optimization of radiation\ntherapy, as well as the different effects of adjusting weighting factors versus\nreference doses in the optimization process. The main discoveries are\nthreefold: 1) While in the organ-based model the selection of the objective\nfunction has an impact on the quality of the optimized plans, this is no longer\nan issue for the voxel-based model since the entire Pareto surface could be\ngenerated regardless the specific form of the objective function as long as it\nsatisfies certain mathematical conditions; 2) A larger Pareto surface is\nexplored by adjusting voxel-dependent parameters than by adjusting\norgan-dependent parameters, possibly allowing for the generation of plans with\nbetter trade-offs among different clinical objectives; 3) Adjusting voxel\nweighting factors is preferred to adjusting the voxel reference doses since the\nPareto optimality can be maintained.\n",
        "  In this paper we present an acquisition chain for the measurement of blood\narterial pressure based on the oscillometric method. This method does not\nsuffer from any limitation as the well-known auscultatory method and it is\nsuited for wearable health monitoring systems. The device uses a pressure\nsensor whose signal is filtered, digitalized and analyzed by a microcontroller.\nLocal analysis allows the evaluation of the systolic and diastolic pressure\nvalues which can be used for local alarms, data collection and remote\nmonitoring.\n",
        "  A new critical effect is predicted in population dispersal. It is based on\nthe fact that a trade-off between the advantages of mobility and the cost of\nmobility breaks with a significant deterioration in living conditions. The\nrecently developed model of purposeful kinesis (Gorban \\& \\c{C}abuko\\v{g}lu,\nEcological Complexity 33, 2018) is based on the \"let well enough alone\" idea:\nmobility decreases for high reproduction coefficient and, therefore, animals\nstay longer in good conditions and leave quicker bad conditions. Mobility has a\ncost, which should be measured in the changes of the reproduction coefficient.\nIntroduction of the cost of mobility into the reproduction coefficient leads to\nan equation for mobility. It can be solved in a closed form using Lambert\n$W$-function.\n  Surprisingly, the \"let well enough alone\" models with the simple linear cost\nof mobility have an intrinsic phase transition: when conditions worsen then the\nmobility increases up to some critical value of the reproduction coefficient.\nFor worse conditions, there is no solution for mobility. We interpret this\ncritical effect as the complete loss of mobility that is degeneration of\ndiffusion. Qualitatively, this means that mobility increases with worsening of\nconditions up to some limit, and after that, mobility is nullified.\n",
        "  Microdosimetric single event spectra is a significant parameter in\nradiotherapy, which can be used to evaluate the biological effect of radiation\nfields. This paper simulated microscopic patterns of energy depositions for\nmixed radiation fields which are created by carbon ions with general purpose\nMonte Carlo code FLUKA. . Experimental measured lineal energy spectra produced\nby carbon ions of about 300MeV/u were chosen from literature to compared with\nthe results from simulation one of the measurement set-up. In addition, the\ndose-weighted lineal energy spectra, frequency averaged lineal energy values\nand dose averaged lineal energy values of carbon ions from 120MeV/u to 430MeV/u\nof the measurement set-up were calculated, too. The results of this paper are\nmeaningful for making treatment planning in carbon ion radiotherapy.\n",
        "  Typically used as a tool for Monte Carlo simulation of high energy physics\nexperiments, GEANT4 is increasingly being employed for the simulation of\ncomplex radiotherapy treatments. Often the specification of components within a\nclinical linear accelerator treatment head is provided in a CAD file format.\nDirect import of these CAD files into GEANT4 may not be possible, and complex\ncomponents such as individual leaves within a multi-leaf collimator may be\ndifficult to define via other means. Solutions that allow for users to work\naround the limited support in the GEANT4 toolkit for loading predefined CAD\ngeometries has been presented by others, however these solutions require\nintermediate file format conversion using commercial software. Here within we\ndescribe a technique that allows for CAD models to be directly loaded as\ngeometry without the need for commercial software and intermediate file format\nconversion. Robustness of the interface was tested using a set of CAD models of\nvarious complexity; for the models used in testing, no import errors were\nreported and all geometry was found to be navigable by GEANT4.\n",
        "  With the development of decision systems and specially data warehouses, the\nvisibility of the data warehouse design before its creation has become\nessential, and that because of data warehouse importance as considered as the\nunique data source giving meaning to the decision. In a decision system the\nproper functioning of a data warehouse resides in the smooth running of the\nmiddleware tools ETC step one hand, and the restitution step through the data\nmining, reporting solutions, dashboards... etc other. The large volume of data\nthat passes through these stages require an optimal design for a highly\nefficient decision system, without disregarding the choice of technologies that\nare introduced for the data warehouse implementation such as: database\nmanagement system, the type of server operating systems, physical server\narchitecture (64-bit, for example) that can be a benefit performance of this\nsystem. The designer of the data warehouse should consider the effectiveness of\ndata query, this depends on the selection of relevant indexes and their\ncombination with the materialized views, note that the index selection is a\nNPcomplete problem, because the number of indexes is exponential in the total\nnumber of attributes in the database, So, it is necessary to provide, while the\ndata warehouse design, the suitable type of index for this data warehouse. This\npaper presents a comparative study between the index B-tree type and type\nBitmap, their advantages and disadvantages, with a real experiment showing that\nits index of type Bitmap more advantageous than the index B-tree type.\n",
        "  Adaptation of asexual populations is driven by beneficial mutations and\ntherefore the dynamics of this process, besides other factors, depend on the\ndistribution of beneficial fitness effects. It is known that on uncorrelated\nfitness landscapes, this distribution can only be of three types: truncated,\nexponential and power law. We performed extensive stochastic simulations to\nstudy the adaptation dynamics on rugged fitness landscapes, and identified two\nquantities that can be used to distinguish the underlying distribution of\nbeneficial fitness effects. The first quantity studied here is the fitness\ndifference between successive mutations that spread in the population, which is\nfound to decrease in the case of truncated distributions, remain nearly a\nconstant for exponentially decaying distributions and increase when the fitness\ndistribution decays as a power law. The second quantity of interest, namely,\nthe rate of change of fitness with time also shows quantitatively different\nbehaviour for different beneficial fitness distributions. The patterns\ndisplayed by the two aforementioned quantities are found to hold for both low\nand high mutation rates. We discuss how these patterns can be exploited to\ndetermine the distribution of beneficial fitness effects in microbial\nexperiments.\n",
        "  This report summarizes work done during PhD rotation that proposes an\nalgorithm that uses anode- cathode trigger time difference and cathode to anode\npulse height ratio to resolve photon event positioning degeneracy in a novel\nCadmium Zinc Telluride (CZT) photon detector for positron emission tomography\n(PET). The detectors used comprise 40 mm x 40 mm x 5 mm monolithic CZT crystals\nthat employ a cross-strip electrode pattern with interspersed steering\nelectrodes to obtain high spatial and energy resolution. The specific\npositioning degeneracy studied results when a photon event interacts in two\ndetector pixels, triggering two anode and two cathode strips, hence presenting\nfour pixels as potential interaction locations. An experiment was performed\nwhere a 1 mm diameter collimated pencil beam of annihilation photons was used\nto target a detector pixel of known location. The data acquired provided an\nempirical basis to train and test the algorithm. The study shows that,\ndepending on the restrictions applied on the data set, the proposed algorithm\naccurately identifies the true photon interaction locations 59.7% to 66.7% of\nthe time, which is 19.4% to 33.4% better than an unbiased Bernoulli trial.\nFurther improvements of the algorithm will enhance the effective system photon\nsensitivity by extracting useful information from events which would otherwise\nbe discarded.\n",
        "  Frame stacking is broadly applied in end-to-end neural network training like\nconnectionist temporal classification (CTC), and it leads to more accurate\nmodels and faster decoding. However, it is not well-suited to conventional\nneural network based on context-dependent state acoustic model, if the decoder\nis unchanged. In this paper, we propose a novel frame retaining method which is\napplied in decoding. The system which combined frame retaining with frame\nstacking could reduces the time consumption of both training and decoding. Long\nshort-term memory (LSTM) recurrent neural networks (RNNs) using it achieve\nalmost linear training speedup and reduces relative 41\\% real time factor\n(RTF). At the same time, recognition performance is no degradation or improves\nsightly on Shenma voice search dataset in Mandarin.\n",
        "  Drawing from the LEGA-C dataset, we present the spectroscopic view of the\nstellar population across a large volume- and mass-selected sample of galaxies\nat large lookback time. We measure the 4000\\AA\\ break (D$_n$4000) and Balmer\nabsorption line strengths (probed by H$\\delta$) from 1019 high-quality spectra\nof $z=0.6 - 1.0$ galaxies with $M_\\ast = 2 \\times 10^{10} M_\\odot - 3 \\times\n10^{11} M_\\odot$. Our analysis serves as a first illustration of the power of\nhigh-resolution, high-S/N continuum spectroscopy at intermediate redshifts as a\nqualitatively new tool to constrain galaxy formation models. The observed\nD$_n$4000-EW(H$\\delta$) distribution of our sample overlaps with the\ndistribution traced by present-day galaxies, but $z\\sim 0.8$ galaxies populate\nthat locus in a fundamentally different manner. While old galaxies dominate the\npresent-day population at all stellar masses $> 2\\times10^{10} M_\\odot$, we see\na bimodal D$_n$4000-EW(H$\\delta$) distribution at $z\\sim0.8$, implying a\nbimodal light-weighted age distribution. The light-weighted age depends\nstrongly on stellar mass, with the most massive galaxies\n$>1\\times10^{11}M_\\odot$ being almost all older than 2 Gyr. At the same time we\nestimate that galaxies in this high mass range are only $\\sim3$ Gyr younger\nthan their $z\\sim0.1$ counterparts, at odd with pure passive evolution given a\ndifference in lookback time of $>5$ Gyr; younger galaxies must grow to\n$>10^{11}M_\\odot$ in the meantime, and/or small amounts of young stars must\nkeep the light-weighted ages young. Star-forming galaxies at $z\\sim0.8$ have\nstronger H$\\delta$ absorption than present-day galaxies with the same\nD$_n$4000, implying larger short-term variations in star-formation activity.\n",
        "  Extracting the relevant information by exploiting the spatial data warehouse\nbecomes increasingly hard. In fact, because of the enormous amount of data\nstored in the spatial data warehouse, the user, usually, don't know what part\nof the cube contain the relevant information and what the forthcoming query\nshould be. As a solution, we propose to study the similarity between the\nbehaviors of the users, in term of the spatial MDX queries launched on the\nsystem, as a basis to recommend the next relevant MDX query to the current\nuser. This paper introduces a new similarity measure for comparing spatial MDX\nqueries. The proposed similarity measure could directly support the development\nof spatial personalization approaches. The proposed similarity measure takes\ninto account the basic components of the similarity assessment models: the\ntopology, the direction and the distance.\n",
        "  A single-phase high pressure (HP) modification of CsxFe2-ySe2 was synthesized\nat 11.8 GPa at ambient temperature. Structurally this polymorph is similar to\nthe minor low pressure (LP) superconducting phase, namely they both crystallize\nin a ThCr2Si2-type structure without ordering of the Fe vacancies within the\nFe-deficient FeSe4 layers. The HP CsxFe2-ySe2 polymorph is found to be less\ncrystalline and nearly twice as soft compared to the parent major and minor\nphases of CsxFe2-ySe2. It can be quenched to low pressures and is stable at\nleast on the scale of weeks. At ambient pressure the HP polymorph of\nCsxFe2-ySe2 is expected to exhibit different superconducting properties\ncompared to its LP minor phase (Tc = 27 K).\n",
        "  This paper presents the evaluation of the architecture of healthcare data\nwarehouse specific to cancer diseases. This data warehouse containing relevant\ncancer medical information and patient data. The data warehouse provides the\nsource for all current and historical health data to help executive manager and\ndoctors to improve the decision making process for cancer patients. The\nevaluation model based on Bill Inmon's definition of data warehouse is proposed\nto evaluate the Cancer data warehouse.\n",
        "  We present a new algorithm to model and investigate the learning process of a\nlearner mastering a set of grammatical rules from an inconsistent source. The\ncompelling interest of human language acquisition is that the learning succeeds\nin virtually every case, despite the fact that the input data are formally\ninadequate to explain the success of learning. Our model explains how a learner\ncan successfully learn from or even surpass its imperfect source without\npossessing any additional biases or constraints about the types of patterns\nthat exist in the language. We use the data collected by Singleton and Newport\n(2004) on the performance of a 7-year boy Simon, who mastered the American Sign\nLanguage (ASL) by learning it from his parents, both of whom were imperfect\nspeakers of ASL. We show that the algorithm possesses a frequency-boosting\nproperty, whereby the frequency of the most common form of the source is\nincreased by the learner. We also explain several key features of Simon's ASL.\n",
        "  Researchers and developers use benchmarks to compare their algorithms and\nproducts. A database benchmark must have a dataset D. To be\napplication-specific, this dataset D should be empirical. However, D may be too\nsmall, or too large, for the benchmarking experiments. D must, therefore, be\nscaled to the desired size.\n  To ensure the scaled D' is similar to D, previous work typically specifies or\nextracts a fixed set of features F = {F_1, F_2, . . . , F_n} from D, then uses\nF to generate synthetic data for D'. However, this approach (D -> F -> D')\nbecomes increasingly intractable as F gets larger, so a new solution is\nnecessary.\n  Different from existing approaches, this paper proposes ASPECT to scale D to\nenforce similarity. ASPECT first uses a size-scaler (S0) to scale D to D'. Then\nthe user selects a set of desired features F'_1, . . . , F'_n. For each desired\nfeature F'_k, there is a tweaking tool T_k that tweaks D' to make sure D' has\nthe required feature F'_k. ASPECT coordinates the tweaking of T_1,...,T_n to\nD', so T_n(...(T_1(D'))...) has the required features F'_1,...,F'_n.\n  By shifting from D -> F -> D' to D -> D' -> F', data scaling becomes\nflexible. The user can customise the scaled dataset with their own interested\nfeatures. Extensive experiments on real datasets show that ASPECT can enforce\nsimilarity in the dataset effectively and efficiently.\n",
        "  We present the strongest known knot invariant that can be computed\neffectively (in polynomial time).\n",
        "  This paper focuses on minimizing the time requirement for CT capture through\ninnovative simultaneous x-ray capture method. The state-of-the-art CT imaging\nmethodology captures a sequence of projections during which the internal organ\nmovements may lead to poor reconstruction due to motion artefacts. Traditional\nCT scanners' minimize such effect by taking more projections than necessary. In\nthis work we focus on an innovative CT capture method that captures projections\nsimultaneously, promising super fast scans along with possible radiation dose\nreductions. While the simultaneous CT capture model has already been proposed\nin our earlier work 'Multi-axial CT Reconstruction from Few View Projections'\n(in SPIE Optical Engineering and Applications, pp. 85000A-85000A. International\nSociety for Optics and Photonics, 2012) and 'A New Imaging Method for Real-time\n3D X-ray Reconstruction' (in SPIE Medical Imaging, pp. 86685G-86685G.\nInternational Society for Optics and Photonics, 2013), in this work we enhance\nthe model through better initialization along with prior smoothing before\nsuccessive iterations of the iterative algorithms. We also elaborate the model\nconsidering different X-ray source/detector configurations. Results show that\nit is possible reconstruct a cross-section slice by considering only four\nangular projections. With eight projections, the reconstruction is further\nimproved. The main promising matter about this method is that, all these\nprojections (i.e. four or eight) can in principle be captured simultaneously,\nimplying CT capture in one go just like a chest X-ray.\n",
        "  This paper investigates the dynamics of Ebola virus transmission in West\nAfrica during 2014. The reproduction numbers for the total period of epidemic\nand for different consequent time intervals are estimated based on a newly\nsuggested linear model. It contains one major variable - the average time of\ninfectiousness (time from onset to hospitalization) that is considered as a\nparameter for controlling the future dynamics of epidemics.\n  Numerical implementations are carried out on data collected from three\ncountries Guinea, Sierra Leone and Liberia as well as the total data collected\nworldwide. Predictions are provided by considering different scenarios\ninvolving the average times of infectiousness for the next few months and the\nend of the current epidemic is estimated according to each scenario.\n",
        "  To compare the dosimetrical differences between plans generated by helical\ntomotherapy using 2D or 3D margining technique in in prostate cancer. Ten\nprostate cancer patients were included in this study. For 2D plans, planning\ntarget volume (PTV) was created by adding 5 mm (lateral/anterior-posterior) to\nclinical target volume (CTV). For 3D plans, 5 mm margin was added not only in\nlateral/anterior-posterior, but also in superior-inferior to CTV. Various\ndosimetrical indices, including the prescription isodose to target volume\n(PITV) ratio, conformity index (CI), homogeneity index (HI), target coverage\nindex (TCI), modified dose homogeneity index (MHI), conformation number (CN),\ncritical organ scoring index (COSI), and quality factor (QF) were determined to\ncompare the different treatment plans. Differences between 2D and 3D PTV\nindices were not significant except for CI (p = 0.023). 3D margin plans (11195\nMUs) resulted in higher (13.0%) monitor units than 2D margin plans (9728 MUs).\nThere were no significant differences in any OARs between the 2D and 3D plans.\nOverall, the average 2D plan dose was slightly lower than the 3D plan dose.\nCompared to the 2D plan, the 3D plan increased average treatment time by 1.5\nminutes; however, this difference was not statistically significant (p =\n0.082). We confirmed that 2D and 3D margin plans are not significantly\ndifferent with regard to various dosimetric indices such as PITV, CI, and HI\nfor PTV, and OARs with tomotherapy.\n",
        "  Many enterprise environments have databases running on network-attached\nserver-storage infrastructure (referred to as Storage Area Networks or SANs).\nBoth the database and the SAN are complex systems that need their own separate\nadministrative teams. This paper puts forth the vision of an innovative\nmanagement framework to simplify administrative tasks that require an in-depth\nunderstanding of both the database and the SAN. As a concrete instance, we\nconsider the task of diagnosing the slowdown in performance of a database query\nthat is executed multiple times (e.g., in a periodic report-generation\nsetting). This task is very challenging because the space of possible causes\nincludes problems specific to the database, problems specific to the SAN, and\nproblems that arise due to interactions between the two systems. In addition,\nthe monitoring data available from these systems can be noisy.\n  We describe the design of DIADS which is an integrated diagnosis tool for\ndatabase and SAN administrators. DIADS generates and uses a powerful\nabstraction called Annotated Plan Graphs (APGs) that ties together the\nexecution path of queries in the database and the SAN. Using an innovative\nworkflow that combines domain-specific knowledge with machine-learning\ntechniques, DIADS was applied successfully to diagnose query slowdowns caused\nby complex combinations of events across a PostgreSQL database and a production\nSAN.\n",
        "  Urea and creatinine are two important diagnostic components of urine. The\nstudy of creatinine in liquid phase is difficult due to its feeble\nconcentration in urine. To bring down the detection limit, Raman spectroscopy\nof dried urine samples was employed. Raman studies in association with partial\nleast square algorithm of artificial urine samples gave improved results in\ndried phase as compared to liquid phase. These findings were further validated\non real urine samples.\n",
        "  This study explores the creation of a machine learning model to automatically\nidentify whether a Neonatal Intensive Care Unit (NICU) patient was diagnosed\nwith neonatal jaundice during a particular hospitalization based on their\nassociated clinical notes. We develop a number of techniques for text\npreprocessing and feature selection and compare the effectiveness of different\nclassification models. We show that using ensemble decision tree\nclassification, both with AdaBoost and with bagging, outperforms support vector\nmachines (SVM), the current state-of-the-art technique for neonatal jaundice\ncoding.\n",
        "  In this paper we numerically study the behavior of the density power spectrum\nin turbulent thermally bistable flows. We analyze a set of five\nthree-dimensional simulations where turbulence is randomly driven in Fourier\nspace at a fixed wave-number and with different Mach numbers M (with respect to\nthe warm medium) ranging from 0.2 to 4.5. The density power spectrum becomes\nshallower as M increases and the same is true for the column density power\nspectrum. This trend is interpreted as a consequence of the simultaneous\nturbulent compressions, thermal instability\n  generated density fluctuations, and the weakening of thermal pressure force\nin diffuse gas. This behavior is consistent with the fact that observationally\ndetermined spectra exhibit different slopes in different regions. The values of\nthe spectral indexes resulting from our simulations are consistent with\nobservational values. We do also explore the behavior of the velocity power\nspectrum, which becomes steeper as M increases. The spectral index goes from a\nvalue much shallower than the Kolmogorov one for M=0.2 to a value steeper than\nthe Kolmogorov one for M=4.5.\n",
        "  We report the direct imaging of a novel modulated flux striped domain phase\nin a nearly twin-free YBCO crystal. These domains arise from instabilities in\nthe vortex structure within a narrow region of tilted magnetic fields at small\nangles from the in-plane direction. By comparing the experimental and\ntheoretically derived vortex phase diagrams we infer that the stripe domains\nemerge from a first order phase transition of the vortex structure. The size of\ndomains containing vortices of certain orientations is controlled by the\nbalance between the vortex stray field energy and the positive energy of the\ndomain boundaries. Our results confirm the existence of the kinked vortex chain\nphase in an anisotropic high temperature superconductor and reveal a sharp\ntransition in the state of this phase resulting in regular vortex domains.\n",
        "  A significant number of neural architectures for reading comprehension have\nrecently been developed and evaluated on large cloze-style datasets. We present\nexperiments supporting the emergence of \"predication structure\" in the hidden\nstate vectors of these readers. More specifically, we provide evidence that the\nhidden state vectors represent atomic formulas $\\Phi[c]$ where $\\Phi$ is a\nsemantic property (predicate) and $c$ is a constant symbol entity identifier.\n",
        "  Ultrasound localization microscopy offers new radiation-free diagnostic tools\nfor vascular imaging deep within the tissue. Sequential localization of echoes\nreturned from inert microbubbles with low-concentration within the bloodstream\nreveal the vasculature with capillary resolution. Despite its high spatial\nresolution, low microbubble concentrations dictate the acquisition of tens of\nthousands of images, over the course of several seconds to tens of seconds, to\nproduce a single super-resolved image. %since each echo is required to be well\nseparated from adjacent microbubbles. Such long acquisition times and stringent\nconstraints on microbubble concentration are undesirable in many clinical\nscenarios. To address these restrictions, sparsity-based approaches have\nrecently been developed. These methods reduce the total acquisition time\ndramatically, while maintaining good spatial resolution in settings with\nconsiderable microbubble overlap. %Yet, non of the reported methods exploit the\nfact that microbubbles actually flow within the bloodstream. % to improve\nrecovery. Here, we further improve sparsity-based super-resolution ultrasound\nimaging by exploiting the inherent flow of microbubbles and utilize their\nmotion kinematics. While doing so, we also provide quantitative measurements of\nmicrobubble velocities. Our method relies on simultaneous tracking and\nsuper-localization of individual microbubbles in a frame-by-frame manner, and\nas such, may be suitable for real-time implementation. We demonstrate the\neffectiveness of the proposed approach on both simulations and {\\it in-vivo}\ncontrast enhanced human prostate scans, acquired with a clinically approved\nscanner.\n",
        "  Matrix tablets are drug delivery devices designed to release a drug in a\ncontrolled manner over an extended period of time. We develop a cellular\nautomaton (CA) model for the dissolution and release of a water-soluble drug\nand excipient from a matrix tablet of water-insoluble polymer. Cells of the CA\nare occupied by drug, excipient, water or polymer and the CA updating rules\nsimulate the dissolution of drug and excipient and the subsequent diffusion of\nthe dissolved substances. In addition we simulate the possible fracture of\nbrittle drug and excipient powders during the tablet compression and the\nmelting of the polymer during a possible thermal curing process. Different\nstirring mechanisms that facilitate the transport of dissolved drug in the\nfluid in which the tablet is immersed are modeled in the water cells adjacent\nto the boundary of the tablet. We find that our simulations can reproduce\nexperimental drug release profiles. Our simulation tool can be used to\nstreamline the formulation and production of sustained release tablets.\n",
        "  In Big data era, information integration often requires abundant data\nextracted from massive data sources. Due to a large number of data sources,\ndata source selection plays a crucial role in information integration, since it\nis costly and even impossible to access all data sources. Data Source selection\nshould consider both efficiency and effectiveness issues. For efficiency, the\napproach should achieve high performance and be scalability to fit large data\nsource amount. From effectiveness aspect, data quality and overlapping of\nsources are to be considered, since data quality varies much from data sources,\nwith significant differences in the accuracy and coverage of the data provided,\nand the overlapping of sources can even lower the quality of data integrated\nfrom selected data sources.\n  In this paper, we study source selection problem in \\textit{Big Data Era} and\npropose methods which can scale to datasets with up to millions of data sources\nand guarantee the quality of results. Motivated by this, we propose a new\nobject function taking the expected number of true values a source can provide\nas a criteria to evaluate the contribution of a data source. Based on our\nproposed index we present a scalable algorithm and two pruning strategies to\nimprove the efficiency without sacrificing precision. Experimental results on\nboth real world and synthetic data sets show that our methods can select\nsources providing a large proportion of true values efficiently and can scale\nto massive data sources.\n",
        "  Satellite Tracking of People (STOP) tracks thousands of GPS-enabled devices\n24 hours a day and 365 days a year. With locations captured for each device\nevery minute, STOP servers receive tens of millions of points each day. In\naddition to cataloging these points in real-time, STOP must also respond to\nquestions from customers such as, \"What devices of mine were at this location\ntwo months ago?\" They often then broaden their question to one such as, \"Which\nof my devices have ever been at this location?\" The processing requirements\nnecessary to answer these questions while continuing to process inbound data in\nreal-time is non-trivial.\n  To meet this demand, STOP developed Adaptive Partitioning to provide a\ncost-effective and highly available hardware platform for the geographical and\ntime-spatial indexing capabilities necessary for responding to customer data\nrequests while continuing to catalog inbound data in real-time.\n",
        "  (Abridged) Gas and star velocity dispersions have been derived for four\ncircumnuclear star-forming regions (CNSFRs) and the nucleus of the spiral\ngalaxy NGC2903 using high resolution spectroscopy in the blue and far red.\nStellar velocity dispersions have been obtained from the CaII triplet (CaT)\nlines at 8494, 8542, 8662A, using cross-correlation techniques while gas\nvelocity dispersions have been measured by Gaussian fits to the Hbeta line.\n  The CNSFRs, with sizes of about 100 to 150pc in diameter, show a complex\nstructure at the Hubble Space Telescope resolution, with a good number of\nsubclusters with linear diameters between 3 and 8pc. Their stellar velocity\ndispersions range from 39 to 67 km/s. These values, together with the sizes\nmeasured on archival HST images yield upper limits to the dynamical masses for\nthe individual star clusters between 1.8 and 8.7 x 10$^6$ M$_\\odot$ and upper\nlimits to the masses for the whole CNSFR between 4.9 x 10$^6$ and 4.3 x 10$^7$\nM$_\\odot$. ...\n",
        "  We investigate the structural properties of the underlying hosts of 34 blue\ncompact dwarf (BCD) galaxies with deep near-infrared (NIR) photometry. The BCD\nsample is selected from the Cosmic Assembly Near-IR Deep Extragalactic Legacy\nSurvey in the Great observatories origins Deep Survey North and South fields.\nWe extract the surface brightness profile (SBP) in the optical F 435W and NIR F\n160W bands. The SBPs of BCDs in the H band reach 26 mag arcsec^-2 at the\n3\\sigma level, which is so far the deepest NIR imaging of BCDs. Then we fit the\nSBPs with one- and two- component Sersic models. About half of the BCDs favour\nthe two-component model which significantly improves the fit quality. The\neffective radii of the underlying hosts of BCDs in the B band are smaller than\nthose of early-type dwarfs (dEs) and dwarf irregulars at a fixed luminosity.\nThis discrepancy is similar to findings in many previous works. However, the\ndifference in structural parameters between BCDs and other dwarf galaxies seems\nto be less significant in the H band. Furthermore, we find a remarkable\nagreement between the underlying hosts of BCDs and dEs. All dwarf galaxies seem\nto follow a similar luminosity-radius relationship which suggests a unified\nstructural evolution for dwarf galaxies. We conclude that a possible evolution\ntrack from BCDs to dEs cannot be ruled out, with no significant change of\nstructure needed in the evolutionary scenario.\n",
        "  Identifying drivers of complex traits from the noisy signals of genetic\nvariation obtained from high throughput genome sequencing technologies is a\ncentral challenge faced by human geneticists today. We hypothesize that the\nvariants involved in complex diseases are likely to exhibit non-neutral\nevolutionary signatures. Uncovering the evolutionary history of all variants is\ntherefore of intrinsic interest for complex disease research. However, doing so\nnecessitates the simultaneous elucidation of the targets of natural selection\nand population-specific demographic history. Here we characterize the action of\nnatural selection operating across complex disease categories, and use\npopulation genetic simulations to evaluate the expected patterns of genetic\nvariation in large samples. We focus on populations that have experienced\nhistorical bottlenecks followed by explosive growth (consistent with most human\npopulations), and describe the differences between evolutionarily deleterious\nmutations and those that are neutral. Genes associated with several complex\ndisease categories exhibit stronger signatures of purifying selection than\nnon-disease genes. In addition, loci identified through genome-wide association\nstudies of complex traits also exhibit signatures consistent with being in\nregions recurrently targeted by purifying selection. Through simulations, we\nshow that population bottlenecks and rapid growth enables deleterious rare\nvariants to persist at low frequencies just as long as neutral variants, but\nlow frequency and common variants tend to be much younger than neutral\nvariants. This has resulted in a large proportion of modern-day rare alleles\nthat have a deleterious effect on function, and that potentially contribute to\ndisease susceptibility.\n",
        "  Radial metallicity gradients are observed in the disks of the Milky Way and\nin several other spiral galaxies. In the case of the Milky Way, many objects\ncan be used to determine the gradients, such as HII regions, B stars, Cepheids,\nopen clusters and planetary nebulae. Several elements can be studied, such as\noxygen, sulphur, neon, and argon in photoionized nebulae, and iron and other\nelements in cepheids, open clusters and stars. As a consequence, the number of\nobservational characteristics inferred from the study of abundance gradients is\nvery large, so that in the past few years they have become one of the main\nobservational constraints of chemical evolution models. In this paper, we\npresent some recent observational evidences of abundance gradients based on\nseveral classes of objects. We will focus on (i) the magnitude of the\ngradients, (ii) the space variations, and (iii) the evidences of a time\nvariation of the abundance gradients. Some comments on recent theoretical\nmodels are also given, in an effort to highlight their predictions concerning\nabundance gradients and their variations.\n",
        "  Despite of decades of work, query optimizers still make mistakes on\n\"difficult\" queries because of bad cardinality estimates, often due to the\ninteraction of multiple predicates and correlations in the data. In this paper,\nwe propose a low-cost post-processing step that can take a plan produced by the\noptimizer, detect when it is likely to have made such a mistake, and take steps\nto fix it. Specifically, our solution is a sampling-based iterative procedure\nthat requires almost no changes to the original query optimizer or query\nevaluation mechanism of the system. We show that this indeed imposes low\noverhead and catches cases where three widely used optimizers (PostgreSQL and\ntwo commercial systems) make large errors.\n",
        "  Competitions can occur on an absolute scale, to be faster or more efficient,\nor they can occur on a relative scale, to \"beat\" one's competitor in a zero-sum\ngame. Ecological models have focused on absolute competitions, in which optima\nexist. Classic evolutionary models such as the Wright-Fisher model, as well as\nmore recent models of travelling waves, have focused on purely relative\ncompetitions, in which fitness continues to increase indefinitely, without\nactually progressing anywhere. This manuscript proposes a new way to describe\nboth at the same time. It begins with a revised version of r/K-selection\ntheory. r continues to describe maximum reproductive speed, but the new version\nof K, with a different subscript, now describes parsimoniousness in territory\nuse, a group-selected, anti-tragedy-of-the-commons trait. A third dimension c\nof fitness is then added to this novel system, one which is unitless and\nnormalized, and hence capable of capturing the population genetics concept w of\na strictly relative, genetically-limited competitive race. MacArthur's original\nversion of r/K-selection theory is shown to confound parsimoniousness K with\ncompetitive ability c, despite the fact that available data suggests a negative\ncorrelation between the two; here they are disentangled. A rotation of the\nresulting three-dimensional system provides a population genetic underpinning\nfor Grime's universal adaptive strategy theory of ruderals (selected for high\nr), stress tolerators (selected for a combination of high r and high K), and\ncompetitors (selected for a combination of high r and high c).\n",
        "  We show a spectral sequence for the rational Khovanov homology of an oriented\nlink in terms of the rational Khovanov complexes and homologies of the link\nsurgeries along an admissible cut. As a non trivial corollary, we give an\nexplicit splitting formula for the Jones polynomial.\n",
        "  In this work we focus on confidence modeling for neural semantic parsers\nwhich are built upon sequence-to-sequence models. We outline three major causes\nof uncertainty, and design various metrics to quantify these factors. These\nmetrics are then used to estimate confidence scores that indicate whether model\npredictions are likely to be correct. Beyond confidence estimation, we identify\nwhich parts of the input contribute to uncertain predictions allowing users to\ninterpret their model, and verify or refine its input. Experimental results\nshow that our confidence model significantly outperforms a widely used method\nthat relies on posterior probability, and improves the quality of\ninterpretation compared to simply relying on attention scores.\n",
        "  Motivated by the need to extract knowledge and value from interconnected\ndata, graph analytics on big data is a very active area of research in both\nindustry and academia. To support graph analytics efficiently a large number of\nin memory graph libraries, graph processing systems and graph databases have\nemerged. Projects in each of these categories focus on particular aspects such\nas static versus dynamic graphs, off line versus on line processing, small\nversus large graphs, etc. While there has been much advance in graph processing\nin the past decades, there is still a need for a fast graph processing, using a\ncluster of machines with distributed storage. In this paper, we discuss a novel\ndistributed graph database called System G designed for efficient graph data\nstorage and processing on modern computing architectures. In particular we\ndescribe a single node graph database and a runtime and communication layer\nthat allows us to compose a distributed graph database from multiple single\nnode instances. From various industry requirements, we find that fast\ninsertions and large volume concurrent queries are critical parts of the graph\ndatabases and we optimize our database for such features. We experimentally\nshow the efficiency of System G for storing data and processing graph queries\non state-of-the-art platforms.\n",
        "  Deep learning embeddings have been successfully used for many natural\nlanguage processing problems. Embeddings are mostly computed for word forms\nalthough a number of recent papers have extended this to other linguistic units\nlike morphemes and phrases. In this paper, we argue that learning embeddings\nfor discontinuous linguistic units should also be considered. In an\nexperimental evaluation on coreference resolution, we show that such embeddings\nperform better than word form embeddings.\n",
        "  Hysteretic losses in MgB2 wound superconducting coils of a 500 kW synchronous\nhybrid generator were estimated as part of the European project SUPRAPOWER led\nby the Spanish company Tecnalia Research and Innovation. Particular interest\nwas given to the losses found in tapes in the superconducting rotor caused by\nthe magnetic flux ripples originating from the conventional stator during\nnominal operation. To compute the losses, a 2D Finite Element Method was\napplied to solve the H-formulation of Maxwell's equations considering the\nnonlinear properties of both the superconducting material and its surrounding\nNi matrix. To be able to model all the different turns composing the winding of\nthe superconducting rotor coils, three geometrical models of single tape cross\nsection of decreasing complexity were studied: 1) the first model reproduced\nclosely the actual cross section obtained from micrographs, 2) the second model\nwas obtained from the computed elastoplastic deformation of a round Ni wire, 3)\nthe last model was based on a simplified elliptic cross section. The last\ngeometry allowed validating the modeling technique by comparing numerical\nlosses with results from well-established analytical expressions. Additionally,\nthe following cases of filament transpositions were studied: no, partial and\nfull transposition. Finally, choosing the right level of geometrical details to\npredict the expected behavior of individual superconducting tapes in the rotor,\nthe following operational regimes were studied: Bias-DC current, ramping\ncurrent under ramping background field, and magnetic flux ripples under DC\nbackground current and field.\n",
        "  We define and study a family of link invariants $\\mathit{HFK}_{n}(L)$.\nAlthough these homology theories are defined using holomorphic disc counts,\nthey share many properties with $sl_{n}$ homology. Using these theories, we\ngive a framework that generalizes the conjectured spectral sequence from\nKhovanov homology to $\\delta$-graded knot Floer homology. In particular, we\nconjecture that for all links $L$ in $S^3$ and all $n\\ge 1$, there is a\nspectral sequence from the $sl_{n}$ homology of $L$ to $\\mathit{HFK}_{n}(L)$.\n",
        "  The Epstein-Baer theory of curve isotopies is basic to the remarkable theorem\nthat homotopic homeomorphisms of surfaces are isotopic. The groundbreaking work\nof R. Baer was carried out on closed, orientable surfaces and extended by D. B.\nA. Epstein to arbitrary surfaces, compact or not, with or without boundary and\norientable or not. We give a new method of deducing the theorem about homotopic\nhomeomorphisms from the results about homotopic curves via the hyperbolic\ngeometry of surfaces. This works on all but 13 surfaces where ad hoc proofs are\nneeded.\n",
        "  Models of Stock Recruitment Relationships (SRRs) are often used to predict\nfish population dynamics. Commonly used SRRs include the Ricker, Beverton-Holt,\nand Cushing functional forms, which differ primarily by the degree of density\ndependent effects (compensation). The degree of compensation determines whether\nrecruitment respectively decreases, saturates, or increases at high levels of\nspawning stock biomass. In 1982 J.G. Shepherd united these dynamics into a\nsingle model, where the degree of compensation is determined by a single\nparameter, however the difficulty in relating this parameter to biological data\nhas limited its usefulness. Here we use a generalized modeling framework to\nshow that the degree of compensation can be related directly to the functional\nelasticity of growth, which is a general quantity that measures the change in\nrecruitment relative to a change in biomass, irrespective of the specific SRR.\nWe show that the elasticity of growth can be calculated from short-term\nfluctuations in fish biomass, is robust to observation error, and can be used\nto determine general attributes of the SRR in both continuous time production\nmodels, as well as discrete time age-structured models. This framework may be\nparticularly useful if fisheries time-series data are limited, and not\nconducive to determining functional relationships using traditional methods of\nstatistical best-fit.\n",
        "  Statistically consistent estimation of phylogenetic trees or gene trees is\npossible if pairwise sequence dissimilarities can be converted to a set of\ndistances that are proportional to the true evolutionary distances. Susko et\nal. (2004) reported some strikingly broad results about the forms of\ninconsistency in tree estimation that can arise if corrected distances are not\nproportional to the true distances. They showed that if the corrected distance\nis a concave function of the true distance, then inconsistency due to long\nbranch attraction will occur. If these functions are convex, then two \"long\nbranch repulsion\" trees will be preferred over the true tree -- though these\ntwo incorrect trees are expected to be tied as the preferred true. Here we\nextend their results, and demonstrate the existence of a tree shape (which we\nrefer to as a \"twisted Farris-zone\" tree) for which a single incorrect tree\ntopology will be guaranteed to be preferred if the corrected distance function\nis convex. We also report that the standard practice of treating gaps in\nsequence alignments as missing data is sufficient to produce non-linear\ncorrected distance functions if the substitution process is not independent of\nthe insertion/deletion process. Taken together, these results imply\ninconsistent tree inference under mild conditions. For example, if some\npositions in a sequence are constrained to be free of substitutions and\ninsertion/deletion events while the remaining sites evolve with independent\nsubstitutions and insertion/deletion events, then the distances obtained by\ntreating gaps as missing data can support an incorrect tree topology even given\nan unlimited amount of data.\n",
        "  Cells generally change their internal state to adapt to an environmental\nchange, and accordingly evolve in response to the new conditions. This process\ninvolves phenotypic changes that occur over several different time scales,\nranging from faster environmental adaptation without a corresponding change in\nthe genomic sequence to slower evolutionary dynamics involving genetic\nmutations and subsequent selection. In this regard, a question arises as to\nwhether there are any relationships between such phenotypic changes over the\ndifferent time scales at which adaptive evolution occurs. In this study, we\nanalyzed simulated adaptive evolution in a simple cell model, and found that\nproportionality between concentration changes in adaptation and evolution over\nall components, and the proportion coefficients were closely linked to the\nchange in the growth rate of a cell. Furthermore, we demonstrated that the\nphenotypic variances in component concentrations due to (non-genetic) noise and\ngenomic alternations are proportional across all components. These global\nrelationships in cellular states were also supported by phenomenological theory\nand transcriptome analysis of laboratory evolution in {\\it Escherichia coli}.\nThese findings provide a basis for the development of a quantitative theory of\nplasticity and robustness, and to determine the general restriction of\nphenotypic changes imposed by evolution.\n",
        "  Investigating the anisotropy of superconductors permits an access to\nfundamental properties. Having succeeded in the fabrication of epitaxial\nsuperconducting LaFeAs(O,F) thin films we performed an extensive study of\nelectrical transport properties. In face of multiband superconductivity we can\ndemonstrate that a Blatter scaling of the angular dependent critical current\ndensities can be adopted, although being originally developed for single band\nsuperconductors. In contrast to single band superconductors the mass anisotropy\nof LaFeAs(O,F) is temperature dependent. A very steep increase of the upper\ncritical field and the irreversibility field can be observed at temperatures\nbelow 6K, indicating that the band with the smaller gap is in the dirty limit.\nThis temperature dependence can be theoretically described by two dominating\nbands responsible for superconductivity. A pinning force scaling provides\ninsight into the prevalent pinning mechanism and can be specified in terms of\nthe Kramer model.\n",
        "  The UV photon escape fraction from molecular clouds is a key parameter for\nunderstanding the ionization of the Interstellar Medium (ISM), and\nextragalactic processes, such as cosmic reionization. We present the ionizing\nphoton flux and the corresponding photon escape fraction (f$_{esc}$) arising as\na consequence of star cluster formation in a turbulent, 10$^6$ M$_{\\odot}$ GMC,\nsimulated using the code FLASH. We make use of sink particles to represent\nyoung, star-forming clusters coupled with a radiative transfer scheme to\ncalculate the emergent UV flux. We find that the ionizing photon flux across\nthe cloud boundary is highly variable in time and space due to the turbulent\nnature of the intervening gas. The escaping photon fraction remains at $\\sim$5%\nfor the first 2.5 Myr, followed by two pronounced peaks at 3.25 and 3.8 Myr\nwith a maximum f$_{esc}$ of 30% and 37%, respectively. These peaks are due to\nthe formation of large HII regions, that expand into regions of lower density\nand some of which reach the cloud surface. However, these phases are short\nlived and f$_{esc}$ drops sharply as the HII regions are quenched by the\ncentral cluster passing through high-density material due to the turbulent\nnature of the cloud. We find an average f$_{esc}$ of 15% with factor of two\nvariations over 1 Myr timescales. Our results suggest that assuming a single\nvalue for f$_{esc}$ from a molecular cloud is in general a poor approximation,\nand that the dynamical evolution of the system leads to large temporal\nvariation.\n",
        "  Detailed chemical abundances are presented for seven M31 outer halo globular\nclusters (with projected distances from M31 greater than 30 kpc), as derived\nfrom high resolution integrated light spectra taken with the Hobby Eberly\nTelescope. Five of these clusters were recently discovered in the Pan-Andromeda\nArchaeological Survey (PAndAS)---this paper presents the first determinations\nof integrated Fe, Na, Mg, Ca, Ti, Ni, Ba, and Eu abundances for these clusters.\nFour of the target clusters (PA06, PA53, PA54, and PA56) are metal-poor ([Fe/H]\n< -1.5), alpha-enhanced (though they are possibly less alpha-enhanced than\nMilky Way stars at the 1 sigma level), and show signs of star-to-star Na and Mg\nvariations. The other three globular clusters (H10, H23, and PA17) are more\nmetal rich, with metallicities ranging from [Fe/H] = -1.4 to -0.9. While H23 is\nchemically similar to Milky Way field stars, Milky Way globular clusters, and\nother M31 clusters, H10 and PA17 have moderately low [Ca/Fe], compared to Milky\nWay field stars and clusters. Additionally, PA17's high [Mg/Ca] and [Ba/Eu]\nratios are distinct from Milky Way stars, and are in better agreement with the\nstars and clusters in the Large Magellanic Cloud (LMC). None of the clusters\nstudied here can be conclusively linked to any of the identified streams from\nPAndAS; however, based on their locations, kinematics, metallicities, and\ndetailed abundances, the most metal-rich PAndAS clusters H23 and PA17 may be\nassociated with the progenitor of the Giant Stellar Stream, H10 may be\nassociated with the SW Cloud, and PA53 and PA56 may be associated with the\nEastern Cloud.\n",
        "  We introduce a new class of possibly noncompact n-dimensional manifolds\nwithout boundary associated to finite data which we call topological automata.\nThis class is large enough to contain many interesting examples of open\n2-dimensional and 3-dimensional manifolds of interest to low-dimensional\ntopologists. Our main result is that the homeomorphism problem in this class is\ndecidable for n = 2.\n",
        "  Marine protected areas (MPAs) have attracted much attention as a tool for\nsustainable fisheries management, restoring depleted fisheries stocks and\nmaintaining ecosystems. However, even with total exclusion of fishing effort,\ndepleted stocks sometimes show little or no recovery over a long time period.\nHere, using a mathematical model, we show that multiple stable states may hold\nthe key to understanding the tendency for fisheries stocks to recover because\nof MPAs. We find that MPAs can have either a positive effect or almost no\neffect on the recovery of depleted fishing stocks, depending on the fish\nmigration patterns and the fishing policies. MPAs also reinforce ecological\nresilience, particularly for migratory species. In contrast to previous\nreports, our results show that MPAs have small or sometimes negative effects on\nthe recovery of sedentary species. Unsuitable MPA planning might result in low\neffectiveness or even deterioration of the existing condition.\n",
        "  We present a new approach for neural machine translation (NMT) using the\nmorphological and grammatical decomposition of the words (factors) in the\noutput side of the neural network. This architecture addresses two main\nproblems occurring in MT, namely dealing with a large target language\nvocabulary and the out of vocabulary (OOV) words. By the means of factors, we\nare able to handle larger vocabulary and reduce the training time (for systems\nwith equivalent target language vocabulary size). In addition, we can produce\nnew words that are not in the vocabulary. We use a morphological analyser to\nget a factored representation of each word (lemmas, Part of Speech tag, tense,\nperson, gender and number). We have extended the NMT approach with attention\nmechanism in order to have two different outputs, one for the lemmas and the\nother for the rest of the factors. The final translation is built using some\n\\textit{a priori} linguistic information. We compare our extension with a\nword-based NMT system. The experiments, performed on the IWSLT'15 dataset\ntranslating from English to French, show that while the performance do not\nalways increase, the system can manage a much larger vocabulary and\nconsistently reduce the OOV rate. We observe up to 2% BLEU point improvement in\na simulated out of domain translation setup.\n",
        "  We investigate the structure of the characteristic polynomial det(xI-T) of a\ntransition matrix T that is associated to a train track representative of a\npseudo-Anosov map [F] acting on a surface. As a result we obtain three new\npolynomial invariants of [F], one of them being the product of the other two,\nand all three being divisors of det(xI-T). The degrees of the new polynomials\nare invariants of [F ] and we give simple formulas for computing them by a\ncounting argument from an invariant train track. We give examples of genus 2\npseudo-Anosov maps having the same dilatation, and use our invariants to\ndistinguish them.\n",
        "  Cross lingual projection of linguistic annotation suffers from many sources\nof bias and noise, leading to unreliable annotations that cannot be used\ndirectly. In this paper, we introduce a novel approach to sequence tagging that\nlearns to correct the errors from cross-lingual projection using an explicit\ndebiasing layer. This is framed as joint learning over two corpora, one tagged\nwith gold standard and the other with projected tags. We evaluated with only\n1,000 tokens tagged with gold standard tags, along with more plentiful parallel\ndata. Our system equals or exceeds the state-of-the-art on eight simulated\nlow-resource settings, as well as two real low-resource languages, Malagasy and\nKinyarwanda.\n",
        "  For every orientable surface of finite negative Euler characteristic, we find\na right-angled Artin group of cohomological dimension two which does not embed\ninto the associated mapping class group. For a right-angled Artin group on a\ngraph $\\gam$ to embed into the mapping class group of a surface $S$, we show\nthat the chromatic number of $\\gam$ cannot exceed the chromatic number of the\nclique graph of the curve graph $\\mathcal{C}(S)$. Thus, the chromatic number of\n$\\gam$ is a global obstruction to embedding the right-angled Artin group\n$A(\\gam)$ into the mapping class group $\\Mod(S)$.\n",
        "  Large variety of optical full-field measurement techniques are being\ndeveloped and applied to solve mechanical problems. Since each technique\npossess its own merits, it is important to know the capabilities and\nlimitations of such techniques. Among these optical full-field methods,\ninterferometry techniques take an important place. They are based on\nillumination with coherent light (laser). In shearing interferometry the\ndifference of the out of-plane displacement in two neighboring object points is\ndirectly measured. Since object displacement does not result in interferometry\nfringes, the method is suited for localization of strain concentrations and is\nindeed used in industry for this purpose. Used quantitatively DSPSI possesses\nthe advantage over conventional out-of-plane displacement-sensitive\ninterferometry that only a single difference of the unwrapped phase map is\nrequired to obtain flexural strains, thereby relieving problems with noise and\nreduction in the field of view. The first publication on (DSPSI) was made in\n1973, but the emergence of a system providing quantitative measurements is more\nrecent. This work aims to present the results of strain measurements using\ndigital speckle pattern shearing interferometry (DSPSI).\n",
        "  While traditional RDBMSes offer a lot of advantages, they require significant\neffort to setup and to use. Because of these challenges, many data scientists\nand analysts have switched to using alternative data management solutions.\nThese alternatives, however, lack features that are standard for RDBMSes, e.g.\nout-of-core query execution. In this paper, we introduce the embedded\nanalytical database MonetDBLite. MonetDBLite is designed to be both highly\nefficient and easy to use in conjunction with standard analytical tools. It can\nbe installed using standard package managers, and requires no configuration or\nserver management. It is designed for OLAP scenarios, and offers\nnear-instantaneous data transfer between the database and analytical tools, all\nthe while maintaining the transactional guarantees and ACID properties of a\nstandard relational system. These properties make MonetDBLite highly suitable\nas a storage engine for data used in analytics, machine learning and\nclassification tasks.\n",
        "  A closed hyperbolic surface of genus $g\\ge 2$ can be decomposed into pairs of\npants along shortest closed geodesics and if these curves are sufficiently\nshort (and with lengths uniformly bounded away from 0), then the geometry of\nthe surface is essentially determined by the combinatorics of the pants\ndecomposition. These combinatorics are determined by a trivalent graph, so we\ncall such surfaces {\\em trivalent}.\n  In this paper, in a first attempt to understand the \"shape\" of the subset\n$\\ts$ of moduli space consisting of surfaces whose systoles fill, we compare it\nmetrically, asymptotically in g, with the set $\\tri$ of trivalent surfaces. As\nour main result, we find that the set $\\ts \\cap \\tri$ is metrically \"sparse\" in\n$\\ts$ (where we equip $\\moduli$ with either the Thurston or the Teichm\\\"uller\nmetric).\n",
        "  With the multiplication of XML data sources, many XML data warehouse models\nhave been proposed to handle data heterogeneity and complexity in a way\nrelational data warehouses fail to achieve. However, XML-native database\nsystems currently suffer from limited performances, both in terms of manageable\ndata volume and response time. Fragmentation helps address both these issues.\nDerived horizontal fragmentation is typically used in relational data\nwarehouses and can definitely be adapted to the XML context. However, the\nnumber of fragments produced by classical algorithms is difficult to control.\nIn this paper, we propose the use of a k-means-based fragmentation approach\nthat allows to master the number of fragments through its $k$ parameter. We\nexperimentally compare its efficiency to classical derived horizontal\nfragmentation algorithms adapted to XML data warehouses and show its\nsuperiority.\n",
        "  R. S. Kulkarni showed that a finite group acting pseudofreely, but not\nfreely, preserving orientation, on an even-dimensional sphere (or suitable\nsphere-like space) is either a periodic group acting semifreely with two fixed\npoints, a dihedral group acting with three singular orbits, or one of the\npolyhedral groups, occurring only in dimension 2. It is shown here that the\ndihedral group does not act pseudofreely and locally linearly on an actual\nn-sphere when n is congruent to 0 mod 4. The possibility of such an action when\nn is congruent to 2 mod 4 and n>2 remains open. Orientation-reversing actions\nare also considered.\n",
        "  Privacy definitions provide ways for trading-off the privacy of individuals\nin a statistical database for the utility of downstream analysis of the data.\nIn this paper, we present Blowfish, a class of privacy definitions inspired by\nthe Pufferfish framework, that provides a rich interface for this trade-off. In\nparticular, we allow data publishers to extend differential privacy using a\npolicy, which specifies (a) secrets, or information that must be kept secret,\nand (b) constraints that may be known about the data. While the secret\nspecification allows increased utility by lessening protection for certain\nindividual properties, the constraint specification provides added protection\nagainst an adversary who knows correlations in the data (arising from\nconstraints). We formalize policies and present novel algorithms that can\nhandle general specifications of sensitive information and certain count\nconstraints. We show that there are reasonable policies under which our privacy\nmechanisms for k-means clustering, histograms and range queries introduce\nsignificantly lesser noise than their differentially private counterparts. We\nquantify the privacy-utility trade-offs for various policies analytically and\nempirically on real datasets.\n",
        "  It is known that the maximal homological degree of the Khovanov homology of a\nknot gives a lower bound of the minimal positive crossing number of the knot.\nIn this paper, we show that the maximal homological degree of the Khovanov\nhomology of a cabling of a knot gives a lower bound of the minimal positive\ncrossing number of the knot.\n",
        "  Searching for superconducting materials with high transition temperature (TC)\nis one of the most exciting and challenging fields in physics and materials\nscience. Although superconductivity has been discovered for more than 100\nyears, the copper oxides are so far the only materials with TC above 77 K, the\nliquid nitrogen boiling point. Here we report an interface engineering method\nfor dramatically raising the TC of superconducting films. We find that one\nunit-cell (UC) thick films of FeSe grown on SrTiO3 (STO) substrates by\nmolecular beam epitaxy (MBE) show signatures of superconducting transition\nabove 50 K by transport measurement. A superconducting gap as large as 20 meV\nof the 1 UC films observed by scanning tunneling microcopy (STM) suggests that\nthe superconductivity could occur above 77 K. The occurrence of\nsuperconductivity is further supported by the presence of superconducting\nvortices under magnetic field. Our work not only demonstrates a powerful way\nfor finding new superconductors and for raising TC, but also provides a\nwell-defined platform for systematic study of the mechanism of unconventional\nsuperconductivity by using different superconducting materials and substrates.\n",
        "  An article (J Trauma 29:10-18, 1989) cites unpublished wound ballistics data\nto support the authors' view that distant injuries are a myth in wound\nballistics. The actual data, published in 1990, actually contains a number of\ndetailed examples of distant injuries. (Bellamy RF, Zajtchuk R. The physics and\nbiophysics of wound ballistics. In: Zajtchuk R, ed. Textbook of Military\nMedicine, Part I: Warfare, Weaponry, and the Casualty, Vol. 5, Conventional\nWarfare: Ballistic, Blast, and Burn Injuries. Washington, DC: Office of the\nSurgeon General, Department of the Army, United States of America; 1990:\n107-162.)\n",
        "  We compared the properties of 56 elliptical galaxies selected from three\nclusters at $1.2<z<1.4$ with those of field galaxies in the GOODS-S (~30),\nCOSMOS (~180) and CANDELS (~220) fields. We studied the relationships among\neffective radius, surface brightness, stellar mass, stellar mass density\n$\\Sigma_{Re}$ and central mass density $\\Sigma_{1kpc}$ within 1 kpc radius. We\nfind that cluster ellipticals do not differ from field ellipticals: they share\nthe same structural parameters at fixed mass and the same scaling relations. On\nthe other hand, the population of field ellipticals at $z\\sim1.3$ shows a\nsignificant lack of massive ($M_*> 2\\times 10^{11}$ M$_\\odot$) and large (R$_e\n> 4-5$ kpc) ellipticals with respect to the cluster. Nonetheless, at\n$M*<2\\times 10^{11}$ M$_\\odot$, the two populations are similar. The size-mass\nrelation of ellipticals at z~1.3 defines two different regimes, above and below\na transition mass $m_t\\sim 2-3\\times10^{10}$ M$_\\odot$: at lower masses the\nrelation is nearly flat (R$_e\\propto M_*^{-0.1\\pm 0.2}$), the mean radius is\nconstant at ~1 kpc and $\\Sigma_{Re}\\sim \\Sigma_{1kpc}$ while, at larger masses,\nthe relation is R$_e\\propto M*^{0.64\\pm0.09}$. The transition mass marks the\nmass at which galaxies reach the maximum $\\Sigma_{Re}$. Also the\n$\\Sigma_{1kpc}$-mass relation follows two different regimes,\n$\\Sigma_{1kpc}\\propto M*^{0.64\\ >m_t}_{1.07\\ <m_t}$, defining a transition mass\ndensity $\\Sigma_{1kpc}\\sim 2-3\\times10^3$ M$_\\odot$ pc$^{-2}$. The mass density\n$\\Sigma_{Re}$ does not correlate with mass, dense/compact galaxies can be\nassembled over a wide mass regime, independently of the environment. The\ncentral mass density, $\\Sigma_{1kpc}$, besides to be correlated with the mass,\nis correlated to the age of the stellar population: the higher the central\nstellar mass density, the higher the mass, the older the age of the stellar\npopulation. [Abridged]\n",
        "  Suppose $S$ is a closed orientable surface and $\\tilde{S}$ is a finite\nsheeted regular cover of $S$. The following question was posed by Juli\\'{e}n\nMarch\\'{e} in Mathoverflow: Do the lifts of simple curves from $S$ generate\n$H_{1}(\\tilde{S},\\mathbb{Z})$? A family of examples is given for which the\nanswer is \"no\".\n",
        "  We consider the standard three-component differential equation model for the\ngrowth of an HIV virion population in an infected host in the absence of drug\ntherapy. The dynamical properties of the model are determined by the set of\nvalues of six parameters which vary across host populations. There may be one\nor two critical points whose natures play a key role in determining the outcome\nof infection and in particular whether the HIV population will persist or\nbecome extinct. There are two cases which may arise. In the first case, there\nis only one critical point P_1 at biological values and this is an\nasymptotically stable node. The system ends up with zero virions and so the\nhost becomes HIV-free. In the second case, there are two critical points P_1\nand P_2 at biological values. Here P_1 is an unstable saddle point and P_2 is\nan asymptotically stable spiral point with a non-zero virion level. In this\ncase the HIV population persists unless parameters change. We let the parameter\nvalues take random values from distributions based on empirical data, but\nsuitably truncated, and determine the probabilities of occurrence of the\nvarious combinations of critical points. From these simulations the probability\nthat an HIV infection will persist, across a population, is estimated. It is\nfound that with conservatively estimated distributions of parameters, within\nthe framework of the standard 3-component model, the chances that a within host\nHIV population will become extinct is between 0.6% and 6.9%. With less\nconservative parameter estimates, the probability is estimated to be as high as\n24%. The many factors related to the transmission and possible spontaneous\nelimination of the virus are discussed.\n",
        "  The GEANT4 Monte Carlo simulation and experimental characterization of the\nSiemens E.Cam Dual Head gamma camera hosted in the Particular Hospital of\nAlgarve have been done. Imaging tests of thyroid and other phantoms have been\nmade \"in situ\" and compared with the results obtained with the Monte Carlo\nsimulation.\n",
        "  Thermo plastic polyurethane (TPU) provides excellent bio-compatibility,\nflexibility and good irradiation resistance; however, extremely high\nirradiation doses can alter the structure and function of macromolecules,\nresulting in oxidation, chain scission and cross-linking. In this study, the\neffects of e-beam irradiation on the medical grade thermo plastic polyurethane\nwere studied. The changes in the chain length and their distribution as well as\nthe changes in molecular structure were studied. The GPC (Gel Permeation\nChromatography) results show that the oxidative decomposition is followed by a\ndecrease in molecular mass together with an increase in polydispersity. This\nindicates a very inhomogeneous degradation, which is a consequence of the\nspecific course and of the intensity of oxidative degradation. This was\nconfirmed by means of mechanical property measurements. Overall, this study\ndemonstrated that the medical grade TPU was affected by radiation exposure,\nparticularly at high irradiation doses.\n",
        "  Word embeddings -- distributed representations of words -- in deep learning\nare beneficial for many tasks in natural language processing (NLP). However,\ndifferent embedding sets vary greatly in quality and characteristics of the\ncaptured semantics. Instead of relying on a more advanced algorithm for\nembedding learning, this paper proposes an ensemble approach of combining\ndifferent public embedding sets with the aim of learning meta-embeddings.\nExperiments on word similarity and analogy tasks and on part-of-speech tagging\nshow better performance of meta-embeddings compared to individual embedding\nsets. One advantage of meta-embeddings is the increased vocabulary coverage. We\nwill release our meta-embeddings publicly.\n",
        "  We provide a new obstruction for a rational homology 3-sphere to arise by\nDehn surgery on a given knot in the 3-sphere. The obstruction takes the form of\nan inequality involving the genus of the knot, the surgery coefficient, and a\ncount of L-structures on the 3-manifold, that is spin-c structures with the\nsimplest possible associated Heegaard Floer group. Applications include an\nobstruction for two framed knots to yield the same 3-manifold, an obstruction\nthat is particularly effective when working with families of framed knots. We\nintroduce the rational and integral Dehn surgery genera for a rational homology\n3-sphere, and use our inequality to provide bounds, and in some cases exact\nvalues, for these genera. We also demonstrate that the difference between the\nintegral and rational Dehn surgery genera can be arbitrarily large.\n",
        "  Purpose : To relate the physical transmittance parameters of the water\nequivalent Gafchromic EBT 2 Film with the delivered dose in a transparent\nabsolute calibration protocol. The protocol should be easy to understand, easy\nto perform, and should be able to predict the residual dose error.\n  Conclussions : The gafchromic EBT2 Films are properly calibrated with an\naccessible robust calibration protocol. The protocol largely deals with the\nuniformity problems of the Film. The proposed method allowed to relate the dose\nwith the red channel transmittance using only T0, T_inf, and a dose scaling\nfactor. Based on the local and global uniformity the red channel dose errors\ncould be predicted to be smaller than 5%.\n",
        "  Collaborative working is increasingly popular, but it presents challenges due\nto the need for high responsiveness and disconnected work support. To address\nthese challenges the data is optimistically replicated at the edges of the\nnetwork, i.e. personal computers or mobile devices. This replication requires a\nmerge mechanism that preserves the consistency and structure of the shared data\nsubject to concurrent modifications. In this paper, we propose a generic design\nto ensure eventual consistency (every replica will eventually view the same\ndata) and to maintain the specific constraints of the replicated data. Our\nlayered design provides to the application engineer the complete control over\nsystem scalability and behavior of the replicated data in face of concurrent\nmodifications. We show that our design allows replication of complex data types\nwith acceptable performances.\n",
        "  A pedigree is a directed graph that describes how individuals are related\nthrough ancestry in a sexually-reproducing population. In this paper we explore\nthe question of whether one can reconstruct a pedigree by just observing\nsequence data for present day individuals. This is motivated by the increasing\navailability of genomic sequences, but in this paper we take a more theoretical\napproach and consider what models of sequence evolution might allow pedigree\nreconstruction (given sufficiently long sequences). Our results complement\nrecent work that showed that pedigree reconstruction may be fundamentally\nimpossible if one uses just the degrees of relatedness between different extant\nindividuals. We find that for certain stochastic processes, pedigrees can be\nrecovered up to isomorphism from sufficiently long sequences.\n",
        "  Tumours are made up of a mixed population of different types of cells that\ninclude normal struc- tures as well as ones associated with the malignancy, and\nthere are multiple interactions between the malignant cells and the local\nmicroenvironment. These intercellular interactions, modulated by the\nmicroenvironment, effect tumour progression and represent a largely under\nappreciated therapeutic target. We use observations of primary tumor biology\nfrom prostate cancer to extrapolate a math- ematical model: specifically; it\nhas been observed that in prostate cancer three disparate cellular outcomes\npredominate: (i) the tumour remains well differentiated and clinically indolent\n- in this case the local stromal cells may act to restrain the growth of the\ncancer; (ii) early in its genesis the tumour acquires a highly malignant\nphenotype, growing rapidly and displacing the original stromal population\n(often referred to as small cell prostate cancer) - these less common\naggressive tumours are relatively independent of the local microenvironment;\nand, (iii) the tumour co-opts the local stroma - taking on a classic\nstromagenic phenotype where interactions with the local microenviron- ment are\ncritical to the cancer growth. We present an evolutionary game theoretical\nconstruct that models the influence of tumour-stroma interactions in driving\nthese outcomes. We consider three characteristic and distinct cellular\npopulations: stromal cells, tumour cells that are self-reliant in terms of\nmicroenvironmental factors and tumour cells that depend on the environment for\nresources but can also co-opt stroma. Using evolutionary game theory we explore\na number of different sce- narios that elucidate the impact of tumour-stromal\ninteractions on the dynamics of prostate cancer growth and progression and how\ndifferent treatments in the metastatic setting can affect different types of\ntumors.\n",
        "  Nearest neighbor searching of large databases in high-dimensional spaces is\ninherently difficult due to the curse of dimensionality. A flavor of\napproximation is, therefore, necessary to practically solve the problem of\nnearest neighbor search. In this paper, we propose a novel yet simple indexing\nscheme, HD-Index, to solve the problem of approximate k-nearest neighbor\nqueries in massive high-dimensional databases. HD-Index consists of a set of\nnovel hierarchical structures called RDB-trees built on Hilbert keys of\ndatabase objects. The leaves of the RDB-trees store distances of database\nobjects to reference objects, thereby allowing efficient pruning using distance\nfilters. In addition to triangular inequality, we also use Ptolemaic inequality\nto produce better lower bounds. Experiments on massive (up to billion scale)\nhigh-dimensional (up to 1000+) datasets show that HD-Index is effective,\nefficient, and scalable.\n",
        "  Despite the number of NLP studies dedicated to thematic fit estimation,\nlittle attention has been paid to the related task of composing and updating\nverb argument expectations. The few exceptions have mostly modeled this\nphenomenon with structured distributional models, implicitly assuming a\nsimilarly structured representation of events. Recent experimental evidence,\nhowever, suggests that human processing system could also exploit an\nunstructured \"bag-of-arguments\" type of event representation to predict\nupcoming input. In this paper, we re-implement a traditional structured model\nand adapt it to compare the different hypotheses concerning the degree of\nstructure in our event knowledge, evaluating their relative performance in the\ntask of the argument expectations update.\n",
        "  A computer-aided interpretation approach is proposed to detect rheumatic\narthritis (RA) of human finger joints in optical tomographic images. The image\ninterpretation method employs a multi-variate signal detection analysis aided\nby a machine learning classification algorithm, called Self-Organizing Mapping\n(SOM). Unlike in previous studies, this allows for combining multiple physical\nimage parameters, such as minimum and maximum values of the absorption\ncoefficient for identifying affected and not affected joints. Classification\nperformances obtained by the proposed method were evaluated in terms of\nsensitivity, specificity, Youden index, and mutual information. Different\nmethods (i.e., clinical diagnostics, ultrasound imaging, magnet resonance\nimaging and inspection of optical tomographic images), were used as \"ground\ntruth\"-benchmarks to determine the performance of image interpretations. Using\ndata from 100 finger joints, findings suggest that some parameter combinations\nlead to higher sensitivities while others to higher specificities when compared\nto single parameter classifications employed in previous studies. Maximum\nperformances were reached when combining minimum/maximum-ratio and image\nvariance with respect to ultra sound as benchmark. In this case, sensitivity\nand specificity of 0.94 and 0.96 respectively were achieved. These values are\nmuch higher than results reported when a) other classification techniques were\napplied or b) single parameter classifications were used, where sensitivities\nand specificities of 0.71 were achieved.\n",
        "  The radii of young (<100 Myr) star clusters correlate only weakly with their\nmasses. This shallow relation has been used to argue that impulsive tidal\nperturbations, or `shocks', by passing giant molecular clouds (GMCs)\npreferentially disrupt low-mass clusters. We show that this mass-radius\nrelation is in fact the result of the combined effect of two-body relaxation\nand repeated tidal shocks. Clusters in a broad range of environments including\nthose like the solar neighbourhood evolve towards a typical radius of a few\nparsecs, as observed, independent of the initial radius. This equilibrium\nmass-radius relation is the result of a competition between expansion by\nrelaxation and shrinking due to shocks. Interactions with GMCs are more\ndisruptive for low-mass clusters, which helps to evolve the globular cluster\nmass function (GCMF). However, the properties of the interstellar medium in\nhigh-redshift galaxies required to establish a universal GCMF shape are more\nextreme than previously derived, challenging the idea that all GCs formed with\nthe same power-law mass function.\n",
        "  The pressure dependence of the magnetic and superconducting transitions, as\nwell as that of the superconducting upper critical field is reported for\nCaK(Fe$_{1-x}$Ni$_{x}$)$_4$As$_4$, the first example of an Fe-based\nsuperconductor with spin-vortex-crystal-type magnetic ordering. Resistance\nmeasurements were performed on single crystals with two substitution levels\n($x=0.033, 0.050$) under hydrostatic pressures up to 5.12 GPa and in magnetic\nfields up to 9 T. Our results show that, for both compositions, magnetic\ntransition temperatures, $T_\\textrm{N}$, are suppressed upon applying pressure,\nthe superconducting transition temperatures $T_\\textrm{c}$ are suppressed by\npressure as well, except for $x=0.050$ in the pressure region where\n$T_\\textrm{N}$ and $T_\\textrm{c}$ cross. Furthermore, the pressure associated\nwith the crossing of the $T_\\textrm{N}$ and $T_\\textrm{c}$ lines also coincides\nwith a minimum in the normalized slope of the superconducting upper critical\nfield, consistent with a likely Fermi-surface reconstruction associated with\nthe loss of magnetic ordering. Finally, at $p \\sim$ 4 GPa, both Ni-substituted\nCaK(Fe$_{1-x}$Ni$_{x}$)$_4$As$_4$ samples likely go through a\nhalf-collapsed-tetragonal (hcT) phase transition, similar to the parent\ncompound CaKFe$_4$As$_4$.\n",
        "  Flat surfaces that correspond to $k$-differentials on compact Riemann\nsurfaces are of finite area provided there is no pole of order $k$ or higher.\nWe denote by \\textit{flat surfaces with poles of higher order} those surfaces\nwith flat structures defined by a $k$-differential with at least one pole of\norder at least $k$. Flat surfaces with poles of higher order have different\ngeometrical and dynamical properties than usual flat surfaces of finite area.\nIn particular, they can have a finite number of saddle connections. We give\nlower and upper bounds for the number of saddle connections and related\nquantities. In the case $k=1\\ or\\ 2$, we provide a combinatorial\ncharacterization of the strata for which there can be an infinite number of\nsaddle connections.\n",
        "  Users suffering from mental health conditions often turn to online resources\nfor support, including specialized online support communities or general\ncommunities such as Twitter and Reddit. In this work, we present a neural\nframework for supporting and studying users in both types of communities. We\npropose methods for identifying posts in support communities that may indicate\na risk of self-harm, and demonstrate that our approach outperforms strong\npreviously proposed methods for identifying such posts. Self-harm is closely\nrelated to depression, which makes identifying depressed users on general\nforums a crucial related task. We introduce a large-scale general forum dataset\n(\"RSDD\") consisting of users with self-reported depression diagnoses matched\nwith control users. We show how our method can be applied to effectively\nidentify depressed users from their use of language alone. We demonstrate that\nour method outperforms strong baselines on this general forum dataset.\n",
        "  NoSQL databases are becoming increasingly popular as more developers seek new\nways for storing information. The popularity of these databases has risen due\nto their flexibility and scalability needed in domains like Big Data and Cloud\nComputing. This paper examines asynchronous replication, one of the key\nfeatures for a scalable and flexible system. Three of the most popular\nDocument-Oriented Databases, MongoDB, CouchDB, and Couchbase, are examined. For\ntesting, the execution time for CRUD operations for a single database instance\nand for a distributed environment with two nodes is taken into account and the\nresults are compared with tests outcomes obtained for three relational database\nmanagement systems: Microsoft SQL Server, MySQL, and PostgreSQL.\n",
        "  Many successful approaches to semantic parsing build on top of the syntactic\nanalysis of text, and make use of distributional representations or statistical\nmodels to match parses to ontology-specific queries. This paper presents a\nnovel deep learning architecture which provides a semantic parsing system\nthrough the union of two neural models of language semantics. It allows for the\ngeneration of ontology-specific queries from natural language statements and\nquestions without the need for parsing, which makes it especially suitable to\ngrammatically malformed or syntactically atypical text, such as tweets, as well\nas permitting the development of semantic parsers for resource-poor languages.\n",
        "  We give a concrete example of an infinite sequence of $(p_n, q_n)$-lens\nspaces $L(p_n, q_n)$ with natural triangulations $T(p_n, q_n)$ with $p_n$\ntaterahedra such that $L(p_n, q_n)$ contains a certain non-orientable closed\nsurface which is fundamental with respect to $T(p_n, q_n)$ and of minimal\ncrosscap number among all closed non-orientable surfaces in $L(p_n, q_n)$ and\nhas $n-2$ parallel sheets of normal disks of a quadrilateral type disjoint from\nthe pair of core circles of $L(p_n, q_n)$. Actually, we can set $p_0=0, q_0=1,\np_{k+1}=3p_k+2q_k$ and $q_{k+1}=p_k+q_k$.\n",
        "  Galaxies covering several orders of magnitude in stellar mass and a variety\nof Hubble types have been shown to follow the \"Radial Acceleration Relation\"\n(RAR), a relationship between $g_{\\rm obs}$, the observed circular acceleration\nof the galaxy, and $g_{\\rm bar}$, the acceleration due to the total baryonic\nmass of the galaxy. For accelerations above $10^{10}~{\\rm m \\, s}^{-2}$,\n$g_{\\rm obs}$ traces $g_{\\rm bar}$, asymptoting to the 1:1 line. Below this\nscale, there is a break in the relation such that $\\rm g_{\\rm obs} \\sim g_{\\rm\nbar}^{1/2}$. We show that the RAR slope, scatter and the acceleration scale are\nall natural consequences of the well-known baryonic Tully-Fisher relation\n(BTFR). We further demonstrate that galaxies with a variety of baryonic and\ndark matter (DM) profiles and a wide range of dark halo and galaxy properties\n(well beyond those expected in CDM) lie on the RAR if we simply require that\ntheir rotation curves satisfy the BTFR. We explore conditions needed to break\nthis degeneracy: sub-kpc resolved rotation curves inside of \"cored\"\nDM-dominated profiles and/or outside $\\gg 100\\,$kpc could lie on BTFR but\ndeviate in the RAR, providing new constraints on DM.\n",
        "  The zero temperature phase diagram of Cooper pairs exposed to disorder and\nmagnetic field is found to exhibit four distinct phases: a Bose and a Fermi\ninsulating, a metallic and a superconducting phase, respectively. The results\nexplain the giant negative magneto-resistance found experimentally in In-O,\nTiN, Bi and high-$T_c$ materials.\n",
        "  Evolutionary dynamics and patterns of molecular evolution are strongly\ninfluenced by selection on linked regions of the genome, but our quantitative\nunderstanding of these effects remains incomplete. Recent work has focused on\npredicting the distribution of fitness within an evolving population, and this\nforms the basis for several methods that leverage the fitness distribution to\npredict the patterns of genetic diversity when selection is strong. However, in\nweakly selected populations random fluctuations due to genetic drift are more\nsevere, and neither the distribution of fitness nor the sequence diversity\nwithin the population are well understood. Here, we briefly review the\nmotivations behind the fitness-distribution picture, and summarize the general\napproaches that have been used to analyze this distribution in the\nstrong-selection regime. We then extend these approaches to the case of weak\nselection, by outlining a perturbative treatment of selection at a large number\nof linked sites. This allows us to quantify the stochastic behavior of the\nfitness distribution and yields exact analytical predictions for the sequence\ndiversity and substitution rate in the limit that selection is weak.\n",
        "  We present a detailed study of the electrical transport properties of a\nrecently discovered iron-based superconductor:\nSm$_4$Fe$_2$As$_2$Te$_{0.72}$O$_{2.8}$F$_{1.2}$. We followed the temperature\ndependence of the upper critical field by resistivity measurement of single\ncrystals in magnetic fields up to 16 T, oriented along the two main\ncrystallographic directions. This material exhibits a zero-temperature upper\ncritical field of 90 T and 65 T parallel and perpendicular to the Fe$_2$As$_2$\nplanes, respectively. An unprecedented superconducting magnetic anisotropy\n$\\gamma_H=H_{c2}^{ab}/H_{c2}^c \\sim 14$ is observed near Tc, and it decreases\nat lower temperatures as expected in multiband superconductors. Direct\nmeasurement of the electronic anisotropy was performed on microfabricated\nsamples, showing a value of $\\rho_c/\\rho_{ab}(300K) \\sim 5$ that raises up to\n19 near Tc. Finally, we have studied the pressure and temperature dependence of\nthe in-plane resistivity. The critical temperature decreases linearly upon\napplication of hydrostatic pressure (up to 2 GPa) similarly to overdoped\ncuprate superconductors. The resistivity shows saturation at high temperatures,\nsuggesting that the material approaches the Mott-Ioffe-Regel limit for metallic\nconduction. Indeed, we have successfully modelled the resistivity in the normal\nstate with a parallel resistor model that is widely accepted for this state.\nAll the measured quantities suggest strong pressure dependence of the density\nof states.\n",
        "  Empathy, as defined in behavioral sciences, expresses the ability of human\nbeings to recognize, understand and react to emotions, attitudes and beliefs of\nothers. The lack of an operational definition of empathy makes it difficult to\nmeasure it. In this paper, we address two related problems in automatic\naffective behavior analysis: the design of the annotation protocol and the\nautomatic recognition of empathy from spoken conversations. We propose and\nevaluate an annotation scheme for empathy inspired by the modal model of\nemotions. The annotation scheme was evaluated on a corpus of real-life, dyadic\nspoken conversations. In the context of behavioral analysis, we designed an\nautomatic segmentation and classification system for empathy. Given the\ndifferent speech and language levels of representation where empathy may be\ncommunicated, we investigated features derived from the lexical and acoustic\nspaces. The feature development process was designed to support both the fusion\nand automatic selection of relevant features from high dimensional space. The\nautomatic classification system was evaluated on call center conversations\nwhere it showed significantly better performance than the baseline.\n",
        "  For an oriented surface link $S$, we can take a satellite construction called\na 2-dimensional braid over $S$, which is a surface link in the form of a\ncovering over $S$. We demonstrate that 2-dimensional braids over surface links\nare useful for showing the distinctness of surface links. We investigate\nnon-trivial examples of surface links with free abelian group of rank two,\nconcluding that their link types are infinitely many.\n",
        "  This paper describes the monomodal and multimodal Neural Machine Translation\nsystems developed by LIUM and CVC for WMT17 Shared Task on Multimodal\nTranslation. We mainly explored two multimodal architectures where either\nglobal visual features or convolutional feature maps are integrated in order to\nbenefit from visual context. Our final systems ranked first for both En-De and\nEn-Fr language pairs according to the automatic evaluation metrics METEOR and\nBLEU.\n",
        "  While conversing with chatbots, humans typically tend to ask many questions,\na significant portion of which can be answered by referring to large-scale\nknowledge graphs (KG). While Question Answering (QA) and dialog systems have\nbeen studied independently, there is a need to study them closely to evaluate\nsuch real-world scenarios faced by bots involving both these tasks. Towards\nthis end, we introduce the task of Complex Sequential QA which combines the two\ntasks of (i) answering factual questions through complex inferencing over a\nrealistic-sized KG of millions of entities, and (ii) learning to converse\nthrough a series of coherently linked QA pairs. Through a labor intensive\nsemi-automatic process, involving in-house and crowdsourced workers, we created\na dataset containing around 200K dialogs with a total of 1.6M turns. Further,\nunlike existing large scale QA datasets which contain simple questions that can\nbe answered from a single tuple, the questions in our dialogs require a larger\nsubgraph of the KG. Specifically, our dataset has questions which require\nlogical, quantitative, and comparative reasoning as well as their combinations.\nThis calls for models which can: (i) parse complex natural language questions,\n(ii) use conversation context to resolve coreferences and ellipsis in\nutterances, (iii) ask for clarifications for ambiguous queries, and finally\n(iv) retrieve relevant subgraphs of the KG to answer such questions. However,\nour experiments with a combination of state of the art dialog and QA models\nshow that they clearly do not achieve the above objectives and are inadequate\nfor dealing with such complex real world settings. We believe that this new\ndataset coupled with the limitations of existing models as reported in this\npaper should encourage further research in Complex Sequential QA.\n",
        "  Multidimensional databases are a great asset for decision making. Their users\nexpress complex OLAP (On-Line Analytical Processing) queries, often returning\nhuge volumes of facts, sometimes providing little or no information.\nFurthermore, due to the huge volume of historical data stored in DWs, the OLAP\napplications may return a big amount of irrelevant information that could make\nthe data exploration process not efficient and tardy. OLAP personalization\nsystems play a major role in reducing the effort of decision-makers to find the\nmost interesting information. Several works dealing with OLAP personalization\nwere presented in the last few years. This paper aims to provide a\ncomprehensive review of literature on OLAP personalization approaches. A\nbenchmarking study of OLAP personalization methods is proposed. Several\nevaluation criteria are used to identify the existence of trends as well as\npotential needs for further investigations.\n",
        "  Aims. The aim of this paper is to study deuterated water in the solar-type\nprotostars NGC1333 IRAS4A and IRAS4B, to compare their HDO abundance\ndistribution with other star-forming regions, and to constrain their HDO/H2O\nratios. Methods. Using the Herschel/HIFI instrument as well as ground-based\ntelescopes, we observed several HDO lines covering a large excitation range\n(Eup/k=22-168 K) towards these protostars and an outflow position. Non-LTE\nradiative transfer codes were then used to determine the HDO abundance profiles\nin these sources. Results. The HDO fundamental line profiles show a very broad\ncomponent, tracing the molecular outflows, in addition to a narrower emission\ncomponent and a narrow absorbing component. In the protostellar envelope of\nNGC1333 IRAS4A, the HDO inner (T>100 K) and outer (T<100 K) abundances with\nrespect to H2 are estimated at 7.5x10^{-9} and 1.2x10^{-11}, respectively,\nwhereas, in NGC1333 IRAS4B, they are 1.0x10^{-8} and 1.2x10^{-10},\nrespectively. Similarly to the low-mass protostar IRAS16293-2422, an absorbing\nouter layer with an enhanced abundance of deuterated water is required to\nreproduce the absorbing components seen in the fundamental lines at 465 and 894\nGHz in both sources. This water-rich layer is probably extended enough to\nencompass the two sources as well as parts of the outflows. In the outflows\nemanating from NGC1333 IRAS4A, the HDO column density is estimated at about\n(2-4)x10^{13} cm^{-2}, leading to an abundance of about (0.7-1.9)x10^{-9}. An\nHDO/H2O ratio between 7x10^{-4} and 9x10^{-2} is derived in the outflows. In\nthe warm inner regions of these two sources, we estimate the HDO/H2O ratios at\nabout 1x10^{-4}-4x10^{-3}. This ratio seems higher (a few %) in the cold\nenvelope of IRAS4A, whose possible origin is discussed in relation to formation\nprocesses of HDO and H2O.\n",
        "  Sequence-to-sequence models have shown strong performance across a broad\nrange of applications. However, their application to parsing and generating\ntext usingAbstract Meaning Representation (AMR)has been limited, due to the\nrelatively limited amount of labeled data and the non-sequential nature of the\nAMR graphs. We present a novel training procedure that can lift this limitation\nusing millions of unlabeled sentences and careful preprocessing of the AMR\ngraphs. For AMR parsing, our model achieves competitive results of 62.1SMATCH,\nthe current best score reported without significant use of external semantic\nresources. For AMR generation, our model establishes a new state-of-the-art\nperformance of BLEU 33.8. We present extensive ablative and qualitative\nanalysis including strong evidence that sequence-based AMR models are robust\nagainst ordering variations of graph-to-sequence conversions.\n",
        "  Motivated by the study of ribbon knots we explore symmetric unions, a\nbeautiful construction introduced by Kinoshita and Terasaka in 1957. For\nsymmetric diagrams we develop a two-variable refinement $W_D(s,t)$ of the Jones\npolynomial that is invariant under symmetric Reidemeister moves. Here the two\nvariables $s$ and $t$ are associated to the two types of crossings,\nrespectively on and off the symmetry axis. From sample calculations we deduce\nthat a ribbon knot can have essentially distinct symmetric union presentations\neven if the partial knots are the same.\n  If $D$ is a symmetric union diagram representing a ribbon knot $K$, then the\npolynomial $W_D(s,t)$ nicely reflects the geometric properties of $K$. In\nparticular it elucidates the connection between the Jones polynomials of $K$\nand its partial knots $K_\\pm$: we obtain $W_D(t,t) = V_K(t)$ and $W_D(-1,t) =\nV_{K_-}(t) \\cdot V_{K_+}(t)$, which has the form of a symmetric product $f(t)\n\\cdot f(t^{-1})$ reminiscent of the Alexander polynomial of ribbon knots.\n",
        "  We present far-infrared spectral line observations of five galaxies from the\nLITTLE THINGS sample: DDO 69, DDO 70, DDO 75, DDO 155, and WLM. While most\nstudies of dwarfs focus on bright systems or starbursts due to observational\nconstraints, our data extend the observed parameter space into the regime of\nlow surface brightness dwarf galaxies with low metallicities and moderate star\nformation rates. Our targets were observed with Herschel at the [CII] 158um,\n[OI] 63um, [OIII] 88um, and NII 122um emission lines using the PACS\nSpectrometer. These high-resolution maps allow us for the first time to study\nthe far-infrared properties of these systems on the scales of larger\nstar-forming complexes. The spatial resolution in our maps, in combination with\nstar formation tracers, allows us to identify separate PDRs in some of the\nregions we observed. Our systems have widespread [CII] emission that is bright\nrelative to continuum, averaging near 0.5% of the total infrared budget -\nhigher than in solar-metallicity galaxies of other types. [NII] is weak,\nsuggesting that the [CII] emission in our galaxies comes mostly from PDRs\ninstead of the diffuse ionized ISM. These systems exhibit efficient cooling at\nlow dust temperatures, as shown by ([OI]+[CII])/TIR in relation to 60um/100um,\nand low [OI]/[CII] ratios which indicate that [CII] is the dominant coolant of\nthe ISM. We observe [OIII]/[CII] ratios in our galaxies that are lower than\nthose published for other dwarfs, but similar to levels noted in spirals.\n",
        "  In this work we implement a training of a Language Model (LM), using\nRecurrent Neural Network (RNN) and GloVe word embeddings, introduced by\nPennigton et al. in [1]. The implementation is following the general idea of\ntraining RNNs for LM tasks presented in [2], but is rather using Gated\nRecurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM\n[4].\n",
        "  We present a general scenario for high-temperature superconducting cuprates,\nbased on the presence of dynamical charge density waves (CDWs) and to the\noccurrence of a CDW quantum critical point, which occurs, e.g., at doping\np~0.16 in YBCO. In this framework, even the pseudogap temperature T* is\ninterpreted in terms of a reduction of the density of states due to incipient\nCDW and, at lower temperature to the possible formation of incoherent\nsuperconducting pairs. The dynamically fluctuating character of CDW accounts\nfor the different temperatures at which the pseudogap (T*), the CDW onset\nrevealed by X-ray scattering (T_{ons}(p)), and the static three-dimensional CDW\nordering appear. We also investigate the anisotropic character of the\nCDW-mediated scattering. We find that this is strongly anisotropic only close\nto the CDW quantum critical point (QCP) at low temperature and very low energy.\nIt rapidly becomes nearly isotropic and marginal-Fermi-liquid-like away from\nthe CDW QCP and at finite (even rather small) energies. This may reconcile the\ninterpretation of Hall measurements in terms of anisotropic CDW scattering with\nrecent photoemission experiments.\n",
        "  Motivated by a recent paper of Gabai on the Whitehead contractible\n3-manifold, we investigate contractible manifolds $M^n$ which decompose or\nsplit as $M^n = A \\cup_C B$ where $A,B,C \\approx \\mathbb{R}^n$ or $A,B,C\n\\approx \\mathbb{B}^n$. Of particular interest to us is the case $n=4.$ Our main\nresults exhibit large collections of $4$-manifolds that split in this manner.\n",
        "  Statistical properties of the site frequency spectrum associated with\nLambda-coalescents are our objects of study. In particular, we derive\nrecursions for the expected value, variance, and covariance of the spectrum,\nextending earlier results of Fu (1995) for the classical Kingman coalescent.\nEstimating coalescent parameters introduced by certain Lambda-coalescents for\ndatasets too large for full likelihood methods is our focus. The recursions for\nthe expected values we obtain can be used to find the parameter values which\ngive the best fit to the observed frequency spectrum. The expected values are\nalso used to approximate the probability a (derived) mutation arises on a\nbranch subtending a given number of leaves (DNA sequences), allowing us to\napply a pseudo-likelihood inference to estimate coalescence parameters\nassociated with certain subclasses of Lambda coalescents. The properties of the\npseudo-likelihood approach are investigated on simulated as well as real mtDNA\ndatasets for the high fecundity Atlantic cod (\\emph{Gadus morhua}). Our results\nfor two subclasses of Lambda coalescents show that one can distinguish these\nsubclasses from the Kingman coalescent, as well as between the\nLambda-subclasses, even for moderate sample sizes.\n",
        "  From social networks to language modeling, the growing scale and importance\nof graph data has driven the development of numerous new graph-parallel systems\n(e.g., Pregel, GraphLab). By restricting the computation that can be expressed\nand introducing new techniques to partition and distribute the graph, these\nsystems can efficiently execute iterative graph algorithms orders of magnitude\nfaster than more general data-parallel systems. However, the same restrictions\nthat enable the performance gains also make it difficult to express many of the\nimportant stages in a typical graph-analytics pipeline: constructing the graph,\nmodifying its structure, or expressing computation that spans multiple graphs.\nAs a consequence, existing graph analytics pipelines compose graph-parallel and\ndata-parallel systems using external storage systems, leading to extensive data\nmovement and complicated programming model.\n  To address these challenges we introduce GraphX, a distributed graph\ncomputation framework that unifies graph-parallel and data-parallel\ncomputation. GraphX provides a small, core set of graph-parallel operators\nexpressive enough to implement the Pregel and PowerGraph abstractions, yet\nsimple enough to be cast in relational algebra. GraphX uses a collection of\nquery optimization techniques such as automatic join rewrites to efficiently\nimplement these graph-parallel operators. We evaluate GraphX on real-world\ngraphs and workloads and demonstrate that GraphX achieves comparable\nperformance as specialized graph computation systems, while outperforming them\nin end-to-end graph pipelines. Moreover, GraphX achieves a balance between\nexpressiveness, performance, and ease of use.\n",
        "  Due to the ever increasing importance of the internet, interoperability of\nheterogeneous data sources is as well of ever increasing importance.\nInteroperability can be achieved e.g. through data integration and data\nexchange. Common to both approaches is the need for the DBMS to be able to\nstore and query incomplete databases. In this report we present PossDB, a DBMS\ncapable of storing and querying incomplete databases. The system is wrapper\nover PostgreSQL, and the query language is an extension of a subset of standard\nSQL. Our experimental results show that our system scales well, actually better\nthan comparable systems.\n",
        "  We study quotients $\\Gamma\\backslash \\mathbb H^n$ of the $n$-fold product of\nthe upper half plane $\\mathbb H$ by irreducible and torsion-free lattices\n$\\Gamma < PSL_2(\\mathbb R)^n$ with the same Betti numbers as the $n$-fold\nproduct $(\\mathbb P^1)^n$ of projective lines. Such varieties are called fake\nproducts of projective lines or fake $(\\mathbb P^1)^n$. These are higher\ndimensional analogs of fake quadrics. In this paper we show that the number of\nfake $(\\mathbb P^1)^n$ is finite (independently of $n$), we give examples of\nfake $(\\mathbb P^1)^4$ and show that for $n>4$ there are no fake $(\\mathbb\nP^1)^n$ of the form $\\Gamma\\backslash \\mathbb H^n$ with $\\Gamma$ contained in\nthe norm-1 group of a maximal order of a quaternion algebra over a real number\nfield.\n",
        "  A brief review of unconventional superconductivity is given, stretching from\nthe halcyon days of helium-3 to the modern world of Majorana fermions. Along\nthe way, we will encounter such strange beasts as heavy fermion\nsuperconductors, cuprates, and their iron-based cousins. Emphasis will be put\non the fact that in almost all cases, an accepted microscopic theory has yet to\nemerge. This is attributed to the difficulty of constructing a theory of\nsuperconductivity outside the Migdal-Eliashberg framework.\n",
        "  We introduce a toy model of ecosystem assembly for which we are able to map\nout all assembly pathways generated by external invasions. The model allows to\ndisplay the whole phase space in the form of an assembly graph whose nodes are\ncommunities of species and whose directed links are transitions between them\ninduced by invasions. We characterize the process as a finite Markov chain and\nprove that it exhibits a unique set of recurrent states (the endstate of the\nprocess), which is therefore resistant to invasions. This also shows that the\nendstate is independent on the assembly history. The model shares all features\nwith standard assembly models reported in the literature, with the advantage\nthat all observables can be computed in an exact manner.\n",
        "  We study the incompressible surfaces in the exterior of a cable knot and use\nthis to compute the representativity and waist of most cable knots.\n",
        "  A problem of impedance mismatch between applications written in OO languages\nand relational DB is not a problem of discrepancy between object-oriented and\nrelational approaches themselves. Its real causes can be found in usual\nimplementation of the OO approach. Direct comparison of the two approaches\ncannot be used as a base for the conclusion that they are discrepant or\nmismatched. Experimental proof of absence of contradiction between\nobject-oriented paradigm and relational data model is also presented\n",
        "  Video reviews are the natural evolution of written product reviews. In this\npaper we target this phenomenon and introduce the first dataset created from\nclosed captions of YouTube product review videos as well as a new attention-RNN\nmodel for aspect extraction and joint aspect extraction and sentiment\nclassification. Our model provides state-of-the-art performance on aspect\nextraction without requiring the usage of hand-crafted features on the SemEval\nABSA corpus, while it outperforms the baseline on the joint task. In our\ndataset, the attention-RNN model outperforms the baseline for both tasks, but\nwe observe important performance drops for all models in comparison to SemEval.\nThese results, as well as further experiments on domain adaptation for aspect\nextraction, suggest that differences between speech and written text, which\nhave been discussed extensively in the literature, also extend to the domain of\nproduct reviews, where they are relevant for fine-grained opinion mining.\n",
        "  Results from the simulations of a Compton gamma camera based on compact\nconfiguration of detectors consisting in two detection modules, each of them\nhaving two stages of high-resolution position- and energy-sensitive radiation\ndetectors operated in time-coincidence are presented. Monolithic scintillation\ncrystals instead of pixelated crystals in order to reduce dead areas have been\nsimulated. In order to study the system feasibility to produce real-time\nimages, different setups are considered. Performance in terms of acquisition\ntimes have been calculated to determine the real-time capabilities and\nlimitations of such a system.\n",
        "  pymorphy2 is a morphological analyzer and generator for Russian and Ukrainian\nlanguages. It uses large efficiently encoded lexi- cons built from OpenCorpora\nand LanguageTool data. A set of linguistically motivated rules is developed to\nenable morphological analysis and generation of out-of-vocabulary words\nobserved in real-world documents. For Russian pymorphy2 provides\nstate-of-the-arts morphological analysis quality. The analyzer is implemented\nin Python programming language with optional C++ extensions. Emphasis is put on\nease of use, documentation and extensibility. The package is distributed under\na permissive open-source license, encouraging its use in both academic and\ncommercial setting.\n",
        "  RDF is increasingly being used to encode data for the semantic web and for\ndata exchange. There have been a large number of works that address RDF data\nmanagement. In this paper we provide an overview of these works.\n",
        "  Superconducting properties of hypothetical simple hexagonal CaB2 are studied\nusing the fully anisotropic Eliashberg formalism based on electronic and\nphononic structures and electron-phonon interactions which are obtained from ab\ninitio pseudopotential density functional calculations. The superconducting\ntransition temperature Tc, the superconducting energy gap Delta(k) on the Fermi\nsurface, and the specific heat are obtained and compared with corresponding\nproperties of MgB2. Our results suggest that CaB2 will have a higher Tc and a\nstronger two-gap nature, with a larger Delta(k) in the sigma bands but a\nsmaller Delta(k) in the pi bands than MgB2.\n",
        "  We present a dynamical model for gas transport, star formation, and winds in\nthe nuclear regions of galaxies, focusing on the Milky Way's Central Molecular\nZone (CMZ). In our model angular momentum and mass are transported by a\ncombination of gravitational and bar-driven acoustic instabilities. In\ngravitationally-unstable regions the gas can form stars, and the resulting\nfeedback drives both turbulence and a wind that ejects mass from the CMZ. We\nshow that the CMZ is in a quasi-steady state where mass deposited at large\nradii by the bar is transported inward to a star-forming, ring-shaped region at\n$\\sim 100$ pc from the Galactic Centre, where the shear reaches a minimum. This\nring undergoes episodic starbursts, with bursts lasting $\\sim 5-10$ Myr\noccurring at $\\sim 20-40$ Myr intervals. During quiescence the gas in the ring\nis not fully cleared, but is driven out of a self-gravitating state by the\nmomentum injected by expanding supernova remnants. Starbursts also drive a wind\noff the star-forming ring, with a time-averaged mass flux comparable to the\nstar formation rate. We show that our model agrees well with the observed\nproperties of the CMZ, and places it near a star formation minimum within the\nevolutionary cycle. We argue that such cycles of bursty star formation and\nwinds should be ubiquitous in the nuclei of barred spiral galaxies, and show\nthat the resulting distribution of galactic nuclei on the Kennicutt-Schmidt\nrelation is in good agreement with that observed in nearby galaxies.\n",
        "  In this note we give a new lower bound on the virtual crossing number via the\nwrithe polynomial, which refines a result of B. Mellor. The proof is based on a\nnew interpretation of the writhe polynomial. The characterization of the writhe\npolynomial is also discussed.\n",
        "  It is a theorem of Casson and Rivin that the complete hyperbolic metric on a\ncusp end ideal triangulated 3-manifold maximizes volume in the space of all\npositive angle structures. We show that the conclusion still holds if some of\nthe tetrahedra in the complete metric are flat.\n",
        "  We present an experimental approach using magnetic force microscopy for\nmeasurements of the absolute value of the magnetic penetration depth $\\lambda$\nin superconductors. $\\lambda$ is obtained in a simple and robust way without\nintroducing any tip modeling procedure via direct comparison of the Meissner\nresponse curves for a material of interest to those measured on a reference\nsample. Using a well-characterized Nb film as a reference, we determine the\nabsolute value of $\\lambda$ in a Ba(Fe$_{0.92}$Co$_{0.08}$)$_{2}$As$_{2}$\nsingle crystal and a MgB$_2$ thin film through a comparative experiment. Our\napparatus features simultaneous loading of multiple samples, and allows\nstraightforward measurement of the absolute value of $\\lambda$ in\nsuperconducting thin film or single crystal samples.\n",
        "  Drug-drug interaction (DDI) is a vital information when physicians and\npharmacists intend to co-administer two or more drugs. Thus, several DDI\ndatabases are constructed to avoid mistakenly combined use. In recent years,\nautomatically extracting DDIs from biomedical text has drawn researchers'\nattention. However, the existing work utilize either complex feature\nengineering or NLP tools, both of which are insufficient for sentence\ncomprehension. Inspired by the deep learning approaches in natural language\nprocessing, we propose a recur- rent neural network model with multiple\nattention layers for DDI classification. We evaluate our model on 2013 SemEval\nDDIExtraction dataset. The experiments show that our model classifies most of\nthe drug pairs into correct DDI categories, which outperforms the existing NLP\nor deep learning methods.\n",
        "  We report the design and test of Reciprocal Quantum Logic shift-register\nyield vehicles consisting of up to 72,800 Josephson junction devices per die,\nthe largest digital superconducting circuits ever reported. Multiple physical\nlayout styles were matched to the MIT Lincoln Laboratory foundry, which\nsupports processes with both four and eight metal layers and minimum feature\nsize of 0.5 {\\mu}m. The largest individual circuits with 40,400 junctions\nindicate large operating margins of $\\pm$20% on AC clock amplitude. In one case\nthe data were reproducible to the accuracy of the measurement, $\\pm$1% across\nfive thermal cycles using only the rudimentary precautions of passive mu-metal\nmagnetic shielding and a controlled cool-down rate of 3 mK/s in the test\nfixture. We conclude that with proper mitigation techniques, flux-trapping is\nno longer a limiting consideration for very-large-scale-integration of\nsuperconductor digital logic.\n",
        "  This work is to develop a general framework, namely analytical iterative\nreconstruction (AIR) method, to incorporate analytical reconstruction (AR)\nmethod into iterative reconstruction (IR) method, for enhanced CT image quality\nand reconstruction efficiency. Specifically, AIR is established based on the\nmodified proximal forward-backward splitting (PFBS) algorithm, and its\nconnection to the filtered data fidelity with sparsity regularization is\ndiscussed. As a result, AIR decouples data fidelity and image regularization\nwith a two-step iterative scheme, during which an AR-projection step updates\nthe filtered data fidelity term, while a denoising solver updates the sparsity\nregularization term. During the AR-projection step, the image is projected to\nthe data domain to form the data residual, and then reconstructed by certain AR\nto a residual image which is then weighted together with previous image iterate\nto form next image iterate. Intuitively since the eigenvalues of AR-projection\noperator are close to the unity, PFBS based AIR has a fast convergence. Such an\nadvantage is rigorously established through convergence analysis and numerical\ncomputation of convergence rate. The proposed AIR method is validated in the\nsetting of circular cone-beam CT with AR being FDK and total-variation sparsity\nregularization, and has improved image quality from both AR and IR. For\nexample, AIR has improved visual assessment and quantitative measurement in\nterms of both contrast and resolution, and reduced axial and half-fan\nartifacts.\n",
        "  We explore a knot invariant derived from colorings of corresponding\n$1$-tangles with arbitrary connected quandles. When the quandle is an abelian\nextension of a certain type the invariant is equivalent to the quandle\n$2$-cocycle invariant. We construct many such abelian extensions using\ngeneralized Alexander quandles without explicitly finding $2$-cocycles. This\npermits the construction of many $2$-cocycle invariants without exhibiting\nexplicit $2$-cocycles. We show that for connected generalized Alexander\nquandles the invariant is equivalent to Eisermann's knot coloring polynomial.\nComputations using this technique show that the $2$-cocycle invariant\ndistinguishes all of the oriented prime knots up to 11 crossings and most\noriented prime knots with 12 crosssings including classification by symmetry:\nmirror images, reversals, and reversed mirrors.\n",
        "  A promising supermassive black hole seed formation channel is that of direct\ncollapse from primordial gas clouds. We perform a suite of 3D hydrodynamics\nsimulations of an isolated turbulent gas cloud to investigate conditions\nconducive to forming massive black hole seeds via direct collapse, probing the\nimpact of cloud metallicity, gas temperature floor and cooling physics on cloud\nfragmentation. We find there is no threshold in metallicity which produces a\nsharp drop in fragmentation. When molecular cooling is not present, metallicity\nhas little effect on fragmentation. When molecular cooling is present,\nfragmentation is suppressed by at most $\\sim 25\\%$, with the greatest\nsuppression seen at metallicities below $2\\%$ solar. A gas temperature floor\n$\\sim 10^{4}$K produces the largest drop in fragmentation of any parameter\nchoice, reducing fragmentation by $\\sim 60\\%$. At metallicities below $2\\%$\nsolar or at temperatures $\\sim 10^{3}$K we see a reduction in fragmentation\n$\\sim 20-25 \\%$. For a cloud of metallicity $2\\%$ solar above and a temperature\nbelow $10^3$K, the detailed choices of temperature floor, metallicity, and\ncooling physics have little impact on fragmentation.\n",
        "  We present a set of numerical experiments designed to systematically\ninvestigate how turbulence and magnetic fields influence the morphology,\nenergetics, and dynamics of filaments produced in wind-cloud interactions. We\ncover 3D magnetohydrodynamic systems of supersonic winds impacting clouds with\nturbulent density, velocity, and magnetic fields. We find that log-normal\ndensity distributions aid shock propagation through clouds, increasing their\nvelocity dispersion and producing filaments with expanded cross sections and\nhighly-magnetised knots and sub-filaments. In self-consistently turbulent\nscenarios the ratio of filament to initial cloud magnetic energy densities is\n~1. The effect of Gaussian velocity fields is bound to the turbulence Mach\nnumber: Supersonic velocities trigger a rapid cloud expansion; subsonic\nvelocities only have a minor impact. The role of turbulent magnetic fields\ndepends on their tension and is similar to the effect of radiative losses: the\nstronger the magnetic field or the softer the gas equation of state, the\ngreater the magnetic shielding at wind-filament interfaces and the suppression\nof Kelvin-Helmholtz instabilities. Overall, we show that including turbulence\nand magnetic fields is crucial to understanding cold gas entrainment in\nmulti-phase winds. While cloud porosity and supersonic turbulence enhance the\nacceleration of clouds, magnetic shielding protects them from ablation and\ncauses Rayleigh-Taylor-driven sub-filamentation. Wind-swept clouds in turbulent\nmodels reach distances ~15-20 times their core radius and acquire bulk speeds\n~0.3-0.4 of the wind speed in one cloud-crushing time, which are three times\nlarger than in non-turbulent models. In all simulations the ratio of turbulent\nmagnetic to kinetic energy densities asymptotes at ~0.1-0.4, and convergence of\nall relevant dynamical properties requires at least 64 cells per cloud radius.\n",
        "  We investigate the interplay between gap oscillations and damping in the\ndynamics of superconductors taken out of equilibrium by strong optical pulses\nwith sub-gap Terahertz frequencies. A semi-phenomenological formalism is\ndeveloped to include the damping within the electronic subsystem that arises\nfrom effects beyond BCS, such as interactions between Bogoliubov quasiparticles\nand decay of the Higgs mode. Such processes are conveniently expressed as\n$T_{1}$ and $T_{2}$ times in the standard pseudospin language for\nsuperconductors. Comparing with data on NbN that we report here, we argue that\nthe superconducting dynamics in the picosecond time scale, after the pump is\nturned off, is governed by the $T_{2}$ process.\n",
        "  We review Giroux's contact handles and contact handle attachments in\ndimension three and show that a bypass attachment consists of a pair of contact\n1 and 2-handles. As an application we describe explicit contact handle\ndecompositions of infinitely many pairwise non-isotopic overtwisted 3-spheres.\nWe also give an alternative proof of the fact that every compact contact\n3-manifold (closed or with convex boundary) admits a contact handle\ndecomposition, which is a result originally due to Giroux.\n",
        "  Spatial data mining or Knowledge discovery in spatial database is the\nextraction of implicit knowledge, spatial relations and spatial patterns that\nare not explicitly stored in databases. Co-location patterns discovery is the\nprocess of finding the subsets of features that are frequently located together\nin the same geographic area. In this paper, we discuss the different approaches\nlike Rule based approach, Join-less approach, Partial Join approach and\nConstraint neighborhood based approach for finding co-location patterns.\n",
        "  In last few years, the volume of the data has grown manyfold. The data\nstorages have been inundated by various disparate potential data outlets,\nleading by social media such as Facebook, Twitter, etc. The existing data\nmodels are largely unable to illuminate the full potential of Big Data; the\ninformation that may serve as the key solution to several complex problems is\nleft unexplored. The existing computation capacity falls short for the\nincreasingly expanded storage capacity. The fast-paced volume expansion of the\nunorganized data entails a complete paradigm shift in new age data computation\nand witnesses the evolution of new capable data engineering techniques such as\ncapture, curation, visualization, analyses, etc. In this paper, we provide the\nfirst level classification for modern Big Data models. Some of the leading\nrepresentatives of each classification that claim to best process the Big Data\nin reliable and efficient way are also discussed. Also, the classification is\nfurther strengthened by the intra-class and inter-class comparisons and\ndiscussions of the undertaken Big Data models.\n",
        "  Most of the organizations put information on the web because they want it to\nbe seen by the world. Their goal is to have visitors come to the site, feel\ncomfortable and stay a while and try to know completely about the running\norganization. As educational system increasingly requires data mining, the\nopportunity arises to mine the resulting large amounts of student information\nfor hidden useful information (patterns like rule, clustering, and\nclassification, etc). The education domain offers ground for many interesting\nand challenging data mining applications like astronomy, chemistry,\nengineering, climate studies, geology, oceanography, ecology, physics, biology,\nhealth sciences and computer science. Collecting the interesting patterns using\nthe required interestingness measures, which help us in discovering the\nsophisticated patterns that are ultimately used for developing the site. We\nstudy the application of data mining to educational log data collected from\nGuru Nanak Institute of Technology, Ibrahimpatnam, India. We have proposed a\ncustom-built apriori algorithm to find the effective pattern analysis. Finally,\nanalyzing web logs for usage and access trends can not only provide important\ninformation to web site developers and administrators, but also help in\ncreating adaptive web sites.\n",
        "  In Multimodal Neural Machine Translation (MNMT), a neural model generates a\ntranslated sentence that describes an image, given the image itself and one\nsource descriptions in English. This is considered as the multimodal image\ncaption translation task. The images are processed with Convolutional Neural\nNetwork (CNN) to extract visual features exploitable by the translation model.\nSo far, the CNNs used are pre-trained on object detection and localization\ntask. We hypothesize that richer architecture, such as dense captioning models,\nmay be more suitable for MNMT and could lead to improved translations. We\nextend this intuition to the word-embeddings, where we compute both linguistic\nand visual representation for our corpus vocabulary. We combine and compare\ndifferent confi\n",
        "  In this paper we describe a new approach to data modelling called the\nconcept-oriented model (CoM). This model is based on the formalism of nested\nordered sets which uses inclusion relation to produce hierarchical structure of\nsets and ordering relation to produce multi-dimensional structure among its\nelements. Nested ordered set is defined as an ordered set where an each element\ncan be itself an ordered set. Ordering relation in CoM is used to define data\nsemantics and operations with data such as projection and de-projection. This\ndata model can be applied to very different problems and the paper describes\nsome its uses such grouping with aggregation and multi-dimensional analysis.\n",
        "  Electron tomography has been studied in various fields. Various methods have\nbeen developed to align projection sets to construct ideally focused\nreconstruction. In this paper, we present how to align the projection set to\ndistinguish whether it has an ideal sinogram pattern or not by removing\ntranslation errors and vertical tilt errors. We also analyze some important\nproperties for certain types of samples to identify whether the reconstruction\nimage can be made through an ideal sinogram pattern. We provide a guideline for\nhow to construct a better reconstruction image by scanning the sample through\nthese properties.\n",
        "  Galactic nuclei are expected to be densely populated with stellar and\nintermediate mass black holes. Exploring this population will have important\nconsequences for the observation prospects of gravitational waves as well as\nunderstanding galactic evolution. The gas cloud G2 currently approaching Sgr A*\nprovides an unprecedented opportunity to probe the black hole and neutron star\npopulation of the Galactic nucleus. We examine the possibility of a G2-black\nhole encounter and its detectability with current X-ray satellites, such as\nChandra and NuSTAR. We find that multiple encounters are likely to occur close\nto the pericenter, which may be detectable upon favorable circumstances. This\nopportunity provides an additional, important science case for leading X-ray\nobservatories to closely follow G2 on its way to the nucleus.\n",
        "  This study elaborates some examples of a simple evolutionary stochastic rate\nprocess where the population rate of change depends on the distribution of\nproperties--so different cohorts change at different rates. We investigate the\neffect on the evolution arising from parametrized perturbations of uniformity\nfor the initial inhomogeneity. The information geometric neighbourhood system\nyields also solutions for a wide range of other initial inhomogeneity\ndistributions, including approximations to truncated Gaussians of arbitrarily\nsmall variance and distributions with pronounced extreme values. It is found\nthat, under quite considerable alterations in the shape and variance of the\ninitial distribution of inhomogeneity in unfitness, the decline of the mean\ndoes change markedly with the variation in starting conditions, but the net\npopulation evolution seems surprisingly stable.\n",
        "  Most pattern mining methods output a very large number of frequent patterns\nand isolating a small but relevant subset is a challenging problem of current\ninterest in frequent pattern mining. In this paper we consider discovery of a\nsmall set of relevant frequent episodes from data sequences. We make use of the\nMinimum Description Length principle to formulate the problem of selecting a\nsubset of episodes. Using an interesting class of serial episodes with\ninter-event constraints and a novel encoding scheme for data using such\nepisodes, we present algorithms for discovering small set of episodes that\nachieve good data compression. Using an example of the data streams obtained\nfrom distributed sensors in a composable coupled conveyor system, we show that\nour method is very effective in unearthing highly relevant episodes and that\nour scheme also achieves good data compression.\n",
        "  Genetic drift is stochastic fluctuations of alleles frequencies in a\npopulation due to sampling effects. We consider a model of drift in an\nequilibrium population, with high mutation rates: few functional mutations per\ngeneration. Such mutation rates are common in multicellular organisms including\nhumans, however they are not explicitly considered in most population genetics\nmodels. Under these assumptions the drift shows properties distinct from the\nclassical drift models, which ignore realistic mutation rates: i) All\n(non-lethal) variants of a site have a characteristic average frequencies,\nwhich are independent of population size, however the magnitude of fluctuations\naround these frequencies depends on population size. ii) There is no\n\"mutational meltdown\" due to \"low efficiency of selection\" for small population\nsize. Population average fitness does not depend on population size. iii) Drift\n(and molecular clock) can be represented as wandering by compensatory\nmutations, postulate of neutral mutations is not necessary for explaining the\nhigh rate of mutation accumulation. Our results, which adjust the meaning of\nthe neutral theory from the individual neutrality of the majority of mutations,\nto the collective neutrality of compensatory mutations, are applicable to\ninvestigations in phylogeny and coalescent and for GWAS design and analysis.\n",
        "  We theoretically study the magnetic field orientation dependence of\nlongitudinal and transverse flux line lattice form factors in uniaxial\nsuperconductors with anisotropy ratio corresponding to YBa_2_Cu_3_O_{7-delta}.\nWe discuss influences of the anisotropy ratio of coherence length, and\ndifferences between the s-wave and the d_{x^2-y^2}-wave pairings. The\ncalculations are performed by two methods, the Eilenberger theory and the\nLondon theory comparatively, and we study the cutoff function of the extended\nLondon theory, which will be helpful in the analysis of the small angle neutron\nscattering in the vortex states.\n",
        "  Molecular clock (MC) is a central concept of molecular evolution according to\nwhich each gene evolves at a characteristic, near constant rate. Numerous\nevolutionary studies have demonstrated the validity of MC but also have shown\nthat MC is substantially overdispersed, i.e. lineage-specific deviations of the\nevolutionary rate of the given gene from the clock greatly exceed the\nexpectation from the sampling error. A fundamental observation of comparative\ngenomics that appears to complement the MC is that the distribution of\nevolution rates across orthologous genes in pairs of related genomes remains\nvirtually unchanged throughout the evolution of life, from bacteria to mammals.\nThe conservation of this distribution implies that the relative evolution rates\nof all genes remain nearly constant, or in other words, that evolutionary rates\nof different genes are strongly correlated within each evolving genome. We\nhypothesized that this correlation is not a simple consequence of MC but could\nbe better explained by a model we dubbed Universal PaceMaker (UPM) of genome\nevolution. The UPM model posits that the rate of evolution changes\nsynchronously across genome-wide sets of genes in all evolving lineages. We\nsought to differentiate between the MC and UPM models by fitting thousands of\nphylogenetic trees for bacterial and archaeal genes to supertrees that reflect\nthe dominant trend of vertical descent in the evolution of archaea and bacteria\nand that were constrained according to the two models. The goodness of fit for\nthe UPM model was better than the fit for the MC model, with overwhelming\nstatistical significance. These results reveal a universal pacemaker of genome\nevolution that could have been in operation throughout the history of life.\n",
        "  In this article we focus firstly on the principle of pedagogical indexing and\ncharacteristics of Arabic language and secondly on the possibility of adapting\nthe standard for describing learning resources used (the LOM and its\nApplication Profiles) with learning conditions such as the educational levels\nof students and their levels of understanding,... the educational context with\ntaking into account the representative elements of text, text length, ... in\nparticular, we put in relief the specificity of the Arabic language which is a\ncomplex language, characterized by its flexion, its voyellation and\nagglutination.\n",
        "  Amyotrophic lateral sclerosis (ALS) is the most common adult motor neuron\ndisease, causing motor neuron degeneration, muscle atrophy, paralysis, and\ndeath. Despite this degenerative process, a stable hypermetabolic state has\nbeen observed in a large subset of patients. Mice expressing a mutant form of\nCu/Zn-superoxide dismutase (mSOD1 mice) constitute an animal model of ALS that,\nlike patients, exhibits unexpectedly increased energy expenditure.\nCounterbalancing for this increase with a high-fat diet extends lifespan and\nprevents motor neuron loss. Here, we investigated whether lipid metabolism is\ndefective in this animal model. Hepatic lipid metabolism was roughly normal,\nwhereas gastrointestinal absorption of lipids as well as peripheral clearance\nof triglyceride-rich lipoproteins were markedly increased, leading to decreased\npostprandial lipidemia. This defect was corrected by the high-fat regimen that\ntypically induces neuroprotection in these animals. Together, our findings show\nthat energy metabolism in mSOD1 mice shifts toward an increase in the\nperipheral use of lipids. This metabolic shift probably accounts for the\nprotective effect of dietary lipids in this model.\n",
        "  In the setting of finite type invariants for null-homologous knots in\nrational homology 3-spheres with respect to null Lagrangian-preserving\nsurgeries, there are two candidates to be universal invariants, defined\nrespectively by Kricker and Lescop. In a previous paper, the second author\ndefined maps between spaces of Jacobi diagrams. Injectivity for these maps\nwould imply that Kricker and Lescop invariants are indeed universal invariants;\nthis would prove in particular that these two invariants are equivalent. In the\npresent paper, we investigate the injectivity status of these maps for degree 2\ninvariants, in the case of knots whose Blanchfield modules are direct sums of\nisomorphic Blanchfield modules of Q-dimension two. We prove that they are\nalways injective except in one case, for which we determine explicitly the\nkernel.\n",
        "  It is known that annotating named entities in unstructured and\nsemi-structured data sets by their concepts improves the effectiveness of\nanswering queries over these data sets. As every enterprise has a limited\nbudget of time or computational resources, it has to annotate a subset of\nconcepts in a given domain whose costs of annotation do not exceed the budget.\nWe call such a subset of concepts a {\\it conceptual design} for the annotated\ndata set. We focus on finding a conceptual design that provides the most\neffective answers to queries over the annotated data set, i.e., a {\\it\ncost-effective conceptual design}. Since, it is often less time-consuming and\ncostly to annotate general concepts than specific concepts, we use information\non superclass/subclass relationships between concepts in taxonomies to find a\ncost-effective conceptual design. We quantify the amount by which a conceptual\ndesign with concepts from a taxonomy improves the effectiveness of answering\nqueries over an annotated data set. If the taxonomy is a tree, we prove that\nthe problem is NP-hard and propose an efficient approximation and\npseudo-polynomial time algorithms for the problem. We further prove that if the\ntaxonomy is a directed acyclic graph, given some generally accepted hypothesis,\nit is not possible to find any approximation algorithm with reasonably small\napproximation ratio for the problem. Our empirical study using real-world data\nsets, taxonomies, and query workloads shows that our framework effectively\nquantifies the amount by which a conceptual design improves the effectiveness\nof answering queries. It also indicates that our algorithms are efficient for a\ndesign-time task with pseudo-polynomial algorithm being generally more\neffective than the approximation algorithm.\n",
        "  Halls of Fame are fascinating constructs. They represent the elite of an\noften very large amount of entities---persons, companies, products, countries\netc. Beyond their practical use as static rankings, changes to them are\nparticularly interesting---for decision making processes, as input to common\nmedia or novel narrative science applications, or simply consumed by users. In\nthis work, we aim at detecting events that can be characterized by changes to a\nHall of Fame ranking in an automated way. We describe how the schema and data\nof a database can be used to generate Halls of Fame. In this database scenario,\nby Hall of Fame we refer to distinguished tuples; entities, whose\ncharacteristics set them apart from the majority. We define every Hall of Fame\nas one specific instance of an SQL query, such that a change in its result is\nconsidered a noteworthy event. Identified changes (i.e., events) are ranked\nusing lexicographic tradeoffs over event and query properties and presented to\nusers or fed in higher-level applications. We have implemented a full-fledged\nprototype system that uses either database triggers or a Java based middleware\nfor event identification. We report on an experimental evaluation using a\nreal-world dataset of basketball statistics.\n",
        "  In this report, we comprehensively study the effect of H$^+$ irradiation on\nthe critical current density, $J_c$, and vortex pinning in FeSe single crystal.\nIt is found that the value of $J_c$ for FeSe is enhanced more than twice after\n3-MeV H$^+$ irradiation. The scaling analyses of the vortex pinning force based\non the Dew-Hughes model reveal that the H$^+$ irradiation successfully\nintroduce point pinning centers into the crystal. We also find that the vortex\ncreep rates are strongly suppressed after irradiation. Detailed analyses of the\ncritical current dependent pinning energy based on the collective creep theory\nand extend Maley's method show that the H$^+$ irradiation enhances the value of\n$J_c$ before the flux creep, and also reduces the size of flux bundle, which\nwill further reduce the field dependence of $J_c$ due to vortex motion.\n",
        "  Data quality and data cleaning are context dependent activities. Starting\nfrom this observation, in previous work a context model for the assessment of\nthe quality of a database instance was proposed. In that framework, the context\ntakes the form of a possibly virtual database or data integration system into\nwhich a database instance under quality assessment is mapped, for additional\nanalysis and processing, enabling quality assessment. In this work we extend\ncontexts with dimensions, and by doing so, we make possible a multidimensional\nassessment of data quality assessment. Multidimensional contexts are\nrepresented as ontologies written in Datalog+-. We use this language for\nrepresenting dimensional constraints, and dimensional rules, and also for doing\nquery answering based on dimensional navigation, which becomes an important\nauxiliary activity in the assessment of data. We show ideas and mechanisms by\nmeans of examples.\n",
        "  This paper reviews the state-of-the-art of semantic change computation, one\nemerging research field in computational linguistics, proposing a framework\nthat summarizes the literature by identifying and expounding five essential\ncomponents in the field: diachronic corpus, diachronic word sense\ncharacterization, change modelling, evaluation data and data visualization.\nDespite the potential of the field, the review shows that current studies are\nmainly focused on testifying hypotheses proposed in theoretical linguistics and\nthat several core issues remain to be solved: the need for diachronic corpora\nof languages other than English, the need for comprehensive evaluation data for\nevaluation, the comparison and construction of approaches to diachronic word\nsense characterization and change modelling, and further exploration of data\nvisualization techniques for hypothesis justification.\n",
        "  Purpose: To develop a fast magnetic resonance fingerprinting (MRF) method for\nquantitative chemical exchange saturation transfer (CEST) imaging.\n  Methods: We implemented a CEST-MRF method to quantify the chemical exchange\nrate and volume fraction of the N${\\alpha}$-amine protons of L-arginine (L-Arg)\nphantoms and the amide and semi-solid exchangeable protons of in vivo rat brain\ntissue. L-Arg phantoms were made with different concentrations (25-100 mM) and\npH (pH 4-6). The MRF acquisition schedule varied the saturation power randomly\nfor 30 iterations (phantom: 0-6 ${\\mu}$T; in vivo: 0-4 ${\\mu}$T) with a total\nacquisition time of <=2 minutes. The signal trajectories were pattern-matched\nto a large dictionary of signal trajectories simulated using the\nBloch-McConnell equations for different combinations of exchange rate,\nexchangeable proton volume fraction, and water T1 and T2* relaxation times.\n  Results: The chemical exchange rates of the N${\\alpha}$-amine protons of\nL-Arg were significantly (p<0.0001) correlated with the rates measured with the\nQuantitation of Exchange using Saturation Power method. Similarly, the L-Arg\nconcentrations determined using MRF were significantly (p<0.0001) correlated\nwith the known concentrations. The pH dependence of the exchange rate was well\nfit (R2=0.9186) by a base catalyzed exchange model. The amide proton exchange\nrate measured in rat brain cortex (36.3+-12.9 Hz) was in good agreement with\nthat measured previously with the Water Exchange spectroscopy method (28.6+-7.4\nHz). The semi-solid proton volume fraction was elevated in white (11.2+-1.7%)\ncompared to gray (7.6+-1.8%) matter brain regions in agreement with previous\nmagnetization transfer studies.\n  Conclusion: CEST-MRF provides a method for fast, quantitative CEST imaging.\n",
        "  In this work, we present a simple and elegant approach to language modeling\nfor bilingual code-switched text. Since code-switching is a blend of two or\nmore different languages, a standard bilingual language model can be improved\nupon by using structures of the monolingual language models. We propose a novel\ntechnique called dual language models, which involves building two\ncomplementary monolingual language models and combining them using a\nprobabilistic model for switching between the two. We evaluate the efficacy of\nour approach using a conversational Mandarin-English speech corpus. We prove\nthe robustness of our model by showing significant improvements in perplexity\nmeasures over the standard bilingual language model without the use of any\nexternal information. Similar consistent improvements are also reflected in\nautomatic speech recognition error rates.\n",
        "  Preprocessing tools for automated text analysis have become more widely\navailable in major languages, but non-English tools are often still limited in\ntheir functionality. When working with Spanish-language text, researchers can\neasily find tools for tokenization and stemming, but may not have the means to\nextract more complex word features like verb tense or mood. Yet Spanish is a\nmorphologically rich language in which such features are often identifiable\nfrom word form. Conjugation rules are consistent, but many special verbs and\nnouns take on different rules. While building a complete dictionary of known\nwords and their morphological rules would be labor intensive, resources to do\nso already exist, in spell checkers designed to generate valid forms of known\nwords. This paper introduces a set of tools for Spanish-language morphological\nanalysis, built using the COES spell checking tools, to label person, mood,\ntense, gender and number, derive a word's root noun or verb infinitive, and\nconvert verbs to their nominal form.\n",
        "  Resistivities of single-crystalline as well as poly-crystalline samples of\nCeOBiS2 without fluorine doping were measured at temperatures down to 0.13 K,\nand were compared with those of poly-crystalline LaOBiS2 and PrOBiS2. Both\npoly-crystalline and single-crystalline CeOBiS2 exhibited zero resistivity\nbelow 1.2 K while poly-crystalline LaOBiS2 and PrOBiS2 did not show zero\nresistivity down to 0.13 K. Superconducting transition temperature of CeOBiS2\nwas reduced by increasing the applied current density. The superconductivity of\nCeOBiS2 without chemical doping is likely triggered by the carriers induced by\nthe valence fluctuation between Ce3+ and Ce4+.\n",
        "  This paper studies the mutation-selection balance in three simplified\nreplication models. The first model considers a population of organisms\nreplicating via the production of asexual spores. The second model considers a\nsexually replicating population that produces identical gametes. The third\nmodel considers a sexually replicating population that produces distinct sperm\nand egg gametes. All models assume diploid organisms whose genomes consist of\ntwo chromosomes, each of which is taken to be functional if equal to some\nmaster sequence, and defective otherwise. In the asexual population, the\nasexual diploid spores develop directly into adult organisms. In the sexual\npopulations, the haploid gametes enter a haploid pool, where they may fuse with\nother haploids. The resulting immature diploid organisms then proceed to\ndevelop into mature organisms. Based on an analysis of all three models, we\nfind that, as organism size increases, a sexually replicating population can\nonly outcompete an asexually replicating population if the adult organisms\nproduce distinct sperm and egg gametes. A sexual replication strategy that is\nbased on the production of large numbers of sperm cells to fertilize a small\nnumber of eggs is found to be necessary in order to maintain a sufficiently low\ncost for sex for the strategy to be selected for over a purely asexual\nstrategy. We discuss the usefulness of this model in understanding the\nevolution and maintenance of sexual replication as the preferred replication\nstrategy in complex, multicellular organisms.\n",
        "  Given an $m$-component link $L$ in $S^3$ ($m \\ge 2$), we construct a family\nof links which are link homotopic, but not link isotopic, to $L$. Every proper\nsublink of such a link is link isotopic to the corresponding sublink of $L$.\nMoreover, if $L$ is an unlink then there exist links that in addition to the\nabove properties have all Milnor invariants zero.\n",
        "  In this paper, a 3D discrete model is presented to model the movements of the\ntrunk during breathing. In this model, objects are represented by physical\nparticles on their contours. A simple notion of force generated by a linear\nactuator allows the model to create forces on each particle by way of a\ngeometrical attractor. Tissue elasticity and contractility are modeled by local\nshape memory and muscular fibers attractors. A specific dynamic MRI study was\nused to build a simple trunk model comprised of by three compartments: lungs,\ndiaphragm and abdomen. This model was registered on the real geometry.\nSimulation results were compared qualitatively as well as quantitatively to the\nexperimental data, in terms of volume and geometry. A good correlation was\nobtained between the model and the real data. Thanks to this model, pathology\nsuch as hemidiaphragm paralysis can also be simulated.\n",
        "  Phylogenetic networks are rooted, labelled directed acyclic graphs which are\ncommonly used to represent reticulate evolution. There is a close relationship\nbetween phylogenetic networks and multi-labelled trees (MUL-trees). Indeed, any\nphylogenetic network $N$ can be 'unfolded' to obtain a MUL-tree $U(N)$ and,\nconversely, a MUL-tree $T$ can in certain circumstances be 'folded' to obtain a\nphylogenetic network $F(T)$ that exhibits $T$. In this paper, we study\nproperties of the operations $U$ and $F$ in more detail. In particular, we\nintroduce the class of stable networks, phylogenetic networks $N$ for which\n$F(U(N))$ is isomorphic to $N$, characterise such networks, and show that that\nthey are related to the well-known class of tree-sibling networks. We also\nexplore how the concept of displaying a tree in a network $N$ can be related to\ndisplaying the tree in the MUL-tree $U(N)$. To do this, we develop a\nphylogenetic analogue of graph fibrations. This allows us to view $U(N)$ as the\nanalogue of the universal cover of a digraph, and to establish a close\nconnection between displaying trees in $U(N)$ and reconciling phylogenetic\ntrees with networks.\n",
        "  We report a theoretical study of the macroscopic quantum tunneling (MQT) in\nsmall Josephson junctions containing randomly distributed two-level systems. We\nfocus on the magnetic field dependent crossover temperature $T_{cr}$ between\nthe thermal fluctuation and quantum regimes of switching from the\nsuperconducting (the zero-voltage) state to a resistive one. In the absence of\ntwo-levels systems the crossover temperature shows a smooth decrease with an\napplied magnetic field characterized by an external flux $\\Phi$. Beyond that we\npredict a narrow peak in the dependence of $T_{cr}(\\Phi)$ occurring in the\nintermediate range of $\\Phi$. The effect becomes more pronounced as the\njunction size increases. We explain this effect quantitatively by a strong\nresonant suppression of a potential barrier for the Josephson phase escape that\nis due to the coherent quantum Rabi oscillations in two-level systems present\nin the junction.\n",
        "  DR21(OH) is a pc-scale massive, 7000 Msun clump hosting three massive dense\ncores (MDCs) at an early stage of their evolution. We present a high\nangular-resolution mosaic, covering 70\" by 100\", with the IRAM PdBI at 3 mm to\ntrace the dust continuum emission and the N2H+ (J=1-0) and CH3CN (J=5-4)\nmolecular emission. The cold, dense gas traced by the compact emission in N2H+\nis associated with the three MDCs and shows several velocity components towards\neach MDC. These velocity components reveal local shears in the velocity fields\nwhich are best interpreted as convergent flows. Moreover, we report the\ndetection of weak extended emission from CH3CN at the position of the N2H+\nvelocity shears. We propose that this extended CH3CN emission is tracing warm\ngas associated with the low-velocity shocks expected at the location of\nconvergence of the flows where velocity shears are observed. This is the first\ndetection of low-velocity shocks associated with small (sub-parsec) scale\nconvergent flows which are proposed to be at the origin of the densest\nstructures and of the formation of (high-mass) stars. In addition, we propose\nthat MDCs may be active sites of star-formation for more than a crossing time\nas they continuously receive material from larger scale flows as suggested by\nthe global picture of dynamical, gravity driven evolution of massive clumps\nwhich is favored by the present observations.\n",
        "  We present an exact, closed expression for the expected neutral Site\nFrequency Spectrum for two neutral sites, 2-SFS, without recombination. This\nspectrum is the immediate extension of the well known single site $\\theta/f$\nneutral SFS. Similar formulae are also provided for the case of the expected\nSFS of sites that are linked to a focal neutral mutation of known frequency.\nFormulae for finite samples are obtained by coalescent methods and remarkably\nsimple expressions are derived for the SFS of a large population, which are\nalso solutions of the multi-allelic Kolmogorov equations. Besides the general\ninterest of these new spectra, they relate to interesting biological cases such\nas structural variants and introgressions. As an example, we present the\nexpected neutral frequency spectrum of regions with a chromosomal inversion.\n",
        "  Inelastic neutron scattering measurements on Ba(Fe0.925Mn0.075)2As2 manifest\nspin fluctuations at two different wavevectors in the Fe square lattice,\n(1/2,0) and (1/2,1/2), corresponding to the expected stripe spin-density wave\norder and checkerboard antiferromagnetic order, respectively. Below T_N=80 K,\nlong-range stripe magnetic ordering occurs and sharp spin wave excitations\nappear at (1/2,0) while broad and diffusive spin fluctuations remain at\n(1/2,1/2) at all temperatures. Low concentrations of Mn dopants nucleate local\nmoment spin fluctuations at (1/2,1/2) that compete with itinerant spin\nfluctuations at (1/2,0) and may disrupt the development of superconductivity.\n",
        "  Dynamical properties of spherically symmetric galaxy models where both the\nstellar and total mass density distributions are described by the Jaffe (1983)\nprofile (with different scale-lenghts and masses), are presented. The orbital\nstructure of the stellar component is described by Osipkov--Merritt anisotropy,\nand a black hole (BH) is added at the center of the galaxy; the dark matter\nhalo is isotropic. First, the conditions required to have a nowhere negative\nand monothonically decreasing dark matter halo density profile, are derived. We\nthen show that the phase-space distribution function can be recovered by using\nthe Lambert-Euler $W$ function, while in absence of the central BH only\nelementary functions appears in the integrand of the inversion formula. The\nminimum value of the anisotropy radius for consistency is derived in terms of\nthe galaxy parameters. The Jeans equations for the stellar component are solved\nanalytically, and the projected velocity dispersion at the center and at large\nradii are also obtained analytically for generic values of the anisotropy\nradius. Finally, the relevant global quantities entering the Virial Theorem are\ncomputed analytically, and the fiducial anisotropy limit required to prevent\nthe onset of Radial Orbit Instability is determined as a function of the galaxy\nparameters. The presented models, even though highly idealized, represent a\nsubstantial generalization of the models presentd in Ciotti et al. (2009), and\ncan be useful as starting point for more advanced modeling the dynamics and the\nmass distribution of elliptical galaxies.\n",
        "  We investigate galactic winds driven by supernova (SN) explosions in an\nisolated dwarf galaxy using high-resolution (particle mass $m_{\\rm gas} = 1{\\rm\nM_\\odot}$, number of neighbor $N_{\\rm ngb} = 100$) smoothed-particle\nhydrodynamics simulations that include non-equilibrium cooling and chemistry,\nindividual star formation, stellar feedback and metal enrichment. Clustered SNe\nlead to the formation of superbubbles which break out of the disk and vent out\nhot gas, launching the winds. We find much weaker winds than what cosmological\nsimulations typically adopt at this mass scale. At the virial radius, the\ntime-averaged loading factors of mass, momentum and energy are 3, 1 and 0.05,\nrespectively, and the metal enrichment factor is 1.5. Winds that escape the\nhalo consist of two populations that differ in their launching temperatures.\nHot gas acquires enough kinetic energy to escape when launched while warm gas\ndoes not. However, warm gas can be further accelerated by the ram pressure of\nthe subsequently launched hot gas and eventually escape. The strong\ninteractions between different temperature phases highlight the caveat of\nextrapolating properties of warm gas to large distances based on its local\nconditions (e.g. the Bernoulli parameter). Our convergence study finds that\nwind properties converge when the cooling masses of individual SNe are\nresolved, which corresponds to $m_{\\rm gas}=5 {\\rm M_\\odot}$ with an injection\nmass of $500 {\\rm M_\\odot}$. The winds weaken dramatically once the SNe become\nunresolved. We demonstrate that injecting the terminal momentum of SNe, a\npopular sub-grid model in the literature, fails to capture SN winds\nirrespective of the inclusion of residual thermal energy.\n",
        "  We show that for any two values $\\alpha, \\beta >0 $ for which\n$\\alpha+\\beta>1$ then there is a value $N$ so that for all $n \\geq N$ the\nfollowing holds. For any binary phylogenetic tree $T$ on $n$ leaves there is a\nset of $\\lfloor n^\\alpha \\rfloor$ characters that capture $T$, and for which\neach character takes at most $\\lfloor n^\\beta \\rfloor$ distinct states. Here\n`capture' means that $T$ is the unique perfect phylogeny for these characters.\nOur short proof of this combinatorial result is based on the probabilistic\nmethod.\n",
        "  To date no direct detection of Lyman continuum emission has been measured for\nintermediate--redshift z~1 star-forming galaxies . We combine HST grism\nspectroscopy with GALEX UV and ground--based optical imaging to extend the\nsearch for escaping Lyman continuum to a large (~600) sample of z~1 low-mass,\nmoderately star-forming galaxies selected initially on H$\\alpha$ emission. The\ncharacteristic escape fraction of LyC from SFGs that populate this parameter\nspace remains weakly constrained by previous surveys, but these faint SFGs are\nassumed to play a significant role in the reionization of neutral hydrogen in\nthe intergalactic medium (IGM) at high redshift (z>6). We do not make an\nunambiguous detection of escaping LyC radiation from this $z\\sim1$ sample,\nindividual non--detections to constrain the absolute Lyman continuum escape\nfraction, $f_{esc}$<2.1% (3$\\sigma$). We measure upper limits of $f_{esc}$<9.6%\nfrom a sample of SFGs selected on high H$\\alpha$ equivalent width (EW>200\\AA),\nwhich are thought to be close analogs of high redshift sources of reionization.\nFor reference, we also present an emissivity--weighted escape fraction which is\nuseful as a measurement of the general contribution of the SFGs to the z~1\nionizing UV background. In the discussion, we consider the implications of\nthese intermediate redshift constraints for the reionization of hydrogen in the\nintergalactic medium at high (z > 6) redshift. If we assume our $z\\sim1$ SFGs,\nfor which we measure this emissivity-weighted $f_{esc}$, are analogs to the\nhigh redshift sources of reionization, we find is difficult reconcile\nreionization by faint (M$_{UV}<-13$) SFGs with a low escape fraction\n($f_{esc}<$3%), with constraints from independent high redshift observations.\nIf $f_{esc}$ evolves with redshift, reionization by SFGs may be consistent with\nobservations from Planck.\n",
        "  Finite subdivision rules in high dimensions can be difficult to visualize and\nrequire complex topological structures to be constructed explicitly. In many\napplications, only the history graph is needed. We characterize the history\ngraph of a subdivision rule, and define a combinatorial subdivision rule based\non such graphs. We use this to show that a finite subdivision rule of arbitrary\ndimension is combinatorially equivalent to a three-dimensional subdivision\nrule. We use this to show that the Gromov boundary of special cubulated\nhyperbolic groups is a quotient of a compact subset of three-dimensional space,\nwith connected preimages at each point.\n",
        "  We show that the minimum of asymptotic translation lengths of all\npoint-pushing pseudo-Anosov maps on any one punctured Riemann surface is one.\n",
        "  Fast expansion of natural language functionality of intelligent virtual\nagents is critical for achieving engaging and informative interactions.\nHowever, developing accurate models for new natural language domains is a time\nand data intensive process. We propose efficient deep neural network\narchitectures that maximally re-use available resources through transfer\nlearning. Our methods are applied for expanding the understanding capabilities\nof a popular commercial agent and are evaluated on hundreds of new domains,\ndesigned by internal or external developers. We demonstrate that our proposed\nmethods significantly increase accuracy in low resource settings and enable\nrapid development of accurate models with less data.\n",
        "  For applications in computing, Bezier curves are pervasive and are defined by\na piecewise linear curve L which is embedded in R^3 and yields a smooth\npolynomial curve C embedded in R^3. It is of interest to understand when L and\nC have the same embeddings. One class of counterexamples is shown for L being\nunknotted, while C is knotted. Another class of counterexamples is created\nwhere L is equilateral and simple, while C is self-intersecting. These\ncounterexamples were discovered using curve visualizing software and numerical\nalgorithms that produce general procedures to create more examples.\n",
        "  Multiple-breath-washout (MBW) measurements are regarded as a sensitive\ntechnique which can reflect the ventilation inhomogeneity of respiratory\nairways. Typically nitrogen is used as the tracer gas and is washed out by pure\noxygen in multi-breath-nitrogen (MBNW) washout tests. In this work, instead of\nusing nitrogen, helium is used as the tracer gas and a\nmultiple-helium-breath-washout (MBHW) system has been developed for the lung\nfunction study. A commercial quartz tuning fork with a resonance frequency of\n32768 Hz has been used for detecting the change of the respiratory gas density.\nThe resonance frequency of the tuning fork decreases linearly with increasing\ndensity of the surrounding gas. Knowing the CO2 concentration from the infrared\ncarbon dioxide detector, the helium concentration can be determined. Results\nfrom 12 volunteers (3 mild asthmatics, 2 smokers, 1 with asthma history, 1 with\nCOPD history, 5 normal) have shown that mild asthmatics have higher ventilation\ninhomogeneity in either conducting or acinar airways (or both). A feature has\nbeen found in single breath washout curve from 4 smokers with different length\nof smoking history which may indicate the early stage of respiratory\nventilation inhomogeneity.\n",
        "  The Kilo Degree survey (KiDS) is a large-scale optical imaging survey carried\nout with the VLT Survey Telescope (VST), which is the ideal tool for galaxy\nevolution studies. We expect to observe millions of galaxies for which we\nextract the structural parameters in four wavebands (u, g, r and i). This\nsample will represent the largest dataset with measured structural parameters\nup to a redshift $z=0.5$. In this paper we will introduce the sample, and\ndescribe the 2D fitting procedure using the 2DPHOT environment and the\nvalidation of the parameters with an external catalog.\n",
        "  Under a simple assumption on Seifert surfaces, we characterise knots whose\nstable topological 4-genus coincides with the genus.\n",
        "  The diffuse far-ultraviolet (FUV) continuum radiation \"longward\" of Ly-alpha\n(1216A) is well known to correlate with the dust emission at 100 um. However,\nit has been claimed that the FUV continuum background \"shortward\" of Ly-alpha\nshows very weak or no correlation with the 100 um emission. In this paper, the\nobservational data of the diffuse FUV radiation by the Far Ultraviolet\nSpectroscopic Explorer is reexamined in order to investigate the correlation\nbetween the diffuse FUV radiation shortward of Ly-alpha and the 100 um\nemission. Large fluctuations were confirmed in the linear-linear correlation\nplots, but good correlations were found in the log-log plots. The large\nfluctuations in the linear-linear plots, and thus poor correlations, between\nthe FUV and 100 um intensities were attributed to the lognormal property of the\nFUV intensity distribution. The standard deviation of the intensity\ndistribution of the FUV radiation shortward of Ly-alpha was found to be\nsigma_log I = 0.16-0.25. The result is consistent with that obtained not only\nfor the FUV radiation longward of 1216A, but also with the dust column density\nmeasurements of various molecular clouds. This implies that most of the diffuse\nFUV radiation shortward of Ly-alpha is dust-scattered light in the turbulent\ninterstellar medium. The diffuse FUV data obtained from the Voyager missions\nwas also investigated. However, much wider random fluctuations were found\ncompared with the FUSE data, which is most likely due to the systematic\ndifficulties in data reduction of the Voyager data.\n",
        "  The ubiquity of oscillations in epidemics presents a long standing challenge\nfor the formulation of epidemic models. Whether they are external and\nseasonally driven, or arise from the intrinsic dynamics is an open problem. It\nis known that fixed time delays destabilize the steady state solution of the\nstandard SIRS model, giving rise to stable oscillations for certain parameters\nvalues. In this contribution, starting from the classical SIRS model, we make a\ngeneral treatment of the recovery and loss of immunity terms. We present\noscillation diagrams (amplitude and period) in terms of the parameters of the\nmodel, showing how oscillations can be destabilized by the shape of the\ndistributions of the two characteristic (infectious and immune) times. The\nformulation is made in terms of delay equation which are both numerical\nintegrated and linearized. Results from simulation are included showing where\nthey support the linear analysis and explaining why not where they do not.\nConsiderations and comparison with real diseases are presented along.\n",
        "  In this paper we introduce a new type of pattern -- a flipping correlation\npattern. The flipping patterns are obtained from contrasting the correlations\nbetween items at different levels of abstraction. They represent surprising\ncorrelations, both positive and negative, which are specific for a given\nabstraction level, and which \"flip\" from positive to negative and vice versa\nwhen items are generalized to a higher level of abstraction. We design an\nefficient algorithm for finding flipping correlations, the Flipper algorithm,\nwhich outperforms naive pattern mining methods by several orders of magnitude.\nWe apply Flipper to real-life datasets and show that the discovered patterns\nare non-redundant, surprising and actionable. Flipper finds strong contrasting\ncorrelations in itemsets with low-to-medium support, while existing techniques\ncannot handle the pattern discovery in this frequency range.\n",
        "  The morphological, spectroscopic and kinematical properties of the warm\ninterstellar medium (wim) in early-type galaxies (ETGs) hold key observational\nconstraints to nuclear activity and the buildup history of these massive\nquiescent systems. High-quality integral field spectroscopy (IFS) data with a\nwide spectral and spatial coverage, such as those from the CALIFA survey, offer\na precious opportunity for advancing our understanding in this respect. We use\ndeep IFS data from CALIFA (califa.caha.es) to study the wim over the entire\nextent and optical spectral range of 32 nearby ETGs. We find that all ETGs in\nour sample show faint (H\\alpha\\ equivalent width EW~0.5...2 {\\AA}) extranuclear\nnebular emission extending out to >= 2 Petrosian_50 radii. Confirming and\nstrengthening our conclusions in Papaderos et al. (2013) we argue that ETGs\nspan a broad continuous sequence with regard to the properties of their wim,\nand they can be roughly subdivided into two characteristic classes. The first\none (type i) comprises ETGs with a nearly constant EW~1-3 {\\AA} in their\nextranuclear component, in quantitative agreement with (even though, no proof\nfor) the hypothesis of photoionization by pAGB stars. The second class (type\nii) consists of virtually wim-evacuated ETGs with a large Lyman continuum (Lyc)\nphoton escape fraction and a very low (<= 0.5 {\\AA}) EW in their nuclear zone.\nThese two classes appear indistinguishable from one another by their\nLINER-specific emission-line ratios. Additionally, here we extend the\nclassification by the class i+ which stands for a subset of type i ETGs with\nlow-level star-fomation in contiguous spiral-arm like features in their\noutermost periphery. These faint features, together with traces of localized\nstar formation in several type i&i+ systems point to a non-negligible\ncontribution from young massive stars to the global ionizing photon budget in\nETGs.\n",
        "  Electrical resistivities, Hall coefficients and thermoelectric powers have\nbeen measured for polycrystalline samples of LaFe1-yCoyAsO1-xFx (x=0.11) with\nvarious values of y. The results show that there exists clear distinction of\nthese transport behaviors between the superconducting and nonsuperconducting\nmetallic regions of y divided by the boundary value yc~0.05. We have found that\nthe behaviors in both regions are very similar to those of high-Tc Cu oxides in\nthe corresponding phases. If they reflect, as in the case of Cu oxides, effects\nof strong magnetic fluctuations, the energy scale of the fluctuations is\nconsidered to be smaller than that of the high Cu oxides by a factor of ~1/2.\nArguments on the electronic nature and superconducting symmetry are presented\non the basis of the observed small rate of the Tc suppression rate by the Co\ndoping.\n",
        "  The human immune system depends on a highly diverse collection of\nantibody-making B cells. B cell receptor sequence diversity is generated by a\nrandom recombination process called \"rearrangement\" forming progenitor B cells,\nthen a Darwinian process of lineage diversification and selection called\n\"affinity maturation.\" The resulting receptors can be sequenced in high\nthroughput for research and diagnostics. Such a collection of sequences\ncontains a mixture of various lineages, each of which may be quite numerous, or\nmay consist of only a single member. As a step to understanding the process and\nresult of this diversification, one may wish to reconstruct lineage membership,\ni.e. to cluster sampled sequences according to which came from the same\nrearrangement events. We call this clustering problem \"clonal family\ninference.\" In this paper we describe and validate a likelihood-based framework\nfor clonal family inference based on a multi-hidden Markov Model (multi-HMM)\nframework for B cell receptor sequences. We describe an agglomerative algorithm\nto find a maximum likelihood clustering, two approximate algorithms with\nvarious trade-offs of speed versus accuracy, and a third, fast algorithm for\nfinding specific lineages. We show that under simulation these algorithms\ngreatly improve upon existing clonal family inference methods, and that they\nalso give significantly different clusters than previous methods when applied\nto two real data sets.\n",
        "  We investigate the global neutral hydrogen (HI) content of isolated galaxies\nselected from the SDSS spectroscopic survey with optical evidence of Active\nGalactic Nuclei (AGN). Our sample includes galaxies with unresolved HI\nobservations from the ALFALFA 70% data release combined with deeper HI\nobservations of low-mass galaxies with 7.0 < log(M*) < 9.5. We examine the HI\nmasses of this sample using the distance from the star-forming sequence on the\nOIII\\Hb and NII\\Ha Baldwin Phillips Terlevich (BPT) diagram as a measurement of\nAGN activity. In agreement with previous studies, we find that, for galaxies\nwith log(M*) > 9.5, AGN activity does not correlate with the global HI content.\nHowever, for galaxies with 9.2 < log(M*) < 9.5, we identify a set of objects at\nlarge distances from the BPT star-forming sequence and lower than expected HI\nmasses. This gas-depleted sample is red in both g-r and NUV-r colors and\ncompact without distinguishable signs of star formation. This is surprising\nbecause the vast majority of isolated galaxies in this stellar mass regime are\nboth star-forming and gas-rich. These galaxies are greater than 1.5 Mpc from\nany massive galaxy, ruling out environmental processes as a source of the\ngas-depletion. We suggest that either black hole feedback or shocks from\nextremely bursty star formation cause the emission lines and have destroyed or\notherwise consumed the cold gas.\n",
        "  Phenotypic variability in a population of cells can work as the bet-hedging\nof the cells under an unpredictably changing environment, the typical example\nof which is the bacterial persistence. To understand the strategy to control\nsuch phenomena, it is indispensable to identify the phenotype of each cell and\nits inheritance. Although recent advancements in microfluidic technology offer\nus useful lineage data, they are insufficient to directly identify the\nphenotypes of the cells. An alternative approach is to infer the phenotype from\nthe lineage data by latent-variable estimation. To this end, however, we must\nresolve the bias problem in the inference from lineage called survivorship\nbias. In this work, we clarify how the survivor bias distorts statistical\nestimations. We then propose a latent-variable estimation algorithm without the\nsurvivorship bias from lineage trees based on an expectation-maximization (EM)\nalgorithm, which we call Lineage EM algorithm (LEM). LEM provides a statistical\nmethod to identify the traits of the cells applicable to various kinds of\nlineage data.\n",
        "  The Interstellar Boundary Explorer (IBEX) mission is exploring the frontiers\nof the heliosphere where energetic neutral atoms (ENAs) are formed from charge\nexchange between interstellar neutral hydrogen atoms and solar wind ions and\npickup ions. The geography of this frontier is dominated by an unexpected\nnearly complete arc of ENA emission, now known as the IBEX 'Ribbon'. While\nthere is no consensus agreement on the Ribbon formation mechanism, it seems\ncertain this feature is seen for sightlines that are perpendicular to the\ninterstellar magnetic field as it drapes over the heliosphere. At the lowest\nenergies, IBEX also measures the flow of interstellar H, He, and O atoms\nthrough the inner heliosphere. The asymmetric oxygen profile suggests that a\nsecondary flow of oxygen is present, such as would be expected if some fraction\nof oxygen is lost through charge exchange in the heliosheath regions. The\ndetailed spectra characterized by the ENAs provide time-tagged samples of the\nenergy distributions of the underlying ion distributions, and provide a wealth\nof information about the outer heliosphere regions, and beyond.\n",
        "  Suppose F is a compact orientable surface, K is a knot in F x I, and N is the\n3-manifold obtained by some non-trivial surgery on K. If F x {0} compresses in\nN, then there is an annulus in F x I with one end K and the other end an\nessential simple closed curve in F x {0}. Moreover, the end of the annulus at K\ndetermines the surgery slope.\n  An application: suppose M is a compact orientable 3-manifold that fibers over\nthe circle. If surgery on a knot K in M yields a reducible manifold, then\neither: the projection of K to S^1 has non-trivial winding number; or K lies in\na ball; or K lies in a fiber; or K is a cabled knot.\n",
        "  A population genetics model based on a multitype branching process, or\nequivalently a Galton-Watson branching process for multiple alleles, is pre-\nsented. The diffusion limit forward Kolmogorov equation is derived for the case\nof neutral mutations. The asymptotic stationary solution is obtained and has\nthe property that the extant population partitions into subpopulations whose\nrelative sizes are determined by mutation rates. An approximate time-dependent\nsolution is obtained in the limit of low mutation rates. This solution has the\nproperty that the system undergoes a rapid transition from a drift-dominated\nphase to a mutation-dominated phase in which the distribution collapses onto\nthe asymptotic stationary distribution. The changeover point of the transition\nis determined by the per-generation growth factor and mutation rate. The\napproximate solution is confirmed using numerical simulations.\n",
        "  Multiple epidemiological models have been proposed to predict the spread of\nEbola in West Africa. These models include consideration of counter-measures\nmeant to slow and, eventually, stop the spread of the disease. Here, we examine\none component of Ebola dynamics that is of growing concern -- the transmission\nof Ebola from the dead to the living. We do so by applying the toolkit of\nmathematical epidemiology to analyze the consequences of post-death\ntransmission. We show that underlying disease parameters cannot be inferred\nwith confidence from early-stage incidence data (that is, they are not\n\"identifiable\") because different parameter combinations can produce virtually\nthe same epidemic trajectory. Despite this identifiability problem, we find\nrobustly that inferences that don't account for post-death transmission tend to\nunderestimate the basic reproductive number -- thus, given the observed rate of\nepidemic growth, larger amounts of post-death transmission imply larger\nreproductive numbers. From a control perspective, we explain how improvements\nin reducing post-death transmission of Ebola may reduce the overall epidemic\nspread and scope substantially. Increased attention to the proportion of\npost-death transmission has the potential to aid both in projecting the course\nof the epidemic and in evaluating a portfolio of control strategies.\n",
        "  The effect of pinning on self-heating triggering the Larkin-Ovchinnikov (LO)\nflux-flow instability (FFI) in superconducting thin films is theoretically\ninvestigated. The problem is considered relying upon the Bezuglyj-Shklovskij\n(BS) generalization of the LO theory, accounting for a finite heat removal from\nthe quasiparticles at temperature $T^\\ast$ to the bath at temperature $T_0$.\nThe FFI critical parameters, namely the current density $j^{\\ast}$, the\nelectric field $E^{\\ast}$, the power density $P^{\\ast}$, and the vortex\nvelocity $v^{\\ast}$ are calculated and graphically analyzed as functions of the\nmagnetic field and the pinning strength. With increasing pinning strength at a\nfixed magnetic field value $E^{\\ast}$ \\emph{decreases}, $j^{\\ast}$\n\\emph{increases}, while $P^{\\ast}$ and $T^{\\ast}$ \\emph{remain practically\nconstant}. Vortex pinning may hence be the cause for eventual discrepancies\nbetween experiments on superconductors with \\emph{strong pinning} and the\ngeneralized LO and BS results.\n",
        "  A new family of Intensional RDBs (IRDBs), introduced in [1], extends the\ntraditional RDBs with the Big Data and flexible and 'Open schema' features,\nable to preserve the user-defined relational database schemas and all\npreexisting user's applications containing the SQL statements for a deployment\nof such a relational data. The standard RDB data is parsed into an internal\nvector key/value relation, so that we obtain a column representation of data\nused in Big Data applications, covering the key/value and column-based Big Data\napplications as well, into a unifying RDB framework. Such an IRDB architecture\nis adequate for the massive migrations from the existing slow RDBMSs into this\nnew family of fast IRDBMSs by offering a Big Data and new flexible schema\nfeatures as well. Here we present the interoperability features of the IRDBs by\npermitting the queries also over the internal vector relations created by\nparsing of each federated database in a given Multidatabase system. We show\nthat the SchemaLog with the second-order syntax and ad hoc Logic Programming\nand its querying fragment can be embedded into the standard SQL IRDBMSs, so\nthat we obtain a full interoperabilty features of IRDBs by using only the\nstandard relational SQL for querying both data and meta-data.\n",
        "  Lymph node metastasis (LNM) is a significant prognostic factor in patients\nwith head and neck cancer, and the ability to predict it accurately is\nessential for treatment optimization. PET and CT imaging are routinely used for\nLNM identification. However, uncertainties of LNM always exist especially for\nsmall size or reactive nodes. Radiomics and deep learning are the two preferred\nimaging-based strategies for node malignancy prediction. Radiomics models are\nbuilt based on handcrafted features, and deep learning can learn the features\nautomatically. We proposed a hybrid predictive model that combines\nmany-objective radiomics (MO-radiomics) and 3-dimensional convolutional neural\nnetwork (3D-CNN) through evidential reasoning (ER) approach. To build a more\nreliable model, we proposed a new many-objective radiomics model. Meanwhile, we\ndesigned a 3D-CNN that fully utilizes spatial contextual information. Finally,\nthe outputs were fused through the ER approach. To study the predictability of\nthe two modalities, three models were built for PET, CT, and PET&CT. The\nresults showed that the model performed best when the two modalities were\ncombined. Moreover, we showed that the quantitative results obtained from the\nhybrid model were better than those obtained from MO-radiomics and 3D-CNN.\n",
        "  A nearest-neighbor-interchange (NNI) walk is a sequence of unrooted\nphylogenetic trees, T_0, T_1, T_2,... where each consecutive pair of trees\ndiffer by a single NNI move. We give tight bounds on the length of the shortest\nNNI-walks that visit all trees in an subtree-prune-and-regraft (SPR)\nneighborhood of a given tree. For any unrooted, binary tree, T, on n leaves,\nthe shortest walk takes {\\theta}(n^2) additional steps than the number of trees\nin the SPR neighborhood. This answers Bryant's Second Combinatorial Conjecture\nfrom the Phylogenetics Challenges List, the Isaac Newton Institute, 2011, and\nthe Penny Ante Problem List, 2009.\n",
        "  This paper presents new alternatives to the well-known Bloom filter data\nstructure. The Bloom filter, a compact data structure supporting set insertion\nand membership queries, has found wide application in databases, storage\nsystems, and networks. Because the Bloom filter performs frequent random reads\nand writes, it is used almost exclusively in RAM, limiting the size of the sets\nit can represent. This paper first describes the quotient filter, which\nsupports the basic operations of the Bloom filter, achieving roughly comparable\nperformance in terms of space and time, but with better data locality.\nOperations on the quotient filter require only a small number of contiguous\naccesses. The quotient filter has other advantages over the Bloom filter: it\nsupports deletions, it can be dynamically resized, and two quotient filters can\nbe efficiently merged. The paper then gives two data structures, the buffered\nquotient filter and the cascade filter, which exploit the quotient filter\nadvantages and thus serve as SSD-optimized alternatives to the Bloom filter.\nThe cascade filter has better asymptotic I/O performance than the buffered\nquotient filter, but the buffered quotient filter outperforms the cascade\nfilter on small to medium data sets. Both data structures significantly\noutperform recently-proposed SSD-optimized Bloom filter variants, such as the\nelevator Bloom filter, buffered Bloom filter, and forest-structured Bloom\nfilter. In experiments, the cascade filter and buffered quotient filter\nperformed insertions 8.6-11 times faster than the fastest Bloom filter variant\nand performed lookups 0.94-2.56 times faster.\n",
        "  Purpose: To verify dose delivery and quality assurance of volumetric\nmodulated arc therapy (VMAT) for head and neck cancer.\n  Method: The Imaging and Radiation Oncology Core Houston (IROC-H) head and\nneck phantom with thermo- luminescent dosimeters (TLDs) and films, were imaged\nwith computed tomography scan and the reconstructed image was transferred to\npinnacle treatment planning system (TPS). On TPS the planning target volume\n(PTV), secondary target volume (STV) and organ at risk (OAR) were delineated\nmanually and a treatment plan was made. The dose constraints were determined\nfor the concerned organs according to IROC-H prescription. The treatment plan\nwas optimized using adoptive convolution algorithm to improve dose homogeneity\nand conformity. The dose calculation was performed using C.C Convolution\nalgorithm and a Varian True Beam linear accelerator was used to deliver the\ntreatment plan to the head and neck phantom. The delivered radiation dose to\nthe phantom was measured through TLDs and GafChromic EBT2 films. The dosimetric\nperformance of the VMAT delivery was studied by analysing percent dose\ndifference, iso-dose line profile and gamma analysis of the TPS computed dose\nand linac delivered doses.\n  Result: the percent dose difference of 3.8\\% was observed between the planned\nand measured doses of TLDs and a 1.5mm distance to agreement (DTA) was observed\nby comparing iso-dose line profiles. Passed the gamma criteria of 3\\%/3 mm was\nwith good percentages.\n  Conclusion: The dosimetric performance of VMAT delivery for a challenging\nhead and neck radiotherapy can be verified using TLDs and films imbedded in an\nanthropomorphic H\\&N phantom.\n",
        "  We present a study of the star formation and AGN activity for galaxies in the\nAbell 901/2 multi-cluster system at z~0.167 as part of the OMEGA survey. Using\nTuneable Filter data obtained with the OSIRIS instrument at the GTC we produce\nspectra covering the Halpha and [N II] spectral lines for more than 400\ngalaxies. Using optical emission-line diagnostics, we identify a significant\nnumber of galaxies hosting AGN, which tend to have high masses and a broad\nrange of morphologies. Moreover, within the environmental densities probed by\nour study, we find no environmental dependence on the fraction of galaxies\nhosting AGN. The analysis of the integrated Halpha emission shows that the\nspecific star formation rates (SSFRs) of a majority of the cluster galaxies are\nbelow the field values for a given stellar mass. We interpret this result as\nevidence for a slow decrease in the star formation activity of star-forming\ngalaxies as they fall into higher-density regions, contrary to some previous\nstudies which suggested a rapid truncation of star formation. We find that most\nof the intermediate- and high-mass spiral galaxies go through a phase in which\ntheir star formation is suppressed but still retain significant star-formation\nactivity. During this phase, these galaxies tend to retain their spiral\nmorphology while their colours become redder. The presence of this type of\ngalaxies in high density regions indicates that the physical mechanism\nresponsible for suppressing star-formation affects mainly the gas component of\nthe galaxies, suggesting that ram-pressure stripping or starvation are\npotentially responsible.\n",
        "  Hydrostatic pressure Raman measurements have been carried out on the SmFeAsO\nseries of oxypnictides with varying amount of doping (F substitution for O and\nCo for Fe) and transition temperature (T_{c}), in order to investigate lattice\nmodifications and their connection to doping and superconductivity. Synchrotron\nXRD data on some of these compounds indicated that at low doping the lattice\nconstants vary smoothly with pressure, but with increasing F concentration\nthere is a deviation from the normal equation of state and these effects are\nrelated with modifications in the superconducting FeAs4 tetrahedra. The\nhydrostatic pressure Raman measurements show that the A1g mode of the rare\nearth atom for the superconducting compounds deviates from the linear pressure\ndependence at the same pressures where the XRD results indicate\npressure-induced lattice anomalies. A similar anomaly is found for the As\nphonon of same symmetry. As in cuprates, the effect is diminished in the\nundoped compounds and it is not related with the F substitution being present\nin the Sm(Fe_{1-x}Co_{x})AsO as well. The calculated Gr\\\"uneisen parameter for\nthe Sm phonon ({\\gamma \\approx}1.5) is very similar to the corresponding values\nof cuprates and it does not vary with doping. For the Fe mode it has higher\nvalue ({\\gamma \\approx}1.8) than for As ({\\gamma \\approx}1) indicating a more\nanharmonic phonon.\n",
        "  We report the structure and magnetism of PrOFeAs, one of the parent phases of\nthe newly discovered Fe-As superconductors, as measured by neutron powder\ndiffraction. In common with other REOFeAs materials, a tetragonal-orthorhombic\nphase transition is found on cooling below 136 K and striped Fe magnetism with\n$k =$(1,0,1) is detected below $\\sim$ 85 K. Our magnetic order parameter\nmeasurements show that the ordered Fe moment along the a axis reaches a maximum\nat $\\sim$ 40 K, below which an anomalous expansion of the c axis sets in, which\nresults in a negative thermal volume expansion of 0.015 % at 2 K. We propose\nthat this effect, which is suppressed in superconducting samples, is driven by\na delicate interplay between Fe and Pr ordered moments.\n",
        "  We examine the relation between breaks in the surface brightness profiles and\nradial abundance gradients within the optical radius in the discs of 134 spiral\ngalaxies from the CALIFA survey. The distribution of the radial abundance (in\nlogarithmic scale) in each galaxy was fitted by simple and broken linear\nrelations. The surface brightness profile was fitted assuming pure and broken\nexponents for the disc. We find that the maximum absolute difference between\nthe abundances in a disc given by broken and pure linear relations is less than\n0.05 dex in the majority of our galaxies and exceeds the scatter in abundances\nfor 26 out of 134 galaxies considered. The scatter in abundances around the\nbroken linear relation is close (within a few percent) to that around the pure\nlinear relation. The breaks in the surface brightness profiles are more\nprominent. The scatter around the broken exponent in a number of galaxies is\nlower by a factor of two or more than that around the pure exponent. The shapes\nof the abundance gradients and surface brightness profiles within the optical\nradius in a galaxy may be different. A pure exponential surface brightness\nprofile may be accompanied by a broken abundance gradient and vise versa. There\nis no correlation between the break radii of the abundance gradients and\nsurface brightness profiles. Thus, a break in the surface brightness profile\ndoes not need to be accompanied by a break in the abundance gradient.\n",
        "  In this paper, we present Singlish sentiment lexicon, a concept-level\nknowledge base for sentiment analysis that associates multiword expressions to\na set of emotion labels and a polarity value. Unlike many other sentiment\nanalysis resources, this lexicon is not built by manually labeling pieces of\nknowledge coming from general NLP resources such as WordNet or DBPedia.\nInstead, it is automatically constructed by applying graph-mining and\nmulti-dimensional scaling techniques on the affective common-sense knowledge\ncollected from three different sources. This knowledge is represented\nredundantly at three levels: semantic network, matrix, and vector space.\nSubsequently, the concepts are labeled by emotions and polarity through the\nensemble application of spreading activation, neural networks and an emotion\ncategorization model.\n",
        "  Conceptual combination performs a fundamental role in creating the broad\nrange of compound phrases utilized in everyday language. This article provides\na novel probabilistic framework for assessing whether the semantics of\nconceptual combinations are compositional, and so can be considered as a\nfunction of the semantics of the constituent concepts, or not. While the\nsystematicity and productivity of language provide a strong argument in favor\nof assuming compositionality, this very assumption is still regularly\nquestioned in both cognitive science and philosophy. Additionally, the\nprinciple of semantic compositionality is underspecified, which means that\nnotions of both \"strong\" and \"weak\" compositionality appear in the literature.\nRather than adjudicating between different grades of compositionality, the\nframework presented here contributes formal methods for determining a clear\ndividing line between compositional and non-compositional semantics. In\naddition, we suggest that the distinction between these is contextually\nsensitive. Utilizing formal frameworks developed for analyzing composite\nsystems in quantum theory, we present two methods that allow the semantics of\nconceptual combinations to be classified as \"compositional\" or\n\"non-compositional\". Compositionality is first formalised by factorising the\njoint probability distribution modeling the combination, where the terms in the\nfactorisation correspond to individual concepts. This leads to the necessary\nand sufficient condition for the joint probability distribution to exist. A\nfailure to meet this condition implies that the underlying concepts cannot be\nmodeled in a single probability space when considering their combination, and\nthe combination is thus deemed \"non-compositional\". The formal analysis methods\nare demonstrated by applying them to an empirical study of twenty-four\nnon-lexicalised conceptual combinations.\n",
        "  The genetic code markup is the assignment of stop codons. The standard\ngenetic code markup ensures the maximum possible stability of genetic\ninformation with respect to two fault classes: frameshift and nonsense\nmutations. There are only 528 (about 1,3% of total number) optimal markups in\nthe set of markups having 3 stop codons. Among the sets of markups with\n1,2,...,8 stop codons, the standard case having 3 stop codons has maximum\nabsolute number of optimal markups.\n",
        "  We examine the fraction of massive ($M_{*}>10^{10} M_{\\odot}$), compact\nstar-forming galaxies (cSFGs) that host an active galactic nucleus (AGN) at\n$z\\sim2$. These cSFGs are likely the direct progenitors of the compact\nquiescent galaxies observed at this epoch, which are the first population of\npassive galaxies to appear in large numbers in the early Universe. We identify\ncSFGs that host an AGN using a combination of Hubble WFC3 imaging and Chandra\nX-ray observations in four fields: the Chandra Deep Fields, the Extended Groth\nStrip, and the UKIDSS Ultra Deep Survey field. We find that\n$39.2^{+3.9}_{-3.6}$\\% (65/166) of cSFGs at $1.4<z<3.0$ host an X-ray detected\nAGN. This fraction is 3.2 times higher than the incidence of AGN in extended\nstar-forming galaxies with similar masses at these redshifts. This difference\nis significant at the $6.2\\sigma$ level. Our results are consistent with models\nin which cSFGs are formed through a dissipative contraction that triggers a\ncompact starburst and concurrent growth of the central black hole. We also\ndiscuss our findings in the context of cosmological galaxy evolution\nsimulations that require feedback energy to rapidly quench cSFGs. We show that\nthe AGN fraction peaks precisely where energy injection is needed to reproduce\nthe decline in the number density of cSFGs with redshift. Our results suggest\nthat the first abundant population of massive, quenched galaxies emerged\ndirectly following a phase of elevated supermassive black hole growth and\nfurther hints at a possible connection between AGN and the rapid quenching of\nstar formation in these galaxies.\n",
        "  The ratio of non-synonymous to synonymous substitutions\n$\\omega(=d_{N}/d_{S})$ has been widely used as a measure of adaptive evolution\nin protein coding genes. Omega can be defined in terms of population genetics\nparameters as the fixation ratio of selected vs. neutral mutants. Here it is\nargued that approaches based on the infinite sites model are not appropriate to\ndefine $\\omega$ for single codon locations. Simple models of amino acid\nsubstitution with reversible mutation and selection are analysed, and used to\ndefine $\\omega$ under several evolutionary scenarios. In most practical cases\n$\\omega<1$ when selection is constant throughout time. However, it is shown\nthat when the pattern of selection on amino acids changes, for example after an\nenvironment shift, a temporary burst of adaptive evolution ($\\omega\\gg1$) can\nbe observed. The fixation probability of a novel mutant under frequency\ndependent selection is calculated, and it is used to show why $\\omega>1$ can be\nsometimes expected for single locations at equilibrium. An example with\ninfluenza data is discussed.\n",
        "  In the present paper we give a simple proof of the fact that the set of\nvirtual links with orientable atoms is closed. More precisely, the theorem\nstates that if two virtual diagrams $K$ and $K'$ have orientable atoms and they\nare equivalent by Reidemeister moves, then there is a sequence of diagrams $K =\nK_1 \\to...\\to K_n=K'$ all having orientable atoms where $K_i$ is obtained from\n$K_{i-1}$ by a Reidemeister move. The initial proof heavily relies on the\ntopology of virtual links and was published in \\cite{IM}. Our proof is based on\nthe notion of parity which was introduced by the second named author in 2009.\n  We split the set of crossings of a virtual link diagram into sets of {\\it\nodd} and {\\it even} in accordance with a fixed rule. The rule must only satisfy\nseveral conditions of Reidemeister's type. Then one can construct functorial\nmappings of link diagrams by using parity. The concept of parity allows one to\nintroduce new invariants and strengthen well-known ones \\cite{Ma1}.\n",
        "  This master thesis presents a new type of Positron Emission TOF Apparatus\nusing Liquid xenOn (PETALO). The detector is based in the Liquid Xenon\nScintillating Cell (LXSC). The cell is a box filled with liquid xenon (LXe)\nwhose transverse dimensions are chosen to optimize packing and with a thickness\noptimized to contain a large fraction of the incoming photons. The entry and\nexit faces of the box (relative to the incoming gammas direction) are\ninstrumented with large silicon photomultipliers (SiPMs), coated with a\nwavelength shifter, tetraphenyl butadiene (TPB). The non-instrumented faces are\ncovered by reflecting Teflon coated with TPB. In this thesis we show that the\nLXSC can display an energy resolution of 5% FWHM, much better than that of\nconventional solid scintillators such as LSO/LYSO. The LXSC can measure the\ninteraction point of the incoming photon with a resolution in the three\ncoordinates of 1 mm. The very fast scintillation time of LXe (2 ns) and the\navailability of suitable sensors and electronics permits a coincidence\nresolution time (CRT) in the range of 100-200 ps, again much better than any\ncurrent PET-TOF system. The LXSC constitutes the core of a high-sensitivity,\nnuclear magnetic resonance compatible, PET device, with enhanced Time Of Flight\n(TOF) sensitivity.\n",
        "  We use the largest sample of z~6 galaxies to date from the first four Hubble\nFrontier Fields clusters to set constraints on the shape of the z~6 luminosity\nfunctions (LFs) to fainter than Muv=-14 mag. We quantify, for the first time,\nthe impact of magnification uncertainties on LF results and thus provide more\nrealistic constraints than other recent work. Our simulations reveal that for\nthe highly-magnified sources the systematic uncertainties can become extremely\nlarge fainter than -14 mag, reaching several orders of magnitude at 95%\nconfidence at ~-12 mag. Our new forward-modeling formalism incorporates the\nimpact of magnification uncertainties into the LF results by exploiting the\navailability of many independent magnification models for the same cluster. One\npublic magnification model is used to construct a mock high-redshift galaxy\nsample that is then analyzed using the other magnification models to construct\na LF. Large systematic errors occur at high magnifications (mu>30) because of\ndifferences between the models. The volume densities we derive for faint (>-17\nmag) sources are ~3-4x lower than one recent report and give a faint-end slope\nalpha=-1.92+/-0.04, which is 3.0-3.5sigma shallower (including or not including\nthe size uncertainties, respectively). We introduce a new curvature parameter\ndelta to model the faint end of the LF and demonstrate that the observations\npermit (at 68% confidence) a turn-over at z~6 in the range -15.3 to -14.2 mag,\ndepending on the assumed lensing model. The present consideration of\nmagnification errors and new size determinations raise doubts about previous\nreports regarding the form of the LF at >-14 mag. We discuss the implications\nof our turn-over constraints in the context of recent theoretical predictions.\n",
        "  Using the near-infrared spectral stellar library of Cenarro et al. (2001a,b),\nthe behaviours of the Mg I line at 8807 angstrom and nearby TiO bands are\nanalyzed in terms of the effective temperature, surface gravity, and\nmetallicity of the library stars. New spectroscopic indices for both spectral\nfeatures -namely MgI and sTiO- are defined, and their sensitivities to\ndifferent signal-to-noise ratios, spectral resolutions, flux calibrations, and\nsky emission line residuals are characterized. The new two indices exhibit\ninteresting properties. In particular, MgI reveals as a good indicator of the\nMg abundance, whereas sTiO is a powerful dwarf-to-giant discriminator for cold\nspectral types. Empirical fitting polynomials that reproduce the strength of\nthe new indices as a function of the stellar atmospheric parameters are\ncomputed, and a FORTRAN routine with the fitting functions predictions is made\navailable. A thorough study of several error sources, non-solar [Mg/Fe] ratios,\nand their influence on the fitting function residuals is also presented. From\nthis analysis, a [Mg/Fe] underabundance of ~ -0.04 is derived for the Galactic\nopen cluster M67.\n",
        "  The most studied and most successful language models were developed and\nevaluated mainly for English and other close European languages, such as\nFrench, German, etc. It is important to study applicability of these models to\nother languages. The use of vector space models for Russian was recently\nstudied for multiple corpora, such as Wikipedia, RuWac, lib.ru. These models\nwere evaluated against word semantic similarity task. For our knowledge Twitter\nwas not considered as a corpus for this task, with this work we fill the gap.\nResults for vectors trained on Twitter corpus are comparable in accuracy with\nother single-corpus trained models, although the best performance is currently\nachieved by combination of multiple corpora.\n",
        "  In 2006 Mom\\v{c}ilovi\\'c et al. measured radioactvity levels of 210Po and\n210Bi by alpha- and beta-measurements resp. to determine the natural\ndistribuion of environmental radon daughters in the different brain areas of an\nAlzheimer Disease victim. We show that gamma-ray spectrometry of the mother\nnucleus 210Pb can be used. It is a direct measurement without tampering with\nthe sample. However, because this technique is far less sensitive than alpha-\nand beta-spectrometry, activity values previously published were out-of-reach\nfor gamma-ray spectrometry. This is no longer the case with our ultra-low\nbackground high purity Ge detectors which have been developed in the frame of\nthe neutrino experiments. To ascertain this possibility, ten brain samples from\nAlzheimer patients and blanks as well as 10 samples of brain tumors have been\ninvestigated and results were cross-checked with two different spectrometers.\nLocalization of the samples in the brain was not known to avoid biased\ninterpretation of the results. Data taking lasted between 1 and 4 weeks for\neach sample. All ten Alzheimer/blank samples were below the limit of detection\n(LOD), 3 out of the 10 tumors lead to positive results (3.8 to 4.9 mBq/g of\n210Pb) with a statistical significance of at least 3 standard deviations.\nPotential followup of this method is to enhance the detector sensitivity,\nhaving the spectrometer installed in an underground laboratory such as the one\nin Modane under the Alps where the background noise is 30- fold less than at\nthe present location in Bordeaux. The final aim is to map a brain in\ncollaboration with Alzheimer/Parkinson experts.\n",
        "  In this paper, we evaluate various French lexica with the parser FRMG: the\nLefff, LGLex, the lexicon built from the tables of the French Lexicon-Grammar,\nthe lexicon DICOVALENCE and a new version of the verbal entries of the Lefff,\nobtained by merging with DICOVALENCE and partial manual validation. For this,\nall these lexica have been converted to the format of the Lefff, Alexina\nformat. The evaluation was made on the part of the EASy corpus used in the\nfirst evaluation campaign Passage.\n",
        "  We present CAVaT, a tool that performs Corpus Analysis and Validation for\nTimeML. CAVaT is an open source, modular checking utility for statistical\nanalysis of features specific to temporally-annotated natural language corpora.\nIt provides reporting, highlights salient links between a variety of general\nand time-specific linguistic features, and also validates a temporal annotation\nto ensure that it is logically consistent and sufficiently annotated. Uniquely,\nCAVaT provides analysis specific to TimeML-annotated temporal information.\nTimeML is a standard for annotating temporal information in natural language\ntext. In this paper, we present the reporting part of CAVaT, and then its\nerror-checking ability, including the workings of several novel TimeML document\nverification methods. This is followed by the execution of some example tasks\nusing the tool to show relations between times, events, signals and links. We\nalso demonstrate inconsistencies in a TimeML corpus (TimeBank) that have been\ndetected with CAVaT.\n",
        "  Spherical deconvolution (SD) methods are widely used to estimate the\nintra-voxel white-matter fiber orientations from diffusion MRI data. However,\nwhile some of these methods assume a zero-mean Gaussian distribution for the\nunderlying noise, its real distribution is known to be non-Gaussian and to\ndepend on the methodology used to combine multichannel signals. Indeed, the two\nprevailing methods for multichannel signal combination lead to Rician and\nnoncentral Chi noise distributions. Here we develop a Robust and Unbiased\nModel-BAsed Spherical Deconvolution (RUMBA-SD) technique, intended to deal with\nrealistic MRI noise, based on a Richardson-Lucy (RL) algorithm adapted to\nRician and noncentral Chi likelihood models. To quantify the benefits of using\nproper noise models, RUMBA-SD was compared with dRL-SD, a well-established\nmethod based on the RL algorithm for Gaussian noise. Another aim of the study\nwas to quantify the impact of including a total variation (TV) spatial\nregularization term in the estimation framework. To do this, we developed TV\nspatially-regularized versions of both RUMBA-SD and dRL-SD algorithms. The\nevaluation was performed by comparing various quality metrics on 132\nthree-dimensional synthetic phantoms involving different inter-fiber angles and\nvolume fractions, which were contaminated with noise mimicking patterns\ngenerated by data processing in multichannel scanners. The results demonstrate\nthat the inclusion of proper likelihood models leads to an increased ability to\nresolve fiber crossings with smaller inter-fiber angles and to better detect\nnon-dominant fibers. The inclusion of TV regularization dramatically improved\nthe resolution power of both techniques. The above findings were also verified\nin brain data.\n",
        "  Iron-chalcogenide single crystals with the nominal composition\nFeSe$_{0.5}$Te$_{0.5}$ and a transition temperature of $T_{c}\\simeq14.6$ K were\nsynthesized by the Bridgman method. The structural and anisotropic\nsuperconducting properties of those crystals were investigated by means of\nsingle crystal X-ray and neutron powder diffraction, SQUID and torque\nmagnetometry, and muon-spin rotation. Room temperature neutron powder\ndiffraction reveals that 95% of the crystal volume is of the same tetragonal\nstructure as PbO. The structure refinement yields a stoichiometry of\nFe_1.045Se_0.406Te_0.594. Additionally, a minor hexagonal Fe_7Se_8 impurity\nphase was identified. The magnetic penetration depth \\lambda at zero\ntemperature was found to be 491(8) nm in the ab-plane and 1320(14) nm along the\nc-axis. The zero-temperature value of the superfluid density \\rho_s(0)\n\\lambda^-2(0) obeys the empirical Uemura relation observed for various\nunconventional superconductors, including cuprates and iron-pnictides. The\ntemperature dependences of both \\lambda_ab and \\lambda_c are well described by\na two-gap s+s-wave model with the zero-temperature gap values of\n\\Delta_S(0)=0.51(3) meV and \\Delta_L(0)=2.61(9) meV for the small and the large\ngap, respectively. The magnetic penetration depth anisotropy parameter\n\\gamma_\\lambda(T)=\\lambda_c(T)/\\lambda_{ab}(T) increases with decreasing\ntemperature, in agreement with \\gamma_\\lambda(T) observed in the iron-pnictide\nsuperconductors.\n",
        "  A long-term goal of machine learning research is to build an intelligent\ndialog agent. Most research in natural language understanding has focused on\nlearning from fixed training sets of labeled data, with supervision either at\nthe word level (tagging, parsing tasks) or sentence level (question answering,\nmachine translation). This kind of supervision is not realistic of how humans\nlearn, where language is both learned by, and used for, communication. In this\nwork, we study dialog-based language learning, where supervision is given\nnaturally and implicitly in the response of the dialog partner during the\nconversation. We study this setup in two domains: the bAbI dataset of (Weston\net al., 2015) and large-scale question answering from (Dodge et al., 2015). We\nevaluate a set of baseline learning strategies on these tasks, and show that a\nnovel model incorporating predictive lookahead is a promising approach for\nlearning from a teacher's response. In particular, a surprising result is that\nit can learn to answer questions correctly without any reward-based supervision\nat all.\n",
        "  Observations by the Atacama Large Millimetre/sub-millimetre Array of the 358\nGHz continuum emission of the gravitationally lensed quasar host RX\nJ0911.4+0551 have been analysed. They complement earlier Plateau de Bure\nInterferometer observations of the CO(7-6) emission. The good knowledge of the\nlensing potential obtained from Hubble Space Telescope observations of the\nquasar makes a joint analysis of the three emissions possible. It gives\nevidence for the quasar source to be concentric with the continuum source\nwithin 0.31 kpc and with the CO(7-6) source within 1.10 kpc. It also provides a\nmeasurement of the size of the continuum source, 0.76 $\\pm$ 0.04 kpc FWHM,\nmaking RX J0911.4+0551 one of the few high redshift galaxies for which the dust\nand gas components are resolved with dimensions being measured. Both are found\nto be very compact, the former being smaller than the latter by a factor of\n$\\sim$3.4$\\pm$0.4. Moreover, new measurements of the CO ladder $-$ CO(10-9) and\nCO(11-10) $-$ are presented that confirm the extreme narrowness of the CO line\nwidth (107$\\pm$20 km s$^{-1}$ on average). Their mere detection implies higher\ntemperature and/or density than for typical quasar hosts at this redshift and\nsuggests a possible contribution of the central AGN to gas and dust heating.\nThe results are interpreted in terms of current understanding of galaxy\nevolution at the peak of star formation. They suggest that RX J0911.4+0551 is a\nyoung galaxy in an early stage of its evolution, having experienced no recent\nmajor mergers, star formation being concentrated in its centre.\n",
        "  We propose to raise the critical temperature $T_c$ for superconductivity in\ndoped C$_{60}$ molecular crystals by increasing the electronic density of\nstates at the Fermi level $N(E_F)$ and thus the electron-phonon coupling\nconstant in low-dimensional C$_{60}$ nanoarrays. We consider both electron and\nhole doping and present numerical results for $N(E_F)$, which increases with\ndecreasing bandwidth of the partly filled $h_u$ and $t_{1u}$ derived frontier\nbands with decreasing coordination number of C$_{60}$. Whereas a significant\nincrease of $N(E_F)$ occurs in 2D arrays of doped C$_{60}$ intercalated\nin-between graphene layers, we propose that the highest $T_c$ values\napproaching room temperature may occur in bundles of nanotubes filled by 1D\narrays of externally doped C$_{60}$ or La@C$_{60}$, or in diluted 3D crystals,\nwhere quasi-1D arrangements of C$_{60}$ form percolation paths.\n",
        "  Let $M$ be a closed, oriented, connected 3--manifold and $(B,\\pi)$ an open\nbook decomposition on $M$ with page $\\Sigma$ and monodromy $\\varphi$. It is\neasy to see that the first Betti number of $\\Sigma$ is bounded below by the\nnumber of $S^2\\times S^1$--factors in the prime factorization of $M$. Our main\nresult is that equality is realized if and only if $\\varphi$ is trivial and $M$\nis a connected sum of $S^2\\times S^1$'s. We also give some applications of our\nmain result, such as a new proof of the result by Birman and Menasco that if\nthe closure of a braid with $n$ strands is the unlink with $n$ components then\nthe braid is trivial.\n",
        "  We show that if K is a satellite knot which admits a generalized cosmetic\ncrossing change of order q with |q| \\geq 6, then K admits a pattern knot with a\ngeneralized cosmetic crossing change of the same order. As a consequence of\nthis, we find that any prime satellite knot which admits a pattern knot that is\nfibered cannot admit a generalized cosmetic crossing changes of order q with\n|q| \\geq 6. We also show that if there is any knot admitting a generalized\ncosmetic crossing change of order q with |q| \\geq 6, then there must be such a\nknot which is hyperbolic.\n",
        "  A neutron scattering study of heavily hole-overdoped superconducting\nKFe$_2$As$_2$ revealed a well-defined low-energy incommensurate spin\nfluctuation at [$\\pi(1\\pm2\\delta$),0] with $\\delta$ = 0.16. The incommensurate\nstructure differs from the previously observed commensurate peaks in\nelectron-doped $A$Fe$_2$As$_2$ ($A$ = Ba, Ca, or Sr) at low energies. The\ndirection of the peak splitting is perpendicular to that observed in Fe(Te,Se)\nor in Ba(Fe,Co)$_2$As$_2$ at high energies. A band structure calculation\nsuggests interband scattering between bands around the $\\Gamma$ and X points as\nan origin of this incommensurate peak. The perpendicular direction of the peak\nsplitting can be understood within the framework of multiorbital band\nstructure. The results suggest that spin fluctuation is more robust in\nhole-doped than in electron-doped samples, which can be responsible for the\nappearance of superconductivity in the heavily hole-doped samples.\n",
        "  This paper is replaced by arXiv:0907.2826, by these authors and M. Vavilov\nand A. Chubukov. A sign mistake in the original paper led to incorrect\nconclusions. The paper arXiv:0907.2826 supersedes this paper arXiv:0904.3926.\n",
        "  Organizations continuously accumulate data, often according to some business\nprocesses. If one poses a query over such data for decision support, it is\nimportant to know whether the query is stable, that is, whether the answers\nwill stay the same or may change in the future because business processes may\nadd further data. We investigate query stability for conjunctive queries. To\nthis end, we define a formalism that combines an explicit representation of the\ncontrol flow of a process with a specification of how data is read and inserted\ninto the database. We consider different restrictions of the process model and\nthe state of the system, such as negation in conditions, cyclic executions,\nread access to written data, presence of pending process instances, and the\npossibility to start fresh process instances. We identify for which facet\ncombinations stability of conjunctive queries is decidable and provide\nencodings into variants of Datalog that are optimal with respect to the\nworst-case complexity of the problem.\n",
        "  We report the detection of the radio and infrared counterparts of the ring\nnebula around the WN3(h) star HD211564 (WR152), located to the southwest of the\nHII region Sh2132. Using radio continuum data from the Canadian Galactic Plane\nSurvey, we identified the radio counterparts of the two concentric rings, of\nabout 9' and 16' in radius, related to the star. After applying a filling\nfactor f = 0.05-0.12, electron densities and ionized masses are in the range\n10-16 cm^-3 and 450-700 Mo, respectively. The analysis of the HI gas emission\ndistribution allowed the identification of 5900 Mo of neutral atomic gas with\nvelocities between -52 and -43 km/s probably linked to the nebula. The region\nof the nebula is almost free of molecular gas. Only four small clumps were\ndetected, with a total molecular mass of 790 Mo. About 310 Mo are related to a\nsmall infrared shell-like source linked to the inner ring, which is also\ndetected in the MSX band A. An IRAS YSO candidate is detected in coincidence\nwith the shell-like IR source.\n  We suggest that the optical nebula and its neutral counterparts originated\nfrom the stellar winds from the WR star and its massive progenitor, and are\nevolving in the envelope of a slowly expanding shell centered at (l,b) = (102\n30, -0 50), of about 31 pc in radius. The bubble's energy conversion efficiency\nis in agreement with recent numerical analysis and with observational results.\n",
        "  New iron-arsenide superconductors of REFeAsO1-delta (RE = Ho, Y, Dy and Tb)\nwere successfully synthesized by a high pressure synthesizing method with a\nspecial rapid quenching process, with the onset superconducting critical\ntemperatures at 50.3 K, 46.5 K, 52.2K and 48.5 K for RE = Ho, Y, Dy and Tb\nrespectively.\n",
        "  Using 3D numerical simulations we study the evolution of the H$\\alpha$\nintensity and velocity dispersion for single and multiple supenova (SN)\nexplosions. We find that the $I_{\\rm H\\alpha}-\\sigma$ diagram obtained for\nsimulated gas flows is similar in shape to that observed in dwarf galaxies. We\nconclude that colliding SN shells with significant difference in age are\nresposible for high velocity dispersion that reaches values high as $\\simgt\n100$kms$^{-1}$. Such a high velocity dispersion could be hardly got for a\nsingle SN remnant. Peaks of velocity dispersion on the $I_{\\rm H\\alpha}-\\sigma$\ndiagram may correspond to several stand-alone or merged SN remnants with\nmoderately different ages. The procedure of the spatial resolution degrading in\nthe H$\\alpha$ intensity and velocity dispersion maps makes the simulated\n$I_{\\rm H\\alpha}-\\sigma$ diagrams close to those observed in dwarf galaxies not\nonly in shape, but also quantitatively.\n",
        "  Using photometry from Data Release 10 of the northern footprint of the Sloan\nDigital Sky Survey, we detect two new stellar streams with lengths of between\n$25\\arcdeg$ and $50\\arcdeg$. The streams, which we designate Hermus and Hyllus,\nare at distances of between 15 and 23 kpc from the Sun and pass primarily\nthrough Hercules and Corona Borealis. Stars in the streams appear to be metal\npoor, with [Fe/H] $\\sim -2.3$, though we cannot rule out metallicities as high\nas [Fe/H] = -1.2. While Hermus passes within $1\\arcdeg$ (in projection) of the\nglobular cluster NGC 6229, a roughly one magnitude difference in distance\nmodulus, combined with no signs of connecting with NGC 6229's Roche lobe, argue\nagainst any physical association between the two. Though the two streams almost\ncertainly had different progenitors, similarities in preliminary orbit\nestimates suggest that those progenitors may themselves have been a product of\na single accretion event.\n",
        "  A Geant4-based Monte Carlo model for Heavy-Ion Therapy (MCHIT) is used to\nstudy radiation fields of H-1, He-4, Li-7 and C-12 beams with similar ranges\n(~160-180 mm) in water. Microdosimetry spectra are simulated for wall-less and\nwalled Tissue Equivalent Proportional Counters (TEPCs) placed outside or inside\na phantom, as in experiments performed, respectively, at NIRS, Japan and GSI,\nGermany. The impact of fragmentation reactions on microdosimetry spectra is\ninvestigated for He-4, Li-7 and C-12, and contributions from nuclear fragments\nof different charge are evaluated for various TEPC positions in the phantom.\nThe microdosimetry spectra measured on the beam axis are well described by\nMCHIT, in particular, in the vicinity of the Bragg peak. However, the simulated\nspectra for the walled TEPC far from the beam axis are underestimated. Relative\nBiological Effectiveness (RBE) of the considered beams is estimated using a\nmodified microdosimetric-kinetic model. Calculations show a similar rise of the\nRBE up to 2.2-2.9 close to the Bragg peak for helium, lithium and carbon beams\ncompared to the modest values of 1-1.2 at the plateau region. Our results\nsuggest that helium and lithium beams are also promising options for cancer\ntherapy.\n",
        "  A large amount of data resulting from trajectories of moving objects\nactivities are collected thanks to localization based services and some\nassociated automated processes. Trajectories data can be used either for\ntransactional and analysis purposes in various domains (heath care, commerce,\nenvironment, etc.). For this reason, modeling trajectory data at the conceptual\nlevel is an important stair leading to global vision and successful\nimplementations. However, current modeling tools fail to fulfill specific\nmoving objects activities requirements. In this paper, we propose a new profile\nbased on UML in order to enhance the conceptual modeling of trajectory data\nrelated to mobile objects by new stereotypes and icons. As illustration, we\npresent a mobile hospital use case.\n",
        "  Species evolution is essentially a random process of interaction between\nbiological populations and their environments. As a result, some physical\nparameters in evolution models are subject to statistical fluctuations. In this\npaper, two important parameters in the Eigen model, the fitness and mutation\nrate, are treated as Gaussian distributed random variables simultaneously to\nexamine the property of the error threshold. Numerical simulation results show\nthat the error threshold in the fully random model appears as a crossover\nregion instead of a phase transition point, and as the fluctuation strength\nincreases the crossover region becomes smoother and smoother. Furthermore, it\nis shown that the randomization of the mutation rate plays a dominant role in\nchanging the error threshold in the fully random model, which is consistent\nwith the existing experimental data. The implication of the threshold change\ndue to the randomization for antiviral strategies is discussed.\n",
        "  Point contacts (PC) Andreev reflection dV/dI spectra for the\nantiferromagnetic (T_N =6K) superconductor (Tc=11K) ErNi2B2C have been measured\nfor the two main crystallographic directions. Observed retention of the Andreev\nreflection minima in dV/dI up to Tc directly points to unusual superconducting\norder parameter (OP) vanishing at Tc. Temperature dependence of OP was obtained\nfrom dV/dI using recent theory of Andreev reflection including pair-breaking\neffect. For the first time existence of a two superconducting OPs in ErNi2B2C\nis shown. A distinct decrease of both OPs as temperature is lowered below T_N\nis observed.\n",
        "  We study the problem of evaluating automatic speech recognition (ASR) systems\nthat target dialectal speech input. A major challenge in this case is that the\northography of dialects is typically not standardized. From an ASR evaluation\nperspective, this means that there is no clear gold standard for the expected\noutput, and several possible outputs could be considered correct according to\ndifferent human annotators, which makes standard word error rate (WER)\ninadequate as an evaluation metric. Such a situation is typical for machine\ntranslation (MT), and thus we borrow ideas from an MT evaluation metric, namely\nTERp, an extension of translation error rate which is closely-related to WER.\nIn particular, in the process of comparing a hypothesis to a reference, we make\nuse of spelling variants for words and phrases, which we mine from Twitter in\nan unsupervised fashion. Our experiments with evaluating ASR output for\nEgyptian Arabic, and further manual analysis, show that the resulting WERd\n(i.e., WER for dialects) metric, a variant of TERp, is more adequate than WER\nfor evaluating dialectal ASR.\n",
        "  Carnivores are important components of ecosystems with wide-ranging effects\non ecological communities.We studied the carnivore community in the Apostle\nIslands National Lakeshore (APIS), where the presence, distribution, and\npopulations of carnivores was largely unknown. We developed a systematic method\nto deploy camera traps across a grid while targeting fine-scale features to\nmaximize carnivore detection (Appendix 1), including systematic methods for\norganizing and tagging the photo data (Appendix 2). We deployed 88 cameras on\n13 islands from 2014-2016. We collected 92,694 photographs across 18,721 trap\nnights, including 3,591 wildlife events and 1,070 carnivore events. We had a\nmean of 6.6 cameras per island (range 2-30), and our camera density averaged\n1.23 (range 0.74-3.08) cameras/ km2. We detected 27 species and 10 terrestrial\ncarnivores, including surprising detections of American martens (Martes\namericana) and gray wolves (Canis lupus). The mean richness of carnivores on an\nisland was 3.23 (range 0-10). The best single variable to explain carnivore\nrichness on the Apostle Islands was island size, while the best model was\nisland size (positive correlation) and distance from mainland (negative\ncorrelation) (R2 = 0.92). Relative abundances for carnivores ranged from a low\nof 0.01 for weasels (Mustela spp.) to a high of 2.64 for black bears (Ursus\namericanus), and the relative abundance of a species was significantly\ncorrelated with the number of islands on which they were found. Carnivore\noccupancy ranged from lows of 0.09 for gray wolves and 0.11 for weasels to a\nhigh of 0.82 for black bears. Fuller understanding of APIS ecology will require\non-going monitoring of carnivores to evaluate temporal dynamics as well as\nrelated ecological evaluations (e.g. small mammal dynamics, plant community\ndynamics) to understand trophic effects.\n",
        "  Dynamic interspecific interactions are thought to contribute to fundamental\nstructures of ecological communities. Using the Yodzis framework of dominance\ncontrol versus founder control versus niche control, the importance of\nintransitive loops in sub-communities in a source/sink or metacommunity\ncompetition context is examined. It is found that a three-species sub-community\nsampled from a species pool composed of nothing but dominance controlled\nspecies pairs can result in a stable three species coexistence. It is shown\nthat if effect and response competition are balanced, the overall sub-community\nwill be persistent, anchored by one or more intransitive loops. It is proposed\nthat such an arrangement be referred to as an intransitive structure. It is\nconcluded that persistence of a sub-community (when species pairs in the\nspecies pool are all dominance controlled) of more than three species is\npossible only if it has an intransitive structure.\n",
        "  We study influence of ordinal transformations on results of queries in\nrank-aware databases which derive their operations with ranked relations from\ntotally ordered structures of scores with infima acting as aggregation\nfunctions. We introduce notions of ordinal containment and equivalence of\nranked relations and prove that infima-based algebraic operations with ranked\nrelations are invariant to ordinal transformations: Queries applied to original\nand transformed data yield results which are equivalent in terms of the order\ngiven by scores, meaning that top-k results of queries remain the same. We show\nthis important property is preserved in alternative query systems based of\nrelational calculi developed in context of G\\\"odel logic. We comment on\nrelationship to monotone query evaluation and show that the results can be\nattained in alternative rank-aware approaches.\n",
        "  One approach in phylogenomics to infer the tree of life is based on\nconcatenated multiple sequence alignments from many genes. Unfortunately, the\nresulting so-called supermatrix is usually sparse, that is, not every gene\nsequence is available for all species in the supermatrix. Due to the missing\nsequence information a phylogenetic inference, assuming that each gene evolves\nwith its own substitution model, suffers from phylogenetic terraces on which\nmany phylogenetic trees show the same likelihood. Here, we propose a\nphylogenetic terrace aware (PTA) data structure for efficient supermatrix based\ntree inference under partition models. PTA avoids likelihood computations for\ntrees belonging to the same terrace. PTA is implemented in the IQ-TREE\nsoftware, and leads to an 1.7 to 6-fold speedup for real data sets compared\nwith a na\\\"ive implementation. Speedups are independent on terrace sizes but\ncorrelate with the amount of missing data. Thus, the PTA data structure is well\nsuited for phylogenomic analyses. IQ-TREE source codes, binaries and\ndocumentation are freely available at http://www.cibiv.at/software/iqtree .\n",
        "  The main objective of multichannel radiochromic film dosimetry methods is to\ncorrect, or at least mitigate, spatial heterogeneities in the film-scanner\nresponse, especially variations in the active layer thickness. To this end,\nfilms can also be scanned prior to irradiation. In this study, the abilities of\nvarious single channel and multichannel methods to reduce spatial\nheterogeneities, with and without scanning before irradiation, were tested. A\nnew approach to multichannel dosimetry, based on experimental findings, i.e.,\nthe Multigaussian method, is introduced. The Multigaussian method assumes that\nthe probability density function of the response vector formed by the pixel\nvalues of the different color channels, including irradiated and non-irradiated\nscans, follows a multivariate Gaussian distribution. The Multigaussian method\nprovided more accurate doses than the other models under comparison, especially\nwhen incorporating the information of the film prior to irradiation.\n",
        "  Lexical entailment, such as hyponymy, is a fundamental issue in the semantics\nof natural language. This paper proposes distributional semantic models which\nefficiently learn word embeddings for entailment, using a recently-proposed\nframework for modelling entailment in a vector-space. These models postulate a\nlatent vector for a pseudo-phrase containing two neighbouring word vectors. We\ninvestigate both modelling words as the evidence they contribute about this\nphrase vector, or as the posterior distribution of a one-word phrase vector,\nand find that the posterior vectors perform better. The resulting word\nembeddings outperform the best previous results on predicting hyponymy between\nwords, in unsupervised and semi-supervised experiments.\n",
        "  Dictionaries are essence of any language providing vital linguistic recourse\nfor the language learners, researchers and scholars. This paper focuses on the\nmethodology and techniques used in developing software architecture for a\nUBSESD (Unicode Based Sindhi to English and English to Sindhi Dictionary). The\nproposed system provides an accurate solution for construction and\nrepresentation of Unicode based Sindhi characters in a dictionary implementing\nHash Structure algorithm and a custom java Object as its internal data\nstructure saved in a file. The System provides facilities for Insertion,\nDeletion and Editing of new records of Sindhi. Through this framework any type\nof Sindhi to English and English to Sindhi Dictionary (belonging to different\ndomains of knowledge, e.g. engineering, medicine, computer, biology etc.) could\nbe developed easily with accurate representation of Unicode Characters in font\nindependent manner.\n",
        "  We present a formula for the full Cheeger-Chern-Simons class of the\ntautological flat complex vector bundle of rank two over BSL(2,\\C^\\delta). Our\nformula improves the formula by Dupont and Zickert, where the class is only\ncomputed modulo 2-torsion.\n",
        "  To cope with the massive growth of semantic data streams, several RDF Stream\nProcessing (RSP) engines have been implemented. The efficiency of their\nthroughput, latency and memory consumption can be evaluated using available\nbenchmarks such as LSBench and City- Bench. Nevertheless, these benchmarks lack\nan in-depth performance evaluation as some measurement metrics have not been\nconsidered. The main goal of this paper is to analyze the performance of two\npopular RSP engines, namely C-SPARQL and CQELS, when varying a set of\nperformance metrics. More precisely, we evaluate the impact of stream rate,\nnumber of streams and window size on execution time as well as on memory\nconsumption.\n",
        "  A high-quality, comprehensive product catalog is essential to the success of\nProduct Search engines and shopping sites such as Yahoo! Shopping, Google\nProduct Search or Bing Shopping. But keeping catalogs up-to-date becomes a\nchallenging task, calling for the need of automated techniques. In this paper,\nwe introduce the problem of product synthesis, a key component of catalog\ncreation and maintenance. Given a set of offers advertised by merchants, the\ngoal is to identify new products and add them to the catalog together with\ntheir (structured) attributes. A fundamental challenge is the scale of the\nproblem: a Product Search engine receives data from thousands of merchants and\nmillions of products; the product taxonomy contains thousands of categories,\nwhere each category comes in a different schema; and merchants use\nrepresentations for products that are different from the ones used in the\ncatalog of the Product Search engine.\n  We propose a system that provides an end-to-end solution to the product\nsynthesis problem, and includes components for extraction, and addresses issues\ninvolved in data extraction from offers, schema reconciliation, and data\nfusion. We developed a novel and scalable technique for schema matching which\nleverages knowledge about previously-known instance-level associations between\noffers and products; and it is trained using automatically created training\nsets (no manually-labeled data is needed). We present an experimental\nevaluation of our system using data from Bing Shopping for more than 800K\noffers, a thousand merchants, and 400 categories. The evaluation confirms that\nour approach is able to automatically generate a large number of accurate\nproduct specifications, and that our schema reconciliation component\noutperforms state-of-the-art schema matching techniques in terms of precision\nand recall.\n",
        "  In evolutionary game dynamics, reproductive success increases with the\nperformance in an evolutionary game. If strategy $A$ performs better than\nstrategy $B$, strategy $A$ will spread in the population. Under stochastic\ndynamics, a single mutant will sooner or later take over the entire population\nor go extinct. We analyze the mean exit times (or average fixation times)\nassociated with this process. We show analytically that these times depend on\nthe payoff matrix of the game in an amazingly simple way under weak selection,\nie strong stochasticity: The payoff difference $\\Delta \\pi$ is a linear\nfunction of the number of $A$ individuals $i$, $\\Delta \\pi = u i + v$. The\nunconditional mean exit time depends only on the constant term $v$. Given that\na single $A$ mutant takes over the population, the corresponding conditional\nmean exit time depends only on the density dependent term $u$. We demonstrate\nthis finding for two commonly applied microscopic evolutionary processes.\n",
        "  To investigate the disk formation and jet launch in protostars is crucial to\ncomprehend the earliest stages of star and planet formation. We aim to\nconstrain the properties of the molecular jet and the disk of the HH 212\nprotostellar system at unprecedented angular scales through ALMA observations\nof sulfur-bearing molecules, SO 9(8)-8(7), SO 10(11)-10(10), SO2 8(2,6)-7(1,7).\nSO 9(8)-8(7) and SO2 8(2,6)-7(1,7) show broad velocity profiles. At systemic\nvelocity they probe the circumstellar gas and the cavity walls. Going from low\nto high blue-/red-shifted velocities the emission traces the wide-angle outflow\nand the fast (~100-200 km/s) and collimated (~90 AU) molecular jet revealing\nthe inner knots with timescales <50 years. The jet transports a mass loss rate\n>0.2-2e-6 Msun/yr, implying high ejection efficiency (>0.03-0.3). The SO and\nSO2 abundances in the jet are ~1e-7-1e-6. SO 10(11)-10(10) emission is compact\nand shows small-scale velocity gradients indicating that it originates partly\nfrom the rotating disk previously seen in HCO+ and C17O, and partly from the\nbase of the jet. The disk mass is >0.002-0.013 Msun, and the SO abundance in\nthe disk is ~1e-8-1e-7. SO and SO2 are effective tracers of the molecular jet\nin the inner few hundreds AU from the protostar. Their abundances indicate that\n1% - 40% of sulfur is in SO and SO2 due to shocks in the jet/outflow and/or to\nambipolar diffusion at the wind base. The SO abundance in the disk is 3-4\norders of magnitude larger than in evolved protoplanetary disks. This may be\ndue to an SO enhancement in the accretion shock at the envelope-disk interface\nor in spiral shocks if the disk is partly gravitationally unstable.\n",
        "  Guided by the duality of turbulence (random versus coherent we seek coherent\nstructures in the turbulent velocity field of molecular clouds, anticipating\ntheir importance in cloud evolution. We analyse a large map (40' by 20')\nobtained with the HERA multibeam receiver (IRAM-30m telescope) in a high\nlatitude cloud of the Polaris Flare at an unprecedented spatial (11\") and\nspectral (0.05 km/s) resolutions in the 12CO(2-1) line. We find that two\nparsec-scale components of velocities differing by ~2 km/s, share a narrow\ninterface ($<0.15$ pc) that appears as an elongated structure of intense\nvelocity-shear, ~15 to 30 km/s/pc. The locus of the extrema of\nline--centroid-velocity increments (E-CVI) in that field follows this\nintense-shear structure as well as that of the 12CO(2-1) high-velocity line\nwings. The tiny spatial overlap in projection of the two parsec-scale\ncomponents implies that they are sheets of CO emission and that discontinuities\nin the gas properties (CO enrichment and/or increase of gas density) occur at\nthe position of the intense velocity shear. These results disclose spatial and\nkinematic coherence between scales as small as 0.03 pc and parsec scales. They\nconfirm that the departure from Gaussianity of the probability density\nfunctions of E-CVIs is a powerful statistical tracer of the intermittency of\nturbulence. They disclose a link between large scale turbulence, its\nintermittent dissipation rate and low-mass dense core formation.\n",
        "  Evaluation plays a vital role in checking the quality of MT output. It is\ndone either manually or automatically. Manual evaluation is very time consuming\nand subjective, hence use of automatic metrics is done most of the times. This\npaper evaluates the translation quality of different MT Engines for\nHindi-English (Hindi data is provided as input and English is obtained as\noutput) using various automatic metrics like BLEU, METEOR etc. Further the\ncomparison automatic evaluation results with Human ranking have also been\ngiven.\n",
        "  Accurate measurement of the fat-to-muscle ratio in animal model is important\nfor obesity research. An efficient way to measure the fat to muscle ratio in\nanimal model using dual-energy absorptiometry is presented in this paper. A\nradioactive source exciting x-ray fluorescence from a target material is used\nto provide the two x-ray energies needed. The x-rays, after transmitting\nthrough the sample, are measured with an energy-sensitive Ge detector. Phantoms\nand specimens were measured. The results showed that the method was sensitive\nto the fat to muscle ratios with good linearity. A standard deviation of a few\npercent in the fat to muscle ratio could be observed with the x-ray dose of\n0.001 mGy.\n",
        "  Hubbell's neutral theory of biodiversity has successfully explained the\nobserved composition of many ecological communities but it relies on strict\ndemographic equivalence among species and provides no room for evolutionary\nprocesses like selection, adaptation and speciation. Here we show how to embed\nthe neutral theory within the Darwinian framework. In a fitness landscape with\na quadratic maximum, typical of quantitative traits, selection restricts the\nextant species to have traits close to optimal, so that the fitness differences\nbetween surviving species are small. For sufficiently small mutation steps, the\ncommunity structure fits perfectly to the Fisher log-series species abundance\ndistribution. The theory is relatively insensitive to moderate amounts of\nenvironmental noise, wherein the location of the fitness maximum changes by\namounts of order the width of the noise-free distribution. Adding very large\nenvironmental noise to the model qualitatively changes the abundance\ndistributions, converting the exponential fall-off of large species to a\npower-law decay, typical of a neutral model with environmental noise.\n",
        "  Over 50 years of work on group actions on $4$-manifolds, from the 1960's to\nthe present, from knotted fixed point sets to Seiberg-Witten invariants, is\nsurveyed. Locally linear actions are emphasized, but differentiable and purely\ntopological actions are also discussed. The presentation is organized around\nsome of the fundamental general questions that have driven the subject of\ncompact transformation groups over the years and their interpretations in the\ncase of $4$-manifolds. Many open problems are formulated. Updates to previous\nproblems sets are given. A substantial bibliography is included.\n",
        "  Ecosystems are complex systems, currently experiencing several threats\nassociated with global warming, intensive exploitation, and human-driven\nhabitat degradation. Such threats are pushing ecosystems to the brink of\ncollapse. Because of a general presence of multiple stable states, including\nstates involving population extinction, and due to intrinsic nonlinearities\nassociated with feedback loops, collapse can occur in a catastrophic manner.\nSuch catastrophic shifts have been suggested to pervade many of the future\ntransitions affecting ecosystems at many different scales. Many studies have\ntried to delineate potential warning signals predicting such ongoing shifts but\nlittle is known about how such transitions might be effectively prevented. It\nhas been recently suggested that a potential path to prevent or modify the\noutcome of these transitions would involve designing synthetic organisms and\nsynthetic ecological interactions that could push these endangered systems out\nof the critical boundaries. Four classes of such ecological engineering designs\nor {\\em Terraformation motifs} have been defined in a qualitative way. Here we\ndevelop the simplest mathematical models associated with these motifs, defining\nthe expected stability conditions and domains where the motifs shall properly\nwork.\n",
        "  This note deals with arbitrary Morse-Smale diffeomorphisms in dimension 3 and\nextends ideas from \\cite{GrLaPo}, \\cite{GrLaPo1}, where gradient-like case was\nconsidered. We introduce a kind of Morse-Lyapunov function, called dynamically\nordered, which fits well dynamics of diffeomorphism. The paper is devoted to\nfinding conditions to the existence of such an energy function, that is, a\nfunction whose set of critical points coincides with the non-wandering set of\nthe considered diffeomorphism. We show that the necessary and sufficient\nconditions to the existence of a dynamically ordered energy function reduces to\nthe type of embedding of one-dimensional attractors and repellers of a given\nMorse-Smale diffeomorphism on a closed 3-manifold.\n",
        "  We exploit data from the Pan-Andromeda Archaeological Survey (PAndAS) to\nstudy the extended structures of M31's dwarf elliptical companions, NGC147 and\nNGC185. Our wide-field, homogeneous photometry allows to construct deep\ncolour-magnitude diagrams (CMDs) which reach down to $\\sim3$ mag below the red\ngiant branch (RGB) tip. We trace the stellar components of the galaxies to\nsurface brightness of $\\mu_g \\sim 32$ mag arcsec$^{-2}$ and show they have much\nlarger extents ($\\sim5$ kpc radii) than previously recognised. While NGC185\nretains a regular shape in its peripheral regions, NGC147 exhibits pronounced\nisophotal twisting due to the emergence of symmetric tidal tails. We fit single\nSersic models to composite surface brightness profiles constructed from diffuse\nlight and star counts and find that NGC147 has an effective radius almost 3\ntimes that of NGC185. In both cases, the effective radii that we calculate are\nlarger by a factor of $\\sim2$ compared to most literature values. We also\ncalculate revised total magnitudes of $M_g=-15.36\\pm0.04$ for NGC185 and\n$M_g=-16.36\\pm0.04$ for NGC147. Using photometric metallicities computed for\nRGB stars, we find NGC185 to exhibit a metallicity gradient of\n[Fe/H]$\\sim-0.15$ dex/kpc over the radial range 0.125 to 0.5 deg. On the other\nhand, NGC147 exhibits almost no metallicity gradient, $\\sim-0.02$ dex/kpc from\n0.2 to 0.6 deg. The differences in the structure and stellar populations in the\noutskirts of these systems suggest that tidal influences have played an\nimportant role in governing the evolution of NGC147.\n",
        "  Ultrasound computed tomography (USCT) holds great promise for breast cancer\nscreening. Waveform inversion-based image reconstruction methods account for\nhigher order diffraction effects and can produce high-resolution USCT images,\nbut are computationally demanding. Recently, a source encoding technique was\ncombined with stochastic gradient descent to greatly reduce image\nreconstruction times. However, this method bundles the stochastic data fidelity\nterm with the deterministic regularization term. This limitation can be\novercome by replacing stochastic gradient descent (SGD) with a structured\noptimization method, such as the regularized dual averaging (RDA) method, that\nexploits knowledge of the composition of the cost function. In this work, the\ndual averaging method is combined with source encoding techniques to improve\nthe effectiveness of regularization while maintaining the reduced\nreconstruction times afforded by source encoding. It is demonstrated that each\niteration can be decomposed into a gradient descent step based on the data\nfidelity term and a proximal update step corresponding to the regularization\nterm. Furthermore, the regularization term is never explicitly differentiated,\nallowing non-smooth regularization penalties to be naturally incorporated. The\nwave equation is solved by use of a time-domain method. The effectiveness of\nthis approach is demonstrated through computer-simulation and experimental\nstudies. The results suggest that the dual averaging method can produce images\nwith less noise and comparable resolution to those obtained by use of\nstochastic gradient descent.\n",
        "  We present spectral line images of [CI] 809 GHz, CO J=1-0 115 GHz and HI 1.4\nGHz line emission, and calculate the corresponding C, CO and H column\ndensities, for a sinuous, quiescent Giant Molecular Cloud about 5 kpc distant\nalong the l=328{\\deg} sightline (hereafter G328) in our Galaxy. The [CI] data\ncomes from the High Elevation Antarctic Terahertz (HEAT) telescope, a new\nfacility on the summit of the Antarctic plateau where the precipitable water\nvapor falls to the lowest values found on the surface of the Earth. The CO and\nHI datasets come from the Mopra and Parkes/ATCA telescopes, respectively. We\nidentify a filamentary molecular cloud, ~75 x 5 pc long with mass ~4 x 10E4\nMsun and a narrow velocity emission range of just 4 km/s. The morphology and\nkinematics of this filament are similar in CO, [CI] and HI, though in the\nlatter appears as self-absorption. We calculate line fluxes and column\ndensities for the three emitting species, which are broadly consistent with a\nPDR model for a GMC exposed to the average interstellar radiation field. The\n[C/CO] abundance ratio averaged through the filament is found to be\napproximately unity. The G328 filament is constrained to be cold (Tdust < 20K)\nby the lack of far-IR emission, to show no clear signs of star formation, and\nto only be mildly turbulent from the narrow line width. We suggest that it may\nrepresent a GMC shortly after formation, or perhaps still be in the process of\nformation.\n",
        "  Ionization feedback should impact the probability distribution function (PDF)\nof the column density around the ionized gas. We aim to quantify this effect\nand discuss its potential link to the Core and Initial Mass Function (CMF/IMF).\nWe used in a systematic way Herschel column density maps of several regions\nobserved within the HOBYS key program: M16, the Rosette and Vela C molecular\ncloud, and the RCW 120 H ii region. We fitted the column density PDFs of all\nclouds with two lognormal distributions, since they present a double-peak or\nenlarged shape in the PDF. Our interpretation is that the lowest part of the\ncolumn density distribution describes the turbulent molecular gas while the\nsecond peak corresponds to a compression zone induced by the expansion of the\nionized gas into the turbulent molecular cloud. The condensations at the edge\nof the ionized gas have a steep compressed radial profile, sometimes\nrecognizable in the flattening of the power-law tail. This could lead to an\nunambiguous criterion able to disentangle triggered from pre-existing star\nformation. In the context of the gravo-turbulent scenario for the origin of the\nCMF/IMF, the double peaked/enlarged shape of the PDF may impact the formation\nof objects at both the low-mass and the high-mass end of the CMF/IMF. In\nparticular a broader PDF is required by the gravo-turbulent scenario to fit\nproperly the IMF with a reasonable initial Mach number for the molecular cloud.\nSince other physical processes (e.g. the equation of state and the variations\namong the core properties) have already been suggested to broaden the PDF, the\nrelative importance of the different effects remains an open question.\n",
        "  Recent work of M. Yoshinaga shows that in some instances certain higher\nhomotopy groups of arrangements map onto non-resonant homology. This is in\ncontrast to the usual Hurewicz map to untwisted homology, which is always the\nzero homomorphism in degree greater than one. In this work we examine this\ndichotomy, generalizing both results.\n",
        "  We consider evolution of a large population, where fitness of each organism\nis defined by many phenotypical traits. These traits result from expression of\nmany genes. We propose a new model of gene regulation, where gene expression is\ncontrolled by a gene network with a threshold mechanism and there is a feedback\nbetween that threshold and gene expression. We show that this regulation is\nvery powerful: depending on parameters we can obtain any functional connection\nbetween thresholds and genes. Under general assumptions on fitness we prove\nthat such model organisms are capable, to some extent, to recognize the fitness\nlandscape. That fitness landscape learning sharply reduces the number of\nmutations necessary for adaptation and thus accelerates of evolution. Moreover,\nthis learning increases phenotype robustness with respect to mutations.\nHowever, this acceleration leads to an additional risk since learning procedure\ncan produce errors. Finally evolution acceleration reminds races on a rugged\nhighway: when you speed up, you have more chances to crash. These results\nexplain recent experimental data on anticipation of environment changes by some\norganisms.\n",
        "  Quality assurance (QA) for medical linear accelerators is indispensable for\nappropriate cancer treatment. Some international organizations and western\nadvanced countries provide QA guidelines for linear accelerators. Currently, QA\nregulations for linear accelerators in Korean hospitals specify a system in\nwhich each hospital stipulates its independent hospital-based protocols for QA\nprocedures (HP_QAPs) and conducts QA based on these HP_QAPs while regulatory\nauthorities verify whether items under these HP_QAPs have been performed.\nHowever, because this regulatory method cannot guarantee the quality of\nuniversal treatment, and QA items with tolerance criteria are different in many\nhospitals, the presentation of standardized QA items and tolerance criteria is\nessential. In this study, QA items in HP_QAPs from various hospitals and those\npresented by international organizations. Concordance rates between QA items\nfor linear accelerators that were presented by the aforementioned organizations\nand those currently being implemented in Korean hospitals were shown to exhibit\na daily QA of 50%, a weekly QA of 22%, a monthly QA of 43%, and an annual QA of\n65%, and the overall concordance rates of all QA items were approximately 48%.\nIn comparison between QA items being implemented in Korean hospitals and those\nbeing implemented in western advanced countries, concordance rates were shown\nto exhibit a daily QA of 50%, a weekly QA of 33%, a monthly QA of 60%, and an\nannual QA of 67%, and the overall concordance rate of all QA items were\napproximately 57%. The results of this study indicate that the HP_QAPs\ncurrently implemented by Korean hospitals as QA standards for linear\naccelerators used in radiation therapy do not meet international standards. To\nsolve this problem, it is necessary to develop national standardized QA items\nand procedures for linear accelerators.\n",
        "  This article describes the following results which relate to each other; i)\nconvergence of high dimensional contact structure to codimension one foliation\nwith Reeb component, ii) relation between Nil-type and Sol-type contact\nsubmanifolds of S^5, iii) definition of convex Thurston-Bennequin inequality,\nand iv) generalization of Lutz twist via convex hypersurface theory. The other\narticle concerning non-convex hypersurfaces is included as an appendix.\n",
        "  Computed tomography (CT) to cone-beam computed tomography (CBCT) deformable\nimage registration (DIR) is a crucial step in adaptive radiation therapy.\nCurrent intensity-based registration algorithms, such as demons, may fail in\nthe context of CT-CBCT DIR because of inconsistent intensities between the two\nmodalities. In this paper, we propose a variant of demons, called Deformation\nwith Intensity Simultaneously Corrected (DISC), to deal with CT-CBCT DIR. DISC\ndistinguishes itself from the original demons algorithm by performing an\nadaptive intensity correction step on the CBCT image at every iteration step of\nthe demons registration. Specifically, the intensity correction of a voxel in\nCBCT is achieved by matching the first and the second moments of the voxel\nintensities inside a patch around the voxel with those on the CT image. It is\nexpected that such a strategy can remove artifacts in the CBCT image, as well\nas ensuring the intensity consistency between the two modalities. DISC is\nimplemented on computer graphics processing units (GPUs) in compute unified\ndevice architecture (CUDA) programming environment. The performance of DISC is\nevaluated on a simulated patient case and six clinical head-and-neck cancer\npatient data. It is found that DISC is robust against the CBCT artifacts and\nintensity inconsistency and significantly improves the registration accuracy\nwhen compared with the original demons.\n",
        "  Recent years have seen a rapid expansion of the model space explored in\nstatistical phylogenetics, emphasizing the need for new approaches to\nstatistical model representation and software development. Clear communication\nand representation of the chosen model is crucial for: (1) reproducibility of\nan analysis, (2) model development and (3) software design. Moreover, a\nunified, clear and understandable framework for model representation lowers the\nbarrier for beginners and non-specialists to grasp complex phylogenetic models,\nincluding their assumptions and parameter/variable dependencies.\n  Graphical modeling is a unifying framework that has gained in popularity in\nthe statistical literature in recent years. The core idea is to break complex\nmodels into conditionally independent distributions. The strength lies in the\ncomprehensibility, flexibility, and adaptability of this formalism, and the\nlarge body of computational work based on it. Graphical models are well-suited\nto teach statistical models, to facilitate communication among phylogeneticists\nand in the development of generic software for simulation and statistical\ninference.\n  Here, we provide an introduction to graphical models for phylogeneticists and\nextend the standard graphical model representation to the realm of\nphylogenetics. We introduce a new graphical model component, tree plates, to\ncapture the changing structure of the subgraph corresponding to a phylogenetic\ntree. We describe a range of phylogenetic models using the graphical model\nframework and introduce modules to simplify the representation of standard\ncomponents in large and complex models. Phylogenetic model graphs can be\nreadily used in simulation, maximum likelihood inference, and Bayesian\ninference using, for example, Metropolis-Hastings or Gibbs sampling of the\nposterior distribution.\n",
        "  Lobb observed in [arXiv:1103.1412] that each equivariant sl(N)\nKhovanov-Rozansky homology over C[a] admits a standard decomposition of a\nsimple form.\n  In the present paper, we derive a formula for the corresponding Lee-Gornik\nspectral sequence in terms of this decomposition. Based on this formula, we\ngive a simple alternative definition of the Lee-Gornik spectral sequence using\nexact couples. We also demonstrate that an equivariant sl(N) Khovanov-Rozansky\nhomology over C[a] can be recovered from the corresponding Lee-Gornik spectral\nsequence via this formula. Therefore, these two algebraic invariants are\nequivalent and contain the same information about the link.\n  As a byproduct of the exact couple construction, we generalize Lee's\nendomorphism on the rational Khovanov homology to a natural exterior algebra\naction on the sl(N) Khovanov-Rozansky homology.\n  A numerical link invariant called torsion width comes up naturally in our\nwork. It determines when the corresponding Lee-Gornik spectral sequence\ncollapses and is bounded from above by the homological thickness of the sl(N)\nKhovanov-Rozansky homology. We use the torsion width to explain why the Lee\nspectral sequences of certain H-thick links collapse so fast.\n",
        "  A novel phase-space source implementation has been designed for GPU-based\nMonte Carlo dose calculation engines. Due to the parallelized nature of GPU\nhardware, it is essential to simultaneously transport particles of the same\ntype and similar energies but separated spatially to yield a high efficiency.\nWe present three methods for phase-space implementation that have been\nintegrated into the most recent version of the GPU-based Monte Carlo\nradiotherapy dose calculation package gDPM v3.0. The first method is to\nsequentially read particles from a patient-dependent phase-space and sort them\non-the-fly based on particle type and energy. The second method supplements\nthis with a simple secondary collimator model and fluence map implementation so\nthat patient-independent phase-space sources can be used. Finally, as the third\nmethod (called the phase-space-let, or PSL, method) we introduce a novel\nstrategy to pre-process patient-independent phase-spaces and bin particles by\ntype, energy and position. Position bins located outside a rectangular region\nof interest enclosing the treatment field are ignored, substantially decreasing\nsimulation time. The three methods were validated in absolute dose against\nBEAMnrc/DOSXYZnrc and compared using gamma-index tests (2%/2mm above the 10%\nisodose). It was found that the PSL method has the optimal balance between\naccuracy and efficiency and thus is used as the default method in gDPM v3.0.\nUsing the PSL method, open fields of 4x4, 10x10 and 30x30 cm2 in water resulted\nin gamma passing rates of 99.96%, 99.92% and 98.66%, respectively. Relative\noutput factors agreed within 1%. An IMRT patient plan using the PSL method\nresulted in a passing rate of 97%, and was calculated in 50 seconds using a\nsingle GPU compared to 8.4 hours (per CPU) for BEAMnrc/DOSXYZnrc.\n",
        "  In this article, a new conceptual biomedical engineering strategy to tackle\nmodern disease challenges, termed as liquid metal enabled electrobiology, is\nproposed. This generalized and easy going way is based on the physiological\nfact that specially administrated electricity would induce a series of\nsubsequent desired biological effects, either shortly, transitional or\npermanently. Owing high compliance within any part of the biological tissues,\nthe liquid metal would aid to mold a pervasive way to treat physiological or\npsychological diseases. As highly conductive and non-toxic multifunctional\nflexible materials, such liquid metals (LMs) consist of the core to generate\nany requested electric treating fields (ETFields) which can adapt to various\nsites inside the human body. The basic mechanisms of electrobiology in\ndelivering electricity to the target tissues and then inducing expected outputs\nfor disease treatment are interpreted. The methods toward realizing soft and\nconformable electronics based on liquid metal are illustrated. Further, a group\nof typical disease challenges were taken to illustrate the basic strategies to\nperform liquid metal electrobiology therapy, which include but are not limited\nto: tissue electronics, brain disorder, immunotherapy, neural functional\nrecovery, muscle stimulation, skin rejuvenation, cosmetology and dieting,\nartificial organs, cardiac pacing and cancer therapy etc. Some practical issues\ninvolved in the electrobiology for future disease therapy were discussed.\nPerspective along this direction to incubate an easy-going biomedical tool for\nhealth care was pointed out.\n",
        "  Quantifying diversity is of central importance for the study of structure,\nfunction and evolution of microbial communities. The estimation of microbial\ndiversity has received renewed attention with the advent of large-scale\nmetagenomic studies. Here, we consider what the diversity observed in a sample\ntells us about the diversity of the community being sampled. First, we argue\nthat one cannot reliably estimate the absolute and relative number of microbial\nspecies present in a community without making unsupported assumptions about\nspecies abundance distributions. The reason for this is that sample data do not\ncontain information about the number of rare species in the tail of species\nabundance distributions. We illustrate the difficulty in comparing species\nrichness estimates by applying Chao's estimator of species richness to a set of\nin silico communities: they are ranked incorrectly in the presence of large\nnumbers of rare species. Next, we extend our analysis to a general family of\ndiversity metrics (\"Hill diversities\"), and construct lower and upper estimates\nof diversity values consistent with the sample data. The theory generalizes\nChao's estimator, which we retrieve as the lower estimate of species richness.\nWe show that Shannon and Simpson diversity can be robustly estimated for the in\nsilico communities. We analyze nine metagenomic data sets from a wide range of\nenvironments, and show that our findings are relevant for empirically-sampled\ncommunities. Hence, we recommend the use of Shannon and Simpson diversity\nrather than species richness in efforts to quantify and compare microbial\ndiversity.\n",
        "  Named Entity Disambiguation (NED) is the task of linking a named-entity\nmention to an instance in a knowledge-base, typically Wikipedia. This task is\nclosely related to word-sense disambiguation (WSD), where the supervised\nword-expert approach has prevailed. In this work we present the results of the\nword-expert approach to NED, where one classifier is built for each target\nentity mention string. The resources necessary to build the system, a\ndictionary and a set of training instances, have been automatically derived\nfrom Wikipedia. We provide empirical evidence of the value of this approach, as\nwell as a study of the differences between WSD and NED, including ambiguity and\nsynonymy statistics.\n",
        "  We define a link homology theory that is readily seen to be both isomorphic\nto reduced odd Khovanov homology and fully determined by data impervious to\nConway mutation. This gives an elementary proof that odd Khovanov homology is\nmutation invariant, and therefore that mod 2 Khovanov homology is mutation\ninvariant. We also establish mutation invariance for the entire Ozsvath-Szabo\nspectral sequence from reduced Khovanov homology to the Heegaard Floer homology\nof the branched double cover.\n",
        "  Developing an application with some tables must concern the validation of\ninput (specially in Table Child). In order to maximize the accuracy and data\ninput validation. Its called lookup (took data from other dataset). There are\ntwo ways to look up data from Table Parent: 1) Using Objects (DBLookupComboBox\nand DBookupListBox), or 2) Arranging the properties of data types fields (shown\nby using DBGrid). In this article is using Borland Delphi software (Inprise\nproduct). The method is offered using 5 (five) practise steps: 1) Relational\nDatabase Scheme, 2) Form Design, 3) Object DatabasesRelationships Scheme, 4)\nProperties and Field Type Arrangement, and 5) Procedures. The result of this\npaper are: 1) The relationship that using lookup objects are valid, and 2)\nDelphi Lookup Objects can be used for 1-1, 1-N, and M-N relationship.\n",
        "  The two definitions of radioactive equilibrium are revisited in this paper.\nThe terms activity equilibrium and effective life equilibrium are proposed to\ntake the place of currently used terms transient equilibrium and secular\nequilibrium. The proposed new definitions have the advantage of providing a\nclearer physics meaning. Besides the well known instant activity equilibrium,\nanother class of exact effective life-time equilibrium is also discussed in\nthis letter.\n",
        "  Recent observations suggest ongoing planet formation in the innermost parsec\nof our Galaxy. The super-massive black hole (SMBH) might strip planets or\nplanetary embryos from their parent star, bringing them close enough to be\ntidally disrupted. We investigate the chance of planet tidal captures by\nrunning three-body encounters of SMBH-star-planet systems with a high-accuracy\nregularized code. We show that tidally captured planets have orbits close to\nthose of their parent star. We conclude that the final periapsis distance of\nthe captured planet from the SMBH will be much larger than 200 AU, unless its\nparent star was already on a highly eccentric orbit.\n",
        "  We prove that if a link admits non-trivial (2k+1)-colorings, with prime\n2k+1>7, it also admits non-trivial (2k+1)-colorings not involving colors 2k,\n2k-1, nor k.\n",
        "  Treatment with high energy ionizing radiation is one of the main methods in\nmodern cancer therapy that is in clinical use. During the last decades, two\nmain approaches to dose calculation were used, Monte Carlo simulations and\nsemi-empirical models based on Fermi-Eyges theory. A third way to dose\ncalculation has only recently attracted attention in the medical physics\ncommunity. This approach is based on the deterministic kinetic equations of\nradiative transfer. Starting from these, we derive a macroscopic partial\ndifferential equation model for electron transport in tissue. This model\ninvolves an angular closure in the phase space. It is exact for the\nfree-streaming and the isotropic regime. We solve it numerically by a newly\ndeveloped HLLC scheme based on [BerCharDub], that exactly preserves key\nproperties of the analytical solution on the discrete level. Several numerical\nresults for test cases from the medical physics literature are presented.\n",
        "  Evidence-based medicine is critically dependent on three sources of\ninformation: a medical knowledge base, the patients medical record and\nknowledge of available resources, including where appropriate, clinical\nprotocols. Patient data is often scattered in a variety of databases and may,\nin a distributed model, be held across several disparate repositories.\nConsequently addressing the needs of an evidence-based medicine community\npresents issues of biomedical data integration, clinical interpretation and\nknowledge management. This paper outlines how the Health-e-Child project has\napproached the challenge of requirements specification for (bio-) medical data\nintegration, from the level of cellular data, through disease to that of\npatient and population. The approach is illuminated through the requirements\nelicitation and analysis of Juvenile Idiopathic Arthritis (JIA), one of three\ndiseases being studied in the EC-funded Health-e-Child project.\n",
        "  This article describes a method called Lorentz Force Electrical Impedance\nTomography. The electrical conductivity of biological tissues can be measured\nthrough their sonication in a magnetic field: the vibration of the tissues\ninside the field induces an electrical current by Lorentz force. This current,\ndetected by electrodes placed around the sample, is proportional to the\nultrasonic pressure, to the strength of the magnetic field and to the\nelectrical conductivity gradient along the acoustic axis. By focusing at\ndifferent places inside the sample, a map of the electrical conductivity\ngradient can be established. In this study experiments were conducted on a\ngelatin phantom and on a beef sample, successively placed in a 300 mT magnetic\nfield and sonicated with an ultrasonic transducer focused at 21 cm emitting 500\nkHz bursts. Although all interfaces are not visible, in this exploratory study\na good correlation is observed between the electrical conductivity image and\nthe ultrasonic image. This method offers an alternative to detecting\npathologies invisible to standard ultrasonography.\n",
        "  These are notes for the lecture on Transport Theory in a one-week intensive\ncourse, \"Techniques of Proton Radiotherapy\". Topics are:\n  Phase space diagrams: model beam line-effect of a scatterer-effect of a\ndrift-the beam ellipse-phase space diagrams for the model beam line-emittance\nchange in a drift-emittance change in a scatterer-phase space for a more\nrealistic beam line-summary\n  Miscellaneous topics: review of Gaussians-the Gaussian approximation to\nmultiple Coulomb scattering (MCS)-relativistic single particle\nkinematics-completing the square-scattering power\n  Fermi-Eyges theory: history-the basic theory-the beam ellipse-drawing the\nellipse given the moments-ellipse examples-transporting the beam ellipse\nthrough a slab-emittance change in a drift-emittance change in a\nscatterer-differential form of the transport equations-equivalent sources-beam\ncontained in the beam ellipse-summary\n  Beam spreading in matter: theory of Preston and Koehler-generalization to\nheavy ions-experimental tests (Preston and Koehler, Phillips, Wong et\nal.)-summary\n  Analytical geometry of the ellipse (appendix): tilted ellipse-transformation\nto principal frame-summary and explicit procedures-area enclosed by the ellipse\n",
        "  We define the notion of a braided link cobordism in $S^3 \\times [0,1]$, which\ngeneralizes Viro's closed surface braids in $\\mathbb{R}^4$. We prove that any\nproperly embedded oriented surface $W \\subset S^3 \\times [0,1]$ is isotopic to\na surface in this special position, and that the isotopy can be taken rel\nboundary when $\\partial W$ already consists of closed braids. These surfaces\nare closely related to another notion of surface braiding in $D^2 \\times D^2$,\ncalled braided surfaces with caps, which are a generalization of Rudolph's\nbraided surfaces. We mention several applications of braided surfaces with\ncaps, including using them to apply algebraic techniques from braid groups to\nstudying surfaces in 4-space, as well as constructing singular fibrations on\nsmooth 4-manifolds from a given handle decomposition.\n",
        "  In this note, we collect various properties of Seifert homology spheres from\nthe viewpoint of Dehn surgery along a Seifert fiber. We expect that many of\nthese are known to various experts, but include them in one place which we hope\nto be useful in the study of concordance and homology cobordism.\n",
        "  The dynamics of the interstellar medium (ISM) are strongly affected by\nturbulence, which shows increased anisotropy in the presence of a magnetic\nfield. We expand upon the Esquivel & Lazarian method to estimate the Alfven\nMach number using the structure function anisotropy in velocity centroid data\nfrom position-position-velocity maps. We utilize 3D magnetohydrodynamic (MHD)\nsimulations of fully developed turbulence, with a large range of sonic and\nAlfvenic Mach numbers, to produce synthetic observations of velocity centroids\nwith observational characteristics such as thermal broadening, cloud\nboundaries, noise, and radiative transfer effects of carbon monoxide. In\naddition, we investigate how the resulting anisotropy-Alfven Mach number\ndependency found in Esquivel & Lazarian (2011) might change when taking the\nsecond moment of the position-position-velocity cube or when using different\nexpressions to calculate the velocity centroids. We find that the degree of\nanisotropy is related primarily to the magnetic field strength (i.e. Alfven\nMach number) and the line-of-sight orientation, with a secondary effect on\nsonic Mach number. If the line-of-sight is parallel to up to ~45 deg off of the\nmean field direction, the velocity centroid anisotropy is not prominent enough\nto distinguish different Alfvenic regimes. The observed anisotropy is not\nstrongly affected by including radiative transfer, although future studies\nshould include additional tests for opacity effects. These results open up the\npossibility of studying the magnetic nature of the ISM using statistical\nmethods in addition to existing observational techniques.\n",
        "  We study temperature dependence of the critical current modulation Ic(H) for\ntwo types of planar Josephson junctions: a low-Tc Nb/CuNi/Nb and a high-Tc\nYBa2Cu3O7 bicrystal grain-boundary junction. At low T both junctions exhibit a\nconventional behavior, described by the local sine-Gordon equation. However, at\nelevated T the behavior becomes qualitatively different: the Ic(H) modulation\nfield deltaH becomes almost T-independent and neither deltaH nor the critical\nfield for penetration of Josephson vortices vanish at Tc. Such an unusual\nbehavior is in good agreement with theoretical predictions for junctions with\nnonlocal electrodynamics. We extract absolute values of the London penetration\ndepth from our data and show that a crossover from local to nonlocal\nelectrodynamics occurs with increasing T when London penetration depth becomes\nlarger than the electrode thickness.\n",
        "  We present the theoretical investigation of the single walled carbon\nnanotubes (SWNTs) as the heating agent of photothermal therapy. In our model,\nthe SWNT is modeled by rigid tube surrounded by cancer cells. In this model, we\nneglect the angle dependence of temperature and assume that the length of SWNT\nis much longer than the radius of tube. We calculated the temperature rise of\nthe SWNT and its surrounding cancer cells during the laser heating by solving\none-dimensional heat conduction equation in steady state condition. We found\nthat the maximum temperature is located at the interface between SWNT and\ncancer cells. This maximum temperature is proportional to the square of SWNTs\ndiameter and diameter of SWNTs depends on their chirality. These results extend\nour understanding of the temperature distribution in SWNT during the laser\nheating process and provide the suggested specification of SWNT for the\nimprovement the photothermal therapy in the future.\n",
        "  Unlike the random radial orientation distribution of field elliptical\ngalaxies, galaxies in a cluster are expected to point preferentially towards\nthe center of the cluster, as a result of the cluster's tidal force on its\nmember galaxies. In this work an analytic model is formulated to simulate this\neffect. The deformation time scale of a galaxy in a cluster is usually much\nshorter than the time scale of change of the tidal force; the dynamical process\nof the tidal interaction within the galaxy can thus be ignored. An equilibrium\nshape of a galaxy is then assumed to be the surface of equipotential, which is\nthe sum of the self-gravitational potential of the galaxy and the tidal\npotential of the cluster at this location. We use a Monte-Carlo method to\ncalculate the radial orientation distribution of these galaxies, by assuming\nthe NFW mass profile of the cluster and the initial ellipticity of field\ngalaxies. The radial angles show a single peak distribution centered at zero.\nThe Monte-Carlo simulations also show that a shift of the reference center from\nthe real cluster center weakens the anisotropy of the radial angle\ndistribution. Therefore, the expected radial alignment cannot be revealed if\nthe distribution of spatial position angle is used instead of that of radial\nangle. The observed radial orientations of elliptical galaxies in cluster\nAbell~2744 are consistent with the simulated distribution.\n",
        "  Natural language generation plays a critical role in spoken dialogue systems.\nWe present a new approach to natural language generation for task-oriented\ndialogue using recurrent neural networks in an encoder-decoder framework. In\ncontrast to previous work, our model uses both lexicalized and delexicalized\ncomponents i.e. slot-value pairs for dialogue acts, with slots and\ncorresponding values aligned together. This allows our model to learn from all\navailable data including the slot-value pairing, rather than being restricted\nto delexicalized slots. We show that this helps our model generate more natural\nsentences with better grammar. We further improve our model's performance by\ntransferring weights learnt from a pretrained sentence auto-encoder. Human\nevaluation of our best-performing model indicates that it generates sentences\nwhich users find more appealing.\n",
        "  The compressed sensing (CS) framework leverages the sparsity of MR images to\nreconstruct from undersampled acquisitions. CS reconstructions involve one or\nmore regularization parameters that weigh sparsity in transform domains against\nfidelity to acquired data. While parameter selection is critical for\nreconstruction quality, the optimal parameters are subject and dataset\nspecific. Thus, commonly practiced heuristic parameter selection generalizes\npoorly to independent datasets. Recent studies have proposed to tune parameters\nby estimating the risk of removing significant image coefficients. Line\nsearches are performed across the parameter space to identify the parameter\nvalue that minimizes this risk. Although effective, these line searches yield\nprolonged reconstruction times. Here, we propose a new self-tuning CS method\nfor multi-coil multi-acquisition reconstructions. The proposed method uses\ncomputationally efficient projections onto epigraph sets of the $l_1$ and\ntotal-variation norms to simultaneously achieve parameter selection and\nregularization. In vivo demonstrations are provided for balanced steady-state\nfree precession, time-of-flight, and T1-weighted imaging. The proposed method\nachieves nearly an order of magnitude improvement in computational efficiency\nover line-search methods while maintaining near-optimal parameter selection.\n",
        "  We study stochastic evolution of optional games on simple graphs. There are\ntwo strategies, A and B, whose interaction is described by a general payoff\nmatrix. In addition there are one or several possibilities to opt out from the\ngame by adopting loner strategies. Optional games lead to relaxed social\ndilemmas. Here we explore the interaction between spatial structure and\noptional games. We find that increasing the number of loner strategies (or\nequivalently increasing mutational bias toward loner strategies) facilitates\nevolution of cooperation both in well-mixed and in structured populations. We\nderive various limits for weak selection and large population size. For some\ncases we derive analytic results for strong selection. We also analyze strategy\nselection numerically for finite selection intensity and discuss combined\neffects of optionality and spatial structure.\n",
        "  In a 3-manifold M, let K be a knot and R be an annulus which meets K\ntransversely. We define the notion of the pair (R,K) being caught by a surface\nQ in the exterior of the link given by K and the boundary curves of R. For a\ncaught pair (R,K), we consider the knot K^n gotten by twisting K n times along\nR and give a lower bound on the bridge number of K^n with respect to Heegaard\nsplittings of M -- as a function of n, the genus of the splitting, and the\ncatching surface Q. As a result, the bridge number of K^n tends to infinity\nwith n. In application, we look at a family of knots K^n found by Teragaito\nthat live in a small Seifert fiber space M and where each K^n admits a Dehn\nsurgery giving the 3-sphere. We show that the bridge number of K^n with respect\nto any genus 2 Heegaard splitting of M tends to infinity with n. This contrasts\nwith other work of the authors as well as with the conjectured picture for\nknots in lens spaces that admit Dehn surgeries giving the 3-sphere.\n",
        "  Fintushel and Stern have proved that if S \\subset X is a symplectic surface\nin a symplectic 4-manifold such that S has simply-connected complement and\nnonnegative self-intersection, then there are infinitely many topologically\nequivalent but smoothly distinct embedded surfaces homologous to S. Here we\nextend this result to include symplectic surfaces whose self-intersection is\nbounded below by 2-2g, where g is the genus of S.\n  We make use of tools from Heegaard Floer theory, and include several results\nthat may be of independent interest. Specifically we give an analogue for\nOzsvath-Szabo invariants of the Fintushel-Stern knot surgery formula for\nSeiberg-Witten invariants, both for closed 4-manifolds and manifolds with\nboundary. This is based on a formula for the Ozsvath-Szabo invariants of the\nresult of a logarithmic transformation, analogous to one obtained by\nMorgan-Mrowka-Szab\\'o for Seiberg-Witten invariants, and the results on\nOzsvath-Szabo invariants of fiber sums due to the author and Jabuka. In\naddition, we give a calculation of the twisted Heegaard Floer homology of\ncircle bundles of \"large\" degree over Riemann surfaces.\n",
        "  Let $\\mathrm{Mod}(S_g)$ denote the mapping class group of the closed\norientable surface $S_g$ of genus $g\\geq 2$. Given a finite subgroup $H$ of\n$\\mathrm{Mod}(S_g)$, let $\\mathrm{Fix}(H)$ denote the set of fixed points\ninduced by the action of $H$ on the Teichm\\\"{u}ller space\n$\\mathrm{Teich}(S_g)$. When $H$ is cyclic with $|H| \\geq 3$, we show that\n$\\mathrm{Fix}(H)$ admits a decomposition as a product of two-dimensional strips\nat least one of which is of bounded width. For an arbitrary $H$ with at least\none generator of order $\\geq 3$, we derive a computable optimal upper bound for\nthe restriction $\\mathrm{sys} : \\mathrm{Fix}(H) \\to \\mathbb{R}^+$ of the\nsystole function. Furthermore, we show that in such a case, $\\mathrm{Fix}(H)$\nis not symplectomorphic to the Euclidean space of the same dimension. Finally,\nwe apply our theory to recover three well-known results, namely: (a) Harvey's\nresult giving the dimension of $\\mathrm{Fix}(H)$, (b) Gilman's result that $H$\nis irreducible if and only if the corresponding orbifold is a sphere with three\ncone points, and (c) the Nielsen realization theorem for cyclic groups.\n",
        "  We present high-resolution CARMA 230 GHz continuum imaging of nine deeply\nembedded protostars in the Serpens Molecular Cloud, including six of the nine\nknown Class 0 protostars in Serpens. This work is part of a program to\ncharacterize disk and envelope properties for a complete sample of Class 0\nprotostars in nearby low-mass star forming regions. Here we present CARMA maps\nand visibility amplitudes as a function of uv-distance for the Serpens sample.\nObservations are made in the B, C, D, and E antenna configurations, with B\nconfiguration observations utilizing the CARMA Paired Antenna Calibration\nSystem. Combining data from multiple configurations provides excellent\nuv-coverage (4-500 klam), allowing us to trace spatial scales from 1e2 to 1e4\nAU. We find evidence for compact disk components in all of the observed Class 0\nprotostars, suggesting that disks form at very early times (t<0.2 Myr) in\nSerpens. We make a first estimate of disk masses using the flux at 50 klam,\nwhere the contribution from the envelope should be negligible, assuming an\nunresolved disk. The resulting disk masses range from 0.04 Msun to 1.7 Msun,\nwith a mean of approximately 0.2 Msun. Our high resolution maps are also\nsensitive to binary or multiple sources with separations > 250 AU, but\nsignificant evidence of multiplicity on scales <2000 AU is seen in only one\nsource.\n",
        "  We present cosmological N-body resimulations of the assembly of the Brightest\nCluster Galaxies (BCGs) in rich clusters. At $z=2$ we populate dark matter\nsubhalos with self-gravitating stellar systems whose abundance and structure\nmatch observed high-redshift galaxies. By $z=0$, mergers have built much larger\ngalaxies at cluster centre. Their dark matter density profiles are shallower\nthan in corresponding dark-matter-only simulations, but their total mass\ndensity profiles (stars + dark matter) are quite similar. Differences are found\nonly at radii where the effects of central black holes may be significant. Dark\nmatter density slopes shallower than $\\gamma=1.0$ occur for $r/r_{200} <\n0.015$, close to the half-light radii of the BCGs. Our experiments support\nearlier suggestions that NFW-like profiles are an attractor for the\nhierarchical growth of structure in collisionless systems -- total mass density\nprofiles asymptote to the solution found in dark-matter-only simulations over\nthe radial range where mergers produce significant mixing between stars and\ndark matter. Simulated dark matter fractions are substantially higher in BCGs\nthan in field ellipticals, reaching 80\\% within the half-light radius. We also\nestimate that supermassive black hole mergers should create BCG cores as large\nas $r_{c}\\sim 3 \\,\\mathrm{kpc}$. The good agreement of all these properties\nwith recent observational studies of BCG structure suggests that dissipational\nprocesses have not played a dominant role in the assembly of the observed\nsystems.\n",
        "  We show that a strongly irreducible and boundary-strongly irreducible surface\ncan be isotoped to be almost normal in a triangulated 3-manifold.\n",
        "  In this paper we introduce the tied links, i.e. ordinary links provided with\nsome ties between strands. The motivation for introducing such objects\noriginates from a diagrammatical interpretation of the defining generators of\nthe so-called algebra of braids and ties; indeed, one half of such generators\ncan be interpreted as the usual generators of the braid algebra, and the\nremaining generators can be interpreted as ties between consecutive strands;\nthis interpretation leads the definition of tied braids. We define an invariant\npolynomial for the tied links via a skein relation. Furthermore, we introduce\nthe monoid of tied braids and we prove the corresponding theorems of Alexander\nand Markov for tied links. Finally, we prove that the invariant of tied links\nthat we have defined can be obtained also by using the Jones recipe.\n",
        "  In several members of the ferro-pnictides, spin density wave (SDW) order\ncoexists with superconductivity over a range of dopings. In this letter we\nstudy the anomalous magnetic Zeeman response of this coexistence state and show\nthat it can be used to confirm the extended s-wave gap structure as well as\nstructure of superconducting (SC) gap in coexisting phase. On increasing the\nfield, a strongly anisotropic reduction of SC gap is found. The anisotropy is\ndirectly connected to the gap structure of superconducting phase. The signature\nof this effect in quasiparticle interference measured by STM, as well as heat\ntransport in magnetic field is discussed. For the compounds with the nodal SC\ngap we show that the nodes are removed upon formation of SDW. Interestingly the\nsize of the generated gap in the originally nodal areas is anisotropic in the\nposition of the nodes over the Fermi surface in direct connection with the form\nof SC pairing.\n",
        "  We propose to learn acoustic word embeddings with temporal context for\nquery-by-example (QbE) speech search. The temporal context includes the leading\nand trailing word sequences of a word. We assume that there exist spoken word\npairs in the training database. We pad the word pairs with their original\ntemporal context to form fixed-length speech segment pairs. We obtain the\nacoustic word embeddings through a deep convolutional neural network (CNN)\nwhich is trained on the speech segment pairs with a triplet loss. Shifting a\nfixed-length analysis window through the search content, we obtain a running\nsequence of embeddings. In this way, searching for the spoken query is\nequivalent to the matching of acoustic word embeddings. The experiments show\nthat our proposed acoustic word embeddings learned with temporal context are\neffective in QbE speech search. They outperform the state-of-the-art\nframe-level feature representations and reduce run-time computation since no\ndynamic time warping is required in QbE speech search. We also find that it is\nimportant to have sufficient speech segment pairs to train the deep CNN for\neffective acoustic word embeddings.\n",
        "  Association rule has been an area of active research in the field of\nknowledge discovery. Data mining researchers had improved upon the quality of\nassociation rule mining for business development by incorporating influential\nfactors like value (utility), quantity of items sold (weight) and more for the\nmining of association patterns. In this paper, we propose an efficient approach\nto find maximal frequent itemset first. Most of the algorithms in literature\nused to find minimal frequent item first, then with the help of minimal\nfrequent itemsets derive the maximal frequent itemsets. These methods consume\nmore time to find maximal frequent itemsets. To overcome this problem, we\npropose a navel approach to find maximal frequent itemset directly using the\nconcepts of subsets. The proposed method is found to be efficient in finding\nmaximal frequent itemsets.\n",
        "  This paper deals with certain results on the number of smooth structures on\nquaternionic projective spaces, obtained through the computation of inertia\ngroup and its analogues, which in turn are computed using techniques from\nstable homotopy theory. We show that the concordance inertia group is trivial\nin dimension 20, but there are many examples in high dimensions where the\nconcordance inertia group is non-trivial. We extend these to computations of\nconcordance classes of smooth structures. These have applications to 3-sphere\nactions on homotopy spheres and tangential homotopy structures.\n",
        "  We present a complex data handling system for the COMPASS tokamak, operated\nby IPP ASCR Prague, Czech Republic [1]. The system, called CDB (Compass\nDataBase), integrates different data sources as an assortment of data\nacquisition hardware and software from different vendors is used. Based on\nwidely available open source technologies wherever possible, CDB is vendor and\nplatform independent and it can be easily scaled and distributed. The data is\ndirectly stored and retrieved using a standard NAS (Network Attached Storage),\nhence independent of the particular technology; the description of the data\n(the metadata) is recorded in a relational database. Database structure is\ngeneral and enables the inclusion of multi-dimensional data signals in multiple\nrevisions (no data is overwritten). This design is inherently distributed as\nthe work is off-loaded to the clients. Both NAS and database can be implemented\nand optimized for fast local access as well as secure remote access. CDB is\nimplemented in Python language; bindings for Java, C/C++, IDL and Matlab are\nprovided. Independent data acquisitions systems as well as nodes managed by\nFireSignal [2] are all integrated using CDB. An automated data post-processing\nserver is a part of CDB. Based on dependency rules, the server executes, in\nparallel if possible, prescribed post-processing tasks.\n",
        "  We report the phase diagram for the superconducting system\n(${^{7}}$Li${_{1-x}}$Fe${_{x}}$OD)FeSe and contrast it with that of\n(Li${_{1-x}}$Fe${_{x}}$OH)FeSe both in single crystal and powder forms. Samples\nwere prepared via hydrothermal methods and characterized with laboratory and\nsynchrotron X-ray diffraction, high-resolution neutron powder diffraction\n(NPD), and high intensity NPD. We find a correlation between the tetragonality\nof the unit cell parameters and the critical temperature, $T_{c}$, which is\nindicative of the effects of charge doping on the lattice and formation of iron\nvacancies in the FeSe layer. We observe no appreciable isotope effect on the\nmaximum $T_{c}$ in substituting H by by D. The NPD measurements definitively\nrule out an antiferromagnetic ordering in the non-superconducting\n(Li${_{1-x}}$Fe${_{x}}$OD)FeSe samples below 120 K, which has been reported in\nnon-superconducting (Li${_{1-x}}$Fe${_{x}}$OH)FeSe.$^{1}$ A likely explanation\nfor the observed antiferromagnetic transition in (Li${_{1-x}}$Fe${_{x}}$OH)FeSe\nsamples is the formation of impurities during their preparation such as\nFe${_{3}}$O${_{4}}$ and LixFeO2, which express a charge ordering transition\nknown as the Verwey transition near 120 K. The concentration of these oxide\nimpurities is found to be dependent on the concentration of the lithium\nhydroxide reagent and the use of H${_{2}}$O vs. D${_{2}}$O as the solvent\nduring synthesis. We also describe the reaction conditions that lead to some of\nour superconducting samples to exhibit ferromagnetism below $T_{c}$.\n",
        "  In this paper, we investigate the problem of computing a multiway join in one\nround of MapReduce when the data may be skewed. We optimize on communication\ncost, i.e., the amount of data that is transferred from the mappers to the\nreducers. We identify join attributes values that appear very frequently, Heavy\nHitters (HH). We distribute HH valued records to reducers avoiding skew by\nusing an adaptation of the Shares~\\cite{AfUl} algorithm to achieve minimum\ncommunication cost. Our algorithm is implemented for experimentation and is\noffered as open source software. Furthermore, we investigate a class of\nmultiway joins for which a simpler variant of the algorithm can handle skew. We\noffer closed forms for computing the parameters of the algorithm for chain and\nsymmetric joins.\n",
        "  We present a new scheme, $\\it{galtag}$, for refining the photometric redshift\nmeasurements of faint galaxies by probabilistically tagging them to observed\ngalaxy groups constructed from a brighter, magnitude-limited spectroscopy\nsurvey. First, this method is tested on the DESI light-cone data constructed on\nthe GALFORM galaxy formation model to tests its validity. We then apply it to\nthe photometric observations of galaxies in the Kilo-Degree Imaging Survey\n(KiDS) over a 1 deg$^2$ region centred at 15$^\\mathrm{h}$. This region contains\nGalaxy and Mass Assembly (GAMA) deep spectroscopic observations (i-band<22) and\nan accompanying group catalogue to r-band<19.8. We demonstrate that even with\nsome trade-off in sample size, an order of magnitude improvement on the\naccuracy of photometric redshifts is achievable when using $\\it{galtag}$. This\napproach provides both refined photometric redshift measurements and group\nrichness enhancement. In combination these products will hugely improve the\nscientific potential of both photometric and spectroscopic datasets. The\n$\\it{galtag}$ software will be made publicly available at\nhttps://github.com/pkaf/galtag.git.\n",
        "  We prove that for particular infinite families of $L$-spaces, arising as\nbranched double covers, the $d$-invariants defined by Ozsv\\'ath and Szab\\'o are\narbitrarily large and small. As a consequence, we generalise a result by Greene\nand Watson by proving, for every odd number $\\Delta \\geq 5$, the existence of\ninfinitely many non-quasi-alternating homologically thin knots with determinant\n$\\Delta^2$, and a result by Hoffman and Walsh concerning the existence of\nhyperbolic weight $1$ manifolds that are not surgery on a knot in $S^3$.\n",
        "  Males are often the \"sicker\" sex with male biased parasitism found in a\ntaxonomically diverse range of species. There is considerable interest in the\nprocesses that could underlie the evolution of sex-biased parasitism. Mating\nsystem differences along with differences in lifespan may play a key role. We\nexamine whether these factors are likely to lead to male-biased parasitism\nthrough natural selection taking into account the critical role that ecological\nfeedbacks play in the evolution of defence. We use a host-parasite model with\ntwo-sexes and the techniques of adaptive dynamics to investigate how mating\nsystem and sexual differences in competitive ability and longevity can select\nfor a bias in the rates of parasitism. Male-biased parasitism is selected for\nwhen males have a shorter average lifespan or when males are subject to greater\ncompetition for resources. Male-biased parasitism evolves as a consequence of\nsexual differences in life history that produce a greater proportion of\nsusceptible females than males and therefore reduce the cost of avoiding\nparasitism in males. Different mating systems such as monogamy, polygamy or\npolyandry did not produce a bias in parasitism through these ecological\nfeedbacks but may accentuate an existing bias.\n",
        "  We report measurements of Rabi oscillations and spectroscopic coherence times\nin an Al/AlOx/Al and three Nb/AlOx/Nb dc SQUID phase qubits. One junction of\nthe SQUID acts as a phase qubit and the other junction acts as a\ncurrent-controlled nonlinear isolating inductor, allowing us to change the\ncoupling to the current bias leads in situ by an order of magnitude. We found\nthat for the Al qubit a spectroscopic coherence time T2* varied from 3 to 7 ns\nand the decay envelope of Rabi oscillations had a time constant T' = 25 ns on\naverage at 80 mK. The three Nb devices also showed T2* in the range of 4 to 6\nns, but T' was 9 to 15 ns, just about 1/2 the value we found in the Al device.\nFor all the devices, the time constants were roughly independent of the\nisolation from the bias lines, implying that noise and dissipation from the\nbias leads were not the principal sources of dephasing and inhomogeneous\nbroadening.\n",
        "  Emission from the 6.7 GHz methanol maser transition is very strong, is\nrelatively stable, has small internal motions, and is observed toward numerous\nmassive star-forming regions in the Galaxy. Our goal is to perform\nhigh-precision astrometry using this maser transition to obtain accurate\ndistances to their host regions. Eight strong masers were observed during five\nepochs of VLBI observations with the European VLBI Network between 2006 June,\nand 2008 March. We report trigonometric parallaxes for five star-forming\nregions, with accuracies as good as $\\sim22 \\mathrm{\\mu}$as. Distances to these\nsources are $2.57^{+0.34}_{-0.27}$ kpc for ON 1, $0.776^{+0.104}_{-0.083}$ kpc\nfor L 1206, $0.929^{+0.034}_{-0.033}$ kpc for L 1287, $2.38^{+0.13}_{-0.12}$\nkpc for NGC 281-W, and $1.59^{+0.07}_{-0.06}$ kpc for S 255. The distances and\nproper motions yield the full space motions of the star-forming regions hosting\nthe masers, and we find that these regions lag circular rotation on average by\n$\\sim$17 km s$^{-1}$, a value comparable to those found recently by similar\nstudies.\n",
        "  In 1933, Meissner and Ochsenfeld reported the expulsion of magnetic flux, the\ndiamagnetic Meissner effect, from the interior of superconducting lead. This\ndiscovery was crucial in formulating the Bardeen-Cooper-Schrieffer (BCS) theory\nof superconductivity. In exotic superconducting systems BCS theory does not\nstrictly apply. A classical example is a superconductor-magnet hybrid system\nwhere magnetic ordering breaks time-reversal symmetry of the superconducting\ncondensate and results in the stabilisation of an odd-frequency superconducting\nstate. It has been predicted that under appropriate conditions, odd-frequency\nsuperconductivity should manifest in the Meissner state as fluctuations in the\nsign of the magnetic susceptibility meaning that the superconductivity can\neither repel (diamagnetic) or attract (paramagnetic) external magnetic flux.\nHere we report local probe measurements of faint magnetic fields in a Au/Ho/Nb\ntrilayer system using low energy muons, where antiferromagnetic Ho (4.5 nm)\nbreaks time-reversal symmetry of the proximity induced pair correlations in Au.\nFrom depth-resolved measurements below the superconducting transition of Nb we\nobserve a local enhancement of the magnetic field in Au that exceeds the\nexternally applied field, thus proving the existence of an intrinsic\nparamagnetic Meissner effect arising from an odd-frequency superconducting\nstate.\n",
        "  In this paper, we would like to quantitatively measure the tumor volume\ncontained in the breast imaged by the Digital Breast Tomosynthesis (DBT), a\nreconstructed 3D image. The estimated volume will add to the prognostic value\nof risk classification of breast cancer. We develop an algorithm that offers an\nalternative way of estimating the volume of the tumor in a breast. We segment\nthe region of interest by expressing the ratio of tumor region to normal region\nas a function of the threshold value in the image. Next, we determine the\nvolume of the tumor region as a function of the threshold. We then find the\noptimal threshold value that yields the volume of the tumor contained in the\nbreast with the rate of growth of the tumor volume function.\n",
        "  The \"Law of the Minimum\" states that growth is controlled by the scarcest\nresource (limiting factor). This concept was originally applied to plant or\ncrop growth (Justus von Liebig, 1840) and quantitatively supported by many\nexperiments. Some generalizations based on more complicated \"dose-response\"\ncurves were proposed. Violations of this law in natural and experimental\necosystems were also reported. We study models of adaptation in ensembles of\nsimilar organisms under load of environmental factors and prove that violation\nof Liebig's law follows from adaptation effects. If the fitness of an organism\nin a fixed environment satisfies the Law of the Minimum then adaptation\nequalizes the pressure of essential factors and therefore acts against the\nLiebig's law. This is the the Law of the Minimum paradox: if for a randomly\nchosen pair \"organism-environment\" the Law of the Minimum typically holds,\nthen, in a well-adapted system, we have to expect violations of this law.\n  For the opposite interaction of factors (a synergistic system of factors\nwhich amplify each other) adaptation leads from factor equivalence to\nlimitations by a smaller number of factors.\n  For analysis of adaptation we develop a system of models based on Selye's\nidea of the universal adaptation resource (adaptation energy). These models\npredict that under the load of an environmental factor a population separates\ninto two groups (phases): a less correlated, well adapted group and a highly\ncorrelated group with a larger variance of attributes, which experiences\nproblems with adaptation. Some empirical data are presented and evidences of\ninterdisciplinary applications to econometrics are discussed.\n",
        "  Using an idea of Doug Lind, we give a lower bound for the Perron-Frobenius\ndegree of a Perron number that is not totally-real. As an application, we prove\nthat there are cubic Perron numbers whose Perron-Frobenius degrees are\narbitrary large; a result known to Lind, McMullen and Thurston. A similar\nresult is proved for biPerron numbers.\n",
        "  Data fusion has played an important role in data mining because high-quality\ndata is required in a lot of applications. As on-line data may be out-of-date\nand errors in the data may propagate with copying and referring between\nsources, it is hard to achieve satisfying results with merely applying existing\ndata fusion methods to fuse Web data. In this paper, we make use of the crowd\nto achieve high-quality data fusion result. We design a framework selecting a\nset of tasks to ask crowds in order to improve the confidence of data. Since\ndata are correlated and crowds may provide incorrect answers, how to select a\nproper set of tasks to ask the crowd is a very challenging problem. In this\npaper, we design an approximation solution to address this challenge since we\nprove that the problem is at NP-hard. To further improve the efficiency, we\ndesign a pruning strategy and a preprocessing method, which effectively improve\nthe performance of the proposed approximation solution. Furthermore, we find\nthat under certain scenarios, we are not interested in all the facts, but only\na specific set of facts. Thus, for these specific scenarios, we also develop\nanother approximation solution which is much faster than the general\napproximation solution. We verify the solutions with extensive experiments on a\nreal crowdsourcing platform.\n",
        "  The penetration depth l_j in superconducting junctions is identified within\nthe Ginzburg-Landau theory as a function of the interfacial pair breaking, of\nthe magnetic field and of the Josephson coupling strength. When the interfacial\npair breaking goes up, l_j increases and an applicability of the local\nJosephson electrodynamics to junctions with a strong Josephson coupling is\nextended. In the junctions with strongly anharmonic current-phase relations,\nthe magnetic field dependence of l_j is shown to lead to a significant\ndifference between the weak-field penetration depth and the characteristic size\nof the Josephson vortex. For such junctions a nonmonotonic dependence of l_j\nand of the lower critical field on the Josephson coupling constant is found,\nand the specific features of spatial profiles of the supercurrent and the\nmagnetic field in the Josephson vortex are established.\n",
        "  Virtual assistants and text chatbots have recently been gaining popularity.\nGiven the short message nature of text-based chat interactions, the language\nidentification systems of these bots might only have 15 or 20 characters to\nmake a prediction. However, accurate text language identification is important,\nespecially in the early stages of many multilingual natural language processing\npipelines.\n  This paper investigates the use of a naive Bayes classifier, to accurately\npredict the language family that a piece of text belongs to, combined with a\nlexicon based classifier to distinguish the specific South African language\nthat the text is written in. This approach leads to a 31% reduction in the\nlanguage detection error.\n  In the spirit of reproducible research the training and testing datasets as\nwell as the code are published on github. Hopefully it will be useful to create\na text language identification shared task for South African languages.\n",
        "  The discovery of frequent itemsets can serve valuable economic and research\npurposes. Releasing discovered frequent itemsets, however, presents privacy\nchallenges. In this paper, we study the problem of how to perform frequent\nitemset mining on transaction databases while satisfying differential privacy.\nWe propose an approach, called PrivBasis, which leverages a novel notion called\nbasis sets. A theta-basis set has the property that any itemset with frequency\nhigher than theta is a subset of some basis. We introduce algorithms for\nprivately constructing a basis set and then using it to find the most frequent\nitemsets. Experiments show that our approach greatly outperforms the current\nstate of the art.\n",
        "  The males of the specie of frogs Engystomops pustulosus produce simple and\ncom- plex calls to lure females, as a way of Intersexual selection. Complex\ncalls lead males to a greater reproductive success than simple calls do.\nHowever, the complex calls are also more attractive to the main predator of\nthese amphibians, the bat Trachops cirrhosus. Therefore, as M. Ryan suggests,\nthe complexity of the calls let the frogs keep a trade off between reproductive\nsuccess and predation. In this paper, we first propose to model the proportion\nof simple to complex calls as a symmetric game of two strategies. We also\npropose a model with three strategies (simple callers, complex callers and\nquiet males), where we assess the effect of a male that keeps quiet and\nintercepts females, which would play a role of Intrasexual selection. We\nanalyze the stable points of the replicator equations of the models that we\npropose. Under the assumption that the decision of the males takes into account\nthis trade off between reproductive success and predation, our model reproduces\nthe observed behavior reported in the literature with minimal assumption on the\nparameters. From the three strategies model, we verify that the quiet strategy\ncould only coexists with the simple and complex strategies as long as the rate\nat which quiet males intercept females is high. We conclude that the\nreproductive strategy of the male frog Engystomops pustulosus is rational.\n",
        "  This paper provides a fast and patient-specific scatter artifact correction\nmethod for cone-beam computed tomography (CBCT) used in image-guided\ninterventional procedures. Due to increased irradiated volume of interest in\nCBCT imaging, scatter radiation has increased dramatically compared to 2D\nimaging, leading to a degradation of image quality. In this study, we propose a\nscatter artifact correction strategy using an analytical convolution-based\nmodel whose free parameters are estimated using a rough estimation of scatter\nprofiles from the acquired cone-beam projections. It was evaluated using Monte\nCarlo simulations with both monochromatic and polychromatic X-ray sources. The\nresults demonstrated that the proposed method significantly reduced the\nscatter-induced shading artifacts and recovered CT numbers.\n",
        "  Current state-of-the-art semantic role labeling (SRL) uses a deep neural\nnetwork with no explicit linguistic features. However, prior work has shown\nthat gold syntax trees can dramatically improve SRL decoding, suggesting the\npossibility of increased accuracy from explicit modeling of syntax. In this\nwork, we present linguistically-informed self-attention (LISA): a neural\nnetwork model that combines multi-head self-attention with multi-task learning\nacross dependency parsing, part-of-speech tagging, predicate detection and SRL.\nUnlike previous models which require significant pre-processing to prepare\nlinguistic features, LISA can incorporate syntax using merely raw tokens as\ninput, encoding the sequence only once to simultaneously perform parsing,\npredicate detection and role labeling for all predicates. Syntax is\nincorporated by training one attention head to attend to syntactic parents for\neach token. Moreover, if a high-quality syntactic parse is already available,\nit can be beneficially injected at test time without re-training our SRL model.\nIn experiments on CoNLL-2005 SRL, LISA achieves new state-of-the-art\nperformance for a model using predicted predicates and standard word\nembeddings, attaining 2.5 F1 absolute higher than the previous state-of-the-art\non newswire and more than 3.5 F1 on out-of-domain data, nearly 10% reduction in\nerror. On ConLL-2012 English SRL we also show an improvement of more than 2.5\nF1. LISA also out-performs the state-of-the-art with contextually-encoded\n(ELMo) word representations, by nearly 1.0 F1 on news and more than 2.0 F1 on\nout-of-domain text.\n",
        "  In 1986, W. Thurston introduced a (possibly degenerate) norm on the first\ncohomology group of a 3-manifold. Inspired by this definition, Turaev\nintroduced in 2002 a analogous norm on the first cohomology group of a finite\n2-complex. We show that if N is the exterior of a link in a rational homology\nsphere, then the Thurston norm agrees with a suitable variation of Turaev's\nnorm defined on any 2-skeleton of N.\n",
        "  We derive an adjunction inequality for any smooth, closed, connected,\noriented 4-manifold $X$ with $b^+=1$. This inequality depends only on the\ncohomology algebra and generalizes the inequality of Strle in the case of\n$b_1=0$. We demonstrate that the inequality is especially powerful when\n$2\\tilde \\chi+3\\sigma\\geq 0$, where $\\tilde \\chi$ is the modified Euler number\ntaking account of the cup product on $H^1$.\n",
        "  The purpose of this study was to use various dosimetrical indices to\ndetermine the best IMRT modality technique for treating patients with prostate\ncancer. Ten patients with prostate cancer were included in this study.\nIntensity modulated radiation therapy plans were designed to include different\nmodalities, including the linac step and shoot, Tomotherapy, RapidArc, and\nProton systems. Various dosimetrical indices, like the prescription isodose to\ntarget volume (PITV) ratio, conformity index (CI), homogeneity index (HI),\ntarget coverage index (TCI), modified dose homogeneity index (MHI),\nconformation number (CN), critical organ scoring index (COSI), and quality\nfactor (QF) were determined to compare the different treatment plans.\nBiological indices such as the generalized equivalent uniform dose (gEUD),\nbased tumor control probability (TCP), and normal tissue complication\nprobability (NTCP) were also calculated and used to compare the treatment\nplans. The RapidArc plan attained better PTV coverage, as evidenced by its\nsuperior PITV, CI, TCI, MHI, and CN values. Regarding OARs, proton therapy\nexhibited superior dose sparing for the rectum and bowel in low dose volumes,\nwhereas the Tomotherapy and RapidArc plans achieved better dose sparing in high\ndose volumes. The QF scores showed no significant difference among these plans\n(p=0.701). The average TCPs for prostate tumors in the RapidArc, Linac, and\nProton plans were higher than the average TCP for Tomotherapy (98.79%, 98.76%,\nand 98.75% vs. 98.70%, respectively). Regarding the rectum NTCP, RapidArc\nshowed the most favorable result (0.09%), whereas Linac resulted in the best\nbladder NTCP (0.08%).\n",
        "  This report is an introduction to mathematical map colouring and the problems\nposed by Heawood in his paper of 1890. There will be a brief discussion of the\nMap Colour Theorem; then we will move towards investigating empire maps in the\nplane and the recent contributions by Wessel. Finally we will conclude with a\ndiscussion of all known results for empire maps on higher genus surfaces and\nprove Heawoods Empire Conjecture in a previously unknown case.\n",
        "  A letter to the editor of the Journal of Environmental Radioactivity on the\narticle: E. Gasser, A. Nachab, A. Nourreddine, Ch. Roy, and A. Sellam, `Update\nof 40K and 226Ra and 232Th series $\\gamma$-to-dose conversion factors for\nsoil', J. Environ. Radioactiv. 138, 68-71 (2014), DOI:\n10.1016/j.jenvrad.2014.08.002.\n",
        "  We present stacked average far-infrared spectra of a sample of 197 dusty,\nstar-forming galaxies (DSFGs) at $0.005 < z < 4$ using close to 90% of the\nSPIRE Fourier Transform Spectrometer (FTS) extragalactic data archive from the\nHerschel Space Observatory based on 3.5 years of science operations. These\nspectra explore an observed-frame $\\rm 447\\,GHz-1568\\,GHz$ ($\\rm 191\\,\\mu\nm-671\\,\\mu m$) frequency (wavelength) range allowing us to observe the main\natomic and molecular lines emitted by gas in the interstellar medium. The\nsample is sub-divided into five redshift bins at $0.005 < z < 0.05$, $0.05 < z\n< 0.2$, $0.2 < z < 0.5$, $0.8 < z <2$, and $2 < z < 4$. To study the dependence\nof observed spectral lines on total infrared luminosity, the sources in a\nsubset of the redshift bins are stacked in luminosity bins. These stacked\nspectra are used to determine the average properties of the interstellar medium\nand dense molecular gas properties of DSFGs, in particular, the fine-structure\nline ([CII] 158 $\\mu$m and [OI] 63 $\\mu$m) luminosity ratios, and the line to\nfar-IR luminosity ratios are used to model the gas density and radiation field\nstrength in the photodissociation regions (PDRs). For the low-redshift sample,\nwe additionally present the average spectral line energy distributions (SLED)\nof CO and $\\rm{H_2O}$ rotational transitions and also consider PDR conditions\nbased on a combination of [CI] 370 $\\mu$m and 609 $\\mu$m and $\\rm CO (7-6)$\nlines. For the high-z ($0.8 < z < 4$) sample PDR models suggest a molecular gas\ndistribution in the presence of a radiation field that is at least a factor of\n10$^3$ larger than the Milky-Way and with a neutral gas density of roughly\n10$^3$ to 10$^5$ cm$^{-3}$. The corresponding PDR models for the low-z sample\nsuggest a UV radiation field and gas density comparable to those at high-z.\n",
        "  We give a characterization of alternating link exteriors in terms of cubed\ncomplexes. To this end, we introduce the concept of a \"signed BW\ncubed-complex\", and give a characterization for a signed BW cubed-complex to\nhave the underlying space which is homeomorphic to an alternating link\nexterior.\n",
        "  This paper describes a new open domain dialogue system Alquist developed as\npart of the Alexa Prize competition for the Amazon Echo line of products. The\nAlquist dialogue system is designed to conduct a coherent and engaging\nconversation on popular topics. We are presenting a hybrid system combining\nseveral machine learning and rule based approaches. We discuss and describe the\nAlquist pipeline, data acquisition, and processing, dialogue manager, NLG,\nknowledge aggregation and hierarchy of sub-dialogs. We present some of the\nexperimental results.\n",
        "  We have developed a model for proton depth dose and lateral distributions\nbased on Monte Carlo calculations (GEANT4) and an integration procedure of the\nBethe-Bloch equation (BBE). The model accounts for the transport of primary and\nsecondary protons, the creation of recoil protons and heavy recoil nuclei as\nwell as lateral scattering of these contributions. The buildup, which is\nexperimentally observed in higher energy depth dose curves, is modeled by\ninclusion of two different origins: 1. Secondary reaction protons with a\ncontribution of ca. 65 % of the buildup (for monoenergetic protons). 2. Landau\ntails as well as Gaussian type of fluctuations for range straggling effects.\nAll parameters of the model for initially monoenergetic proton beams have been\nobtained from Monte Carlo calculations or checked by them. Furthermore, there\nare a few parameters, which can be obtained by fitting the model to measured\ndepth dose curves in order to describe individual characteristics of the\nbeamline - the most important being the initial energy spread. We find that the\nfree parameters of the depth dose model can be predicted for any intermediate\nenergy from a couple of measured curves.\n",
        "  We work on translation from rich-resource languages to low-resource\nlanguages. The main challenges we identify are the lack of low-resource\nlanguage data, effective methods for cross-lingual transfer, and the\nvariable-binding problem that is common in neural systems. We build a\ntranslation system that addresses these challenges using eight European\nlanguage families as our test ground. Firstly, we add the source and the target\nfamily labels and study intra-family and inter-family influences for effective\ncross-lingual transfer. We achieve an improvement of +9.9 in BLEU score for\nEnglish-Swedish translation using eight families compared to the single-family\nmulti-source multi-target baseline. Moreover, we find that training on two\nneighboring families closest to the low-resource language is often enough.\nSecondly, we construct an ablation study and find that reasonably good results\ncan be achieved even with considerably less target data. Thirdly, we address\nthe variable-binding problem by building an order-preserving named entity\ntranslation model. We obtain 60.6% accuracy in qualitative evaluation where our\ntranslations are akin to human translations in a preliminary study.\n",
        "  We present optical photometry of Hubble Space Telescope (HST) ACS/WFC data of\nthe resolved stellar populations in the outer disc of the dwarf irregular\ngalaxy DDO 154. The photometry reveals that young main sequence stars are\nalmost absent from the outermost HI disc. Instead, most are clustered near the\nmain stellar component of the galaxy. We constrain the stellar initial mass\nfunction (IMF) by comparing the luminosity function of the main sequence stars\nto simulated stellar populations assuming a constant star formation rate over\nthe dynamical timescale. The best-fitting IMF is deficient in high mass stars\ncompared to a canonical Kroupa IMF, with a best-fit slope $\\alpha = -2.45$ and\nupper mass limit $M_U = 16\\ M_{\\odot}$. This top-light IMF is consistent with\npredictions of the Integrated Galaxy-wide IMF theory. Combining the HST images\nwith HI data from The HI Nearby Galaxy Survey Treasury (THINGS) we determine\nthe star formation law (SFL) in the outer disc. The fit has a power law\nexponent $N = 2.92 \\pm0.22$ and zero point $A=4.47 \\pm 0.65 \\times 10^{-7} \\\nM_{\\odot} \\ \\text{yr}^{-1} \\ \\text{kpc}^{-2}$. This is depressed compared to\nthe Kennicutt-Schmidt Star Formation Law, but consistent with weak star\nformation observed in diffuse HI environments. Extrapolating the SFL over the\nouter disc implies that there could be significant star formation occurring\nthat is not detectable in H$\\alpha$. Last, we determine the Toomre stability\nparameter $Q$ of the outer disc of DDO 154 using the THINGS HI rotation curve\nand velocity dispersion map. 72% of the HI in our field has $Q\\leq 4$ and this\nincorporates 96% of the observed MS stars. Hence 28% of the HI in the field is\nlargely dormant.\n",
        "  In this work, we report the simulation of C4+ irradiation and its significant\neffects towards the enhancement of the critical current density in\nBaFe1.9Ni0.1As2 single crystals. BaFe1.9Ni0.1As2 single crystals with and\nwithout the C-implantation were characterized by magneto-transport and magnetic\nmeasurements up to 13 T over a wide range of temperatures below and above the\nsuperconducting critical temperature, Tc. It is found that the C-implantation\ncauses little change in Tc, but it can greatly enhance the in-field critical\ncurrent density by a factor of up to 1.5 with enhanced flux jumping at 2 K. Our\nMonte Carlo simulation results show that all the C ions end up in a well\ndefined layer, causing extended defects and vacancies at the layer, but few\ndefects elsewhere on the implantation paths. This type of defect distribution\nis distinct from the columnar defects produced by heavy ion implantation.\nFurthermore, the normal state resistivity is enhanced by the light C4+\nirradiation, while the upper critical field, Hc2, the irreversibility field,\nHirr, and Tc were affected very little.\n",
        "  We used low-temperature synchrotron x-ray diffraction to investigate the\nstructural phase transitions of Fe1+yTe in the vicinity of a tricitical point\nin the phase diagram. Detailed analysis of the powder diffraction patterns and\ntemperature dependence of the peak-widths in Fe1+yTe showed that two-step\nstructural and magnetic phase transitions occur within the compositional range\n0.11 $\\leq y \\leq $ 0.13. The phase transitions are sluggish indicating a\nstrong competition between the orthorhombic and the monoclinic phases. We\ncombine high-resolution diffraction experiments with specific heat,\nresistivity, and magnetization measurements and present a revised\ntemperature-composition phase diagram for Fe1+yTe.\n",
        "  Masses of experiments have shown individual preference for fairness which\nseems irrational. The reason behind it remains a focus for research. The effect\nof spite (individuals are only concerned with their own relative standing) on\nthe evolution of fairness has attracted increasing attention from experiments,\nbut only has been implicitly studied in one evolutionary model. The model did\nnot involve high-offer rejections, which have been found in the form of\nnon-monotonic rejections (rejecting offers that are too high or too low) in\nexperiments. Here, we introduce a high offer and a non-monotonic rejection in\nstructured populations of finite size, and use strategy intervention to\nexplicitly study how spite influences the evolution of fairness: five\nstrategies are in sequence added into the competition of a fair strategy and a\nselfish strategy. We find that spite promotes fairness, altruism inhibits\nfairness, and the non-monotonic rejection can cause fairness to overcome\nselfishness, which cannot happen without high-offer rejections. Particularly\nfor the group-structured population with seven discrete strategies, we\nanalytically study the effect of population size, mutation, and migration on\nfairness, selfishness, altruism, and spite. A larger population size cannot\nchange the dominance of fairness, but it promotes altruism and inhibits\nselfishness and spite. Intermediate mutation maximizes selfishness and\nfairness, and minimizes spite; intermediate mutation maximizes altruism for\nintermediate migration and minimizes altruism otherwise. The existence of\nmigration inhibits selfishness and fairness, and promotes altruism; sufficient\nmigration promotes spite. Our study may provide important insights into the\nevolutionary origin of fairness.\n",
        "  For the alternating knots or links, mutations do not change the arc index. In\nthe case of nonalternating knots, some semi-alternating knots or links have\nthis property. We mainly focus on the problem of mutation invariance of the arc\nindex for nonalternating knots which are not semi-alternating. In this paper,\nwe found families of infinitely many mutant pairs/triples of Montesinos knots\nwith the same arc index.\n",
        "  We construct a mean-field theory for itinerant ferromagnetism coexisting with\na non-unitary superconducting state, where only the majority-spin band is\ngapped and contains line nodes, while the minority-spin band is gapless at the\nFermi level. Our study is motivated by recent experimental results indicating\nthat this may be the physical situation realized in the heavy-fermion compound\nUGe$_2$. We investigate the stability of the mean-field solution of the\nmagnetic and superconducting order parameters. Also, we provide theoretical\npredictions for experimentally measurable properties of such a non-unitary\nsuperconductor: the specific heat capacity, the Knight shift, and the tunneling\nconductance spectra. Our study should be useful for direct comparison with\nexperimental results and also for further predictions of the physics that may\nbe expected in ferromagnetic superconductors.\n",
        "  To understand narrative, humans draw inferences about the underlying\nrelations between narrative events. Cognitive theories of narrative\nunderstanding define these inferences as four different types of causality,\nthat include pairs of events A, B where A physically causes B (X drop, X\nbreak), to pairs of events where A causes emotional state B (Y saw X, Y felt\nfear). Previous work on learning narrative relations from text has either\nfocused on \"strict\" physical causality, or has been vague about what relation\nis being learned. This paper learns pairs of causal events from a corpus of\nfilm scene descriptions which are action rich and tend to be told in\nchronological order. We show that event pairs induced using our methods are of\nhigh quality and are judged to have a stronger causal relation than event pairs\nfrom Rel-grams.\n",
        "  Existing corpora for intrinsic evaluation are not targeted towards tasks in\ninformal domains such as Twitter or news comment forums. We want to test\nwhether a representation of informal words fulfills the promise of eliding\nexplicit text normalization as a preprocessing step. One possible evaluation\nmetric for such domains is the proximity of spelling variants. We propose how\nsuch a metric might be computed and how a spelling variant dataset can be\ncollected using UrbanDictionary.\n",
        "  For an oriented link diagram D, the warping degree d(D) is the smallest\nnumber of crossing changes which are needed to obtain a monotone diagram from\nD. We show that d(D)+d(-D)+sr(D) is less than or equal to the crossing number\nof D, where -D denotes the inverse of D and sr(D) denotes the number of\ncomponents which have at least one self-crossing. Moreover, we give a necessary\nand sufficient condition for the equality. We also consider the minimal\nd(D)+d(-D)+sr(D) for all diagrams D. For the warping degree and linking warping\ndegree, we show some relations to the linking number, unknotting number, and\nthe splitting number.\n",
        "  This work addresses computing techniques for dose calculations in treatment\nplanning with proton and ion beams, based on an efficient kernel-convolution\nmethod referred to as grid-dose spreading (GDS) and accurate\nheterogeneity-correction method referred to as Gaussian beam splitting. The\noriginal GDS algorithm suffered from distortion of dose distribution for beams\ntilted with respect to the dose-grid axes. Use of intermediate grids normal to\nthe beam field has solved the beam-tilting distortion. Interplay of arrangement\nbetween beams and grids was found as another intrinsic source of artifact.\nInclusion of rectangular-kernel convolution in beam transport, to share the\nbeam contribution among the nearest grids in a regulatory manner, has solved\nthe interplay problem. This algorithmic framework was applied to a tilted\nproton pencil beam and a broad carbon-ion beam. In these cases, while the\nelementary pencil beams individually split into several tens, the calculation\ntime increased only by several times with the GDS algorithm. The GDS and\nbeam-splitting methods will complementarily enable accurate and efficient dose\ncalculations for radiotherapy with protons and ions.\n",
        "  In order to solve a system of nonlinear rate equations one can try to use\nsome soliton methods. The procedure involves three steps: (1) Find a `Lax\nrepresentation' where all the kinetic variables are combined into a single\nmatrix $\\rho$, all the kinetic constants are encoded in a matrix $H$; (2) find\na Darboux-Backund dressing transformation for the Lax representation $i\\dot\n\\rho=[H,f(\\rho)]$, where $f$ models a time-dependent environment; (3) find a\nclass of seed solutions $\\rho=\\rho[0]$ that lead, via a nontrivial chain of\ndressings $\\rho[0]\\to \\rho[1]\\to \\rho[2]\\to\\dots$ to new solutions, difficult\nto find by other methods. The latter step is not a trivial one since a\nnon-soliton method has to be employed to find an appropriate initial $\\rho[0]$.\nProcedures that lead to a correct $\\rho[0]$ have been discussed in the\nliterature only for a limited class of $H$ and $f$. Here, we develop a\nformalism that works for practically any $H$, and any explicitly time-dependent\n$f$. As a result, we are able to find exact solutions to a system of equations\ndescribing an arbitrary number of species interacting through (auto)catalytic\nfeedbacks, with general time dependent parameters characterizing the\nnonlinearity. Explicit examples involve up to 42 interacting species.\n",
        "  It is widely believed that the perovskite Sr$_2$RuO$_4$ is an unconventional\nsuperconductor with broken time reversal symmetry. It has been predicted that\nsuperconductors with broken time reversal symmetry should have spontaneously\ngenerated supercurrents at edges and domain walls. We have done careful imaging\nof the magnetic fields above Sr$_2$RuO$_4$ single crystals using scanning Hall\nbar and SQUID microscopies, and see no evidence for such spontaneously\ngenerated supercurrents. We use the results from our magnetic imaging to place\nupper limits on the spontaneously generated supercurrents at edges and domain\nwalls as a function of domain size. For a single domain, this upper limit is\nbelow the predicted signal by two orders of magnitude. We speculate on the\ncauses and implications of the lack of large spontaneous supercurrents in this\nvery interesting superconducting system.\n",
        "  The stability of albumin-bilirubin complex was investigated depending on pH\nof solution. It was shown that the stability of complex increases in presence\nof Mn(II) ions. It was also investigated the paramagnetic composition of\ngallstones by the electron spin resonance (ESR) method. It turned out that all\ninvestigated gallstones contain a free bilirubin radical-the stable product of\nits radical oxidation. Accordingly the paramagnetic composition gallstones\ncould be divided on three main types: cholesterol, brown pigment and black\npigment stones. ESR spectra of cholesterol stones is singlet with g=2.003 and\nsplitting between components 1.0 mT. At the same time the brown gallstones,\nbesides aforementioned signal contain the ESR spectrum which is characteristics\nfor Mn(II) ion complexes with inorganic compounds and, finally, in the black\npigment stones it was found out Fe(III) and Cu(II) complexes with organic\ncompounds and a singlet of bilirubin free radical. It is supposed that\ncrystallization centers of gallstones could be the polymer network of bilirubin\nradical polymerization in complex with different metal ions earlier discovered\nin gallstones.\n",
        "  Dynamics of average length of words in Russian and English is analysed in the\narticle. Words belonging to the diachronic text corpus Google Books Ngram and\ndated back to the last two centuries are studied. It was found out that average\nword length slightly increased in the 19th century, and then it was growing\nrapidly most of the 20th century and started decreasing over the period from\nthe end of the 20th - to the beginning of the 21th century. Words which\ncontributed mostly to increase or decrease of word average length were\nidentified. At that, content words and functional words are analysed\nseparately. Long content words contribute mostly to word average length of\nword. As it was shown, these words reflect the main tendencies of social\ndevelopment and thus, are used frequently. Change of frequency of personal\npronouns also contributes significantly to change of average word length. The\nother parameters connected with average length of word were also analysed.\n",
        "  We study the global spatio-temporal patterns of influenza dynamics. This is\nachieved by analysing and modelling weekly laboratory confirmed cases of\ninfluenza A and B from 138 countries between January 2006 and May 2014. The\ndata were obtained from FluNet, the surveillance network compiled by the the\nWorld Health Organization. We report a pattern of {\\it skip-and-resurgence}\nbehavior between the years 2011 and 2013 for influenza H1N1/09, the strain\nresponsible for the 2009 pandemic, in Europe and Eastern Asia. In particular,\nthe expected H1N1/09 epidemic outbreak in 2011 failed to occur (or\"skipped\") in\nmany countries across the globe, although an outbreak occurred in the following\nyear. We also report a pattern of {\\it well-synchronized} 2010 winter wave of\nH1N1/09 in the Northern Hemisphere countries, and a pattern of replacement of\nstrain H1N1/77 by H1N1/09 between the 2009 and 2012 influenza seasons. Using\nboth a statistical and a mechanistic mathematical model, and through fitting\nthe data of 108 countries (108 countries in a statistical model and 10 large\npopulations with a mechanistic model), we discuss the mechanisms that are\nlikely to generate these events taking into account the role of multi-strain\ndynamics. A basic understanding of these patterns has important public health\nimplications and scientific significance.\n",
        "  Identifiability of evolutionary tree models has been a recent topic of\ndiscussion and some models have been shown to be non-identifiable. A\ncoalescent-based rooted population tree model, originally proposed by Nielsen\net al. 1998 [2], has been used by many authors in the last few years and is a\nsimple tool to accurately model the changes in allele frequencies in the tree.\nHowever, the identifiability of this model has never been proven. Here we prove\nthis model to be identifiable by showing that the model parameters can be\nexpressed as functions of the probability distributions of subsamples. This a\nstep toward proving the consistency of the maximum likelihood estimator of the\npopulation tree based on this model.\n",
        "  Setting a proper margin is crucial for not only delivering the required\nradiation dose to a target volume, but also reducing the unnecessary radiation\nto the adjacent organs at risk. This study investigated the independent\none-dimensional symmetric and asymmetric margins between the clinical target\nvolume (CTV) and the planning target volume (PTV) for linac-based\nsingle-fraction frameless stereotactic radiosurgery (SRS).\n",
        "  Consensus methods are widely used for combining phylogenetic trees into a\nsingle estimate of the evolutionary tree for a group of species. As more taxa\nare added, the new source trees may begin to tell a different evolutionary\nstory when restricted to the original set of taxa. However, if the new trees,\nrestricted to the original set of taxa, were to agree exactly with the earlier\ntrees, then we might hope that their consensus would either agree with or\nresolve the original consensus tree. In this paper, we ask under what\nconditions consensus methods exist that are 'future proof' in this sense. While\nwe show that some methods (e.g. Adams consensus) have this property for\nspecific types of input, we also establish a rather surprising `no-go' theorem:\nthere is no 'reasonable' consensus method that satisfies the future-proofing\nproperty in general. We then investigate a second notion of 'future proofing'\nfor consensus methods, in which trees (rather than taxa) are added, and\nestablish some positive and negative results. We end with some questions for\nfuture work.\n",
        "  After the earthquake and the tsunami occurred in Japan on 11th March 2011,\nfour of the Fukushima reactors had released in air a large amount of\nradioactive isotopes that had been diffused all over the world. The presence of\nairborne 131I, 134Cs, and 137Cs in air particulate due to this accident has\nbeen detected and measured in the Low Radioactivity Laboratory operating in the\nDepartment of Environmental Sciences of the University of Milano-Bicocca. The\nsensitivity of the detecting apparatus is of 0.2 \\mu Bq/m3 of air.\nConcentration and time distribution of these radionuclides were determined and\nsome correlations with the original reactor releases were found. Radioactive\ncontaminations ranging from a few to 400 \\mu Bq/m3 for the 131I and of a few\ntens of \\mu Bq/m3 for the 137Cs and 134Cs have been detected\n",
        "  We show that time reversal symmetry breaking $p_x+ip_y$ wave superconductors\nundergo several phase transitions subjected to external magnetic field or\nsupercurrent. In such system the discrete $Z_2$ symmetry can recover before the\ncomplete destruction of the order parameter. The topological defects associated\nwith $Z_2$ symmetry - domain walls can be created in a controllable way by\nmagnetic field or current sweep according to the Kibble-Zurek scenario. Such\ndomain wall generation can take place in exotic superconductors like\n$Sr_2RuO_4$ and some heavy fermion compounds.\n",
        "  Evolutionary game dynamics of two players with two strategies has been\nstudied in great detail. These games have been used to model many biologically\nrelevant scenarios, ranging from social dilemmas in mammals to microbial\ndiversity. Some of these games may in fact take place between a number of\nindividuals and not just between two. Here, we address one-shot games with\nmultiple players. As long as we have only two strategies, many results from two\nplayer games can be generalized to multiple players. For games with multiple\nplayers and more than two strategies, we show that statements derived for\npairwise interactions do no longer hold. For two player games with any number\nof strategies there can be at most one isolated internal equilibrium. For any\nnumber of players $\\boldsymbol{d}$ with any number of strategies n, there can\nbe at most (d-1)^(n-1) isolated internal equilibria. Multiplayer games show a\ngreat dynamical complexity that cannot be captured based on pairwise\ninteractions. Our results hold for any game and can easily be applied for\nspecific cases, e.g. public goods games or multiplayer stag hunts.\n",
        "  We study a mechanical resonator made of aluminum near the normal to super\nconductivity phase transition. A sharp drop in the rate of mechanical damping\nis observed below the critical temperature. The experimental results are\ncompared with predictions based on the Bardeen Cooper Schrieffer theory of\nsuperconductivity and a fair agreement is obtained.\n",
        "  Graph data models are widely used in many areas, for example, bioinformatics,\ngraph databases. In these areas, it is often required to process queries for\nlarge graphs. Some of the most common graph queries are navigational queries.\nThe result of query evaluation is a set of implicit relations between nodes of\nthe graph, i.e. paths in the graph. A natural way to specify these relations is\nby specifying paths using formal grammars over the alphabet of edge labels. An\nanswer to a context-free path query in this approach is usually a set of\ntriples (A, m, n) such that there is a path from the node m to the node n,\nwhose labeling is derived from a non-terminal A of the given context-free\ngrammar. This type of queries is evaluated using the relational query\nsemantics. Another example of path query semantics is the single-path query\nsemantics which requires presenting a single path from the node m to the node\nn, whose labeling is derived from a non-terminal A for all triples (A, m, n)\nevaluated using the relational query semantics. There is a number of algorithms\nfor query evaluation which use these semantics but all of them perform poorly\non large graphs. One of the most common technique for efficient big data\nprocessing is the use of a graphics processing unit (GPU) to perform\ncomputations, but these algorithms do not allow to use this technique\nefficiently. In this paper, we show how the context-free path query evaluation\nusing these query semantics can be reduced to the calculation of the matrix\ntransitive closure. Also, we propose an algorithm for context-free path query\nevaluation which uses relational query semantics and is based on matrix\noperations that make it possible to speed up computations by using a GPU.\n",
        "  We prove that a closed 3-orbifold that fibers over a hyperbolic polygonal\n2-orbifold admits a family of hyperbolic cone structures that are viewed as\nregeneration of the polygon, provided that the perimeter is minimal.\n",
        "  We report the ac magnetic susceptibility $\\chi_{ac}$ and resistivity $\\rho$\nmeasurements of EuFe$_2$As$_2$ under high pressure $P$. By observing nearly\n100% superconducting shielding and zero resistivity at $P$ = 28 kbar, we\nestablish that $P$-induced superconductivity occurs at $T_c \\sim$~30 K in\nEuFe$_2$As$_2$. $\\rho$ shows an anomalous nearly linear temperature dependence\nfrom room temperature down to $T_c$ at the same $P$. $\\chi_{ac}$ indicates that\nan antiferromagnetic order of Eu$^{2+}$ moments with $T_N \\sim$~20 K persists\nin the superconducting phase. The temperature dependence of the upper critical\nfield is also determined.\n",
        "  We present the results of an imaging observation campaign conducted with the\nSubaru Telescope adaptive optics system (IRCS+AO188) on 28 gravitationally\nlensed quasars (23 doubles, 1 quad, and 1 possible triple, and 3 candidates)\nfrom the SDSS Quasar Lens Search. We develop a novel modelling technique that\nfits analytical and hybrid point spread functions (PSFs), while simultaneously\nmeasuring the relative astrometry, photometry, as well as the lens galaxy\nmorphology. We account for systematics by simulating the observed systems using\nseparately observed PSF stars. The measured relative astrometry is comparable\nwith that typically achieved with the Hubble Space Telescope, even after\nmarginalizing over the PSF uncertainty. We model for the first time the quasar\nhost galaxies in 5 systems, without a-priory knowledge of the PSF, and show\nthat their luminosities follow the known correlation with the mass of the\nsupermassive black hole. For each system, we obtain mass models far more\naccurate than those previously published from low-resolution data, and we show\nthat in our sample of lensing galaxies the observed light profile is more\nelliptical than the mass, for ellipticity > 0.25. We also identify eight\ndoubles for which the sources of external and internal shear are more reliably\nseparated, and should therefore be prioritized in monitoring campaigns aimed at\nmeasuring time-delays in order to infer the Hubble constant.\n",
        "  Language identification has become a prerequisite for all kinds of automated\ntext processing systems. In this paper, we present a rule-based language\nidentifier tool for two closely related Indo-Aryan languages: Hindi and Magahi.\nThis system has currently achieved an accuracy of approx 86.34%. We hope to\nimprove this in the future. Automatic identification of languages will be\nsignificant in the accuracy of output of Web Crawlers.\n",
        "  Purpose: Variations in proton Relative Biological Effectiveness (RBE) with\nLinear Energy Transfer (LET) remain one of the largest sources of uncertainty\nin proton radiotherapy. This work seeks to identify physics-based metrics which\ncan be applied to reduce this biological uncertainty.\n  Materials and Methods: Three different physical metrics - dose, dose $\\times$\nLET and a complexity-weighted dose (CWD, Dose $\\times$ (1+$\\kappa LET_D$) )\nwere compared with in vitro experimental studies of proton RBE and clinical\ntreatment plans analysed using RBE models. The biological effects of protons in\neach system were plotted against these metrics to quantify the degree of\nbiological uncertainty introduced by RBE variations in each case.\n  Results: When the biological effects of protons were plotted against dose\nalone, significant biological uncertainty was introduced as the LET-dependence\nof RBE was neglected. Plotting biological effects against dose $\\times$ LET\nsignificantly over-estimated the impact of LET on cell survival, leading to\nsimilar or greater levels of biological uncertainty. CWD, by contrast,\nsignificantly reduced biological uncertainties in both experiments and clinical\nplans. For prostate and medulloblastoma treatment plans, biological\nuncertainties were reduced from $\\pm$ 5% to less than 1%.\n  Conclusions: While not a replacement for full RBE models, physics-based\nmetrics such as CWD have the potential to significantly reduce the\nuncertainties in proton planning which result from variations in RBE. These\nmetrics may be used to identify regions in normal tissues which may see\nunexpectedly high effects due to end-of-range elevations of RBE, or as a tool\nin optimisation to deliver uniform biological effects.\n",
        "  We theoretically investigate the ground state of trapped neutral fermions\nwith population imbalance in the BCS-BEC crossover regime. On the basis of the\nsingle-channel Hamiltonian, we perform full numerical calculations of the\nBogoliubov-de Gennes equation coupled with the regularized gap and number\nequations. The zero-temperature phase diagram in the crossover regime is\npresented, where the Fulde-Ferrell-Larkin-Ovchinnikov (FFLO) pairing state\ngoverns the weak-coupling BCS region of a resonance. It is found that the FFLO\noscillation vanishes in the BEC side, in which the system under population\nimbalance turns into a phase separation (PS) between locally binding superfluid\nand fully polarized spin domains. We also demonstrate numerical calculations\nwith a large particle number O(10^5), comparable to that observed in recent\nexperiments. The resulting density profile on a resonance yields the PS, which\nis in good agreement with the recent experiments, while the FFLO modulation\nexists in the pairing field. It is also proposed that the most favorable\nlocation for the detection of the FFLO oscillation is in the vicinity of the\ncritical population imbalance in the weak coupling BCS regime, where the\noscillation periodicity becomes much larger than the interparticle spacing.\nFinally, we analyze the radio-frequency (RF) spectroscopy in the imbalanced\nsystem. The clear difference in the RF spectroscopy between BCS and BEC sides\nreveals the structure of the pairing field and local ``magnetization''.\n",
        "  A crop can be represented as a biotechnical system in which components are\neither chosen (cultivar, management) or given (soil, climate) and whose\ncombination generates highly variable stress patterns and yield responses.\nHere, we used modeling and simulation to predict the crop phenotypic plasticity\nresulting from the interaction of plant traits (G), climatic variability (E)\nand management actions (M). We designed two in silico experiments that compared\nexisting and virtual sunflower cultivars (Helianthus annuus L.) in a target\npopulation of cropping environments by simulating a range of indicators of crop\nperformance. Optimization methods were then used to search for GEM combinations\nthat matched desired crop specifications. Computational experiments showed that\nthe fit of particular cultivars in specific environments is gradually\nincreasing with the knowledge of pedo-climatic conditions. At the regional\nscale, tuning the choice of cultivar impacted crop performance the same\nmagnitude as the effect of yearly genetic progress made by breeding. When\nconsidering virtual genetic material, designed by recombining plant traits,\ncultivar choice had a greater positive impact on crop performance and\nstability. Results suggested that breeding for key traits conferring plant\nplasticity improved cultivar global adaptation capacity whereas increasing\ngenetic diversity allowed to choose cultivars with distinctive traits that were\nmore adapted to specific conditions. Consequently, breeding genetic material\nthat is both plastic and diverse may improve yield stability of agricultural\nsystems exposed to climatic variability. We argue that process-based modeling\ncould help enhancing spatial management of cultivated genetic diversity and\ncould be integrated in functional breeding approaches.\n",
        "  Time-calibrated species phylogenies are critical for addressing a wide range\nof questions in evolutionary biology, such as those that elucidate historical\nbiogeography or uncover patterns of coevolution and diversification. Because\nmolecular sequence data are not informative on absolute time, external data,\nmost commonly fossil age estimates, are required to calibrate estimates of\nspecies divergence dates. For Bayesian divergence-time methods, the common\npractice for calibration using fossil information involves placing arbitrarily\nchosen parametric distributions on internal nodes, often disregarding most of\nthe information in the fossil record. We introduce the 'fossilized birth-death'\n(FBD) process, a model for calibrating divergence-time estimates in a Bayesian\nframework, explicitly acknowledging that extant species and fossils are part of\nthe same macroevolutionary process. Under this model, absolute node age\nestimates are calibrated by a single diversification model and arbitrary\ncalibration densities are not necessary. Moreover, the FBD model allows for\ninclusion of all available fossils. We performed analyses of simulated data and\nshow that node-age estimation under the FBD model results in robust and\naccurate estimates of species divergence times with realistic measures of\nstatistical uncertainty, overcoming major limitations of standard divergence\ntime estimation methods. We then used this model to estimate the speciation\ntimes for a dataset composed of all living bears, indicating that the genus\nUrsus diversified in the late Miocene to mid Pliocene.\n",
        "  The vortex lattice (VL) symmetry and orientation in clean type-II\nsuperconductors depends sensitively on the host material anisotropy, vortex\ndensity and temperature, frequently leading to rich phase diagrams. Typically,\na well-ordered VL is taken to imply a ground state configuration for the\nvortex-vortex interaction. Using neutron scattering we studied the VL in MgB2\nfor a number of field-temperature histories, discovering an unprecedented\ndegree of metastability in connection with a known, second-order rotation\ntransition. This allows, for the first time, structural studies of a\nwell-ordered, non-equilibrium VL. While the mechanism responsible for the\nlongevity of the metastable states is not resolved, we speculate it is due to a\njamming of VL domains, preventing a rotation to the ground state orientation.\n",
        "  In April 2013, a workshop entitled \"What Regulates Galaxy Evolution\" was held\nat the Lorentz Center. The aim of the workshop was to bring together the\nobservational and theoretical community working on galaxy evolution, and to\ndiscuss in depth of the current problems in the subject, as well as to review\nthe most recent observational constraints. A total of 42 astrophysicists\nattended the workshop. A significant fraction of the time was devoted to\nidentifying the most interesting \"open questions\" in the field, and to discuss\nhow progress can be made. This review discusses the four questions (one for\neach day of the workshop) that, in our opinion, were the focus of the most\nintense debate. We present each question in its context, and close with a\ndiscussion of what future directions should be pursued in order to make\nprogress on these problems.\n",
        "  We define a \"reduced\" version of the knot Floer complex $CFK^-(K)$, and show\nthat it behaves well under connected sums and retains enough information to\ncompute Heegaard Floer $d$-invariants of manifolds arising as surgeries on the\nknot $K$. As an application to connected sums, we prove that if a knot in the\nthree-sphere admits an $L$-space surgery, it must be a prime knot. As an\napplication of the computation of $d$-invariants, we show that the Alexander\npolynomial is a concordance invariant within the class of $L$-space knots, and\nshow the four-genus bound given by the $d$-invariant of +1-surgery is\nindependent of the genus bounds given by the Ozsv\\'ath-Szab\\'o $\\tau$\ninvariant, the knot signature and the Rasmussen $s$ invariant.\n",
        "  Auroux, Donaldson and Katzarkov introduced broken Lefschetz fibrations as a\ngeneralization of Lefshcetz fibrations in order to describe near-symplectic\n4-manifolds. We first study monodromy representations of higher sides of\ngenus-1 simplified broken Lefschetz fibrations. We then completely classify\ndiffeomorphism types of such fibrations with connected fibers and with less\nthan six Lefschetz singularities. In these studies, we obtain several families\nof genus-1 simplified broken Lefschetz fibrations, which we conjecture contain\nall such fibrations, and determine the diffeomorphism types of the total spaces\nof these fibrations. Our results are generalizations of Kas' classification\ntheorem of genus-1 Lefschetz fibrations, which states that the total space of a\nnon-trivial genus-1 Lefschetz fibration over $S^2$ is diffeomorphic to an\nelliptic surface E(n), for some $n\\geq 1$.\n",
        "  Superfluid density ($n_s$) in the mixed state of an iron pnictide\nsuperconductor Ba$_{0.6}$K$_{0.4}$Fe$_2$As$_2$ is determined by muon spin\nrotation for a sample with optimal doping ($x=0.4$). The temperature dependence\nof $n_s$ is perfectly reproduced by the conventional BCS model for s-wave\nparing, where the order parameter can be either a single-gap with\n$\\Delta=8.35(6)$ meV [$2\\Delta/k_BT_c=5.09(4)$], or double-gap structure with\n$\\Delta_1=12$ meV (fixed) [$2\\Delta_1/k_BT_c=7.3$] and $\\Delta_2=6.8(3)$ meV\n[$2\\Delta_2/k_BT_c=4.1(2)$]. The latter is consistent with the recent result of\nangle-resolved photo-emssion spectroscopy. The large gap parameters\n($2\\Delta/k_BT_c$) indicate extremely strong coupling of carriers to bosons\nthat mediate the Cooper pairing.\n",
        "  Order-disorder transitions take place in many physical systems, but observing\nthem in detail in real materials is difficult. In two- or quasi-two-dimensional\nsystems, the transition has been studied by computer simulations and\nexperimentally in electron sheets, dusty plasmas, colloidal and other systems.\nHere I show the different stages of defect formation in the vortex lattice of a\nsuperconductor while it undergoes an order-disorder transition by presenting\nreal-space images of the lattice from scanning tunneling spectroscopy. When the\nsystem evolves from the ordered to the disordered state, the predominant kind\nof defect changes from dislocation pairs to single dislocations, and finally to\ndefect clusters forming grain boundaries. Correlation functions indicate a\nhexatic-like state preceding the disordered state. The transition in the\nmicroscopic vortex distribution is mirrored by the well-known spectacular\nsecond peak effect observed in the macroscopic current density of the\nsuperconductor.\n",
        "  We report intimate relations between topological properties of full gapped\nspin-triplet superconductors with time-reversal invariance and the Fermi\nsurface topology in the normal states. An efficient method to calculate the Z2\ninvariants and the winding number for the spin-triplet superconductors is\ndeveloped, and connections between these topological invariants and the Fermi\nsurface structures in the normal states are pointed out. We also obtain a\ncorrespondence between the Fermi surface topology and gapless surface states in\nthe superconducting states. The correspondence is inherent to spin-triplet\nsuperconductivity.\n",
        "  We study the properties of a sample of 3967 LINER galaxies selected from\nSDSS-DR7, respect to their proximity to galaxy groups. The host galaxies of\nLINER have been analysed and compared with a well defined control sample of\n3841 non-LINER galaxies matched in redshift, luminosity, colour, morphology,\nage and stellar mass content. We find no difference between LINER and control\ngalaxies in terms of colour and age of stellar population as function of the\nvirial mass and distance to the geometric centre of the group. However, we find\nthat LINER are more likely to populate low density environments in spite of\ntheir morphology, which is typical of high density regions such as rich galaxy\nclusters. For rich (poor) galaxy groups, the occurrence of LINER is $\\sim$2\ntimes lower (higher) than the occurrence of matched, non-LINER galaxies.\nMoreover, LINER hosts do not seem to follow the expected morphology-density\nrelation in groups of high virial mass. The high frequency of LINERS in low\ndensity regions could be due to the combination of a sufficiently ample gas\nreservoir to power the low ionization emission and/or enhanced galaxy\ninteraction rates benefiting the gas flow toward their central regions.\n",
        "  We present CO(1-0) observations of objects within the Shocked POststarburst\nGalaxy Survey taken with the Institut de Radioastronomie Millimetrique (IRAM)\n30m single dish and the Combined Array for Research for Millimeter Astronomy\n(CARMA) interferometer. Shocked Poststarburst Galaxies (SPOGs) represent a\ntransitioning population of galaxies, with deep Balmer absorption (Hdelta>5A),\nconsistent with an intermediate-age (A-star) stellar population, and ionized\ngas line ratios inconsistent with pure star formation. The CO(1-0) subsample\nwas selected from SPOGs detected by the Wide-field Infrared Survey Explorer\nwith 22um flux detected at a signal-to-noise (S/N)>3. Of the 52 objects\nobserved in CO(1-0), 47 are detected with S/N>3. A large fraction (37-46%) of\nour CO-SPOG sample were visually classified as morphologically disrupted. The\nH2 masses detected were between 10^(8.7-10.8) Msuns, consistent with the gas\nmasses found in normal galaxies, though approximately an order of magnitude\nlarger than the range seen in poststarburst galaxies. When comparing the 22um\nand CO(1-0) fluxes, SPOGs diverge from the normal star-forming relation, having\n22um fluxes in excess by a factor of <EMIR,SPOG>=4.91+0.42-0.39. The Na I D\ncharacteristics of CO-SPOGs show that it is likely that many of these objects\nhost interstellar winds. Objects with the large Na I D enhancements also tend\nto emit in the radio, suggesting possible AGN-driving of neutral winds.\n",
        "  Graph databases have emerged as the fundamental technology underpinning\ntrendy application domains where traditional databases are not well-equipped to\nhandle complex graph data. However, current graph databases support basic graph\nstructures and integrity constraints with no standard algebra. In this paper,\nwe introduce GRAD, a native and generic graph database model. GRAD goes beyond\ntraditional graph database models, which support simple graph structures and\nconstraints. Instead, GRAD presents a complete graph database model supporting\nadvanced graph structures, a set of well-defined constraints over these\nstructures and a powerful graph analysis-oriented algebra.\n",
        "  The Wide-field Infrared Survey Explorer (WISE) has scanned the entire sky\nwith unprecedented sensitivity in four infrared bands, at 3.4, 4.6, 12, and 22\nmicron. The WISE Point Source Catalog contains more than 560 million objects,\namong them hundreds of thousands of galaxies with Active Nuclei (AGN). While\ntype 1 AGN, owing to their bright and unobscured nature, are easy to detect and\nconstitute a rather complete and unbiased sample, their type 2 counterparts,\npostulated by AGN unification, are not as straightforward to identify. Matching\nthe WISE catalog with known QSOs in the Sloan Digital Sky Survey we confirm\nprevious identification of the type 1 locus in the WISE color space. Using a\nvery large database of the popular CLUMPY torus models, we find the colors of\nthe putative type 2 counterparts, and also, for the first time, predict their\nnumber vs. flux relation that can be expected to be observed in any given WISE\ncolor range. This will allow us to put statistically very significant\nconstraints on the torus parameters. Our results are a successful test of the\nAGN unification scheme.\n",
        "  The utilization of social media material in journalistic workflows is\nincreasing, demanding automated methods for the identification of mis- and\ndisinformation. Since textual contradiction across social media posts can be a\nsignal of rumorousness, we seek to model how claims in Twitter posts are being\ntextually contradicted. We identify two different contexts in which\ncontradiction emerges: its broader form can be observed across independently\nposted tweets and its more specific form in threaded conversations. We define\nhow the two scenarios differ in terms of central elements of argumentation:\nclaims and conversation structure. We design and evaluate models for the two\nscenarios uniformly as 3-way Recognizing Textual Entailment tasks in order to\nrepresent claims and conversation structure implicitly in a generic inference\nmodel, while previous studies used explicit or no representation of these\nproperties. To address noisy text, our classifiers use simple similarity\nfeatures derived from the string and part-of-speech level. Corpus statistics\nreveal distribution differences for these features in contradictory as opposed\nto non-contradictory tweet relations, and the classifiers yield state of the\nart performance.\n",
        "  Computer systems need to be able to react to stress in order to perform\noptimally on some tasks. This article describes TensiStrength, a system to\ndetect the strength of stress and relaxation expressed in social media text\nmessages. TensiStrength uses a lexical approach and a set of rules to detect\ndirect and indirect expressions of stress or relaxation, particularly in the\ncontext of transportation. It is slightly more effective than a comparable\nsentiment analysis program, although their similar performances occur despite\ndifferences on almost half of the tweets gathered. The effectiveness of\nTensiStrength depends on the nature of the tweets classified, with tweets that\nare rich in stress-related terms being particularly problematic. Although\ngeneric machine learning methods can give better performance than TensiStrength\noverall, they exploit topic-related terms in a way that may be undesirable in\npractical applications and that may not work as well in more focused contexts.\nIn conclusion, TensiStrength and generic machine learning approaches work well\nenough to be practical choices for intelligent applications that need to take\nadvantage of stress information, and the decision about which to use depends on\nthe nature of the texts analysed and the purpose of the task.\n",
        "  Purpose: To quantify the ability of correlation and regression analysis to\nextract the normal lung dose-response function from dose volume histogram (DVH)\ndata. Methods: A local injury model is adopted, in which radiation-induced\ndamage (functional loss) G is the integral of the DVH with function R(D). RP\nrisk is H(G) where H() is the sigmoid cumulative distribution of functional\nreserve. RP incidence is a Bernoulli function of risk. A homogeneous patient\ncohort is assumed, allowing non-dose-related factors to be ignored. Clinically\nrealistic DVHs are combined with the injury model to simulate RP data. Results:\nCorrelation analysis is often used to identify predictor variables that are\ncorrelated with outcome, for inclusion in a predictive model. In the local\ninjury model, all DVH metrics VD contribute to damage. Correlation analysis\ntherefore has limited value. The subset of VD significantly correlated with\nincidence varies randomly from trial to trial due to random variations in the\nDVH set, and does not necessarily reveal anything useful about the patient\ncohort or the underlying biological dose-response relationship. Regression or\nmatrix analysis can extract R(D) from damage or risk data, provided smoothness\nregularization is employed. Extraction of R(D) from incidence data was not\nsuccessful, due to its higher level of statistical variability. Conclusions: To\nthe author's knowledge, smoothness regularization has not been applied to this\nproblem, so represents a novel approach. Dose-response functions can be\nsuccessfully extracted from measurements of integral (as opposed to regional)\nlung damage G, suggesting value in re-visiting available measurements of\nventilation, perfusion and radiographic damage. The techniques developed here\ncan potentially be used to extract the dose-response functions of different\ntissues from multiple types of quantitative volumetric imaging data.\n",
        "  Existing machine translation decoding algorithms generate translations in a\nstrictly monotonic fashion and never revisit previous decisions. As a result,\nearlier mistakes cannot be corrected at a later stage. In this paper, we\npresent a translation scheme that starts from an initial guess and then makes\niterative improvements that may revisit previous decisions. We parameterize our\nmodel as a convolutional neural network that predicts discrete substitutions to\nan existing translation based on an attention mechanism over both the source\nsentence as well as the current translation output. By making less than one\nmodification per sentence, we improve the output of a phrase-based translation\nsystem by up to 0.4 BLEU on WMT15 German-English translation.\n",
        "  All measurements of cosmic star formation must assume an initial distribution\nof stellar masses -- the stellar initial mass function -- in order to\nextrapolate from the star-formation rate measured for typically rare, massive\nstars (> 8 Msun) to the total star-formation rate across the full stellar mass\nspectrum. The shape of the stellar initial mass function in various galaxy\npopulations underpins our understanding of the formation and evolution of\ngalaxies across cosmic time. Classical determinations of the stellar initial\nmass function in local galaxies are traditionally made at ultraviolet, optical\nand near-infrared wavelengths, which cannot be probed in dust-obscured\ngalaxies, especially in distant starbursts, whose apparent star-formation rates\nare hundreds to thousands of times higher than in our Milky Way, selected at\nsubmillimetre (rest-frame far-infrared) wavelengths. The 13C/18O abundance\nratio in the cold molecular gas -- which can be probed via the rotational\ntransitions of the 13CO and C18O isotopologues -- is a very sensitive index of\nthe stellar initial mass function, with its determination immune to the\npernicious effects of dust. Here we report observations of 13CO and C18O\nemission for a sample of four dust-enshrouded starbursts at redshifts of\napproximately two to three, and find unambiguous evidence for a top-heavy\nstellar initial mass function in all of them. A low 13CO/C18O ratio for all our\ntargets -- alongside a well-tested, detailed chemical evolution model\nbenchmarked on the Milky Way -- implies that there are considerably more\nmassive stars in starburst events than in ordinary star-forming spiral\ngalaxies. This can bring these extraordinary starbursts closer to the `main\nsequence' of star-forming galaxies, though such main-sequence galaxies may not\nbe immune to changes in initial stellar mass function, depending upon their\nstar-formation densities.\n",
        "  Recently, result diversification has attracted a lot of attention as a means\nto improve the quality of results retrieved by user queries. In this paper, we\npropose a new, intuitive definition of diversity called DisC diversity. A DisC\ndiverse subset of a query result contains objects such that each object in the\nresult is represented by a similar object in the diverse subset and the objects\nin the diverse subset are dissimilar to each other. We show that locating a\nminimum DisC diverse subset is an NP-hard problem and provide heuristics for\nits approximation. We also propose adapting DisC diverse subsets to a different\ndegree of diversification. We call this operation zooming. We present efficient\nimplementations of our algorithms based on the M-tree, a spatial index\nstructure, and experimentally evaluate their performance.\n",
        "  Native oyster populations in Chesapeake Bay have been the focus of three\ndecades of restoration attempts, which have generally failed to rebuild the\npopulations and oyster reef structure. Recent restoration successes and field\nexperiments suggest that high-relief reefs offset heavy sedimentation and\npromote oyster survival, disease resistance and growth, in contrast to\nlow-relief reefs which degrade in just a few years. These findings suggest the\nexistence of alternative stable states in oyster reef populations. We developed\na mathematical model consisting of three differential equations that represent\nvolumes of live oysters, dead oyster shells (= accreting reef), and sediment.\nBifurcation analysis and numerical simulations demonstrated that multiple\nnonnegative equilibria can exist for live oyster, accreting reef and sediment\nvolume at an ecologically reasonable range of parameter values; the initial\nheight of oyster reefs determined which equilibrium was reached. This\ninvestigation thus provides a conceptual framework for alternative stable\nstates in native oyster populations, and can be used as a tool to improve the\nlikelihood of success in restoration efforts.\n",
        "  We present computational results about quasi-alternating knots and links and\nodd homology obtained by looking at link families in the Conway notation. More\nprecisely, we list quasi-alternating links up to 12 crossings and the first\nexamples of quasi-alternating knots and links with at least two different\nminimal diagrams, where one is quasi-alternating and the other is not. We\nprovide examples of knots and links with $n\\le 12$ crossings which are\nhomologically thin and have no minimal quasi-alternating diagrams. These links\nare candidates for homologically thin links that are not quasi-alternating. For\none of our candidates [JaSa1], knot $11n_{50}$, J. Greene proved that it is not\nquasi-alternating, so this is the first example of homologically thin knot\nwhich is not quasi-alternating [Gr]. Computations were performed by A.\nShumakovitch's program \\emph{KhoHo}, the program \\emph{Knotscape}, and our\nprogram \\emph{LinKnot}.\n",
        "  Scientific endeavors such as large astronomical surveys generate databases on\nthe terabyte scale. These, usually multidimensional databases must be\nvisualized and mined in order to find interesting objects or to extract\nmeaningful and qualitatively new relationships. Many statistical algorithms\nrequired for these tasks run reasonably fast when operating on small sets of\nin-memory data, but take noticeable performance hits when operating on large\ndatabases that do not fit into memory. We utilize new software technologies to\ndevelop and evaluate fast multidimensional indexing schemes that inherently\nfollow the underlying, highly non-uniform distribution of the data: they are\nlayered uniform grid indices, hierarchical binary space partitioning, and\nsampled flat Voronoi tessellation of the data. Our working database is the\n5-dimensional magnitude space of the Sloan Digital Sky Survey with more than\n270 million data points, where we show that these techniques can dramatically\nspeed up data mining operations such as finding similar objects by example,\nclassifying objects or comparing extensive simulation sets with observations.\nWe are also developing tools to interact with the multidimensional database and\nvisualize the data at multiple resolutions in an adaptive manner.\n",
        "  We present the extended GALEX Arecibo SDSS Survey (xGASS), a gas\nfraction-limited census of the atomic (HI) gas content of 1179 galaxies\nselected only by stellar mass ($M_\\star =10^{9}-10^{11.5} M_\\odot$) and\nredshift ($0.01<z<0.05$). This includes new Arecibo observations of 208\ngalaxies, for which we release catalogs and HI spectra. In addition to\nextending the GASS HI scaling relations by one decade in stellar mass, we\nquantify total (atomic+molecular) cold gas fractions and molecular-to-atomic\ngas mass ratios, $R_{mol}$, for the subset of 477 galaxies observed with the\nIRAM 30 m telescope. We find that atomic gas fractions keep increasing with\ndecreasing stellar mass, with no sign of a plateau down to $\\log\nM_\\star/M_\\odot = 9$. Total gas reservoirs remain HI-dominated across our full\nstellar mass range, hence total gas fraction scaling relations closely resemble\natomic ones, but with a scatter that strongly correlates with $R_{mol}$,\nespecially at fixed specific star formation rate. On average, $R_{mol}$ weakly\nincreases with stellar mass and stellar surface density $\\mu_\\star$, but\nindividual values vary by almost two orders of magnitude at fixed $M_\\star$ or\n$\\mu_\\star$. We show that, for galaxies on the star-forming sequence,\nvariations of $R_{mol}$ are mostly driven by changes of the HI reservoirs, with\na clear dependence on $\\mu_\\star$. Establishing if galaxy mass or structure\nplays the most important role in regulating the cold gas content of galaxies\nrequires an accurate separation of bulge and disk components for the study of\ngas scaling relations.\n",
        "  Substantial control of the interlayer spacing in Bi-based high temperature\nsuperconductors has been achieved through the intercalation of guest molecules\nbetween the superconducting layers. Measurements using implanted muons reveal\nthat the penetration depth increases with increasing layer separation while T_c\ndoes not vary appreciably, demonstrating that the bulk superfluid density is\nnot the determining factor controlling T_c. Our results strongly suggest that\nthe superfluid density appearing in the Uemura scaling relation n_s/m* \\propto\nT_c should be interpreted as the two dimensional density within the\nsuperconducting layers, which we find to be constant for each class of system\ninvestigated.\n",
        "  Machine Translation for Indian languages is an emerging research area.\nTransliteration is one such module that we design while designing a translation\nsystem. Transliteration means mapping of source language text into the target\nlanguage. Simple mapping decreases the efficiency of overall translation\nsystem. We propose the use of stemming and part-of-speech tagging for\ntransliteration. The effectiveness of translation can be improved if we use\npart-of-speech tagging and stemming assisted transliteration.We have shown that\nmuch of the content in Gujarati gets transliterated while being processed for\ntranslation to Hindi language.\n",
        "  A birth-death-sampling model gives rise to phylogenetic trees with samples\nfrom the past and the present. Interpreting \"birth\" as branching speciation,\n\"death\" as extinction, and \"sampling\" as fossil preservation and recovery, this\nmodel -- also referred to as the fossilized birth-death (FBD) model -- gives\nrise to phylogenetic trees on extant and fossil samples. The model has been\nmathematically analyzed and successfully applied to a range of datasets on\ndifferent taxonomic levels, such as penguins, plants, and insects. However, the\ncurrent mathematical treatment of this model does not allow for a group of\ntemporally distinct fossil specimens to be assigned to the same species. In\nthis paper, we provide a general mathematical FBD modeling framework that\nexplicitly takes \"stratigraphic ranges\" into account, with a stratigraphic\nrange being defined as the lineage interval associated with a single species,\nranging through time from the first to the last fossil appearance of the\nspecies. To assign a sequence of fossil samples in the phylogenetic tree to the\nsame species, i.e., to specify a stratigraphic range, we need to define the\nmode of speciation. We provide expressions to account for three common\nspeciation modes: budding (or asymmetric) speciation, bifurcating (or\nsymmetric) speciation, and anagenetic speciation. Our equations allow for\nflexible joint Bayesian analysis of paleontological and neontological data.\nFurthermore, our framework is directly applicable to epidemiology, where a\nstratigraphic range is the observed duration of infection of a single patient,\n\"birth\" via budding is transmission, \"death\" is recovery, and \"sampling\" is\nsequencing the pathogen of a patient. Thus, we present a model that allows for\nincorporation of multiple observations through time from a single patient.\n",
        "  We identify all hyperbolic knots whose complements are in the census of\norientable one-cusped hyperbolic manifolds with eight ideal tetrahedra. We also\ncompute their Jones polynomials.\n",
        "  An embedding of a graph into $\\mathbb{R}^3$ is said to be linear, if any edge\nof the graph is sent to be a line segment. And we say that an embedding $f$ of\na graph $G$ into $\\mathbb{R}^3$ is free, if $\\pi_1(\\mathbb{R}^3-f(G))$ is a\nfree group. It was known that for any complete graph its linear embedding is\nalways free.\n  In this paper we investigate the freeness of linear embeddings considering\nthe number of vertices. It is shown that for any simple connected graph with at\nmost 6 vertices, if its minimal valency is at least 3, then its linear\nembedding is always free. On the contrary when the number of vertices is much\nlarger than the minimal valency or connectivity, the freeness may not be an\nintrinsic property of such graphs. In fact we show that for any $n \\geq 1$\nthere are infinitely many connected graphs with minimal valency $n$ which have\nnon-free linear embeddings, and furthermore, that there are infinitely many\n$n$-connected graphs which have non-free linear embeddings.\n",
        "  In this paper we give an introduction to the volume conjecture and its\ngeneralizations. Especially we discuss relations of the asymptotic behaviors of\nthe colored Jones polynomials of a knot with different parameters to\nrepresentations of the fundamental group of the knot complement at the special\nlinear group over complex numbers by taking the figure-eight knot and torus\nknots as examples.\n",
        "  The brain deformation that occurs during neurosurgery is a serious issue\nimpacting the patient \"safety\" as well as the invasiveness of the brain\nsurgery. Model-driven compensation is a realistic and efficient solution to\nsolve this problem. However, a vital issue is the lack of reliable and easily\nobtainable patient-specific mechanical characteristics of the brain which,\naccording to clinicians' experience, can vary considerably. We designed an\naspiration device that is able to meet the very rigorous sterilization and\nhandling process imposed during surgery, and especially neurosurgery. The\ndevice, which has no electronic component, is simple, light and can be\nconsidered as an ancillary instrument. The deformation of the aspirated tissue\nis imaged via a mirror using an external camera. This paper describes the\nexperimental setup as well as its use during a specific neurosurgery. The\nexperimental data was used to calibrate a continuous model. We show that we\nwere able to extract an in vivo constitutive law of the brain elasticity: thus\nfor the first time, measurements are carried out per-operatively on the\npatient, just before the resection of the brain parenchyma. This paper\ndiscloses the results of a difficult experiment and provide for the first time\nin-vivo data on human brain elasticity. The results point out the softness as\nwell as the highly non-linear behavior of the brain tissue.\n",
        "  Entity detection and tracking (EDT) is the task of identifying textual\nmentions of real-world entities in documents, extending the named entity\ndetection and coreference resolution task by considering mentions other than\nnames (pronouns, definite descriptions, etc.). Like NE tagging and coreference\nresolution, most solutions to the EDT task separate out the mention detection\naspect from the coreference aspect. By doing so, these solutions are limited to\nusing only local features for learning. In contrast, by modeling both aspects\nof the EDT task simultaneously, we are able to learn using highly complex,\nnon-local features. We develop a new joint EDT model and explore the utility of\nmany features, demonstrating their effectiveness on this task.\n",
        "  We report on terahertz time-domain spectroscopy on superconducting and\nmetallic iron chalcogenides Rb$_{0.75}$Fe$_{1.6}$Se$_{2-z}$S$_z$. The\nsuperconducting transition is reduced from $T_c=$ 32 K ($z=0$) to 22 K\n($z=1.0$), and finally suppressed ($z=1.4$) by isoelectronic substitution of Se\nwith S. Dielectric constant and optical conductivity exhibit a\nmetal-to-insulator transition associated with an orbital-selective Mott phase.\nThis orbital-selective Mott transition appears at higher temperature $T_{met}$\nwith increasing sulfur content, identifying sulfur substitution as an efficient\nparameter to tune orbital-dependent correlation effects in iron-chalcogenide\nsuperconductors. The reduced correlations of the $d_{xy}$ charge carriers can\naccount for the suppression of the superconductivity and the pseudogap-like\nfeature between $T_c$ and $T_{met}$ that was observed for $z=0$.\n",
        "  $YBaCuO-Ag$ pressure point contacts with direct conduction are investigated.\nThe excess (relative to the normal state) conductivity mainly caused by\nfluctuational pairing of electrons above $T_c$ is measured in the temperature\ninterval 100-200~$K$. The superconductivity above 120~$K$ is found to be of the\ntwo-dimensional type. The obtained preliminary results indicate the presence of\nsmall amount of an unknown phase with $T'_c\\gtrsim 200~K$ in $YBaCuO$.\n",
        "  Seven of ten candidate H-alpha emission-line stars found in an objective\ngrism survey of a 1 square degree region in MBM 18, were observed\nspectroscopically. Four of these have weak H-alpha emission, and 6 out of 7\nhave spectral types M1-M4V. One star is of type F7-G1V, and has H-alpha in\nabsorption. The spectra of three of the M-stars may show an absorption line of\nLiI, although none of these is an unambiguous detection. For the six M-stars a\ngood fit is obtained with pre-main-sequence isochrones indicating ages between\n7.5 and 15Myr. The molecular cloud mass, derived from the integrated 12CO(1-0)\nemission, is 160Mo (for a distance of 120pc), much smaller than the virial mass\n(10^3Mo), and the cloud is not gravitationally bound. Nor are the individual\nclumps we identified through a clump-finding routine. Considering the relative\nweakness or absence of the H-alpha emission, the absence of other emission\nlines, and the lack of clear LiI absorption, the targets are not T Tauri stars.\nWith ages between 7.5 and 15Myr they are old enough to explain the lack of\nlithium in their spectra. Based on the derived distances (60-250pc), some of\nthe stars may lie inside the molecular cloud (120-150pc). From the fact that\nthe cloud as a whole, as well as the individual clumps, are not gravitationally\nbound, in combination with the ages of the stars we conclude that it is not\nlikely that (these) stars were formed in MBM 18.\n",
        "  The importance of geo-spatial data in critical applications such as emergency\nresponse, transportation, agriculture etc., has prompted the adoption of recent\nGeoSPARQL standard in many RDF processing engines. In addition to large\nrepositories of geo-spatial data -- e.g., LinkedGeoData, OpenStreetMap, etc. --\nspatial data is also routinely found in automatically constructed\nknowledgebases such as Yago and WikiData. While there have been research\nefforts for efficient processing of spatial data in RDF/SPARQL, very little\neffort has gone into building end-to-end systems that can holistically handle\ncomplex SPARQL queries along with spatial filters.\n  In this paper, we present Streak, a RDF data management system that is\ndesigned to support a wide-range of queries with spatial filters including\ncomplex joins, top-k, higher-order relationships over spatially enriched\ndatabases. Streak introduces various novel features such as a careful\nidentifier encoding strategy for spatial and non-spatial entities, the use of a\nsemantics-aware Quad-tree index that allows for early-termination and a clever\nuse of adaptive query processing with zero plan-switch cost. We show that\nStreak can scale to some of the largest publicly available semantic data\nresources such as Yago3 and LinkedGeoData which contain spatial entities and\nquantifiable predicates useful for result ranking. For experimental\nevaluations, we focus on top-k distance join queries and demonstrate that\nStreak outperforms popular spatial join algorithms as well as state of the art\nend-to-end systems like Virtuoso and PostgreSQL.\n",
        "  A cell-molecular based evolutionary model of tumor development driven by a\nstochastic Moran birth-death process is developed, where each cell carries\nmolecular information represented by a four-digit binary string, used to\ndifferentiate cells into 16 molecular types. The binary string value determines\ncell fitness, with lower fit cells (e.g. 0000) defined as healthy phenotypes,\nand higher fit cells (e.g. 1111) defined as malignant phenotypes. At each step\nof the birth-death process, the two phenotypic sub-populations compete in a\nprisoner's dilemma evolutionary game with healthy cells (cooperators) competing\nwith cancer cells (defectors). Fitness and birth-death rates are defined via\nthe prisoner's dilemma payoff matrix. Cells are able undergo two types of\nstochastic point mutations passed to the daughter cell's binary string during\nbirth: passenger mutations (conferring no fitness advantage) and driver\nmutations (increasing cell fitness). Dynamic phylogenetic trees show clonal\nexpansions of cancer cell sub-populations from an initial malignant cell. The\ntumor growth equation states that the growth rate is proportional to the\nlogarithm of cellular heterogeneity, here measured using the Shannon entropy of\nthe distribution of binary sequences in the tumor cell population. Nonconstant\ntumor growth rates, (exponential growth during sub-clinical range of the tumor\nand subsequent slowed growth during tumor saturation) are associated with a\nGompertzian growth curve, an emergent feature of the model explained here using\nsimple statistical mechanics principles related to the degree of functional\ncoupling of the cell states. Dosing strategies at early stage development,\nmid-stage (clinical stage), and late stage development of the tumor are\ncompared, showing therapy is most effective during the sub-clinical stage,\nbefore the cancer subpopulation is selected for growth.\n",
        "  Variations in mitochondrial genes are usually considered to infer\nphylogenies. However some of these genes are lesser constraint than other ones,\nand thus may blur the phylogenetic signals shared by the majority of the\nmitochondrial DNA sequences. To investigate such effects, in this research\nwork, the molecular phylogeny of the genus Taenia is studied using 14 coding\nsequences extracted from mitochondrial genomes of 17 species. We constructed\n16,384 trees, using a combination of 1 up to 14 genes. We obtained 131\ntopologies, and we showed that only four particular instances were relevant.\nUsing further statistical investigations, we then extracted a particular\ntopology, which displays more robustness properties.\n",
        "  Recent studies show that graph processing systems on a single machine can\nachieve competitive performance compared with cluster-based graph processing\nsystems. In this paper, we present NXgraph, an efficient graph processing\nsystem on a single machine. With the abstraction of vertex intervals and edge\nsub-shards, we propose the Destination-Sorted Sub-Shard (DSSS) structure to\nstore a graph. By dividing vertices and edges into intervals and sub-shards,\nNXgraph ensures graph data access locality and enables fine-grained scheduling.\nBy sorting edges within each sub-shard according to their destination vertices,\nNXgraph reduces write conflicts among different threads and achieves a high\ndegree of parallelism. Then, three updating strategies, i.e., Single-Phase\nUpdate (SPU), Double-Phase Update (DPU), and Mixed-Phase Update (MPU), are\nproposed in this paper. NXgraph can adaptively choose the fastest strategy for\ndifferent graph problems according to the graph size and the available memory\nresources to fully utilize the memory space and reduce the amount of data\ntransfer. All these three strategies exploit streamlined disk access pattern.\nExtensive experiments on three real-world graphs and five synthetic graphs show\nthat NXgraph can outperform GraphChi, TurboGraph, VENUS, and GridGraph in\nvarious situations. Moreover, NXgraph, running on a single commodity PC, can\nfinish an iteration of PageRank on the Twitter graph with 1.5 billion edges in\n2.05 seconds; while PowerGraph, a distributed graph processing system, needs\n3.6s to finish the same task.\n",
        "  In this report I discuss the relations between systoles and volumes of\nhyperbolic manifolds and a conjecture of Lehmer about the Mahler measure of\nnon-cyclotomic polynomials.\n",
        "  Previously, we developed a population model incorporating the Allee effect\nand periodic environmental fluctuations, in which organisms alternate between\nnomadic and colonial behaviours. This switching strategy is regulated by\nbiological clocks and the abundance of environmental resources, and can lead to\npopulation persistence despite both behaviours being individually losing. In\nthe present study, we consider stochastic noise models in place of the original\nperiodic ones, thereby allowing a wider range of environmental fluctuations to\nbe modelled. The theoretical framework is generalized to account for resource\ndepletion by both nomadic and colonial sub-populations, and an ecologically\nrealistic population size-dependent switching scheme is proposed. We\ndemonstrate the robustness of the modified switching scheme to stochastic\nnoise, and we also present the intriguing possibility of consecutive\nsubsidence-recovery cycles within the resulting population dynamics. Our\nresults have relevance in biological and physical systems.\n",
        "  Plate-like single crystals of SrFe2As2 as large as 3x3x0.5 mm3 have been\ngrown out of Sn flux. The SrFe2As2 single crystals show a structural phase\ntransition from a high temperature tetragonal phase to a low temperature\northorhombic phase at To = 198 K, and do not show any sign of superconductivity\ndown to 1.8 K. The structural transition is accompanied by an anomaly in the\nelectrical resistivity, Hall resistivity, specific heat, and the anisotropic\nmagnetic susceptibility. In an intermediate temperature range from 198 K to 160\nK, single crystal X-ray diffraction suggests a coexistence of the\nhigh-temperature tetragonal and the low-temperature orthorhombic phases.\n",
        "  Inverse dynamics methods for muscle forces prediction are globally unable to\npredict antagonistic activity during a joint motion. This is due to a lack of\nphysiological information describing how forces are shared between flexors and\nextensors. The aim of this study is the definition and the use of a new\nEMG-based cocontraction ratio in an inverse dynamics muscle forces prediction\napproach applied to the elbow flexion motion. Results show the relevance of the\nratio.\n",
        "  Unsupervised representation learning for tweets is an important research\nfield which helps in solving several business applications such as sentiment\nanalysis, hashtag prediction, paraphrase detection and microblog ranking. A\ngood tweet representation learning model must handle the idiosyncratic nature\nof tweets which poses several challenges such as short length, informal words,\nunusual grammar and misspellings. However, there is a lack of prior work which\nsurveys the representation learning models with a focus on tweets. In this\nwork, we organize the models based on its objective function which aids the\nunderstanding of the literature. We also provide interesting future directions,\nwhich we believe are fruitful in advancing this field by building high-quality\ntweet representation learning models.\n",
        "  Employing virtual crystal approximation and super-cell methods for doping, we\nhave performed a comparative study of the electronic structures of various\ndoped BaFe$_2$As$_2$ materials by first principles simulations. Both of these\nmethods give rise to a similar density of states and band structures in case of\nhole doping (K doping in Ba site) and iso-electronic P doping in As site. But\nin case of electron doped systems with higher doping concentration, electronic\nstructures, calculated using virtual crystal approximation approach deviates\nfrom that of the super-cell method. On the other hand in case of iso-electronic\nRu doping implemented by virtual crystal approximation, an extra shift of the\nchemical potential in electronic structure in comparison to super-cell method\nis observed and that shift can be used to predict the correct electronic\nstructure within virtual crystal approximation as reflected in our calculated\nFermi surfaces. But for higher Ru doping concentration, simple shifting of\nchemical potential does not work as the electronic structure calculated by\nvirtual crystal approximation approach is entirely different from that of the\ncalculated by super-cell formalism.\n",
        "  The classical Luria-Delbr\\\"uck model for fluctuation analysis is extended to\nthe case where cells can either divide or die at the end of their generation\ntime. This leads to a family of probability distributions generalizing the\nLuria-Delbr\\\"uck family, and depending on three parameters: the expected number\nof mutations, the relative fitness of normal cells compared to mutants, and the\ndeath probability of mutants. The probabilistic treatment is similar to that of\nthe classical case; simulation and computing algorithms are provided. The\nestimation problem is discussed: if the death probability is known, the two\nother parameters can be reliably estimated. If the death probability is\nunknown, the model can be identified only for large samples.\n",
        "  A growing body of evidence has been supporting the existence of so-called\n\"dark molecular gas\" (DMG), which is invisible in the most common tracer of\nmolecular gas, i.e., CO rotational emission. DMG is believed to be the main gas\ncomponent of the intermediate extinction region between A$\\rm_v$$\\sim$0.05-2,\nroughly corresponding to the self-shielding threshold of H$_2$ and $^{13}$CO.\nTo quantify DMG relative to HI and CO, we are pursuing three observational\ntechniques, namely, HI self-absorption, OH absorption, and TeraHz C$^+$\nemission. In this paper, we focus on preliminary results from a CO and OH\nabsorption survey of DMG candidates. Our analysis show that the OH excitation\ntemperature is close to that of the Galactic continuum background and that OH\nis a good DMG tracer co-existing with molecular hydrogen in regions without CO.\nThrough systematic \"absorption mapping\" by Square Kilometer Array (SKA) and\nALMA, we will have unprecedented, comprehensive knowledge of the ISM components\nincluding DMG in terms of their temperature and density, which will impact our\nunderstanding of galaxy evolution and star formation profoundly.\n",
        "  Recently, considerable effort has been put into developing fast algorithms to\nreconstruct a rooted phylogenetic network that explains two rooted phylogenetic\ntrees and has a minimum number of hybridization vertices. With the standard\napproach to tackle this problem being combinatorial, the reconstructed network\nis rarely unique. From a biological point of view, it is therefore of\nimportance to not only compute one network, but all possible networks. In this\npaper, we make a first step towards approaching this goal by presenting the\nfirst algorithm---called allMAAFs---that calculates all\nmaximum-acyclic-agreement forests for two rooted binary phylogenetic trees on\nthe same set of taxa.\n",
        "  We exploit ALMA 870um observations to measure the star-formation rates (SFRs)\nof eight X-ray detected Active Galactic Nuclei (AGNs) in a z~3.1 protocluster,\nfour of which reside in extended Ly-alpha haloes (often termed Ly-alpha blobs:\nLABs). Three of the AGNs are detected by ALMA and have implied SFRs of\n~220-410~M_sun/yr; the non detection of the other five AGNs places SFR upper\nlimits of <210 M_sun/yr. The mean SFR of the protocluster AGNs (~110-210\nM_sun/yr) is consistent (within a factor of ~0.7-2.3) with that found for\nco-eval AGNs in the field, implying that galaxy growth is not significantly\naccelerated in these systems. However, when also considering ALMA data from the\nliterature, we find evidence for elevated mean SFRs (up-to a factor of ~5.9\nover the field) for AGNs at the protocluster core, indicating that galaxy\ngrowth is significantly accelerated in the central regions of the protocluster.\nWe also show that all of the four protocluster LABs are associated with an ALMA\ncounterpart within the extent of their Ly-alpha emission. The SFRs of the ALMA\nsources within the LABs (~150-410 M_sun/yr) are consistent with those expected\nfor co-eval massive star-forming galaxies in the field. Furthermore, the two\ngiant LABs (with physical extents of >100 kpc) do not host more luminous star\nformation than the smaller LABs, despite being an order of magnitude brighter\nin Ly-alpha emission. We use these results to discuss star formation as the\npower source of LABs.\n",
        "  In order to model entanglements of polymers in a confined region, we consider\nthe linking numbers and writhes of cycles in random linear embeddings of\ncomplete graphs in a cube. Our main results are that for a random linear\nembedding of $K_n$ in a cube, the mean sum of squared linking numbers and the\nmean sum of squared writhes are of the order of $\\theta(n(n!))$. We obtain a\nsimilar result for the mean sum of squared linking numbers in linear embeddings\nof graphs on $n$ vertices, such that for any pair of vertices, the probability\nthat they are connected by an edge is $p$. We also obtain experimental results\nabout the distribution of linking numbers for random linear embeddings of these\ngraphs. Finally, we estimate the probability of specific linking configurations\noccurring in random linear embeddings of the graphs $K_6$ and $K_{3,3,1}$.\n",
        "  High-quality superconducting KxFeySe2 single crystals were synthesized using\nan easy one-step method. Detailed annealing studies were performed to make\nclear the phase formation process in KxFeySe2. Compatible observations were\nfound in temperature-dependent X-ray diffraction patterns, back-scattered\nelectron images and corresponding electromagnetic properties, which proved that\ngood superconductivity performance was close related to the microstructure of\nsuperconducting component. Analysis based on the scaling behavior of flux\npinning force indicated that the dominant pinning mechanism was delta(Tc)\npinning and independent of connectivity. The annealing dynamics studies were\nalso performed, which manifested that the humps in temperature-dependent\nresistance (RT) curves were induced by competition between the\nmetallic/superconducting and the semiconducting/insulating phases.\n",
        "  We investigate the detection efficiency of a spiral layout of a\nSuperconducting Nanowire Single-Photon Detector (SNSPD). The design is less\nsusceptible to the critical current reduction in sharp turns of the nanowire\nthan the conventional meander design. Detector samples with different nanowire\nwidth from 300 to 100 nm are patterned from a 4 nm thick NbN film deposited on\nsapphire substrates. The critical current IC at 4.2 K for spiral, meander, and\nsimple bridge structures is measured and compared. On the 100 nm wide samples,\nthe detection efficiency is measured in the wavelength range 400-1700 nm and\nthe cut-off wavelength of the hot-spot plateau is determined. In the optical\nrange, the spiral detector reaches a detection efficiency of 27.6%, which is\n~1.5 times the value of the meander. In the infrared range the detection\nefficiency is more than doubled.\n",
        "  An explicit construction of closed, orientable, smooth, aspherical\n4-manifolds with any odd Euler characteristic greater than 12 is presented. The\nmanifolds constructed here are all Haken manifolds in the sense of B. Foozwell\nand H. Rubinstein and can be systematically reduced to balls by suitably\ncutting them open along essential codimension-one submanifolds. It is easy to\nconstruct examples with even Euler characteristic from products of surfaces.\nAnd Euler characteristics divisible by 3 are know to arise from complex\nalgebraic geometry considerations. Examples with Euler characteristic 1, 5, 7,\nor 11 appear to be unknown.\n",
        "  We use a simple elastic Hamiltonian for the vortex lattice in a weak impurity\nbackground which includes defects in the form of integer-valued fields to\ncalculate the free energy of a vortex lattice in the deep H_{c2} region. The\nphase diagram in this regime is obtained by applying the variational approach\nof M{\\'e}zard and Parisi developed for random manifolds. We find a first-order\nline between the Bragg-glass and vortex-glass phase as a continuation of the\nmelting line. In the liquid phase, we obtain an almost vertical third-order\nglass transition line near the critical temperature in the H-T plane.\nFurthermore, we find an almost vertical second-order phase transition line in\nthe Bragg-glass as well as the vortex-glass phases which crosses the\nfirst-order Bragg-glass, vortex-glass transition line. We calculate the jump of\nthe temperature derivate of the induction field across this second-order line\nas well as the entropy and magnetic field jumps across the first-order line.\n",
        "  The paper presents the importance of research which characterizes the natural\nforest structure for the forest management. The lessons learned in these\nparticular forest ecosystems can be integrated by the forest management\nobjectives, in order to increase the sustainability of this type of resources.\nThe project NATFORMAN was focused on the structure of the natural forest, thus\nresearch methodologies and modern technology (such as Field-Map) investigation\nand determination were used in order to record information on forest structural\nparameters. The results obtained refer to these structural parameters and to\nthe possibility of transferring such information in practice, in order to\nachieve forest sustainable management.\n",
        "  This article is an English translation of Japanese article \"Musubime to\nKyokumen\", Math. Soc. Japan, Sugaku Vol. 67, No. 4 (2015) 403--423. It surveys\na specific area in Knot Theory concerning surfaces in knot exteriors.\n  In version 2, we added comments on the solutions or counterexamples for\nConjecture 3.5, Conjecture 3.7 and Conjecture 5.30.\n",
        "  By means of direct N-body simulations and simplified numerical models, we\nstudy the formation and characteristics of the tidal tails around Palomar 5,\nalong its orbit in the Milky Way potential. Unlike previous findings, we are\nable to reproduce the substructures observed in the stellar streams of this\ncluster, without including any lumpiness in the dark matter halo. We show that\noverdensities similar to those observed in Palomar 5 can be reproduced by the\nepicyclic motion of stars along its tails, i.e. a simple local accumulation of\norbits of stars that escaped from the cluster with very similar positions and\nvelocities. This process is able to form stellar clumps at distances of several\nkiloparsecs from the cluster, so it is not a phenomenon confined to the inner\npart of Palomar 5's tails, as previously suggested. Our models can reproduce\nthe density contrast between the clumps and the surrounding tails found in the\nobserved streams, without including any lumpiness in the dark halo, suggesting\nnew upper limits on its granularity.\n",
        "  We study the problem of joint question answering (QA) and question generation\n(QG) in this paper.\n  Our intuition is that QA and QG have intrinsic connections and these two\ntasks could improve each other.\n  On one side, the QA model judges whether the generated question of a QG model\nis relevant to the answer.\n  On the other side, the QG model provides the probability of generating a\nquestion given the answer, which is a useful evidence that in turn facilitates\nQA.\n  In this paper we regard QA and QG as dual tasks.\n  We propose a training framework that trains the models of QA and QG\nsimultaneously, and explicitly leverages their probabilistic correlation to\nguide the training process of both models.\n  We implement a QG model based on sequence-to-sequence learning, and a QA\nmodel based on recurrent neural network.\n  As all the components of the QA and QG models are differentiable, all the\nparameters involved in these two models could be conventionally learned with\nback propagation.\n  We conduct experiments on three datasets. Empirical results show that our\ntraining framework improves both QA and QG tasks.\n  The improved QA model performs comparably with strong baseline approaches on\nall three datasets.\n",
        "  Interactive data-intensive applications are becoming ever more pervasive in\ndomains such as finance, web applications, mobile computing, and Internet of\nThings. Increasingly, these applications are being deployed in sophisticated\nparallel and distributed hardware infrastructures. With this growing diversity\nof the software and hardware landscape, there is a pressure on programming\nmodels and systems to enable developers to design modular, scalable, efficient,\nand consistent data-intensive applications. In response to this challenge,\nrecent research has advocated the integration of actor programming models and\ndatabase management. This integration promises to help developers build\nlogically distributed micro-applications well adapted to modern hardware trends\nas opposed to existing approaches targeted at optimizing monolithic\napplications.\n  Towards this aim, in this paper we analyze, make the case for, and present a\nbroad vision of actor-relational database systems. We argue why the time is\nripe today to examine the research opportunities afforded by this emerging\nsystem paradigm. Based on this discussion, we present design principles as well\nas candidate feature sets to help concretize the vision for such systems. To\nillustrate the usefulness of the proposed feature set and motivate the need for\nthis class of systems, we show a detailed case study inspired by a smart\nsupermarket application with self-checkout, along with evidence for performance\nbenefits on modern hardware.\n",
        "  This paper presents a precursory yet novel approach to the question answering\ntask using structural decomposition. Our system first generates linguistic\nstructures such as syntactic and semantic trees from text, decomposes them into\nmultiple fields, then indexes the terms in each field. For each question, it\ndecomposes the question into multiple fields, measures the relevance score of\neach field to the indexed ones, then ranks all documents by their relevance\nscores and weights associated with the fields, where the weights are learned\nthrough statistical modeling. Our final model gives an absolute improvement of\nover 40% to the baseline approach using simple search for detecting documents\ncontaining answers.\n",
        "  Recent observations suggest a scenario in which filamentary structures in the\nISM represent the first step towards clumps/cores and eventually star\nformation. The densest filaments would then fragment into prestellar cores\nowing to gravitational instability. We seek to understand the roles filamentary\nstructures play in high-mass star formation. We mapped the integral-shaped\nfilament (ISF) in NH3 (1, 1) and (2, 2). The whole filamentary structure is\nuniformly and fully sampled. We find that the morphology revealed by the map of\nvelocity-integrated intensity of the NH3 (1, 1) line is closely associated with\nthe dust ridge. We identify 6 \"clumps\" related to the well known OMC-1 to 5 and\n11 \"sub-clumps\" within the map and they are separated not randomly but in\nroughly equal intervals along the ISF. The average spacing of clumps is\n11.30'$\\pm$1.31' (1.36$\\pm$0.16 pc ) and the average spacing of sub-clumps is\n7.18'$\\pm$1.19' (0.86$\\pm$0.14 pc). These spacings agree well with the\npredicted values of the thermal (0.86 pc) and turbulent sausage instability\n(1.43 pc) by adopting a cylindric geometry of the ISF with an inclination of\n$60^{\\circ}$ with respect to the line of sight. We also find a velocity\ngradient of about 0.6 km s-1 pc-1 that runs along the ISF which likely arises\nfrom an overall rotation of the Orion A molecular cloud. The inferred ratio\nbetween rotational and gravitational energy is well below unity. Furthermore,\nfluctuations are seen in the centroid velocity diagram along the ISF. The OMC-1\nto 5 clouds are located close to the local extrema of the fluctuations, which\nsuggests that there exist gas flows associated with these clumps in the ISF.\nThe derived NH3 (1, 1) and (2, 2) rotation temperatures in the OMC-1 are about\n30-40 K. In OMC-2, OMC-3, and the northern part of OMC-4, we find higher and\nlower temperatures at the boundaries and in the interior, respectively.\n",
        "  We report the discovery and constrain the physical conditions of the\ninterstellar medium of the highest-redshift millimeter-selected dusty\nstar-forming galaxy (DSFG) to date, SPT-S J031132-5823.4 (hereafter\nSPT0311-58), at $z=6.900 +/- 0.002$. SPT0311-58 was discovered via its 1.4mm\nthermal dust continuum emission in the South Pole Telescope (SPT)-SZ survey.\nThe spectroscopic redshift was determined through an ALMA 3mm frequency scan\nthat detected CO(6-5), CO(7-6) and [CI](2-1), and subsequently confirmed by\ndetections of CO(3-2) with ATCA and [CII] with APEX. We constrain the\nproperties of the ISM in SPT0311-58 with a radiative transfer analysis of the\ndust continuum photometry and the CO and [CI] line emission. This allows us to\ndetermine the gas content without ad hoc assumptions about gas mass scaling\nfactors. SPT0311-58 is extremely massive, with an intrinsic gas mass of $M_{\\rm\ngas} = 3.3 \\pm 1.9 \\times10^{11}\\,M_{\\odot}$. Its large mass and intense star\nformation is very rare for a source well into the Epoch of Reionization.\n",
        "  Electromagnetic radiation (photons) or particle beam (protons or heavy ions)\nhave similar biological effects, i.e. damage to human cell DNA that eventually\nleads to cell death if not correctly repaired. The biological effects at the\nlevel of organs or organisms are explained by a progressive depletion of\nconstitutive cells; below a given threshold, cell division is no longer\nsufficient to compensate for cell loss, up to a point where the entire organism\n(or organ) breaks down. The quantitative aspects of the biological effects are\nmodulated by the microscopic distribution of energy deposits along the beam or\nparticle tracks. In particular, the ionization density, i.e. the amount of\nenergy deposited by unit path length (measured in keV/{\\mu}m), has an influence\non the biological effectiveness, i.e. the amount of damage per energy unit\ndeposited (measured in gray or Gy, equivalent to 1 joule/kg). The ionization\ndensity is usually represented by the Linear Energy Transfer or LET, also\nexpressed in keV/{\\mu}m. Photon beams (X-rays, g-rays) are low-LET radiation,\nwith a sparsely ionising characteristic. Particle beams have a higher LET, with\na more dense distribution of energy deposits along the particle tracks. Protons\nare intermediary, with a LET larger than the photon one, but still belong to\nthe 'radiobiological' group of low LET. The higher the ionization density, the\nhigher the biological effectiveness per unit of dose.\n",
        "  This paper proposes to address the word sense ambiguity issue in an\nunsupervised manner, where word sense representations are learned along a word\nsense selection mechanism given contexts. Prior work focused on designing a\nsingle model to deliver both mechanisms, and thus suffered from either\ncoarse-grained representation learning or inefficient sense selection. The\nproposed modular approach, MUSE, implements flexible modules to optimize\ndistinct mechanisms, achieving the first purely sense-level representation\nlearning system with linear-time sense selection. We leverage reinforcement\nlearning to enable joint training on the proposed modules, and introduce\nvarious exploration techniques on sense selection for better robustness. The\nexperiments on benchmark data show that the proposed approach achieves the\nstate-of-the-art performance on synonym selection as well as on contextual word\nsimilarities in terms of MaxSimC.\n",
        "  We review the recent progress in the density functional theory for\nsuperconductors (SCDFT). Motivated by the long-studied plasmon mechanism of\nsuperconductivity, we have constructed an exchange-correlation kernel entering\nthe SCDFT gap equation which includes the plasmon effect. For the case of\nlithium under high pressures, we show that the plasmon effect substantially\nenhances the transition temperature (Tc) by cooperating with the conventional\nphonon mechanism and results in a better agreement between the theoretical and\nexperimentally observed Tc. Our present formalism will be a first step to\ndensity functional theory for unconventional superconductors.\n",
        "  Generalization of a disordered metal's theory has been proposed when\nscattering of quasiparticles by impurities is caused with a retarded\ninteraction. It was shown that in this case Anderson's theorem was violated in\nthe sense that embedding of the impurities in s-wave superconductor increases\nits critical temperature. The increasing depends on parameters of the metal,\nimpurities and their concentration. At a specific relation between the\nparameters the critical temperature of the dirty superconductor can essentially\nexceed critical temperature of pure one up to room temperature. Thus the\nimpurities catalyze superconductivity in an originally low-temperature\nsuperconductor.\n",
        "  Cross-sectional imaging of human organ serves as a critical tool to provide\ndiagnostic results of many diseases. Based on a unique body coordinate system,\nwe present a method that we use to reconstruct any cross-sectional imaging of\norgan regardless of its original section going along which scanning or cutting\naxis. In clinical medicine, this method enables a patient to undergo only one\nscanning, and then the doctor can observe the structure of lesion sections\nalong any axis, and it can help find changes of lesions at the same section\nfrom different scanning results and thus quantify diagnosis by cross-sectional\nimaging. Significant progress has thus been made towards quantitative diagnosis\ncross-sectional imaging.\n",
        "  We give a shorter proof of the following theorem of Kathryn Mann \\cite{M}:\nthe identity component of the group of the compactly supported $C^r$\ndiffeomorphisms of $\\R^n$ cannot admit a nontrivial $C^p$-action on $S^1$,\nprovided $n\\geq2$, $r\\neq n+1$ and $p\\geq2$. We also give a new proof of\nanother theorem of Mann: any nontrivial endomorphism of the group of the\norientation preserving $C^r$ diffeomorphisms of the circle is the conjugation\nby a $C^r$ diffeomorphism, if $r\\geq3$.\n",
        "  We examine the possibility that the superconductivity in the newly discovered\nFeAs materials may be caused by the Coulomb interaction between d-electrons of\nthe iron atoms. We find that when the Hund's rule ferromagnetic interaction is\nstrong enough, the leading pairing instability is in spin-triplet p-wave\nchannel in the weak coupling limit. The resulting superconducting gap has nodal\nlines on the 3D Fermi surfaces. The k dependent hybridization of several\norbitals around a Fermi pocket is the key for the appearance of the\nspin-triplet p-wave pairing.\n",
        "  We propose a model to automatically describe changes introduced in the source\ncode of a program using natural language. Our method receives as input a set of\ncode commits, which contains both the modifications and message introduced by\nan user. These two modalities are used to train an encoder-decoder\narchitecture. We evaluated our approach on twelve real world open source\nprojects from four different programming languages. Quantitative and\nqualitative results showed that the proposed approach can generate feasible and\nsemantically sound descriptions not only in standard in-project settings, but\nalso in a cross-project setting.\n",
        "  Fungicide mixtures produced by the agrochemical industry often contain\nlow-risk fungicides, to which fungal pathogens are fully sensitive, together\nwith high-risk fungicides known to be prone to fungicide resistance. Can these\nmixtures provide adequate disease control while minimizing the risk for the\ndevelopment of resistance? We present a population dynamics model to address\nthis question. We found that the fitness cost of resistance is a crucial\nparameter to determine the outcome of competition between the sensitive and\nresistant pathogen strains and to assess the usefulness of a mixture. If\nfitness costs are absent, then the use of the high-risk fungicide in a mixture\nselects for resistance and the fungicide eventually becomes nonfunctional. If\nthere is a cost of resistance, then an optimal ratio of fungicides in the\nmixture can be found, at which selection for resistance is expected to vanish\nand the level of disease control can be optimized.\n",
        "  Epistasis is a key concept in the theory of adaptation. Indicators of\nepistasis are of interest for large system where systematic fitness\nmeasurements may not be possible. Some recent approaches depend on information\ntheory. We show that considering shared entropy for pairs of loci can be\nmisleading. The reason is that shared entropy does not imply epistasis for the\npair. This observation holds true also in the absence of higher order\nepistasis. We discuss a refined approach for identifying pairwise interactions\nusing entropy.\n",
        "  Lumped method (Electrical analogy) is a quick and easy way to model human\ncardiovascular system. In this paper Lumped method is used for simulating a\ncomplete model. It describes a 36-vessel model and cardiac system of human body\nwith details that could show hydrodynamic parameters of cardiovascular system.\nAlso this paper includes modeling of pulmonary, atrium, left and right\nventricles with their equivalent circuits. Exact modeling of right and left\nventricles pressure with division of ascending aorta into 27 segments increases\nthe accuracy of our simulation. In this paper we show that a calculated\npressure for aorta from our complex circuit is near to measured pressure by\nusing advanced medical instruments. Also it is shown that pressure graph from\nbrachial is so near to aortic pressure because of this its pressure signal is\nusable instead of aortic pressure. Furthermore, obstruction in ascending aorta,\nbrachial and its effects has been showed in different figures.\n",
        "  1. In recent years many studies have investigated the relationships between\ndifferent aspects of the immune system and ecology in various organisms. Yet,\nit remains unclear why individuals differ in their ability to mount an immune\nresponse against various antigens (often referred to as 'immunocompetence').\nDifferent kinds of trade-offs may be involved and costs of mounting the immune\nresponse often lead to condition dependent effects. 2. We investigated how\nvariation in condition, morphology and genetic variables influenced the amount\nof antibodies produced against two novel antigens in a migrating bird, the\ngreat snipe (Gallinago media). 3. We found no evidence for condition dependence\nof the antibody response and no effect of MHC genetics. There was, however, a\nweak negative correlation between body size and the amount of antibody\nproduction, which may indicate a trade-off between growth and immune response\nin this species.\n",
        "  The Web of Data is an open environment consisting of a great number of large\ninter-linked RDF datasets from various domains. In this environment,\norganizations and companies adopt the Linked Data practices utilizing Semantic\nWeb (SW) technologies, in order to publish their data and offer SPARQL\nendpoints (i.e., SPARQL-based search services). On the other hand, the dominant\nstandard for information exchange in the Web today is XML. The SW and XML\nworlds and their developed infrastructures are based on different data models,\nsemantics and query languages. Thus, it is crucial to develop interoperability\nmechanisms that allow the Web of Data users to access XML datasets, using\nSPARQL, from their own working environments. It is unrealistic to expect that\nall the existing legacy data (e.g., Relational, XML, etc.) will be transformed\ninto SW data. Therefore, publishing legacy data as Linked Data and providing\nSPARQL endpoints over them has become a major research challenge. In this\ndirection, we introduce the SPARQL2XQuery Framework which creates an\ninteroperable environment, where SPARQL queries are automatically translated to\nXQuery queries, in order to access XML data across the Web. The SPARQL2XQuery\nFramework provides a mapping model for the expression of OWL-RDF/S to XML\nSchema mappings as well as a method for SPARQL to XQuery translation. To this\nend, our Framework supports both manual and automatic mapping specification\nbetween ontologies and XML Schemas. In the automatic mapping specification\nscenario, the SPARQL2XQuery exploits the XS2OWL component which transforms XML\nSchemas into OWL ontologies. Finally, extensive experiments have been conducted\nin order to evaluate the schema transformation, mapping generation, query\ntranslation and query evaluation efficiency, using both real and synthetic\ndatasets.\n",
        "  (ABRIDGED) We report the genome sequencing of 139 wild-derived strains of D.\nmelanogaster, representing 22 population samples from the sub-Saharan ancestral\nrange of this species, along with one European population. Most genomes were\nsequenced above 25X depth from haploid embryos. Results indicated a pervasive\ninfluence of non-African admixture in many African populations, motivating the\ndevelopment and application of a novel admixture detection method. Admixture\nproportions varied among populations, with greater admixture in urban\nlocations. Admixture levels also varied across the genome, with localized peaks\nand valleys suggestive of a non-neutral introgression process. Genomes from the\nsame location differed starkly in ancestry, suggesting that isolation\nmechanisms may exist within African populations. After removing putatively\nadmixed genomic segments, the greatest genetic diversity was observed in\nsouthern Africa (e.g. Zambia), while diversity in other populations was largely\nconsistent with a geographic expansion from this potentially ancestral region.\nThe European population showed different levels of diversity reduction on each\nchromosome arm, and some African populations displayed chromosome arm-specific\ndiversity reductions. Inversions in the European sample were associated with\nstrong elevations in diversity across chromosome arms. Genomic scans were\nconducted to identify loci that may represent targets of positive selection. A\ndisproportionate number of candidate selective sweep regions were located near\ngenes with varied roles in gene regulation. Outliers for Europe-Africa FST were\nfound to be enriched in genomic regions of locally elevated cosmopolitan\nadmixture, possibly reflecting a role for some of these loci in driving the\nintrogression of non-African alleles into African populations.\n",
        "  Bilingual dictionaries are very important in various fields of natural\nlanguage processing. In recent years, research on extracting new bilingual\nlexicons from non-parallel (comparable) corpora have been proposed. Almost all\nuse a small existing dictionary or other resources to make an initial list\ncalled the \"seed dictionary\". In this paper, we discuss the use of different\ntypes of dictionaries as the initial starting list for creating a bilingual\nPersian-Italian lexicon from a comparable corpus. Our experiments apply\nstate-of-the-art techniques on three different seed dictionaries; an existing\ndictionary, a dictionary created with pivot-based schema, and a dictionary\nextracted from a small Persian-Italian parallel text. The interesting challenge\nof our approach is to find a way to combine different dictionaries together in\norder to produce a better and more accurate lexicon. In order to combine seed\ndictionaries, we propose two different combination models and examine the\neffect of our novel combination models on various comparable corpora that have\ndiffering degrees of comparability. We conclude with a proposal for a new\nweighting system to improve the extracted lexicon. The experimental results\nproduced by our implementation show the efficiency of our proposed models.\n",
        "  This paper describes our experience with using Grid files as the main storage\norganization for a relational database management system. We primarily focus on\nthe following two aspects. (i) Strategies for implementing grid files\nefficiently. (ii) Methods for efficiency evaluating queries posed to a database\norganized using grid files.\n",
        "  We present the analysis of the spin vector orientation of 5$\\,$987 SDSS\ngalaxies having negative redshift from $-$87.6 to $-$0.3 km$\\,$s$^{-1}$. Two\ndimensional observed parameters are used to compute three dimensional galaxy\nrotation axes by applying `position angle--inclination' method. We aim to\nexamine the non-random effects in the spatial orientation of blue-shifted\ngalaxies. We generate 5$\\times$10$^6$ virtual galaxies to find expected\nisotropic distributions by performing numerical simulations. We have written\nMATLAB program to facilitate the simulation process and eliminate the manual\nerrors in the process. Chi-square, auto-correlation, and the Fourier tests are\nused to examine non-random effects in the polar and azimuthal angle\ndistributions of the galaxy rotation axes. In general, blue-shifted galaxies\nshow no preferred alignments of galaxy rotation axes. Our results support\nHierarchy model, which suggests a random orientation of angular momentum\nvectors of galaxies. However, local effects are noted suggesting gravitational\ntidal interaction between neighboring galaxies.\n",
        "  In this paper, a branching process model of the Northern Spotted Owl is\nsimulated. We focus on the time until extinction. It is shown how an\napproximation of the model with a multivariate autoregressive process works\nwell near the equilibrium, but does not give a good estimate of the time until\nextinction. We also show that introduction of randomness in some of the\nparameters previously assumed to be constants shortens the time until\nextinction considerably.\n",
        "  We give a method to construct non symmetric solutions of a global tetrahedron\nequation from solutions of the Yang-Baxter equation. The solution in the\nHOMFLYPT case gives rise to the first combinatorial quantum 1-cocycle which\nrepresents a non trivial cohomology class in the topological moduli space of\nlong knots. We conjecture that the quotient of its values on Hatchers loop and\non the rotation around the long axis is related to the simplicial volume of the\nknot complement in the 3-sphere and we prove this for the figure eight knot.\n  Surprisingly, the formula for the solution in the HOMFLYPT case of the\npositive global tetrahedron equation gives also a solution in the case of the\n2-variable Kauffman invariant. But there is also a second solution giving rise\nto yet another non trivial quantum 1-cocycle.\n",
        "  Estimating the phylogeny of the genus Homo is entering a new phase of vastly\nimproved data and methodology. There is increasing evidence of 6 to 10\ncompeting species/lineages at any point in the last half million years, making\nthe elucidation of the relationships of individual specimens particularly\nimportant. Recent estimates of the phylogeny of key specimens include Waddell\n(2013, 2014, 2015, 2016), and Mounier et al. (2016). These are made with quite\ndifferent data (3D skull shapes and discrete morphological characters,\nrespectively) and methods of analysis (unweighted least squares fitting of\ndistances, OLS+, and reweighted maximum parsimony, respectively). Initial\ninspection of the trees in these articles might leave the impression of a great\ndeal of disagreement and confused results. Here it is shown this need not be\nthe case, and that these two types of data and analysis may be indicating a\nvery similar tree, one that is in good agreement also with subjective current\nwisdom/expert opinions on particular parts of the phylogeny. The precise\nlocation of the African LH18 specimen arises as key to a better understanding\nof the likely form of the last common ancestors of H. sapiens and Neanderthals.\nA diverse approach seems to bring forth much more agreement of trees than\notherwise perceived, and argues against being dogmatic about methods of\nphylogenetic analysis particularly when working with difficult problems.\n",
        "  Purpose: The goal is to develop a new architecture for computed tomography\n(CT) which is at an ultra-low-dose for developing countries, especially in\nrural areas. Methods: The proposed scheme is inspired by the recently developed\ncompressive sensing and interior tomography techniques, where the data\nacquisition system targets a region of interest (ROI) to acquire limited and\ntruncated data. The source and detector are translated in opposite directions\nfor either ROI reconstruction with one or more localized linear scans or global\nreconstruction by combining multiple ROI reconstructions. In other words, the\npopular slip ring is replaced by a translation based setup, and the\ninstrumentation cost is reduced by a relaxation of the imaging speed\nrequirement. Results: The various translational scanning modes are\ntheoretically analyzed, and the scanning parameters are optimized. The\nnumerical simulation results from different numbers of linear scans confirm the\nfeasibility of the proposed scheme, and suggest two preferred low-end systems\nfor horizontal and vertical patient positions respectively. Conclusion:\nUltra-low-cost x-ray CT is feasible with our proposed combination of linear\nscanning, compressive sensing, and interior tomography. The proposed\narchitecture can be tailored into permanent, movable, or reconfigurable systems\nas desirable. Advanced image registration and spectral imaging features can be\nincluded as well.\n",
        "  We consider different effects that arise when time-reversal symmetry breaking\nsuperconductors are subjected to an external magnetic field, thus rendering the\nsuperconductor to be in the mixed state. We focus in particular on two\ntime-reversal symmetry breaking order parameters which are believed to be\nrealized in actual materials: $p+\\i p'$-wave and $d+\\i s$- or $d+\\i d'$-wave.\nThe first order parameter is relevant for Sr$_2$RuO$_4$, while the latter order\nparameters have been suggested to exist near surfaces in some of the high-$T_c$\ncuprates. We investigate the interplay between surface states and vortex states\nin the presence of an external magnetic field and their influence on both the\ntunneling conductance and the local density of states. Our findings may be\nhelpful to experimentally identify the symmetry of unconventional time-reversal\nsymmetry breaking superconducting states.\n",
        "  Quantitative plant disease resistance is believed to be more durable than\nqualitative resistance, since it exerts less selective pressure on the\npathogens. However, the process of progressive pathogen adaptation to\nquantitative resistance is poorly understood, which makes it difficult to\npredict its durability or to derive principles for its sustainable deployment.\nHere, we study the dynamics of pathogen adaptation in response to quantitative\nplant resistance affecting pathogen reproduction rate and its carrying\ncapacity. We developed a stochastic model for the continuous evolution of a\npathogen population within a quantitatively resistant host. We assumed that\npathogen can adapt to a host by the progressive restoration of reproduction\nrate or of carrying capacity, or of both. Our model suggests that a combination\nof QTLs affecting distinct pathogen traits was more durable if the evolution of\nrepressed traits was antagonistic. Otherwise, quantitative resistance that\ndepressed only pathogen reproduction was more durable. In order to decelerate\nthe progressive pathogen adaptation, QTLs that decrease the pathogen's ability\nto extend must be combined with QTLs that decrease the spore production per\nlesion or the infection efficiency or that increase the latent period. Our\ntheoretical framework can help breeders to develop principles for sustainable\ndeployment of quantitative trait loci..\n",
        "  We theoretically investigate the coexistence of antiferromagnetism and\nsuperconductivity in the iron-based superconductors by using the mean-field\ntheory for two- and three-orbital models. We find that both the s_{+-}-wave and\ns_{++}-wave superconductivity can coexist with antiferromagnetism in the two\nmodels. On Dirac Fermi surfaces emerging in the antiferromagnetic phase, a\nsuperconducting-gap function has a node for s_{++} wave but is nodeless for\ns_{+-} wave. On the other hand, the gap function on non-Dirac Fermi surfaces is\neither nodeless or accidentally nodal, depending on the parameters of pairing\ninteraction, which is independent of pairing symmetry.\n",
        "  The $^{192}$Ir sources are widely used for high dose rate (HDR) brachytherapy\ntreatments. The aim of this study is to simulate $^{192}$Ir MicroSelectron v2\nHDR brachytherapy source and calculate the air kerma strength, dose rate\nconstant, radial dose function and anisotropy function established in the\nupdated AAPM Task Group 43 protocol. The EGSnrc Monte Carlo (MC) code package\nis used to calculate these dosimetric parameters, including dose contribution\nfrom secondary electron source and also contribution of bremsstrahlung photons\nto air kerma strength. The Air kerma strength, dose rate constant and radial\ndose function while anisotropy functions for the distance greater than 0.5 cm\naway from the source center are in good agreement with previous published\nstudies. Obtained value from MC simulation for air kerma strength is\n$9.762\\times 10^{-8} \\textrm{UBq}^{-1}$and dose rate constant is $1.108\\pm\n0.13\\%\\textrm{cGyh}^{-1} \\textrm{U}^{-1}$.\n",
        "  Most non-synonymous mutations are thought to be deleterious because of their\neffect on protein sequence. These polymorphisms are expected to be removed or\nkept at low frequency by the action of natural selection, and rare deleterious\nvariants have been implicated as a possible explanation for the \"missing\nheritability\" seen in many studies of complex traits. Nonetheless, the effect\nof positive selection on linked sites or drift in small or inbred populations\nmay also impact the evolution of deleterious alleles. Here, we made use of\ngenome-wide genotyping data to characterize deleterious variants in a large\npanel of maize inbred lines. We show that, in spite of small effective\npopulation sizes and inbreeding, most putatively deleterious SNPs are indeed at\nlow frequencies within individual genetic groups. We find that genes showing\nassociations with a number of complex traits are enriched for deleterious\nvariants. Together these data are consistent with the dominance model of\nheterosis, in which complementation of numerous low frequency, weak deleterious\nvariants contribute to hybrid vigor.\n",
        "  Narrowband imaging is a highly successful approach for finding large numbers\nof high redshift Lya emitting galaxies (LAEs) up to z~6.6. However, at z>~7\nthere are as yet only 3 narrowband selected LAEs with spectroscopic\nconfirmations (two at z~6.9-7.0, one at z~7.3), which hinders extensive studies\non cosmic reionization and galaxy evolution at this key epoch. We have selected\n23 candidate z~6.9 LAEs in COSMOS field with the large area narrowband survey\nLAGER (Lyman-Alpha Galaxies at the End of Reionization). In this work we\npresent spectroscopic followup observations of 12 candidates using IMACS on\nMagellan. For 9 of these, the observations are sufficiently deep to detect the\nexpected lines. Lya emission lines are identified in six sources (yielding a\nsuccess rate of 2/3), including 3 luminous LAEs with Lya luminosities of L(Lya)\n~ 10^{43.5} erg/s, the highest among known spectroscopically confirmed galaxies\nat >~7.0. This triples the sample size of spectroscopically confirmed\nnarrowband selected LAEs at z>~7, and confirms the bright end bump in the Lya\nluminosity function we previously derived based on the photometric sample,\nsupporting a patchy reionization scenario. Two luminous LAEs appear physically\nlinked with projected distance of 1.1 pMpc and velocity difference of ~ 170\nkm/s. They likely sit in a common ionized bubble produced by themselves or with\nclose neighbors, which reduces the IGM attenuation of Lya. A tentative narrow\nNV${\\lambda}$1240 line is seen in one source, hinting at activity of a central\nmassive black hole with metal rich line emitting gas.\n",
        "  Distinguishing between the capacity of ecosystems to generate ecosystem\nservices (ES) and the actual use of these service (ES flow) in ES assessment\nand mapping is important to develop an understanding of the sustainability of\nES use. This study assesses the spatial variation in ES capacity and flow in\nthe Mediterranean small island state of Malta. The services included in this\nstudy were crop provisioning, beekeeping and honey production, fodder and\nlivestock production, crop pollination, air quality regulation, and aesthetic\nES. This assessment develops different spatial models, which make use of\navailable datasets, causal relationships between datasets, including a\ngenerated land use land cover (LULC) map, and statistical models and indicators\nbased on direct measurements. Individual ES indicators were mapped to visualise\nand compare their spatial patterns across the case study area. Subsequently, an\nanalysis of ES associations and bundles was carried out using Pearson\nparametric correlation test, for both ES capacity and flow indicators generated\nfrom this study, and through Principal Component Analysis. Results demonstrate\nseveral significant synergistic interactions between ES capacity and flow in\nrural landscapes characterised with agricultural and semi-natural LULC\ncategories, indicating high landscape multifunctionality. In contrast,\npredominantly urban areas tend to be characterised with a low ecosystem\ncapacity and ES flow, suggesting that ES delivery in the landscapes of the\nstudy area is determined by land use intensity. These findings support the\nnotion that multifunctional rural landscapes provide multiple ES, making an\nimportant contribution to human well-being, and that land use planning that\ndevelops green infrastructure in urban areas can significantly contribute to\nsupport biodiversity and ES delivery.\n",
        "  We study the statistical properties of the SIR epidemics in heterogeneous\nnetworks, when an epidemic is defined as only those SIR propagations that reach\nor exceed a minimum size s_c. Using percolation theory to calculate the average\nfractional size <M_SIR> of an epidemic, we find that the strength of the\nspanning link percolation cluster $P_{\\infty}$ is an upper bound to <M_SIR>.\nFor small values of s_c, $P_{\\infty}$ is no longer a good approximation, and\nthe average fractional size has to be computed directly. The value of s_c for\nwhich $P_{\\infty}$ is a good approximation is found to depend on the\ntransmissibility T of the SIR. We also study Q, the probability that an SIR\npropagation reaches the epidemic mass s_c, and find that it is well\ncharacterized by percolation theory. We apply our results to real networks\n(DIMES and Tracerouter) to measure the consequences of the choice s_c on\npredictions of average outcome sizes of computer failure epidemics.\n",
        "  Dual-energy CT (DECT) has been increasingly used in imaging applications\nbecause of its capability for material differentiation. However, material\ndecomposition suffers from magnified noise from two CT images of independent\nscans, leading to severe degradation of image quality. Existing algorithms\nachieve suboptimal decomposition performance since they fail to accurately\ndepict the mapping relationship between DECT and the basis material images.\nInspired by the impressive potential of CNN, we developed a new Butterfly\nnetwork to perform the image domain dual material decomposition due to its\nstrong approximation ability to the mapping functions in DECT. The Butterfly\nnetwork is derived from the image domain DECT decomposition model by exploring\nthe geometric relationship between mapping functions of data model and network\ncomponents. The network is designed as the double-entry double-out crossover\narchitecture based on the decomposition formulation. It enters a pair of\ndual-energy images as inputs and defines the ground true decomposed images as\neach label. The crossover architecture, which plays an important role in\nmaterial decomposition, is designed to implement the information exchange\nbetween the two material generation pathways in the network. Network components\nexhibit different sensitivity to basis materials in the visualization. By\nanalyzing their sensitivity to different basis materials, we determined the\nroles of network components in material decomposition. This visualization\nevaluation reveals what the network can learn and verifies the rationality of\nnetwork design. The qualitative and quantitative evaluations in material\ndecomposition of patient data indicate that the proposed network outperforms\nits counterpart.\n",
        "  We construct a new order 1 invariant for knot diagrams. We use it to\ndetermine the minimal number of Reidemeister moves needed to pass between\ncertain pairs of knot diagrams.\n",
        "  Detection of desaturations on the pulse oximetry signal is of great\nimportance for the diagnosis of sleep apneas. Using the counting of\ndesaturations, an index can be built to help in the diagnosis of severe cases\nof obstructive sleep apnea-hypopnea syndrome. It is important to have automatic\ndetection methods that allows the screening for this syndrome, reducing the\nneed of the expensive polysomnography based studies. In this paper a novel\nrecognition method based on the empirical mode decomposition of the pulse\noximetry signal is proposed. The desaturations produce a very specific wave\npattern that is extracted in the modes of the decomposition. Using this\ninformation, a detector based on properly selected thresholds and a set of\nsimple rules is built. The oxygen desaturation index constructed from these\ndetections produces a detector for obstructive sleep apnea-hypopnea syndrome\nwith high sensitivity ($0.838$) and specificity ($0.855$) and yields better\nresults than standard desaturation detection approaches.\n",
        "  In two dimensions there is a direct superconductor-to-insulator quantum phase\ntransition driven by increasing disorder. We elucidate, using a combination of\ninhomogeneous mean field theory and quantum Monte Carlo techniques, the nature\nof the phases and the mechanism of the transition. We make several testable\npredictions specifically for local spectroscopic probes. With increasing\ndisorder, the system forms superconducting blobs on the scale of the coherence\nlength embedded in an insulating matrix. In the superconducting state, the\nphases on the different blobs are coherent across the system whereas in the\ninsulator long range phase coherence is disrupted by quantum fluctuations. As a\nconsequence of this emergent granularity, we show that the single-particle\nenergy gap in the density of states survives across the transition, but\ncoherence peaks exist only in the superconductor. A characteristic pseudogap\npersists above the critical disorder and critical temperature, in contrast to\nconventional theories. Surprisingly, the insulator has a two-particle gap scale\nthat vanishes at the SIT, despite a robust single-particle gap.\n",
        "  In previous work, we studied host response to a pathogen which uses a cycle\nof immunologically distinct stages to establish and maintain infection. We\nshowed that for generic parameter values, the system has a unique biologically\nmeaningful stable fixed point. That paper used a simplified model of T-cell\nactivation, making proliferation depend linearly on antigen-T-cell encounters.\nHere we generalize the way in which T-cell proliferation depends on the sizes\nof the antigenic populations. In particular, we allow this response to become\nsaturated at high levels of antigen. As a result, we show that this family of\ngeneralized models shares the same steady-state behavior properties with the\nsimpler model contemplated in our previous work.\n",
        "  Let M be a closed oriented 3-manifold with first Betti number one. Its\nequivariant linking pairing may be seen as a two-dimensional cohomology class\nin an appropriate infinite cyclic covering of the configuration space of\nordered pairs of distinct points of M. We show how to define the equivariant\ncube Q(M,K) of this Blanchfield pairing with respect to a framed knot K that\ngenerates H_1(M;Z)/Torsion. We present the invariant Q(M,K) and some of its\nproperties including a surgery formula. Via surgery, the invariant Q is\nequivalent to an invariant Q' of null-homologous knots in rational homology\nspheres, that is conjecturally equivalent to the two-loop part of the\nKontsevich integral. We generalize the construction of Q' to obtain a\ntopological construction for an invariant that is conjecturally equivalent to\nthe whole Kricker rational lift of the Kontsevich integral for null-homologous\nknots in rational homology spheres.\n",
        "  We propose a method that uses genetic data to test for the occurrence of a\nrecent range expansion and to infer the location of the origin of the\nexpansion. We introduce a statistic for pairs of populations $\\psi$ (the\ndirectionality index) that detects asymmetries in the two-dimensional allele\nfrequency spectrum caused by the series of founder events that happen during an\nexpansion. Such asymmetry arises because low frequency alleles tend to be lost\nduring founder events, thus creating clines in the frequencies of surviving\nlow-frequency alleles. Using simulations, we further show that $\\psi$ is more\npowerful for detecting range expansions than both $F_{ST}$ and clines in\nheterozygosity. We illustrate the utility of $\\psi$ by applying it to a data\nset from modern humans and show how we can include more complicated scenarios\nsuch as multiple expansion origins or barriers to migration in the model.\n",
        "  A context-aware language model uses location, user and/or domain metadata\n(context) to adapt its predictions. In neural language models, context\ninformation is typically represented as an embedding and it is given to the RNN\nas an additional input, which has been shown to be useful in many applications.\nWe introduce a more powerful mechanism for using context to adapt an RNN by\nletting the context vector control a low-rank transformation of the recurrent\nlayer weight matrix. Experiments show that allowing a greater fraction of the\nmodel parameters to be adjusted has benefits in terms of perplexity and\nclassification for several different types of context.\n",
        "  Considerable effort has been made to increase the scale of Linked Data.\nHowever, an inevitable problem when dealing with data integration from multiple\nsources is that multiple different sources often provide conflicting objects\nfor a certain predicate of the same real-world entity, so-called object\nconflicts problem. Currently, the object conflicts problem has not received\nsufficient attention in the Linked Data community. In this paper, we first\nformalize the object conflicts resolution problem as computing the joint\ndistribution of variables on a heterogeneous information network called the\nSource-Object Network, which successfully captures the all correlations from\nobjects and Linked Data sources. Then, we introduce a novel approach based on\nnetwork effects called ObResolution(Object Resolution), to identify a true\nobject from multiple conflicting objects. ObResolution adopts a pairwise Markov\nRandom Field (pMRF) to model all evidences under a unified framework. Extensive\nexperimental results on six real-world datasets show that our method exhibits\nhigher accuracy than existing approaches and it is robust and consistent in\nvarious domains. \\keywords{Linked Data, Object Conflicts, Linked Data Quality,\nTruth Discovery\n",
        "  We demonstrated a pressure-induced phase transition from a low-Tc phase (3.5\nK) to a high-Tc phase (8.7 K) in single crystalline PrO0.5F0.5BiS2. The high-Tc\nphase is easily observed even at 0.7 GPa by uniaxial pressure obtained from a\ngeometric effect of a single crystal with an extremely-thin thickness. For this\nphase transition, superconducting anisotropy and the coherence length along the\nc axis change from {\\gamma} = 20 to 9.3 and from {\\xi}c = 0.56 to 0.71 nm\nrespectively. It is suggested that a shrinkage in the c lattice constant by\napplied uniaxial pressure enhances the three dimensionality in the\nsuperconducting state.\n",
        "  Transport properties of a superconductor-semiconductor-superconductor\n(S-Sm-S) junction with superlattice structure are investigated. Differential\nresistance as a function of voltage shows oscillatory behavior under the\nirradiation of radio-frequency (RF) waves with the specific frequency of 1.77\nGHz regardless of the superconducting materials and the junction lengths.\nExperimental data are quantitatively explained in terms of the coupling of\nsuperconducting quasiparticles with long-wavelength acoustic phonons indirectly\nexcited by the RF waves. We propose that the strong coupling causes the\nformation of novel composite particles, Andreev polarons.\n",
        "  Within the past few years, organizations in diverse industries have adopted\nMapReduce-based systems for large-scale data processing. Along with these new\nusers, important new workloads have emerged which feature many small, short,\nand increasingly interactive jobs in addition to the large, long-running batch\njobs for which MapReduce was originally designed. As interactive, large-scale\nquery processing is a strength of the RDBMS community, it is important that\nlessons from that field be carried over and applied where possible in this new\ndomain. However, these new workloads have not yet been described in the\nliterature. We fill this gap with an empirical analysis of MapReduce traces\nfrom six separate business-critical deployments inside Facebook and at Cloudera\ncustomers in e-commerce, telecommunications, media, and retail. Our key\ncontribution is a characterization of new MapReduce workloads which are driven\nin part by interactive analysis, and which make heavy use of query-like\nprogramming frameworks on top of MapReduce. These workloads display diverse\nbehaviors which invalidate prior assumptions about MapReduce such as uniform\ndata access, regular diurnal patterns, and prevalence of large jobs. A\nsecondary contribution is a first step towards creating a TPC-like data\nprocessing benchmark for MapReduce.\n",
        "  The accidental oil spill in the Gulf of Mexico in 2010 has caused perceptible\ndamage to marine and freshwater ecosystems. The large quantity of oil leaking\nat a constant rate and the long duration of the event caused an exponentially\nincreasing mortality of vertebrates. Using data provided by NOAA and USFWS, we\nassessed the effects of this event on birds, sea turtles, and mammals.\nMortality rates (measured as the number of carcasses recorded per day) were\nexponential for all three groups. Birds were the most affected group, as\nindicated by the steepest increase of mortality rates over time. For sea\nturtles and mammals, an exponential increase in mortality was observed after an\ninitial delay. These exponential behaviors are consistent with a unified\nscenario for the mortality rate for tetrapod vertebrates. However, at least for\nmammals, pre-spill data seem to indicate that the growth in the mortality rate\nis not entirely a consequence of the spill.\n",
        "  With the proliferation of the data warehouses as supportive decision making\ntools, organizations are increasingly looking forward for a complete data\nwarehouse success model that would manage the enormous amounts of growing data.\nIt is therefore important to measure the success of these massive projects.\nWhile general IS success models have received great deals of attention, few\nresearch has been conducted to assess the success of data warehouses for\nstrategic business intelligence purposes. The framework developed in this study\nconsists of the following nine measures: Vendors and Consultants, Management\nActions, System Quality, Information Quality, Data Warehouse Usage, Perceived\nutility, Individual Decision Making Impact, Organizational Decision Making\nImpact, and Corporate Strategic Goals Attainment.\n",
        "  We modify the definition of the Khovanov complex for oriented links in a\nthickening of an oriented surface to obtain a triply graded homological link\ninvariant with a new homotopical grading.\n",
        "  Finding a location for a new facility such that the facility attracts the\nmaximal number of customers is a challenging problem. Existing studies either\nmodel customers as static sites and thus do not consider customer movement, or\nthey focus on theoretical aspects and do not provide solutions that are shown\nempirically to be scalable. Given a road network, a set of existing facilities,\nand a collection of customer route traversals, an optimal segment query returns\nthe optimal road network segment(s) for a new facility. We propose a practical\nframework for computing this query, where each route traversal is assigned a\nscore that is distributed among the road segments covered by the route\naccording to a score distribution model. The query returns the road segment(s)\nwith the highest score. To achieve low latency, it is essential to prune the\nvery large search space. We propose two algorithms that adopt different\napproaches to computing the query. Algorithm AUG uses graph augmentation, and\nITE uses iterative road-network partitioning. Empirical studies with real data\nsets demonstrate that the algorithms are capable of offering high performance\nin realistic settings.\n",
        "  Recent work has explored the syntactic abilities of RNNs using the\nsubject-verb agreement task, which diagnoses sensitivity to sentence structure.\nRNNs performed this task well in common cases, but faltered in complex\nsentences (Linzen et al., 2016). We test whether these errors are due to\ninherent limitations of the architecture or to the relatively indirect\nsupervision provided by most agreement dependencies in a corpus. We trained a\nsingle RNN to perform both the agreement task and an additional task, either\nCCG supertagging or language modeling. Multi-task training led to significantly\nlower error rates, in particular on complex sentences, suggesting that RNNs\nhave the ability to evolve more sophisticated syntactic representations than\nshown before. We also show that easily available agreement training data can\nimprove performance on other syntactic tasks, in particular when only a limited\namount of training data is available for those tasks. The multi-task paradigm\ncan also be leveraged to inject grammatical knowledge into language models.\n",
        "  A method is presented for estimating unknown Fourier domain (k-space) data\nusing a small number of samples in that space. The method is derived from\nBochners Theorem, and is termed: Bochner Inequality Completion of K-Space\n(BICKS). It is suitable for filling the k-space of a real and nonnegative\nunknown quantity, and applicable even when the sampling rate is substantially\nlower than the Nyquist sampling rate. The BICKS method is demonstrated in the\ncontext of medical imaging, but it is also applicable to many other scientific\nareas that utilize signal processing in Fourier domain. The results indicate\nthat filling a highly undersampled k-space using BICKS enables high quality\nimage reconstruction.\n",
        "  In this paper, we propose phraseNet, a neural machine translator with a\nphrase memory which stores phrase pairs in symbolic form, mined from corpus or\nspecified by human experts. For any given source sentence, phraseNet scans the\nphrase memory to determine the candidate phrase pairs and integrates tagging\ninformation in the representation of source sentence accordingly. The decoder\nutilizes a mixture of word-generating component and phrase-generating\ncomponent, with a specifically designed strategy to generate a sequence of\nmultiple words all at once. The phraseNet not only approaches one step towards\nincorporating external knowledge into neural machine translation, but also\nmakes an effort to extend the word-by-word generation mechanism of recurrent\nneural network. Our empirical study on Chinese-to-English translation shows\nthat, with carefully-chosen phrase table in memory, phraseNet yields 3.45 BLEU\nimprovement over the generic neural machine translator.\n",
        "  XML query can be modeled by twig pattern query (TPQ) specifying predicates on\nXML nodes and XPath relationships satisfied between them. A lot of TPQ types\nhave been proposed; this paper takes into account a TPQ model extended by a\nspecification of output and non-output query nodes since it complies with the\nXQuery semantics and, in many cases, it leads to a more efficient query\nprocessing. In general, there are two approaches to process the TPQ: holistic\njoins and binary joins. Whereas the binary join approach builds a query plan as\na tree of interconnected binary operators, the holistic join approach evaluates\na whole query using one operator (i.e., using one complex algorithm).\nSurprisingly, a thorough analytical and experimental comparison is still\nmissing despite an enormous research effort in this area. In this paper, we try\nto fill this gap; we analytically and experimentally show that the binary joins\nused in a fully-pipelined plan (i.e., the plan where each join operation does\nnot wait for the complete result of the previous operation and no explicit\nsorting is used) can often outperform the holistic joins, especially for TPQs\nwith a higher ratio of non-output query nodes. The main contributions of this\npaper can be summarized as follows: (i) we introduce several improvements of\nexisting binary join approaches allowing to build a fully-pipelined plan for a\nTPQ considering non-output query nodes, (ii) we prove that for a certain class\nof TPQs such a plan has the linear time complexity with respect to the size of\nthe input and output as well as the linear space complexity with respect to the\nXML document depth (i.e., the same complexity as the holistic join approaches),\n(iii) we show that our improved binary join approach outperforms the holistic\njoin approaches in many situations, and (iv) we propose a simple combined\napproach that uses advantages of both types of approaches.\n",
        "  We propose a one-shot thickness measurement method for sponge-like structures\nusing a propagation-based X-ray phase-contrast imaging (P-PCI) method. In\nP-PCI, the air-material interface refracts the incident X-ray. Refracted many\ntimes along their paths by such a structure, incident X-rays propagate randomly\nwithin a small divergent angle range, resulting in a speckle pattern in the\ncaptured image. We found structure thickness and contrast of a phase-contrast\nprojection are directly related in images. This relationship can be described\nby a natural logarithm equation. Thus, from the one phase-contrast view, depth\ninformation can be retrieved from its contrast. Our preliminary biological\nexperiments indicate promise in its application to measurements requiring in\nvivo and ongoing assessment of lung tumor progression.\n",
        "  We review and complete the kinetic theory of spatially inhomogeneous stellar\nsystems when collective effects (dressing of the stars by their polarization\ncloud) are neglected. We start from the BBGKY hierarchy issued from the\nLiouville equation and consider an expansion in powers of 1/N in a proper\nthermodynamic limit. For $N\\rightarrow +\\infty$, we obtain the Vlasov equation\ndescribing the evolution of collisionless stellar systems like elliptical\ngalaxies. At the order 1/N, we obtain a kinetic equation describing the\nevolution of collisional stellar systems like globular clusters. This equation\ndoes not suffer logarithmic divergences at large scales since spatial\ninhomogeneity is explicitly taken into account. Making a local approximation,\nand introducing an upper cut-off at the Jeans length, it reduces to the\nVlasov-Landau equation which is the standard kinetic equation of stellar\nsystems. Our approach provides a simple and pedagogical derivation of these\nimportant equations from the BBGKY hierarchy which is more rigorous for systems\nwith long-range interactions than the two-body encounters theory. Making an\nadiabatic approximation, we write the generalized Landau equation in\nangle-action variables and obtain a Landau-type kinetic equation that is valid\nfor fully inhomogeneous stellar systems and is free of divergences at large\nscales. This equation is less general than the Lenard Balescu-type kinetic\nequation recently derived by Heyvaerts (2010) since it neglects collective\neffects, but it is substantially simpler and could be useful as a first step.\nWe discuss the evolution of the system as a whole and the relaxation of a test\nstar in a bath of field stars. We derive the corresponding Fokker-Planck\nequation in angle-action variables and provide expressions for the diffusion\ncoefficient and friction force.\n",
        "  No abstract needed since short\n",
        "  We investigate whether and where multi-task learning (MTL) can improve\nperformance on NLP problems related to argumentation mining (AM), in particular\nargument component identification. Our results show that MTL performs\nparticularly well (and better than single-task learning) when little training\ndata is available for the main task, a common scenario in AM. Our findings\nchallenge previous assumptions that conceptualizations across AM datasets are\ndivergent and that MTL is difficult for semantic or higher-level tasks.\n",
        "  We present a map of the diffuse ultraviolet cosmic background in two\nwavelength bands (FUV: 1530 {\\AA}; NUV: 2310 {\\AA}) over almost 75% of the sky\nusing archival data from the GALEX mission. Most of the diffuse flux is due to\ndust-scattered starlight and follows a cosecant law with slopes of 545 photons\ncm-2 s-1 sr-1 {\\AA}-1 and 433 photons cm-2 s-1 sr-1 {\\AA}-1 in the FUV and NUV\nbands, respectively. There is a strong correlation with the 100 {\\mu}m IRAS\nflux with an average UV/IR ratio of 300 photons cm-2 s-1 sr-1 {\\AA}-1 (MJy\nsr-1)-1 in the FUV band and 220 photons cm-2 s-1 sr-1 {\\AA}-1 (MJy sr-1)-1 in\nthe NUV but with significant variations over the sky. In addition to the large\nscale distribution of the diffuse light, we note a number of individual\nfeatures including bright spots around the hot stars Spica and Achernar.\n",
        "  We study superconductivity in doped solid picene (C22H14) with linear\nresponse calculations of the phonon spectrum and electron-phonon (ep)\ninteraction. We show that the coupling of the high-energy C bond-stretching\nphonons to the {\\pi} molecular orbitals for a doping of ~3 electrons per picene\nmolecule is sufficiently strong to reproduce the experimental Tc of 18 K within\nMigdal-Eliashberg theory. For hole doping, we predict a similar coupling\nleading to a maximum Tc of 6 K. However, we argue that, due to its molecular\nnature, picene may belong to the same class of strongly correlated ep\nsuperconductors as fullerides. We propose several experimental tests for this\nhypothesis and suggest that intercalated hydrocarbons with different\narrangements and numbers of benzene rings may be used to study the interplay\nbetween ep interaction and strong electronic correlations in the highly\nnonadiabatic limit.\n",
        "  In this paper, a general moving object trajectories framework is put forward\nto allow independent applications processing trajectories data benefit from a\nhigh level of interoperability, information sharing as well as an efficient\nanswer for a wide range of complex trajectory queries. Our proposed meta-model\nis based on ontology and event approach, incorporates existing presentations of\ntrajectory and integrates new patterns like space-time path to describe\nactivities in geographical space-time. We introduce recursive Region of\nInterest concepts and deal mobile objects trajectories with diverse\nspatio-temporal sampling protocols and different sensors available that\ntraditional data model alone are incapable for this purpose.\n",
        "  We show how the two-band nature of superconductivity in noncentrosymmetric\ncompounds leads to a variety of novel nonuniform superconducting states induced\nby a magnetic field. At low fields, a two-band helical state is realized, with\na distinctly non-BCS quasiparticle spectrum. At high fields, the\nsuperconducting state becomes unstable towards the formation of a lattice of\ntopological phase solitons.\n",
        "  We show that a knot has a non left-orderable surgery if the knot group admits\na generalized Baumslag-Solitar relator and satisfies certain conditions on a\nlongitude of the knot. As an application, it is shown that certain positively\ntwisted torus knots admit non left-orderable surgeries.\n",
        "  This paper considers the degradation of alumina and zirconia toughened\nalumina vs. alumina for hip implants. The materials are as assumed to be load\nbearing surfaces subjected to shocks in wet conditions. The load is a peak of\nforce; 9 kN was applied over 15 ms at 2 Hz for 800,000 cycles. The volumetric\nwear and roughness are lower for ZTA than for alumina. The long ZTA ageing did\nnot seem to have a direct influence on the roughness. The ageing increased the\nwear volumes of ZTA and it was found to have a higher wear resistance compared\nto alumina.\n",
        "  The HST/WFC3 multiband photometry spanning from the UV to the near-IR of four\nfields in the Galactic bulge, together with that for six template globular and\nopen clusters, are used to photometrically tag the metallicity [Fe/H] of stars\nin these fields after proper-motion rejecting most foreground disk\ncontaminants. Color-magnitude diagrams and luminosity functions are then\nconstructed, in particular for the most metal rich and most metal poor stars in\neach field. We do not find any significant difference between the $I$-band and\n$H$-band luminosity functions, hence turnoff luminosity and age, of the metal\nrich and metal poor components which therefore appear essentially coeval. In\nparticular, we find that no more than $\\sim 3\\%$ of the metal-rich component\ncan be $\\sim 5$ Gyr old, or younger. Conversely, theoretical luminosity\nfunctions give a good match to the observed ones for an age of ~10 Gyr.\nAssuming this age is representative for the bulk of bulge stars, we then recall\nthe observed properties of star-forming galaxies at 10 Gyr lookback time, i.e.,\nat z~2, and speculate about bulge formation in that context. We argue that bar\nformation and buckling instabilities leading to the observed boxy/peanut,\nX-shaped bulge may have arisen late in the history of the Milky Way galaxy,\nonce its gas fraction had decreased compared to the high values typical of\nhigh-redshift galaxies. This paper follows the public release of the\nphotometric and astrometric catalogs for the measured stars in the four fields.\n",
        "  We show that all twist knots, certain double twist knots and some other\n2-bridge knots are minimal elements for the partial ordering on the set of\nprime knots. The key to these results are presentations of their character\nvarieties using Chebyshev polynomials and a criterion for irreducibility of a\npolynomial of two variables. These give us an elementary method to discuss the\nnumber of irreducible components of the character varieties, which concludes\nthe result essentially.\n",
        "  Understanding gas-grain chemistry of deuterium in star-forming objects may\nhelp to explain their history and present state. We aim to clarify how\nprocesses in ices affect the deuterium fractionation. In this regard, we\ninvestigate a Solar-mass protostellar envelope using an astrochemical\nrate-equation model that considers bulk-ice chem- istry. The results show a\ngeneral agreement with the molecular D/H abundance ratios observed in low-mass\nprotostars. The simultaneous processes of ice accumulation and rapid synthesis\nof HD on grain surfaces in the prestellar core hampers the deuteration of icy\nspecies. The observed very high D/H ratios exceeding 10 per cent, i.e., super-\ndeuteration, are reproduced for formaldehyde and dimethyl ether, but not for\nother species in the protostellar envelope phase. Chemical transformations in\nbulk ice lower D/H ratios of icy species and do not help explaining the\nsuper-deuteration. In the protostellar phase, the D2O/HDO abundance ratio was\ncalculated to be higher than the HDO/H2O ratio owing to gas-phase chemistry.\nSpecies that undergo evaporation from ices have high molecular D/H ratio and a\nhigh gas-phase abundance.\n",
        "  We prove that there are exactly $6$ Nil Seifert fibred spaces which can be\nobtained by Dehn surgeries on non-trefoil knots in $S^3$, with $\\{60, 144, 156,\n288, 300\\}$ as the exact set of all such surgery slopes up to taking the mirror\nimages of the knots. We conjecture that there are exactly $4$ specific\nhyperbolic knots in $S^3$ which admit Nil Seifert fibred surgery. We also give\nsome more general results and a more general conjecture concerning Seifert\nfibred surgeries on hyperbolic knots in $S^3$.\n",
        "  Monte Carlo (MC) method has been recognized the most accurate dose\ncalculation method for radiotherapy. However, its extremely long computation\ntime impedes clinical applications. Recently, a lot of efforts have been made\nto realize fast MC dose calculation on GPUs. Nonetheless, most of the GPU-based\nMC dose engines were developed in NVidia CUDA environment. This limits the code\nportability to other platforms, hindering the introduction of GPU-based MC\nsimulations to clinical practice. The objective of this paper is to develop a\nfast cross-platform MC dose engine oclMC using OpenCL environment for external\nbeam photon and electron radiotherapy in MeV energy range. Coupled\nphoton-electron MC simulation was implemented with analogue simulations for\nphoton transports and a Class II condensed history scheme for electron\ntransports. To test the accuracy and efficiency of our dose engine oclMC, we\ncompared dose calculation results of oclMC and gDPM, our previously developed\nGPU-based MC code, for a 15 MeV electron beam and a 6 MV photon beam on a\nhomogenous water phantom, one slab phantom and one half-slab phantom.\nSatisfactory agreement was observed in all the cases. The average dose\ndifferences within 10% isodose line of the maximum dose were 0.48-0.53% for the\nelectron beam cases and 0.15-0.17% for the photon beam cases. In terms of\nefficiency, our dose engine oclMC was 6-17% slower than gDPM when running both\ncodes on the same NVidia TITAN card due to both different physics particle\ntransport models and different computational environments between CUDA and\nOpenCL. The cross-platform portability was also validated by successfully\nrunning our new dose engine on a set of different compute devices including an\nNvidia GPU card, two AMD GPU cards and an Intel CPU card using one or four\ncores. Computational efficiency among these platforms was compared.\n",
        "  The AC susceptibility at zero DC magnetic field of a polycrystalline sample\nof LaFeAsO_{0.94}F_{0.06} (T_c = 24 K) has been investigated as a function of\nthe temperature, the amplitude of the AC magnetic field (in the range Hac =\n0.003 - 4 Oe) and the frequency (in the range f = 10 kHz - 100 kHz). The\ntemperature dependence of the AC susceptibility exhibits the typical two-step\ntransition arising from the combined response of superconduncting grains and\nintergranular weak-coupled medium. The intergranular part of the susceptibility\nstrongly depends on both the amplitude and the frequency of the AC driving\nfield, from few Kelvin below T_c down to T = 4.2 K. Our results show that, in\nthe investigated sample, the intergrain critical current is not determined by\npinning of Josephson vortices but by Josephson critical current across\nneighboring grains.\n",
        "  We give a complete description of all order 1 invariants of spherical curves.\nWe also identify the subspaces of all J-invariants and S-invariants, and\npresent two equalities satisfied by any spherical curve.\n",
        "  The reionization of the Universe is one of the most important topics of\npresent day astrophysical research. The most plausible candidates for the\nreionization process are star-forming galaxies, which according to the\npredictions of the majority of the theoretical and semi-analytical models\nshould dominate the HI ionizing background at z~3. We aim at measuring the\nLyman continuum escape fraction, which is one of the key parameters to compute\nthe contribution of star-forming galaxies to the UV background. We have used\nultra-deep U-band imaging (U=30.2mag at 1sigma) by LBC/LBT in the\nCANDELS/GOODS-North field, as well as deep imaging in COSMOS and EGS fields, in\norder to estimate the Lyman continuum escape fraction of 69 star-forming\ngalaxies with secure spectroscopic redshifts at 3.27<z<3.40 to faint magnitude\nlimits (L=0.2L*, or equivalently M1500~-19). We have measured through stacks a\nstringent upper limit (<1.7% at 1sigma) for the relative escape fraction of HI\nionizing photons from bright galaxies (L>L*), while for the faint population\n(L=0.2L*) the limit to the escape fraction is ~10%. We have computed the\ncontribution of star-forming galaxies to the observed UV background at z~3 and\nwe have found that it is not enough to keep the Universe ionized at these\nredshifts, unless their escape fraction increases significantly (>10%) at low\nluminosities (M1500>-19). We compare our results on the Lyman continuum escape\nfraction of high-z galaxies with recent estimates in the literature and discuss\nfuture prospects to shed light on the end of the Dark Ages. In the future,\nstrong gravitational lensing will be fundamental to measure the Lyman continuum\nescape fraction down to faint magnitudes (M1500~-16) which are inaccessible\nwith the present instrumentation on blank fields.\n",
        "  The quality of SPECT Bremsstrahlung images of patients treated with Y-90 is\npoor, mainly because of scattered radiation and collimator septa penetration.\nTo minimize the latter effect, High Energy (HE) or Medium Energy (ME)\ncollimators can be used. Scatter correction is not possible through the methods\ncommonly used for the diagnostic radionuclides (Tc-99m, etc.) because the\nBremsstrahlung radiation does not have distinct photopeaks, but a broad\nspectrum of energies ranging from zero to the maximum one detectable by the\ngamma-camera crystal is registered. Scatter radiation and collimator septa\npenetration affect the Contrast and the Contrast Recovery Coefficient (CRC) :\nour research focused on finding the best energy position for the acquisition\nwindow in order to maximize these parameters. To be guided in this finding, we\nfirst made a Monte Carlo (MC) simulation of a SPECT acquisition of a Y-90\ncylindrical phantom and then we measured at different energies the Line Spread\nFunction (LSF) of a linear Y-90 source inserted in a scatter medium. Finally we\nacquired the NEMA IEC Body phantom filled with Y-90 chloride at different\nenergies and using different setups in order to measure the Contrast and the\nCRC in different conditions with the purpose of optimizing the clinical SPECT\nacquisition procedures. The final results showed that Contrast and CRC are\nhigher for HE collimators compared to ME collimators and they have a maximum\nfor the larger spheres at 110 - 135 keV.\n",
        "  We show that for a special alternating link diagram, the following three\npolynomials are essentially the same: a) the part of the HOMFLY polynomial that\ncorresponds to the leading term in the Alexander polynomial; b) the $h$-vector\nfor a triangulation of the root polytope of the Seifert graph and c) the\nenumerator of parking functions for the planar dual of the Seifert graph. These\nobservations yield formulas for the maximal $z$-degree part of the HOMFLY\npolynomial of an arbitrary homogeneous link as well. Our result is part of a\nprogram aimed at reading HOMFLY coefficients out of Heegaard Floer homology.\n",
        "  We consider the problem of finding equivalent minimal-size reformulations of\nSQL queries in presence of embedded dependencies [1]. Our focus is on\nselect-project-join (SPJ) queries with equality comparisons, also known as safe\nconjunctive (CQ) queries, possibly with grouping and aggregation. For SPJ\nqueries, the semantics of the SQL standard treat query answers as multisets\n(a.k.a. bags), whereas the stored relations may be treated either as sets,\nwhich is called bag-set semantics for query evaluation, or as bags, which is\ncalled bag semantics. (Under set semantics, both query answers and stored\nrelations are treated as sets.)\n  In the context of the above Query-Reformulation Problem, we develop a\ncomprehensive framework for equivalence of CQ queries under bag and bag-set\nsemantics in presence of embedded dependencies, and make a number of conceptual\nand technical contributions. Specifically, we develop equivalence tests for CQ\nqueries in presence of arbitrary sets of embedded dependencies under bag and\nbag-set semantics, under the condition that chase [9] under set semantics\n(set-chase) on the inputs terminates. We also present equivalence tests for\naggregate CQ queries in presence of embedded dependencies. We use our\nequivalence tests to develop sound and complete (whenever set-chase on the\ninputs terminates) algorithms for solving instances of the Query-Reformulation\nProblem with CQ queries under each of bag and bag-set semantics, as well as for\ninstances of the problem with aggregate queries.\n",
        "  A topological theory of d-wave superconductors is derived in this thesis.\nGinzburg-Landau theory describes superconductivity by defining a complex order\nparameter and applying Landau's theory for phase transitions. However, there is\nno local, gauge invariant order parameter for a superconductor and classical\norder is no appropriate description. Topological order has proven to be a\npowerful tool for describing the Quantum Hall effect and it gives also an\nappropriate description of s-wave superconductors. For d-wave superconductors\nthere are gapless excitations at four points on the Fermi surface. The\ntopological theory for superconductors exhibits a similar structure for the\nground state as found in the s-wave case. However, the gapless excitations\ndestroy the topological degeneracy of the ground state and introduce an\nadditional degeneracy in one of the flux sectors. In search for regularities\nwhich reflect the topological order, I include a magnetic point impurity and\nfocus on the effects it has on the Casimir energy. Indeed, it is shown that the\nenergy shift reflects the topology to some extend. However, this is work in\nprogress and further computations will be done.\n",
        "  Growth of human population shows no signs of stagnation. The only small\ndisturbance is identified as being probably associated with the coinciding\nimpacts of five demographic catastrophes. The concept of the Epoch of\nMalthusian Stagnation is convincingly contradicted by empirical evidence.\n",
        "  We present the first variability study of the 1720 MHz OH masers located in\nthe Galactic Center. Most of these masers are associated with the interaction\nbetween the supernova remnant SgrAEast and the interstellar medium, but a few\nmasers are associated with the Circumnuclear Disk. The monitoring program\ncovered five epochs and a timescale of 20-195 days, during which no masers\ndisappeared and no new masers appeared. All masers have previously been\ndetected in a single epoch observation about one year prior to the start of the\nmonitoring experiment, implying relatively stable conditions for the 1720 MHz\nOH masers. No extreme variability was detected. The masers associated with the\nnortheastern interaction region between the supernova remnant and the +50km/s\nmolecular cloud show the highest level of variability. This can be explained\nwith the +50km/s molecular cloud being located behind the supernova remnant and\nwith a region of high OH absorbing column density along the line of sight.\nPossibly the supernova remnant provides additional turbulence to the gas in\nthis region, through which the maser emission must travel. The masers in the\nsouthern interaction region are located on the outermost edge of SgrAEast which\nline of sight is not covered by either absorbing OH gas or a supernova remnant,\nin agreement with the much lower variability level observed. Similarly, the\nmasers associated with the CND show little variability, consistent with them\narising through collisions between relatively large clumps of gas in the CND\nand no significant amount of turbulent gas along the line of sight.\n",
        "  GSH 90-28-17 is a high-latitude galactic HI supershell, identified in the HI\nsupershell catalogs with a velocity of $v_{lsr}\\sim-17$ \\kms. We used the new\nArecibo GALFA-HI survey data which have much higher resolution and sensitivity\nthan what were previously available to re-examine the properties of the\nsupershell. We derived a new distance of 400 pc for GSH 90-28-17 and suggested\nthat it is related to the Lac OB1 association. The radius of GSH 90-28-17 is\n66.0$\\pm$3.5 pc. The HI mass of the shell is (3.1$\\pm0.1)\\times10^{4}$\nM$_{\\odot}$. It has an age of $\\sim4.5$ Myr and a total kinetic energy of\n(8.2$\\pm0.3)\\times10^{48}$ ergs. We extracted radio continuum data for the GSH\n90-28-17 region from the 408 MHz all-sky Survey and Bonn 1420 MHz survey, and\nfiltered the diffuse background Galactic emission. A radio loop-like ridge is\nfound to be associated with the HI shell at both frequencies, and shows a\nnonthermal origin with a TT-plot index of $\\alpha$=-1.35$\\pm$0.69. In addition,\nthe pulsar J2307+2225 with a similar distance is found in the shell region. We\nconclude that GSH 90-28-17 is probably an old, type II supernova remnant in the\nsolar neighborhood.\n",
        "  We compute the $\\mathrm{Pin}(2)$-equivariant Seiberg-Witten Floer homology of\nSeifert rational homology three-spheres in terms of their Heegaard Floer\nhomology. As a result of this computation, we prove Manolescu's conjecture that\n$\\beta=-\\bar{\\mu}$ for Seifert integral homology three-spheres. We show that\nthe Manolescu invariants $\\alpha, \\beta,$ and $\\gamma$ give new obstructions to\nhomology cobordisms between Seifert fiber spaces, and that many Seifert\nhomology spheres $\\Sigma(a_1,...,a_n)$ are not homology cobordant to any\n$-\\Sigma(b_1,...,b_n)$. We then use the same invariants to give an example of\nan integral homology sphere not homology cobordant to any Seifert fiber space.\nWe also show that the $\\mathrm{Pin}(2)$-equivariant Seiberg-Witten Floer\nspectrum provides homology cobordism obstructions distinct from $\\alpha,\\beta,$\nand $\\gamma$. In particular, we identify an $\\mathbb{F}[U]$-module called\nconnected Seiberg-Witten Floer homology, whose isomorphism class is a homology\ncobordism invariant.\n",
        "  The International Electrotechnical Commission (IEC) has previously defined\nstandard rotation operators for positive gantry, collimator and couch rotations\nfor the radiotherapy DICOM coordinate system that is commonly used by treatment\nplanning systems. Coordinate transformations to the coordinate systems of\ncommonly used Monte Carlo (MC) codes (BEAMnrc/DOSXYZnrc and VMC++) have been\nderived and published in the literature. However, these coordinate\ntransformations disregard patient orientation during the computed tomography\n(CT) scan, and assume the most commonly used 'head first, supine' orientation.\nWhile less common, other patient orientations are used in clinics - Monte Carlo\nverification of such treatments can be problematic due to the lack of\nappropriate coordinate transformations.\n  In this work, a solution has been obtained by correcting the CT-derived\nphantom orientation and deriving generalized coordinate transformations for\nfield angles in the DOSXYZnrc and VMC++ codes. The rotation operator that\nincludes any possible patient treatment orientation was determined using the\nDICOM Image Orientation tag (0020,0037). The derived transformations of the\npatient image and beam direction angles were verified by comparison of MC dose\ndistributions with the Eclipse treatment planning system, calculated for each\nof the eight possible patient orientations.\n",
        "  What is a population? This review considers how a population may be defined\nin terms of understanding the structure of the underlying genetics of the\nindividuals involved. The main approach is to consider statistically\nidentifiable groups of randomly mating individuals, which is well defined in\ntheory for any type of (sexual) organism. We discuss generative models using\ndrift, admixture and spatial structure, and the ancestral recombination graph.\nThese are contrasted with statistical models for inference, principle component\nanalysis and other `non-parametric' methods. The relationships between these\napproaches are explored with both simulated and real-data examples. The\nstate-of-the-art practical software tools are discussed and contrasted. We\nconclude that populations are a useful theoretical construct that can be well\ndefined in theory and often approximately exist in practice.\n",
        "  The main result of this paper asserts that if a Seifert fibered 4-manifold\nhas nonzero Seiberg-Witten invariant, the homotopy class of regular fibers has\ninfinite order. This is a nontrivial obstruction to smooth circle actions; as\napplications, we show how to destroy smooth circle actions on a 4-manifold by\nknot surgery, without changing the integral homology, intersection form, and\neven the Seiberg-Witten invariant. Results concerning classification of Seifert\nfibered complex surfaces or symplectic 4-manifolds are included. We also show\nthat every smooth circle action on the 4-torus is smoothly conjugate to a\nlinear action.\n",
        "  Elasticity is highly desirable for stream processing systems to guarantee low\nlatency against workload dynamics, such as surges in data arrival rate and\nfluctuations in data distribution. Existing systems achieve elasticity\nfollowing a resource-centric approach that uses dynamic key partitioning across\nthe parallel instances, i.e. executors, to balance the workload and scale\noperators. However, such operator-level key repartitioning needs global\nsynchronization and prohibits rapid elasticity. To address this problem, we\npropose an executor-centric approach, whose core idea is to avoid\noperator-level key repartitioning while implementing each executor as the\nbuilding block of elasticity. Following this new approach, we design the\nElasticutor framework with two level of optimizations: i) a novel\nimplementation of executors, i.e., elastic executors, that perform elastic\nmulti-core execution via efficient intra-executor load balancing and executor\nscaling and ii) a global model-based scheduler that dynamically allocates CPU\ncores to executors based on the instantaneous workloads. We implemented a\nprototype of Elasticutor and conducted extensive experiments. Our results show\nthat Elasticutor doubles the throughput and achieves an average processing\nlatency up to 2 orders of magnitude lower than previous methods, for a dynamic\nworkload of real-world applications.\n",
        "  This review is written with the goal of informing public health concerns\nrelated to nanoscience, while raising awareness of nanomaterials toxicity among\nscientists and manufacturers handling them. We show that humans have always\nbeen exposed to nanoparticles and dust from natural sources and human\nactivities, the recent development of industry and combustion-based engine\ntransportation profoundly increasing anthropogenic nanoparticulate pollution.\nThe key to understanding the toxicity of nanoparticles is that their minute\nsize, smaller than cells and cellular organelles, allows them to penetrate\nthese basic biological structures, disrupting their normal function. Among\ndiseases associated with nanoparticles are asthma, bronchitis, lung cancer,\nneurodegenerative diseases (such as Parkinson`s and Alzheimer`s diseases),\nCrohn`s disease, colon cancer. Nanoparticles that enter the circulatory system\nare related to occurrence of arteriosclerosis, and blood clots, arrhythmia,\nheart diseases, and ultimately cardiac death. We show that possible adverse\neffects of nanoparticles on human health depend on individual factors such as\ngenetics and existing disease, as well as exposure, and nanoparticle chemistry,\nsize, shape, and agglomeration state. The faster we will understand their\ncauses and mechanisms, the more likely we are to find cures for diseases\nassociated with nanoparticle exposure. We foresee a future with\nbetter-informed, and hopefully more cautious manipulation of engineered\nnanomaterials, as well as the development of laws and policies for safely\nmanaging all aspects of nanomaterial manufacturing, industrial and commercial\nuse, and recycling.\n",
        "  Overranging or overscanning increases the dose delivered to patients\nundergoing helical Computed Tomography examinations. In order to reduce it,\nnowadays most of the multidetector tomographs close the X-ray beam aperture at\nthe scan extremes. This technical innovation, usually referred to as dynamic or\nadaptive collimation, also influences the overranging assessment methods. In\nparticular, the film free approach proposed in previous studies is not suitable\nfor these modern tomographs. The present study aims to introduce a new method\nof estimating overranging with real time dosimetry, even suitable for\ntomographs equipped with adaptive collimation. The approach proposed is very\neasy to implement and time saving because only a pencil chamber is required. It\nis also equivalent in precision and in accuracy to the film based one,\nconsidered an absolute benchmark.\n",
        "  CSPTRQ is an interesting problem and its has attracted much attention. The\nCSPTRQ is a variant of the traditional PTRQ. As objects moving in a\nconstrained-space are common, clearly, it can also find many applications. At\nthe first sight, our problem can be easily tackled by extending existing\nmethods used to answer the PTRQ. Unfortunately, those classical techniques are\nnot well suitable for our problem, due to a set of new challenges. We develop\ntargeted solutions and demonstrate the efficiency and effectiveness of the\nproposed methods through extensive experiments.\n",
        "  Commercially available clinical decision support systems (CDSSs) for skin\ncancer have been designed for the detection of melanoma only. Correct use of\nthe systems requires expert knowledge, hampering their utility for nonexperts.\nFurthermore, there are no systems to detect other common skin cancer types,\nthat is, nonmelanoma skin cancer (NMSC). As early diagnosis of skin cancer is\nessential, there is a need for a CDSS that is applicable to all types of skin\nlesions and is suitable for nonexperts. Nevus Doctor (ND) is a CDSS being\ndeveloped by the authors. We here investigate ND's ability to detect both\nmelanoma and NMSC and the opportunities for improvement. An independent test\nset of dermoscopic images of 870 skin lesions, including 44 melanomas and 101\nNMSCs, were analysed by ND. Its sensitivity to melanoma and NMSC was compared\nto that of Mole Expert (ME), a commercially available CDSS, using the same set\nof lesions. ND and ME had similar sensitivity to melanoma. For ND at 95 percent\nmelanoma sensitivity, the NMSC sensitivity was 100 percent, and the specificity\nwas 12 percent. The melanomas misclassified by ND at 95 percent sensitivity\nwere correctly classified by ME, and vice versa. ND is able to detect NMSC\nwithout sacrificing melanoma sensitivity.\n",
        "  Top-k query processing finds a list of k results that have largest scores\nw.r.t the user given query, with the assumption that all the k results are\nindependent to each other. In practice, some of the top-k results returned can\nbe very similar to each other. As a result some of the top-k results returned\nare redundant. In the literature, diversified top-k search has been studied to\nreturn k results that take both score and diversity into consideration. Most\nexisting solutions on diversified top-k search assume that scores of all the\nsearch results are given, and some works solve the diversity problem on a\nspecific problem and can hardly be extended to general cases. In this paper, we\nstudy the diversified top-k search problem. We define a general diversified\ntop-k search problem that only considers the similarity of the search results\nthemselves. We propose a framework, such that most existing solutions for top-k\nquery processing can be extended easily to handle diversified top-k search, by\nsimply applying three new functions, a sufficient stop condition sufficient(),\na necessary stop condition necessary(), and an algorithm for diversified top-k\nsearch on the current set of generated results, div-search-current(). We\npropose three new algorithms, namely, div-astar, div-dp, and div-cut to solve\nthe div-search-current() problem. div-astar is an A* based algorithm, div-dp is\nan algorithm that decomposes the results into components which are searched\nusing div-astar independently and combined using dynamic programming. div-cut\nfurther decomposes the current set of generated results using cut points and\ncombines the results using sophisticated operations. We conducted extensive\nperformance studies using two real datasets, enwiki and reuters. Our div-cut\nalgorithm finds the optimal solution for diversified top-k search problem in\nseconds even for k as large as 2,000.\n",
        "  The proliferation of imprecise data has motivated both researchers and the\ndatabase industry to push statistical techniques into relational database\nmanagement systems (RDBMSs). We study algorithms to maintain model-based views\nfor a popular statistical technique, classification, inside an RDBMS in the\npresence of updates to the training examples. We make three technical\ncontributions: (1) An algorithm that incrementally maintains classification\ninside an RDBMS. (2) An analysis of the above algorithm that shows that our\nalgorithm is optimal among all deterministic algorithms (and asymptotically\nwithin a factor of 2 of a nondeterministic optimal). (3) An index structure\nbased on the technical ideas that underlie the above algorithm which allows us\nto store only a fraction of the entities in memory. We apply our techniques to\ntext processing, and we demonstrate that our algorithms provide several orders\nof magnitude improvement over non-incremental approaches to classification on a\nvariety of data sets: such as the Cora, UCI Machine Learning Repository data\nsets, Citeseer, and DBLife.\n",
        "  We report the discovery of a family of ternary platinum phosphides APt3P (A =\nCa, Sr and La), which crystallize in an antiperovskite-based structure closely\nrelated to that of the heavy fermion superconductor CePt3Si. All three\nphosphides showed superconductivity at low temperatures and the highest\ncritical temperature Tc = 8.4 K was observed for SrPt3P. The analysis of\nspecific heat C(T) for SrPt3P shows clear evidence for very strong coupling\ns-wave superconductivity with a large ratio between superconducting gap D0 and\nTc, 2D0/kBTc ~ 5, and the presence of low-energy phonons. The presence of\nmultiple Fermi surface pockets was inferred from the nonlinear magnetic field\ndependence of Hall resistivity, which we argue might play a role in realizing\nthe strong coupling of charge carriers with the low lying phonons.\n",
        "  Database management systems (DBMSs) carefully optimize complex multi-join\nqueries to avoid expensive disk I/O. As servers today feature tens or hundreds\nof gigabytes of RAM, a significant fraction of many analytic databases becomes\nmemory-resident. Even after careful tuning for an in-memory environment, a\nlinear disk I/O model such as the one implemented in PostgreSQL may make query\nresponse time predictions that are up to 2X slower than the optimal multi-join\nquery plan over memory-resident data. This paper introduces a memory I/O cost\nmodel to identify good evaluation strategies for complex query plans with\nmultiple hash-based equi-joins over memory-resident data. The proposed cost\nmodel is carefully validated for accuracy using three different systems,\nincluding an Amazon EC2 instance, to control for hardware-specific differences.\nPrior work in parallel query evaluation has advocated right-deep and bushy\ntrees for multi-join queries due to their greater parallelization and\npipelining potential. A surprising finding is that the conventional wisdom from\nshared-nothing disk-based systems does not directly apply to the modern\nshared-everything memory hierarchy. As corroborated by our model, the\nperformance gap between the optimal left-deep and right-deep query plan can\ngrow to about 10X as the number of joins in the query increases.\n",
        "  We discuss a number of open problems about mapping class groups of surfaces.\nIn particular, we discuss problems related to linearity, congruence subgroups,\ncohomology, pseudo-Anosov stretch factors, Torelli subgroups, and normal\nsubgroups.\n",
        "  An open-source Mandarin speech corpus called AISHELL-1 is released. It is by\nfar the largest corpus which is suitable for conducting the speech recognition\nresearch and building speech recognition systems for Mandarin. The recording\nprocedure, including audio capturing devices and environments are presented in\ndetails. The preparation of the related resources, including transcriptions and\nlexicon are described. The corpus is released with a Kaldi recipe. Experimental\nresults implies that the quality of audio recordings and transcriptions are\npromising.\n",
        "  Forward-time models of diversification (i.e., speciation and extinction)\nproduce phylogenetic trees that grow \"vertically\" as time goes by. Pruning the\nextinct lineages out of such trees leads to natural models for reconstructed\ntrees (i.e., phylogenies of extant species). Alternatively, reconstructed trees\ncan be modelled by coalescent point processes (CPP), where trees grow\n\"horizontally\" by the sequential addition of vertical edges. Each new edge\nstarts at some random speciation time and ends at the present time; speciation\ntimes are drawn from the same distribution independently. CPP lead to extremely\nfast computation of tree likelihoods and simulation of reconstructed trees.\nTheir topology always follows the uniform distribution on ranked tree shapes\n(URT). We characterize which forward-time models lead to URT reconstructed\ntrees and among these, which lead to CPP reconstructed trees. We show that for\nany \"asymmetric\" diversification model in which speciation rates only depend on\ntime and extinction rates only depend on time and on a non-heritable trait\n(e.g., age), the reconstructed tree is CPP, even if extant species are\nincompletely sampled. If rates additionally depend on the number of species,\nthe reconstructed tree is (only) URT (but not CPP). We characterize the common\ndistribution of speciation times in the CPP description, and discuss incomplete\nspecies sampling as well as three special model cases in detail: 1) extinction\nrate does not depend on a trait; 2) rates do not depend on time; 3) mass\nextinctions may happen additionally at certain points in the past.\n",
        "  The new results summarized here, including a brief comparison with the\nparaconductivity, further suggest that the anomalous precursor (above Tc)\ndiamagnetism recently observed in the underdoped La1.9Sr0.1CuO4 superconductor\ncould be attributed to the presence, in addition to the conventional\nsuperconducting pair fluctuations, of Tc-inhomogeneities with long\ncharacteristic lengths associated with chemical disorder.\n",
        "  Following the principles of Cognitive Grammar, we concentrate on a model for\nreference resolution that attempts to overcome the difficulties previous\napproaches, based on the fundamental assumption that all reference (independent\non the type of the referring expression) is accomplished via access to and\nrestructuring of domains of reference rather than by direct linkage to the\nentities themselves. The model accounts for entities not explicitly mentioned\nbut understood in a discourse, and enables exploitation of discursive and\nperceptual context to limit the set of potential referents for a given\nreferring expression. As the most important feature, we note that a single\nmechanism is required to handle what are typically treated as diverse\nphenomena. Our approach, then, provides a fresh perspective on the relations\nbetween Cognitive Grammar and the problem of reference.\n",
        "  Computational topology is a vibrant contemporary subfield and this article\nintegrates knot theory and mathematical visualization. Previous work on\ncomputer graphics developed a sequence of smooth knots that were shown to\nconverge point wise to a piecewise linear (PL) approximant. This is extended to\nisotopic convergence, with that discovery aided by computational experiments.\nSufficient conditions to attain isotopic equivalence can be determined a\npriori. These sufficient conditions need not be tight bounds, providing\nopportunities for further optimizations. The results presented will facilitate\nfurther computational experiments on the theory of PL knots (also known as\nstick knots), where this theory is less mature than for smooth knots.\n",
        "  We present an analysis of MUSE observations obtained on the massive Frontier\nFields cluster Abell 2744. This new dataset covers the entire multiply-imaged\nregion around the cluster core. We measure spectroscopic redshifts for\nHST-selected continuum sources together with line emitters blindly detected in\nthe datacube. The combined catalog consists of 514 spectroscopic redshifts\n(with 414 new identifications), including 156 cluster members and 326 magnified\nbackground sources. We use this redshift information to perform a\nstrong-lensing analysis of all multiple images previously found in the deep\nFrontier Field images, and add three new MUSE-detected multiply-imaged systems\nwith no obvious HST counterpart. The combined strong lensing constraints\ninclude a total of 60 systems producing 188 images altogether, out of which 29\nsystems and 83 images are spectroscopically confirmed, making Abell 2744 one of\nthe most well-constrained clusters to date. A parametric mass model including\ntwo cluster-scale components in the core and several group-scale substructures\nat larger radii accurately reproduces all the spectroscopic multiple systems,\nreaching an rms of 0.67\" in the image plane. Overall, the large number of\nspectroscopic redshifts gives us a robust model and we estimate the systematics\non the mass density and magnification within the cluster core to be typically\n~9%.\n",
        "  The superconducting FeSe films were successfully fabricated using the\nelectrochemical synthesis. The composition ratio of Fe and Se can be controlled\nby the electric potential and pH value. We found that the FeSe films deposited\nat the electric potential -1.75 V and pH 2.3 show the superconducting\ntransition at 3.5 K. The establishment of this electrochemical synthesis\ntechnique will provide many advantages for application.\n",
        "  Cone-beam CT (CBCT) has been widely used in image guided radiation therapy\n(IGRT) to acquire updated volumetric anatomical information before treatment\nfractions for accurate patient alignment purpose. However, the excessive x-ray\nimaging dose from serial CBCT scans raises a clinical concern in most IGRT\nprocedures. The excessive imaging dose can be effectively reduced by reducing\nthe number of x-ray projections and/or lowering mAs levels in a CBCT scan. The\ngoal of this work is to develop a fast GPU-based algorithm to reconstruct high\nquality CBCT images from undersampled and noisy projection data so as to lower\nthe imaging dose. The CBCT is reconstructed by minimizing an energy functional\nconsisting of a data fidelity term and a total variation regularization term.\nWe developed a GPU-friendly version of the forward-backward splitting algorithm\nto solve this model. A multi-grid technique is also employed. We test our CBCT\nreconstruction algorithm on a digital NCAT phantom and a head-and-neck patient\ncase. The performance under low mAs is also validated using a physical Catphan\nphantom and a head-and-neck Rando phantom. It is found that 40 x-ray\nprojections are sufficient to reconstruct CBCT images with satisfactory quality\nfor IGRT patient alignment purpose. Phantom experiments indicated that CBCT\nimages can be successfully reconstructed with our algorithm under as low as 0.1\nmAs/projection level. Comparing with currently widely used full-fan\nhead-and-neck scanning protocol of about 360 projections with 0.4\nmAs/projection, it is estimated that an overall 36 times dose reduction has\nbeen achieved with our algorithm. Moreover, the reconstruction time is about\n130 sec on an NVIDIA Tesla C1060 GPU card, which is estimated ~100 times faster\nthan similar iterative reconstruction approaches.\n",
        "  We present a map of the dust reddening to 4.5 kpc derived from Pan-STARRS1\nstellar photometry. The map covers almost the entire sky north of declination\n-30 degrees at a resolution of 7' to 14', and is based on the estimated\ndistances and reddenings to more than 500 million stars. The technique is\ndesigned to map dust in the Galactic plane, where many other techniques are\nstymied by the presence of multiple dust clouds at different distances along\neach line of sight. This reddening-based dust map agrees closely with the\nSchlegel, Finkbeiner, and Davis (SFD; 1998) far-infrared emission-based dust\nmap away from the Galactic plane, and the most prominent differences between\nthe two maps stem from known limitations of SFD in the plane. We also compare\nthe map with Planck, finding likewise good agreement in general at high\nlatitudes. The use of optical data from Pan-STARRS1 yields reddening\nuncertainty as low as 25 mmag E(B-V).\n",
        "  We compute Cayley graphs and automorphism groups for all finite $n$-quandles\nof two-bridge and torus knots and links, as well as torus links with an axis.\n",
        "  In this paper the author presents four methods to treat retinitis related\ndiseases using quantum dot technology.\n",
        "  Relational queries, and in particular join queries, often generate large\noutput results when executed over a huge dataset. In such cases, it is often\ninfeasible to store the whole materialized output if we plan to reuse it\nfurther down a data processing pipeline. Motivated by this problem, we study\nthe construction of space-efficient compressed representations of the output of\nconjunctive queries, with the goal of supporting the efficient access of the\nintermediate compressed result for a given access pattern. In particular, we\ninitiate the study of an important tradeoff: minimizing the space necessary to\nstore the compressed result, versus minimizing the answer time and delay for an\naccess request over the result. Our main contribution is a novel parameterized\ndata structure, which can be tuned to trade off space for answer time. The\ntradeoff allows us to control the space requirement of the data structure\nprecisely, and depends both on the structure of the query and the access\npattern. We show how we can use the data structure in conjunction with query\ndecomposition techniques, in order to efficiently represent the outputs for\nseveral classes of conjunctive queries.\n",
        "  The paper presents many facets of medical imaging investigations radiological\nrisks. The total volume of prescribed medical investigations proves a serious\nlack in monitoring and tracking of the cumulative radiation doses in many\nhealth services. Modern radiological investigations equipment is continuously\nreducing the total dose of radiation due to improved technologies, so a\ndecrease in per caput dose can be noticed, but the increasing number of\ninvestigations has determined a net increase of the annual collective dose.\nHigh doses of radiation are cumulated from Computed Tomography investigations.\nAn integrated system for radiation safety of the patients investigated by\nradiological imaging methods, based on smart cards and Public Key\nInfrastructure allow radiation absorbed dose data storage.\n",
        "  This paper presents a novel semantic-based phrase translation model. A pair\nof source and target phrases are projected into continuous-valued vector\nrepresentations in a low-dimensional latent semantic space, where their\ntranslation score is computed by the distance between the pair in this new\nspace. The projection is performed by a multi-layer neural network whose\nweights are learned on parallel training data. The learning is aimed to\ndirectly optimize the quality of end-to-end machine translation results.\nExperimental evaluation has been performed on two Europarl translation tasks,\nEnglish-French and German-English. The results show that the new semantic-based\nphrase translation model significantly improves the performance of a\nstate-of-the-art phrase-based statistical machine translation sys-tem, leading\nto a gain of 0.7-1.0 BLEU points.\n",
        "  Poetry-writing in Sanskrit is riddled with problems for even those who know\nthe language well. This is so because the rules that govern Sanskrit prosody\nare numerous and stringent. We propose a computational algorithm that converts\nprose given as E-text into poetry in accordance with the metrical rules of\nSanskrit prosody, simultaneously taking care to ensure that sandhi or euphonic\nconjunction, which is compulsory in verse, is handled. The algorithm is\nconsiderably speeded up by a novel method of reducing the target search\ndatabase. The algorithm further gives suggestions to the poet in case what\nhe/she has given as the input prose is impossible to fit into any allowed\nmetrical format. There is also an interactive component of the algorithm by\nwhich the algorithm interacts with the poet to resolve ambiguities. In\naddition, this unique work, which provides a solution to a problem that has\nnever been addressed before, provides a simple yet effective speech recognition\ninterface that would help the visually impaired dictate words in E-text, which\nis in turn versified by our Poetry Composer Engine.\n",
        "  We consider the setting of a Semantic Web database, containing both explicit\ndata encoded in RDF triples, and implicit data, implied by the RDF semantics.\nBased on a query workload, we address the problem of selecting a set of views\nto be materialized in the database, minimizing a combination of query\nprocessing, view storage, and view maintenance costs. Starting from an existing\nrelational view selection method, we devise new algorithms for recommending\nview sets, and show that they scale significantly beyond the existing\nrelational ones when adapted to the RDF context. To account for implicit\ntriples in query answers, we propose a novel RDF query reformulation algorithm\nand an innovative way of incorporating it into view selection in order to avoid\na combinatorial explosion in the complexity of the selection process. The\ninterest of our techniques is demonstrated through a set of experiments.\n",
        "  A substantial period of life after reproduction ends, known as\npostreproductive lifespan (PRLS), is at odds with classical life history\ntheory. Prolonged PRLS has been confirmed in only two non-human mammals, both\nodontocete cetaceans. We investigate the evidence for PRLS in a third species,\nthe false killer whale, Pseudorca crassidens, using a quantitative measure of\nPRLS and morphological evidence from reproductive tissue. We examined specimens\nfrom false killer whales from combined strandings (South Africa, 1981) and\nharvest (Japan 1979-80) and found morphological evidence of changes in the\nactivity of the ovaries in relation to age. Ovulation had ceased in 50% of\nwhales over 45 years, and all whales over 55 years old had ovaries classified\nas postreproductive. We also calculated a measure of PRLS, known as\npostreproductive representation (PrR) as an indication of the effect of\ninter-population demographic variability. PrR for the combined sample was 0.14,\nwhereas the mean of the simulated distribution for PrR under the null\nhypothesis of no PRLS was 0.02. The 99th percentile of the simulated\ndistribution was 0.08 and no simulated value exceeded 0.13. These results\nsuggest that PrR was convincingly different from the measures simulated under\nthe null hypothesis. We found morphological and statistical evidence for PRLS\nin South African and Japanese pods of false killer whales, suggesting that this\nspecies is the third non-human mammal in which this phenomenon has been\ndemonstrated in wild populations. Our estimates for PrR in false killer whales\n(0.12-0.37) spanned the single values available for the short-finned pilot\nwhale (0.28) and the killer whale (0.22) and are comparable to estimates for\nhistorical or hunter-gather human populations (0.3-0.47).\n",
        "  With the rapid growth of large graphs, we cannot assume that graphs can still\nbe fully loaded into memory, thus the disk-based graph operation is inevitable.\nIn this paper, we take the shortest path discovery as an example to investigate\nthe technique issues when leveraging existing infrastructure of relational\ndatabase (RDB) in the graph data management. Based on the observation that a\nvariety of graph search queries can be implemented by iterative operations\nincluding selecting frontier nodes from visited nodes, making expansion from\nthe selected frontier nodes, and merging the expanded nodes into the visited\nones, we introduce a relational FEM framework with three corresponding\noperators to implement graph search tasks in the RDB context. We show new\nfeatures such as window function and merge statement introduced by recent SQL\nstandards can not only simplify the expression but also improve the performance\nof the FEM framework. In addition, we propose two optimization strategies\nspecific to shortest path discovery inside the FEM framework. First, we take a\nbi-directional set Dijkstra's algorithm in the path finding. The bi-directional\nstrategy can reduce the search space, and set Dijkstra's algorithm finds the\nshortest path in a set-at-a-time fashion. Second, we introduce an index named\nSegTable to preserve the local shortest segments, and exploit SegTable to\nfurther improve the performance. The final extensive experimental results\nillustrate our relational approach with the optimization strategies achieves\nhigh scalability and performance.\n",
        "  In their study of fundamental groups of one-dimensional path-connected\ncompact metric spaces, Cannon and Conner have asked: Is there a tree-like\nobject that might be considered the topological Cayley graph? We answer this\nquestion in the positive and provide a combinatorial description of such an\nobject.\n",
        "  Within the complex phase diagram of the hole-doped cuprates, seizing the\nnature of the mysterious pseudo-gap phase is essential to unravel the\nmicroscopic origin of high-temperature superconductivity. Below the pseudo-gap\ntemperature $\\rm T^{\\star}$, evidences for intra-unit-cell orders breaking the\n4-fold rotation symmetry have been provided by neutron diffraction and scanning\ntunneling spectroscopy. Using polarized neutron diffraction on a detwinned $\\rm\nYBa_2Cu_3O_{6.6}$ sample, we here report a distinct a-b anisotropy of the\nintra-unit-cell magnetic structure factor below $\\rm T^{\\star}$, highlighting\nthat intra-unit-cell order in this material breaks the mirror symmetry of the\nCuO$_2$ bilayers. This is likely to originate from a crisscrossed arrangement\nof loop currents within the $\\rm CuO_2$ bilayer, resulting in a bilayer mean\ntoroidal axis along the $\\rm {\\bf b}$ direction.\n",
        "  Data-driven applications rely on the correctness of their data to function\nproperly and effectively. Errors in data can be incredibly costly and\ndisruptive, leading to loss of revenue, incorrect conclusions, and misguided\npolicy decisions. While data cleaning tools can purge datasets of many errors\nbefore the data is used, applications and users interacting with the data can\nintroduce new errors. Subsequent valid updates can obscure these errors and\npropagate them through the dataset causing more discrepancies. Even when some\nof these discrepancies are discovered, they are often corrected superficially,\non a case-by-case basis, further obscuring the true underlying cause, and\nmaking detection of the remaining errors harder. In this paper, we propose\nQFix, a framework that derives explanations and repairs for discrepancies in\nrelational data, by analyzing the effect of queries that operated on the data\nand identifying potential mistakes in those queries. QFix is flexible, handling\nscenarios where only a subset of the true discrepancies is known, and robust to\ndifferent types of update workloads. We make four important contributions: (a)\nwe formalize the problem of diagnosing the causes of data errors based on the\nqueries that operated on and introduced errors to a dataset; (b) we develop\nexact methods for deriving diagnoses and fixes for identified errors using\nstate-of-the-art tools; (c) we present several optimization techniques that\nimprove our basic approach without compromising accuracy, and (d) we leverage a\ntradeoff between accuracy and performance to scale diagnosis to large datasets\nand query logs, while achieving near-optimal results. We demonstrate the\neffectiveness of QFix through extensive evaluation over benchmark and synthetic\ndata.\n",
        "  The ability of genetic isolation to block gene flow plays a key role in the\nspeciation of sexually reproducing organisms. This paper analyses the hybrid\nzone dynamics affected by \"weak\" Haldane's rule, namely the incomplete hybrids\ninferiority (sterility/inviability) against the heterogametic (XY or ZW) sex\ncaused by a Dobzhansky-Muller incompatibility. Different strengths of\nincompatibility, dispersal and density-dependent regulation are considered; and\nthe gene flow and clinal structures of allele frequencies in the presence of\nshort-range dispersal (the stepping-stone model) are examined. I show that a\nweak heterogametic hybrid incompatibility could constitute a substantial\nbarrier that could reduce gene flow and result in non-coincident and discordant\nclines of alleles. It is found that the differential gene flow is more\npronounced under a stronger density-dependent regulation. This study provides a\nmechanistic explanation for how an adaptive mutation, which may only have a\nmarginal fitness effect, could set a gene up as an evolutionary hot-spot.\n",
        "  The article describes a model of automatic analysis of puns, where a word is\nintentionally used in two meanings at the same time (the target word). We\nemploy Roget's Thesaurus to discover two groups of words which, in a pun, form\naround two abstract bits of meaning (semes). They become a semantic vector,\nbased on which an SVM classifier learns to recognize puns, reaching a score\n0.73 for F-measure. We apply several rule-based methods to locate intentionally\nambiguous (target) words, based on structural and semantic criteria. It appears\nthat the structural criterion is more effective, although it possibly\ncharacterizes only the tested dataset. The results we get correlate with the\nresults of other teams at SemEval-2017 competition (Task 7 Detection and\nInterpretation of English Puns) considering effects of using supervised\nlearning models and word statistics.\n",
        "  Positron Emission Mammography (PEM) imaging systems with the ability in\ndetection of millimeter-sized tumors were developed in recent years. And some\nof them have been well used in clinical applications. In consideration of\nbiopsy application, a double-plane detector configuration is practical for the\nconvenience of breast immobilization. However, the serious blurring effect in\nthe double-plane system with changeable spacing for different breast size\nshould be studied. Methods: We study a high resolution reconstruction method\napplicable for a double-plane PET system with a changeable detector spacing.\nGeometric and blurring components should be calculated at real time for\ndifferent detector distance. Accurate geometric sensitivity is obtained with a\ntube area model. Resolution recovery is achieved by estimating blurring effects\nderived from simulated single gamma response information. Results: The results\nshow that the new geometric modeling gives a more finite and smooth sensitivity\nweight in double-plane system. The blurring component yields contrast recovery\nlevels that could not be reached without blurring modeling, as well as better\nvisual recovery of the smallest spheres and better delineation of the\nstructures in the reconstructed images. Statistical noise has lower variance at\nthe voxel level with blurring modeling than without at matched resolution.\nConclusion: In the distance-changeable double-plane PET, the finite resolution\nmodeling during reconstruction achieves resolution recovery, without noise\namplification.\n",
        "  The 1918 influenza pandemic was characterized by multiple epidemic waves. We\ninvestigated into reactive social distancing, a form of behavioral responses,\nand its effect on the multiple influenza waves in the United Kingdom. Two forms\nof reactive social distancing have been used in previous studies: Power\nfunction, which is a function of the proportion of recent influenza mortality\nin a population, and Hill function, which is a function of the actual number of\nrecent influenza mortality. Using a simple epidemic model with a Power function\nand one common set of parameters, we provided a good model fit for the observed\nmultiple epidemic waves in London boroughs, Birmingham and Liverpool. Our\napproach is different from previous studies where separate models are fitted to\neach city. We then applied these model parameters obtained from fitting three\ncities to all 334 administrative units in England and Wales and including the\npopulation sizes of individual administrative units. We computed the Pearson's\ncorrelation between the observed and simulated data for each administrative\nunit. We achieved a median correlation of 0.636, indicating our model\npredictions perform reasonably well. Our modelling approach which requires\nreduced number of parameters resulted in computational efficiency gain without\nover-fitting the model. Our works have both scientific and public health\nsignificance.\n",
        "  Let M be a closed oriented 3-manifold with first Betti number one. Its\nequivariant linking pairing may be seen as a two-dimensional cohomology class\nin an appropriate infinite cyclic covering of the space of ordered pairs of\ndistinct points of M. We show how to define the equivariant cube Q(K) of this\nBlanchfield pairing with respect to a framed knot K that generates\nH_1(M)/Torsion. This article is devoted to the study of the invariant Q. We\nprove many properties for this invariant including two surgery formulae. Via\nsurgery, the invariant Q is equivalent to an invariant of null-homologous knots\nin rational homology spheres, that coincides with the two-loop part of the\nKricker rational lift of the Kontsevich integral, at least for knots with\ntrivial Alexander polynomial in integral homology spheres.\n",
        "  The factors that influence genetic architecture shape the structure of the\nfitness landscape, and therefore play a large role in the evolutionary\ndynamics. Here the NK model is used to investigate how epistasis and pleiotropy\n-- key components of genetic architecture -- affect the structure of the\nfitness landscape, and how they affect the ability of evolving populations to\nadapt despite the difficulty of crossing valleys present in rugged landscapes.\nPopulations are seen to make use of epistatic interactions and pleiotropy to\nattain higher fitness, and are not inhibited by the fact that valleys have to\nbe crossed to reach peaks of higher fitness.\n",
        "  This thesis introduces the sequence to sequence model with Luong's attention\nmechanism for end-to-end ASR. It also describes various neural network\nalgorithms including Batch normalization, Dropout and Residual network which\nconstitute the convolutional attention-based seq2seq neural network. Finally\nthe proposed model proved its effectiveness for speech recognition achieving\n15.8% phoneme error rate on TIMIT dataset.\n",
        "  With the emergence of graph databases, the task of frequent subgraph\ndiscovery has been extensively addressed. Although the proposed approaches in\nthe literature have made this task feasible, the number of discovered frequent\nsubgraphs is still very high to be efficiently used in any further exploration.\nFeature selection for graph data is a way to reduce the high number of frequent\nsubgraphs based on exact or approximate structural similarity. However, current\nstructural similarity strategies are not efficient enough in many real-world\napplications, besides, the combinatorial nature of graphs makes it\ncomputationally very costly. In order to select a smaller yet structurally\nirredundant set of subgraphs, we propose a novel approach that mines the top-k\ntopological representative subgraphs among the frequent ones. Our approach\nallows detecting hidden structural similarities that existing approaches are\nunable to detect such as the density or the diameter of the subgraph. In\naddition, it can be easily extended using any user defined structural or\ntopological attributes depending on the sought properties. Empirical studies on\nreal and synthetic graph datasets show that our approach is fast and scalable.\n",
        "  Superconducting epitaxial films of Fe-based layered arsenide, Co-doped\nSrFe2As2, were grown at 700oC on mixed perovskite (La, Sr)(Al, Ta)O3 (001)\nsingle-crystal substrates by pulsed-laser deposition. Both the epitaxial film\nand an (001)-oriented film grown at 600oC exhibited superconducting transitions\nat ~ 20 K. The zero-resistance states of the epitaxial film were sustained\nunder a magnetic field (H) of 9 T at 9 K when H was parallel to the c-axis,\nwhile they were sustained at higher temperatures up to 10 K for H parallel to\nthe a-axis. This is the first demonstration of superconducting thin films of\nFeAs-based new superconductors.\n",
        "  A common and effective way to train translation systems between related\nlanguages is to consider sub-word level basic units. However, this increases\nthe length of the sentences resulting in increased decoding time. The increase\nin length is also impacted by the specific choice of data format for\nrepresenting the sentences as subwords. In a phrase-based SMT framework, we\ninvestigate different choices of decoder parameters as well as data format and\ntheir impact on decoding time and translation accuracy. We suggest best options\nfor these settings that significantly improve decoding time with little impact\non the translation accuracy.\n",
        "  When evaluating the ecological value of land use within a landscape,\ninvestigators typically rely on measures of habitat selection and habitat\nquality. Traditional measures of habitat selection and habitat quality require\ndata from resource intensive study designs (e.g., telemetry or mark-recapture).\nOften, managers must evaluate ecological value despite only having data from\nless resource intensive study designs. In this paper, we use occupancy data to\nmeasure habitat quality and habitat selection response for the Puerto Rican\nVireo, an endemic songbird whose population growth is depressed by brood\nparasitism from the Shiny Cowbird. We were interested in how vireo habitat\nquality and vireo habitat selection varied among three land uses (forest,\nshaded coffee plantations, and sun coffee) in Puerto Rico. We estimated vireo\noccupancy probability as a measure of habitat selection, and the probability of\ncowbird occurrence given vireo presence as a measure of habitat quality. To\nestimate the latter, we explored different ways of modeling the occurrence of\nthe two species jointly, and compared these models to independent models using\nmeasures of predictive performance. Vireos preferentially selected forested\nsites and shaded coffee sites over sun coffee sites. By our measure of habitat\nquality, either type of coffee plantation was poor quality, and the forested\nsites were high quality. This suggests that shade coffee may be an ecological\ntrap for the vireo in our study area. One joint model performed best by our\nmeasures of predictive ability, thus showing that the cowbird may not occur\nindependently of the vireo. Vireo population dynamics in our study area may\nbenefit from having large amounts of forest relative to coffee plantations.\nIncorporating species interactions into occupancy models has the potential to\nimprove monitoring for conservation.\n",
        "  We formulate a mathematical model of competition for resources between\nrepresentatives of different age groups. A nonlinear kinetic\nintegral-differential equation of the age aggression describes the process of\nredistribution of resources. It is shown that the equation of the age\naggression has a stationary solution, in the absence of age-dependency in the\ninteraction of different age groups. A numerical simulation of the evolution of\nresources for different initial distributions has done. It is shown the\ninstability of the system and the existence of regimes leading to a\nconcentration of resources in the certain age groups.\n",
        "  Phylogenetic analyses which include fossils or molecular sequences that are\nsampled through time require models that allow one sample to be a direct\nancestor of another sample. As previously available phylogenetic inference\ntools assume that all samples are tips, they do not allow for this possibility.\nWe have developed and implemented a Bayesian Markov Chain Monte Carlo (MCMC)\nalgorithm to infer what we call sampled ancestor trees, that is, trees in which\nsampled individuals can be direct ancestors of other sampled individuals. We\nuse a family of birth-death models where individuals may remain in the tree\nprocess after the sampling, in particular we extend the birth-death skyline\nmodel [Stadler et al, 2013] to sampled ancestor trees. This method allows the\ndetection of sampled ancestors as well as estimation of the probability that an\nindividual will be removed from the process when it is sampled. We show that\nsampled ancestor birth-death models where all samples come from different time\npoints are non-identifiable and thus require one parameter to be known in order\nto infer other parameters. We apply this method to epidemiological data, where\nthe possibility of sampled ancestors enables us to identify individuals that\ninfected other individuals after being sampled and to infer fundamental\nepidemiological parameters. We also apply the method to infer divergence times\nand diversification rates when fossils are included among the species samples,\nso that fossilisation events are modelled as a part of the tree branching\nprocess. Such modelling has many advantages as argued in literature. The\nsampler is available as an open-source BEAST2 package\n(https://github.com/gavryushkina/sampled-ancestors).\n",
        "  We consider in this work representations of the of the fundamental group of\nthe 3-punctured sphere in ${\\rm PU}(2,1)$ such that the boundary loops are\nmapped to ${\\rm PU}(2,1)$. We provide a system of coordinates on the\ncorresponding representation variety, and analyse more specifically those\nrepresentations corresponding to subgroups of $(3,3,\\infty)$-groups. In\nparticular we prove that it is possible to construct representations of the\nfree group of rank two $\\la a,b\\ra$ in ${\\rm PU}(2,1)$ for which $a$, $b$,\n$ab$, $ab^{-1}$, $ab^2$, $a^2b$ and $[a,b]$ all are mapped to parabolics.\n",
        "  The existence of complex (multiple-step) genetic adaptations that are\n\"irreducible\" (i.e., all partial combinations are less fit than the original\ngenotype) is one of the longest standing problems in evolutionary biology. In\nstandard genetics parlance, these adaptations require the crossing of a wide\nadaptive valley of deleterious intermediate stages. Here we demonstrate, using\na simple model, that evolution can cross wide valleys to produce \"irreducibly\ncomplex\" adaptations by making use of previously cryptic mutations. When\nrevealed by an evolutionary capacitor, previously cryptic mutants have higher\ninitial frequencies than do new mutations, bringing them closer to a\nvalley-crossing saddle in allele frequency space. Moreover, simple\ncombinatorics imply an enormous number of candidate combinations exist within\navailable cryptic genetic variation. We model the dynamics of crossing of a\nwide adaptive valley after a capacitance event using both numerical simulations\nand analytical approximations. Although individual valley crossing events\nbecome less likely as valleys widen, by taking the combinatorics of genotype\nspace into account, we see that revealing cryptic variation can cause the\nfrequent evolution of complex adaptations. This finding also effectively\ndismantles \"irreducible complexity\" as an argument against evolution by\nproviding a general mechanism for crossing wide adaptive valleys.\n",
        "  Data is often partially known, vague or ambiguous in many real world\napplications. To deal with such imprecise information, fuzziness is introduced\nin the classical model. SQLf is one of the practical language to deal with\nflexible fuzzy querying in Fuzzy DataBases (FDB). However, with a huge amount\nof fuzzy data, the necessity to work with synthetic views became a challenge\nfor many DB community researchers. The present work deals with Flexible SQLf\nquery based on fuzzy linguistic summaries. We use the fuzzy summaries produced\nby our Fuzzy-SaintEtiq approach. It provides a description of objects depending\non the fuzzy linguistic labels specified as selection criteria.\n",
        "  We adapt a fitness function from evolutionary game theory as a mechanism for\naggregation and dispersal in a partial differential equation (PDE) model of two\ninteracting populations, described by density functions $u$ and $v$. We\nconsider a spatial model where individuals migrate up local fitness gradients,\nseeking out locations where their given traits are more advantageous. The\nresulting system of fitness gradient equations is a degenerate system having\nspatially structured, smooth, steady state solutions characterized by constant\nfitness throughout the domain. When populations are viewed as predator and\nprey, our model captures prey aggregation behavior consistent with Hamilton's\nselfish herd hypothesis. We also present weak steady state solutions in 1d that\nare continuous but in general not smooth everywhere, with an associated fitness\nthat is discontinuous, piecewise constant. We give numerical examples of\nsolutions that evolve toward such weak steady states. We also give an example\nof a spatial Lotka--Volterra model, where a fitness gradient flux creates\ninstabilities that lead to spatially structured steady states. Our results also\nsuggest that when fitness has some dependence on local interactions, a\nfitness-based dispersal mechanism may act to create spatial variation across a\nhabitat.\n",
        "  Recently, we determined a lower bound for the Milky Way mass in a point mass\napproximation. This result was obtained for most general spherically symmetric\nphase-space distribution functions consistent with a measured radial velocity\ndispersion. As a stability test of these predictions against a perturbation of\nthe point mass potential, in this paper we make use of a representative of\nthese functions to set the initial conditions for a simulation in a more\nrealistic potential of similar mass and accounting for other observations. The\npredicted radial velocity dispersion profile evolves to forms still consistent\nwith the measured profile, proving structural stability of the point mass\napproximation and the reliability of the resulting mass estimate of\n$2.1\\times10^{11}\\mathrm{M}_{\\odot}$ within $150\\,\\mathrm{kpc}$. We also find\nan interesting coincidence with the recent estimates based on the kinematics of\nthe extended Orphan Stream. As a byproduct, we obtain the equations of motion\nin axial symmetry from a nonstandard Hamiltonian, and derive a formula in the\nspherical symmetry relating the radial velocity dispersion profile to a\ndirectly measured kinematical observable.\n",
        "  The integrated galactic initial mass function (IGIMF) is computed from the\ncombination of the stellar initial mass function (IMF) and the embedded cluster\nmass function, described by a power law with index beta. The result of the\ncombination is a time-varying IMF which depends on the star formation rate. We\napplied the IGIMF formalism to a chemical evolution model for the solar\nneighbourhood and compared the results obtained by assuming three possible\nvalues for beta with the ones obtained by means of a standard, well-tested,\nconstant IMF. In general, a lower absolute value of beta implies a flatter\nIGIMF, hence a larger number of massive stars, higher Type Ia and II supernova\nrates, higher mass ejection rates and higher [alpha/Fe] values at a given\nmetallicity. Our suggested fiducial value for beta is 2, since with this value\nwe can account for most of the local observables. We discuss our results in a\nbroader perspective, with some implications regarding the possible universality\nof the IMF and the importance of the star formation threshold.\n",
        "  We show how the Alexander polynomial of links in lens spaces is related to\nthe classical Alexander polynomial of a link in the 3-sphere, obtained by\ncutting out the exceptional lens space fibre. It follows from these\nrelationship that a certain normalization of the Alexander polynomial satisfies\na skein relation in lens spaces.\n",
        "  The importance of electron-hole interband interactions is widely acknowledged\nfor iron-pnictide superconductors with high transition temperatures (Tc).\nHowever, high-Tc superconductivity without hole carriers has been suggested in\nFeSe single-layer films and intercalated iron-selenides, raising a fundamental\nquestion whether iron pnictides and chalcogenides have different pairing\nmechanisms. Here, we study the properties of electronic structure in the\nhigh-Tc phase induced by pressure in bulk FeSe from magneto-transport\nmeasurements and first-principles calculations. With increasing pressure, the\nlow-Tc superconducting phase transforms into high-Tc phase, where we find the\nnormal-state Hall resistivity changes sign from negative to positive,\ndemonstrating dominant hole carriers in striking contrast to other FeSe-derived\nhigh-Tc systems. Moreover, the Hall coefficient is remarkably enlarged and the\nmagnetoresistance exhibits anomalous scaling behaviors, evidencing strongly\nenhanced interband spin fluctuations in the high-Tc phase. These results in\nFeSe highlight similarities with high-Tc phases of iron pnictides, constituting\na step toward a unified understanding of iron-based superconductivity.\n",
        "  Formality is one of the most important dimensions of writing style variation.\nIn this study we conducted an inter-rater reliability experiment for assessing\nsentence formality on a five-point Likert scale, and obtained good agreement\nresults as well as different rating distributions for different sentence\ncategories. We also performed a difficulty analysis to identify the bottlenecks\nof our rating procedure. Our main objective is to design an automatic scoring\nmechanism for sentence-level formality, and this study is important for that\npurpose.\n",
        "  To assist non-specialists in formulating database queries, multiple\nframeworks that automatically infer queries from a set of examples have been\nproposed. While highly useful, a shortcoming of the approach is that if users\ncan only provide a small set of examples, many inherently different queries may\nqualify, and only some of these actually match the user intentions. Our main\nobservation is that if users further explain their examples, the set of\nqualifying queries may be significantly more focused. We develop a novel\nframework where users explain example tuples by choosing input tuples that are\nintuitively the \"cause\" for their examples. Their explanations are\nautomatically \"compiled\" into a formal model for explanations, based on\npreviously developed models of data provenance. Then, our novel algorithms\ninfer conjunctive queries from the examples and their explanations. We prove\nthe computational efficiency of the algorithms and favorable properties of\ninferred queries. We have further implemented our solution in a system\nprototype with an interface that assists users in formulating explanations in\nan intuitive way. Our experimental results, including a user study as well as\nexperiments using the TPC-H benchmark, indicate the effectiveness of our\nsolution.\n",
        "  A recent study (Fish. Bull. 109:394-401 (2011)) purportedly tests two\nhypotheses: 1. that the capture of elasmobranchs would be reduced with hooks\ncontaining magnets in comparison with control hooks in hook-and-line and\nlongline studies. 2. that the presence of permanent magnets on hooks would not\nalter teleost capture because teleosts lack the ampullary organ. Review of this\npaper shows some inconsistencies in the data supporting the first hypothesis\nand insufficient data and poor experimental design to adequately test the\nsecond hypothesis. Further, since several orders of teleosts are known to\npossess ampullary organs and demonstrate electroreception, grouping all\nteleosts in a study design or data analysis of magnetic hook catch rates is not\nwarranted. Adequate tests of the hypothesis that permanent magnets or\nmagnetized hooks do not alter teleost capture requires a more careful study\ndesign and much larger sample sizes than O'Connell et al. (Fish. Bull.\n109:394-401 (2011)).\n",
        "  Purpose: Cone-beam CT (CBCT) plays an important role in image guided\nradiation therapy (IGRT). However, the large radiation dose from serial CBCT\nscans in most IGRT procedures raises a clinical concern, especially for\npediatric patients who are essentially excluded from receiving IGRT for this\nreason. The goal of this work is to develop a fast GPU-based algorithm to\nreconstruct CBCT from undersampled and noisy projection data so as to lower the\nimaging dose. Methods: The CBCT is reconstructed by minimizing an energy\nfunctional consisting of a data fidelity term and a total variation\nregularization term. We developed a GPU-friendly version of the\nforward-backward splitting algorithm to solve this model. A multi-grid\ntechnique is also employed. Results: It is found that 20~40 x-ray projections\nare sufficient to reconstruct images with satisfactory quality for IGRT. The\nreconstruction time ranges from 77 to 130 sec on a NVIDIA Tesla C1060 GPU card,\ndepending on the number of projections used, which is estimated about 100 times\nfaster than similar iterative reconstruction approaches. Moreover, phantom\nstudies indicate that our algorithm enables the CBCT to be reconstructed under\na scanning protocol with as low as 0.1 mAs/projection. Comparing with currently\nwidely used full-fan head and neck scanning protocol of ~360 projections with\n0.4 mAs/projection, it is estimated that an overall 36~72 times dose reduction\nhas been achieved in our fast CBCT reconstruction algorithm. Conclusions: This\nwork indicates that the developed GPU-based CBCT reconstruction algorithm is\ncapable of lowering imaging dose considerably. The high computation efficiency\nin this algorithm makes the iterative CBCT reconstruction approach applicable\nin real clinical environments.\n",
        "  To reconstruct a radioactive tracer distribution with positron emission\ntomography (PET), the background attenuation correction is needed to eliminate\nimage artifacts. Recent research shows that time-of-flight (TOF) PET data\ndetermine the attenuation sinogram up to a constant, and its gradient can be\ncomputed using an analytic algorithm. In this paper, we study a direct\nestimation of the sinogram only from TOF PET data. First, the gradient of the\nattenuation sinogram is estimated using the aforementioned algorithm. Then, a\nrelationship is established to link the differential attenuation sinogram and\nthe underlying attenuation background. Finally, an iterative algorithm is\ndesigned to determine the attenuation sinogram accurately and stably. A 2D\nnumerical simulation study is conducted to verify the correctness of our\nproposed approach.\n",
        "  Word segmentation plays a pivotal role in improving any Arabic NLP\napplication. Therefore, a lot of research has been spent in improving its\naccuracy. Off-the-shelf tools, however, are: i) complicated to use and ii)\ndomain/dialect dependent. We explore three language-independent alternatives to\nmorphological segmentation using: i) data-driven sub-word units, ii) characters\nas a unit of learning, and iii) word embeddings learned using a character CNN\n(Convolution Neural Network). On the tasks of Machine Translation and POS\ntagging, we found these methods to achieve close to, and occasionally surpass\nstate-of-the-art performance. In our analysis, we show that a neural machine\ntranslation system is sensitive to the ratio of source and target tokens, and a\nratio close to 1 or greater, gives optimal performance.\n",
        "  With XML becoming an ubiquitous language for data interoperability purposes\nin various domains, efficiently querying XML data is a critical issue. This has\nlead to the design of algebraic frameworks based on tree-shaped patterns akin\nto the tree-structured data model of XML. Tree patterns are graphic\nrepresentations of queries over data trees. They are actually matched against\nan input data tree to answer a query. Since the turn of the twenty-first\ncentury, an astounding research effort has been focusing on tree pattern models\nand matching optimization (a primordial issue). This paper is a comprehensive\nsurvey of these topics, in which we outline and compare the various features of\ntree patterns. We also review and discuss the two main families of approaches\nfor optimizing tree pattern matching, namely pattern tree minimization and\nholistic matching. We finally present actual tree pattern-based developments,\nto provide a global overview of this significant research topic.\n",
        "  Machine translation between Arabic and Hebrew has so far been limited by a\nlack of parallel corpora, despite the political and cultural importance of this\nlanguage pair. Previous work relied on manually-crafted grammars or pivoting\nvia English, both of which are unsatisfactory for building a scalable and\naccurate MT system. In this work, we compare standard phrase-based and neural\nsystems on Arabic-Hebrew translation. We experiment with tokenization by\nexternal tools and sub-word modeling by character-level neural models, and show\nthat both methods lead to improved translation performance, with a small\nadvantage to the neural models.\n",
        "  In microbial ecology studies, the most commonly used ways of investigating\nalpha (within-sample) diversity are either to apply count-only measures such as\nSimpson's index to Operational Taxonomic Unit (OTU) groupings, or to use\nclassical phylogenetic diversity (PD), which is not abundance-weighted.\nAlthough alpha diversity measures that use abundance information in a\nphylogenetic framework do exist, but are not widely used within the microbial\necology community. The performance of abundance-weighted phylogenetic diversity\nmeasures compared to classical discrete measures has not been explored, and the\nbehavior of these measures under rarefaction (sub-sampling) is not yet clear.\nIn this paper we compare the ability of various alpha diversity measures to\ndistinguish between different community states in the human microbiome for\nthree different data sets. We also present and compare a novel one-parameter\nfamily of alpha diversity measures, BWPD_\\theta, that interpolates between\nclassical phylogenetic diversity (PD) and an abundance-weighted extension of\nPD. Additionally, we examine the sensitivity of these phylogenetic diversity\nmeasures to sampling, via computational experiments and by deriving a closed\nform solution for the expectation of phylogenetic quadratic entropy under\nre-sampling. In all three of the datasets considered, an abundance-weighted\nmeasure is the best differentiator between community states. OTU-based\nmeasures, on the other hand, are less effective in distinguishing community\ntypes. In addition, abundance-weighted phylogenetic diversity measures are less\nsensitive to differing sampling intensity than their unweighted counterparts.\nBased on these results we encourage the use of abundance-weighted phylogenetic\ndiversity measures, especially for cases such as microbial ecology where\nspecies delimitation is difficult.\n",
        "  Binaural hearing aids communicate with each other through a wireless link for\nsynchronization. A propagation model is needed to estimate the ear-to-ear link\nloss for such binaural hearing aids. The link loss is a critical parameter in a\nlink budget to decide the sensitivity of the transceiver. In this paper, we\nhave presented a model for the deterministic component of the ear-to-ear link\nloss. The model takes into account the dominant paths having most of the power\nof the creeping wave from the transceiver in one ear to the transceiver in\nother ear and the effect of the protruding part of the outer ear called pinna.\nSimulations are done to validate the model using in-the-ear (ITE) placement of\nantennas at 2.45 GHz on two heterogeneous phantoms of different age-group and\nbody size. The model agrees with the simulations. The ear-to-ear link loss\nbetween the antennas for the binaural hearing aids in the homogeneous SAM\nphantom is compared with a heterogeneous phantom. It is found that the absence\nof the pinna and the lossless shell in the SAM phantom underestimate the link\nloss. This is verified by the measurements on a phantom where we have included\nthe pinnas fabricated by 3D-printing.\n",
        "  Fifteen years after the discovery of dynamical friction by Chandrasekhar,\nMichel Henon attempts to solve the longstanding problem of the divergence of\nthe friction suffered by the perturber and caused by the most distant cluster\nstars. His solution laid the foundation to the current understanding of\ndynamical friction as a non-local transitory force.\n",
        "  Given a query graph that represents a pattern of interest, the emerging\npattern detection problem can be viewed as a continuous query problem on a\ndynamic graph. We present an incremental algorithm for continuous query\nprocessing on dynamic graphs. The algorithm is based on the concept of query\ndecomposition; we decompose a query graph into smaller subgraphs and assemble\nthe result of sub-queries to find complete matches with the specified query.\nThe novelty of our work lies in using the subgraph distributional statistics\ncollected from the dynamic graph to generate the decomposition. We introduce a\n\"Lazy Search\" algorithm where the search strategy is decided on a\nvertex-to-vertex basis depending on the likelihood of a match in the vertex\nneighborhood. We also propose a metric named \"Relative Selectivity\" that is\nused to select between different query decomposition strategies. Our\nexperiments performed on real online news, network traffic stream and a\nsynthetic social network benchmark demonstrate 10-100x speedups over competing\napproaches.\n",
        "  We investigate the presence of extended ionized outflows in 18 luminous type\n2 AGNs (11 quasars and 7 high luminosity Seyfert 2s) at 0.3<z<0.6 based on\nVLT-FORS2 spectroscopy. We infer typical lower limits on the radial sizes of\nthe outflows R>=severalx100 pc and upper limits R<1-2 kpc. Our results are\ninconsistent with related studies which suggest that large scale R~several-15\nkpc are ubiquitous in QSO2. We study the possible causes of discrepancy and\npropose that seeing smearing is the cause of the large inferred sizes. The\nimplications in our understanding of the feedback phenomenon are important\nsince the mass M (through the density), mass injection M(dot) and energy\ninjection rates E(dot) of the outflows become highly uncertain. One conclusion\nseems unavoidable: M, M(dot), E(dot) are modest or low compared with previous\nestimations. No evidence is found supporting that typical outflows can affect\nthe interstellar medium of the host galaxies accross spatial scales >~1-2 kpc.\n",
        "  This paper explores coevolution and governance of common goods using models\nof coevolving biospheres, in which adapting populations must collectively\nregulate their planet's climate or face extinction. The results support the\nGaia hypothesis against challenges based on the tragedy of the commons: model\ncreatures are often able to work together to maintain the common good (a\nsuitable climate) without being undermined by \"free riders.\" A long-term\ndynamics appears in which communities that cannot sustain Gaian cooperation\ngive way to communities that can. This result provides an argument why a Gaia\nscenario should generally be observed, rather than a tragedy of the commons\nscenario. Second, a close look at how communities fail reveals failures that do\nnot fit the tragedy of the commons framework and are better described in terms\nof conflict between differently positioned parties, with power over different\naspects of the system. In the context of Norgaard's work, all these\nobservations can be read as narratives of coevolution relevant to social\ncommunities as well as ecological ones, contrasting with pessimistic scenarios\nabout common governance and supporting respect for traditional arrangements and\nrestraint in intervention.\n",
        "  Scattering power (T = d/dx of mean squared multiple Coulomb scattering (MCS)\nangle), as used in proton transport theory, is properly viewed as a\ndifferential description of the Gaussian approximation to MCS theories such as\nMoliere's. That is, we seek a function T which, when integrated over a finite\nslab, will recover the Moliere/Fano/Hanson angle for that slab. To be accurate,\nT must include a single scattering correction, which means mathematically it\nmust be nonlocal, depending on how much MCS has taken place as well as the\nenergy and scattering material at the POI. We review five formulas for T and\nintroduce a sixth, testing each against the Moliere/Fano/Hanson prediction as\nwell as experimental data. We discuss how sensitive some practical problems are\nto the choice of T. That choice is probably most important for general Monte\nCarlo codes, which are expected to address a wide variety of problems.\n",
        "  This article will explore the K- and L-theory of group rings and their\napplications to algebra, geometry and topology. The Farrell-Jones Conjecture\ncharacterizes K- and L-theory groups. It has many implications, including the\nBorel and Novikov Conjectures for topological rigidity. Its current status, and\nmany of its consequences are surveyed.\n",
        "  Multiround algorithms are now commonly used in distributed data processing\nsystems, yet the extent to which algorithms can benefit from running more\nrounds is not well understood. This paper answers this question for a spectrum\nof rounds for the problem of computing the equijoin of $n$ relations.\nSpecifically, given any query $Q$ with width $\\w$, {\\em intersection width}\n$\\iw$, input size $\\mathrm{IN}$, output size $\\mathrm{OUT}$, and a cluster of\nmachines with $M$ memory available per machine, we show that:\n  (1) $Q$ can be computed in $O(n)$ rounds with $O(n\\frac{(\\mathrm{IN}^{\\w} +\n\\mathrm{OUT})^2}{M})$ communication cost.\n  (2) $Q$ can be computed in $O(\\log(n))$ rounds with\n$O(n\\frac{(\\mathrm{IN}^{\\max(\\w, 3\\iw)} + \\mathrm{OUT})^2}{M})$ communication\ncost. \\end{itemize} Intersection width is a new notion of queries and\ngeneralized hypertree decompositions (GHDs) of queries we introduce to capture\nhow connected the adjacent cyclic components of the GHDs are.\n  We achieve our first result by introducing a distributed and generalized\nversion of Yannakakis's algorithm, called GYM. GYM takes as input any GHD of\n$Q$ with width $\\w$ and depth $d$, and computes $Q$ in $O(d + \\log(n))$ rounds\nand $O(n\\frac{(\\mathrm{IN}^{\\w} + \\mathrm{OUT})^2}{M})$ communication cost. We\nachieve our second result by showing how to construct GHDs of $Q$ with width\n$\\max(\\w, 3\\iw)$ and depth $O(\\log(n))$. We describe another technique to\nconstruct GHDs with longer widths and shorter depths, demonstrating a spectrum\nof tradeoffs one can make between communication and the number of rounds.\n",
        "  We present an automatic mortality prediction scheme based on the unstructured\ntextual content of clinical notes. Proposing a convolutional document embedding\napproach, our empirical investigation using the MIMIC-III intensive care\ndatabase shows significant performance gains compared to previously employed\nmethods such as latent topic distributions or generic doc2vec embeddings. These\nimprovements are especially pronounced for the difficult problem of\npost-discharge mortality prediction.\n",
        "  Evolutionary events such as incomplete lineage sorting and lateral gene\ntransfer constitute major problems for inferring species trees from gene trees,\nas they can sometimes lead to gene trees which conflict with the underlying\nspecies tree. One particularly simple and efficient way to infer species trees\nfrom gene trees under such conditions is to combine three-taxon analyses for\nseveral genes using a majority vote approach. For incomplete lineage sorting\nthis method is known to be statistically consistent, however, in the case of\nlateral gene transfer it is known that a zone of inconsistency does exist for a\nspecific four-taxon tree topology. In this paper we analyze all remaining\nfour-taxon topologies and show that no other inconsistencies exist.\n",
        "  This paper introduces ENFrame, a unified data processing platform for\nquerying and mining probabilistic data. Using ENFrame, users can write programs\nin a fragment of Python with constructs such as bounded-range loops, list\ncomprehension, aggregate operations on lists, and calls to external database\nengines. The program is then interpreted probabilistically by ENFrame.\n  The realisation of ENFrame required novel contributions along several\ndirections. We propose an event language that is expressive enough to\nsuccinctly encode arbitrary correlations, trace the computation of user\nprograms, and allow for computation of discrete probability distributions of\nprogram variables. We exemplify ENFrame on three clustering algorithms:\nk-means, k-medoids, and Markov Clustering. We introduce sequential and\ndistributed algorithms for computing the probability of interconnected events\nexactly or approximately with error guarantees. Experiments with k-medoids\nclustering of sensor readings from energy networks show orders-of-magnitude\nimprovements of exact clustering using ENFrame over na\\\"ive clustering in each\npossible world, of approximate over exact, and of distributed over sequential\nalgorithms.\n",
        "  The aim of this work was to simulate the behaviour of hip prostheses under\nmechanical shocks. When hip joint is replaced by prosthesis, during the swing\nphase of the leg, a microseparation between the prosthetic head and the cup\ncould occur. Two different sizes of femoral heads were studied: 28 and 32 mm\ndiameter, made, respectively, in alumina and zirconia. The shock-induced stress\nwas determined numerically using finite element analysis (FEA), Abaqus\nsoftware. The influence of inclination, force, material, and microseparation\nwas studied. In addition, an algorithm was developed from a probabilistic\nmodel, Todinov's approach, to predict lifetime of head and cup. Simulations\nshowed maximum tensile stresses were reached on the cup's surfaces near to rim.\nThe worst case was the cup-head mounted at 30^{\\circ}. All simulations and\ntests showed bulk zirconia had a greater resistance to shocks than bulk\nalumina. The probability of failure could be bigger than 0.9 when a porosity\ngreater than 0.7% vol. is present in the material. Simulating results showed\ngood agreement with experimental results. The tests and simulations are\npromising for predicting the lifetime of ceramic prostheses.\n",
        "  In this thesis we describe how to estimate the distance spanned in the pants\ngraph by a train track splitting sequence on a surface, up to multiplicative\nand additive constants. If some moderate assumptions on a splitting sequence\nare satisfied, each vertex set of a train track in it will represent a vertex\nof a graph which is naturally quasi-isometric to the pants graph; moreover the\nsplitting sequence gives an edge-path in this graph so, more precisely, our\ndistance estimate holds between the extreme points of this path. The present\ndistance estimate is inspired by a result of Masur, Mosher and Schleimer for\ndistances in the marking graph. However, we can apply their line of proof only\nafter some manipulation of the splitting sequence: a rearrangement, changing\nthe order the elementary moves are performed in, so that the ones producing\nDehn twists are brought together; and then an untwisting, which suppresses the\nmajority of these latter moves to give a new sequence, which does not end with\nthe same track as before, but does not include any portion that is almost\nstationary in the pants graph. The required distance is then, up to constants,\nthe number of splits occurring in the untwisted sequence. A consequence of our\nmain theorem together with a result of Brock is that, given a pseudo-Anosov\nself-diffeomorphism $\\psi$ of a surface $S$, the maximal splitting sequence\nintroduced by Agol gives us an estimate for the hyperbolic volume of the\nmapping torus built from $S$ and $\\psi$. There are also some interesting\nconsequences for the hyperbolic volume of a solid torus minus a closed braid,\nvia a machinery employed by Dynnikov and Wiest.\n",
        "  We study the spatial distributions of $\\beta^+$-activity produced by\ntherapeutic beams of $^3$He and $^{12}$C ions in various tissue-like materials.\nThe calculations were performed within a Monte Carlo model for Heavy-Ion\nTherapy (MCHIT) based on the GEANT4 toolkit. The contributions from\n$^{10,11}$C, $^{13}$N, $^{14,15}$O, $^{17,18}$F and $^{30}$P positron-emitting\nnuclei were calculated and compared with experimental data obtained during and\nafter irradiation. Positron emitting nuclei are created by $^{12}$C beam in\nfragmentation reactions of projectile and target nuclei. This leads to a\n$\\beta^+$-activity profile characterised by a noticeable peak located close to\nthe Bragg peak in the corresponding depth-dose distribution. On the contrary,\nas the most of positron-emitting nuclei are produced by $^3$He beam in target\nfragmentation reactions, the calculated total $\\beta^+$-activity during or soon\nafter the irradiation period is evenly distributed within the projectile range.\nHowever, we predict also the presence of $^{13}$N, $^{14}$O, $^{17,18}$F\ncreated in charge-transfer reactions by low-energy $^3$He ions close to the end\nof their range in several tissue-like media. The time evolution of\n$\\beta^+$-activity profiles was investigated for both kinds of beams. Due to\nthe production of $^{18}$F nuclide the $\\beta^+$-activity profile measured 2 or\n3 hours after irradiation with $^{3}$He ions will have a distinct peak\ncorrelated with the maximum of depth-dose distribution. We found certain\nadvantages of low-energy $^{3}$He beams over low-energy proton beams for\nreliable PET monitoring during particle therapy of shallow located tumours. In\nthis case the distal edge of $\\beta^+$-activity distribution from $^{17}$F\nnuclei clearly marks the range of $^{3}$He in tissues.\n",
        "  This paper describes AllenNLP, a platform for research on deep learning\nmethods in natural language understanding. AllenNLP is designed to support\nresearchers who want to build novel language understanding models quickly and\neasily. It is built on top of PyTorch, allowing for dynamic computation graphs,\nand provides (1) a flexible data API that handles intelligent batching and\npadding, (2) high-level abstractions for common operations in working with\ntext, and (3) a modular and extensible experiment framework that makes doing\ngood science easy. It also includes reference implementations of high quality\napproaches for both core semantic problems (e.g. semantic role labeling (Palmer\net al., 2005)) and language understanding applications (e.g. machine\ncomprehension (Rajpurkar et al., 2016)). AllenNLP is an ongoing open-source\neffort maintained by engineers and researchers at the Allen Institute for\nArtificial Intelligence.\n",
        "  We present total and linearly polarized 3 mm Global mm-VLBI Array images of a\nsample of blazars and radio galaxies from the VLBA-BU-BLAZAR 7 mm monitoring\nprogram designed to probe the innermost regions of active galactic nuclei (AGN)\njets and locate the sites of gamma-ray emission observed by the Fermi-LAT. The\nlower opacity at 3 mm and improved angular resolution, on the order of 50\nmicroarcseconds, allow us to distinguish features in the jet not visible in the\n7 mm VLBA data. We also compare two different methods used for the calibration\nof instrumental polarisation and we analyze the resulting images for some of\nthe sources in the sample.\n",
        "  Recently, we have been witnessing huge advancements in the scale of data we\nroutinely generate and collect in pretty much everything we do, as well as our\nability to exploit modern technologies to process, analyze and understand this\ndata. The intersection of these trends is what is called, nowadays, as Big Data\nScience. Cloud computing represents a practical and cost-effective solution for\nsupporting Big Data storage, processing and for sophisticated analytics\napplications. We analyze in details the building blocks of the software stack\nfor supporting big data science as a commodity service for data scientists. We\nprovide various insights about the latest ongoing developments and open\nchallenges in this domain.\n",
        "  The first purpose of this paper is to shed some new light on the old question\nof selecting the number of beams in intensity-modulated radiation therapy\n(IMRT). The second purpose is to illuminate the related issue of discrete\nstatic beam angles vs. rotational techniques, which has recently re-surfaced\ndue to the advancement of volumetric arc therapy (VMAT). A specific objective\nis to find analytical expressions that allow one to address the points raised\nabove. To make the problem mathematically tractable, it is assumed that the\ndepth dose is flat and that the lateral dose profile can be approximated by\npolynomials, specifically Chebyshev polynomials of the first kind, of finite\ndegree. The application of methods known from image reconstruction then allows\none to answer the first question above as follows: The required number of beams\nis determined by the maximum degree of the polynomials used in the\napproximation of the beam profiles, which is a measure of the dose variability.\nThere is nothing to be gained by using more beams. In realistic cases, in which\nthe variability of the lateral dose profile is restricted in several ways, the\nrequired number of beams is of the order of 10 to 20. The consequence of\ndelivering the beams with a `leaf sweep' technique during continuous rotation\nof the gantry, as in VMAT, is also derived in analytical form. The main effect\nis that the beams fan out, but the effect near the axis of rotation is small.\nThis result can serve as a theoretical justification of VMAT. Overall the\nanalytical derivations in this paper, albeit based on strong simplifications,\nprovide new insights into, and a deeper understanding of, the beam angle\nproblem in IMRT.\n",
        "  Translation surfaces with poles correspond to meromorphic differentials on\ncompact Riemann surfaces. They appear in compactifications of strata of the\nmoduli space of Abelian differentials and in the study of stability conditions.\nSuch structures have different geometrical and dynamical properties than usual\ntranslation surfaces. In particular, they always have infinite area and can\nhave a finite number of saddle connections. We provide a combinatorial\ncharacterization of the strata for which there can be an infinite number of\nsaddle connections and give lower and upper bounds for the number of saddle\nconnections and related quantities.\n",
        "  This work uncovers the tropical analogue for measured laminations of the\nconvex hull construction of decorated Teichmueller theory, namely, it is a\nstudy in coordinates of geometric degeneration to a point of Thurston's\nboundary for Teichmueller space. This may offer a paradigm for the extension of\nthe basic cell decomposition of Riemann's moduli space to other contexts for\ngeneral moduli spaces of flat connections on a surface. In any case, this\ndiscussion drastically simplifies aspects of previous related studies as is\nexplained. Furthermore, a new class of measured laminations relative to an\nideal cell decomposition of a surface is discovered in the limit. Finally, the\ntropical analogue of the convex hull construction in Minkowski space is\nformulated as an explicit algorithm that serially simplifies a triangulation\nwith respect to a fixed lamination and has its own independent applications.\n",
        "  We revisit the moving k nearest neighbor (MkNN) query, which computes one's k\nnearest neighbor set and maintains it while at move. Existing MkNN algorithms\nare mostly safe region based, which lack efficiency due to either computing\nsmall safe regions with a high recomputation frequency or computing larger safe\nregions but with a high cost for each computation. In this demonstration, we\nshowcase a system named INSQ that adopts a novel algorithm called the\nInfluential Neighbor Set (INS) algorithm to process the MkNN query in both\ntwo-dimensional Euclidean space and road networks. This algorithm uses a small\nset of safe guarding objects instead of safe regions. As long as the the\ncurrent k nearest neighbors are closer to the query object than the safe\nguarding objects are, the current k nearest neighbors stay valid and no\nrecomputation is required. Meanwhile, the region defined by the safe guarding\nobjects is the largest possible safe region. This means that the recomputation\nfrequency is also minimized and hence, the INS algorithm achieves high overall\nquery processing efficiency.\n",
        "  Table grid user interface element with a vertical scrollbar is a standard way\nof working with database table records. There are two basic operations each\ngrid should support: scrolling records with vertical scrollbar and positioning\nto a record with a given primary key. This paper addresses the case when the\nnumber of records is so large that it is not feasible to load them all into\nmemory, and database functions like \"select..offset\" work insufficiently fast\nand put undue load on RDBMS. Our main idea is to use only queries that involve\nindex lookup (a O(Log(N)-fast operation) and to use statistic properties of\nhypergeometric distribution to \"guess\" primary keys of records given their\nordinal numbers. The proposed method allows us to implement a grid with\nO(Log(N))-fast scrolling and positioning performance.\n",
        "  While theoretical dust condensation models predict that most refractory\nelements produced in core-collapse supernovae (SNe) efficiently condense into\ndust, a large quantity of dust has so far only been observed in SN 1987A. We\npresent the analysis of Spitzer Space Telescope, Herschel Space Observatory,\nStratospheric Observatory for Infrared Astronomy (SOFIA), and AKARI\nobservations of the infrared (IR) shell surrounding the pulsar wind nebula in\nthe supernova remnant G54.1+0.3. We attribute a distinctive spectral feature at\n21 $\\mu$m to a magnesium silicate grain species that has been invoked in\nmodeling the ejecta-condensed dust in Cas A, which exhibits the same spectral\nsignature. If this species is responsible for producing the observed spectral\nfeature and accounts for a significant fraction of the observed IR continuum,\nwe find that it would be the dominant constituent of the dust in G54.1+0.3,\nwith possible secondary contributions from other compositions, such as carbon,\nsilicate, or alumina grains. The smallest mass of SN-formed dust required by\nour models is 1.1 $\\pm$ 0.8 $\\rm M_{\\odot}$. We discuss how these results may\nbe affected by varying dust grain properties and self-consistent grain heating\nmodels. The spatial distribution of the dust mass and temperature in G54.1+0.3\nconfirms the scenario in which the SN-formed dust has not yet been processed by\nthe SN reverse shock and is being heated by stars belonging to a cluster in\nwhich the SN progenitor exploded. The dust mass and composition suggest a\nprogenitor mass of 16$-$27 $\\rm M_{\\odot}$ and imply a high dust condensation\nefficiency, similar to that found for Cas A and SN 1987A. The study provides\nanother example of significant dust formation in a Type IIP SN and sheds light\non the properties of pristine SN-condensed dust.\n",
        "  Primary blast-induced traumatic brain injury (TBI) has increased in\ndocumented incidence and public prominence in recent conflicts. Evidence for a\nthoracic mechanism of blast-induced TBI was recently reviewed and, while the\ntotality is compelling, data from experiments isolating this mechanism is\nsparse. Notably, one recent study showed pericapillar haemorrhage in brain\ntissue from victims of single, fatal gunshot wounds to the chest. Here,\nqualitative results are reported for a small field study that isolated a\nthoracic mechanism for TBI caused by a high strain rate insult in white-tailed\ndeer (Odocoileus virginianus, mass 49-80 kg) in a natural environment. In each\nof three cases, petechiae were present on the surface of the frontal, occipital\nand/or left parietal lobes, along with capillary damage in the choroid plexus.\nThe location of the projectile impact to the thorax seemed to affect the degree\nof damage. This may be due to the proximity to the great vessels. The data\nreported here provides direct evidence of a thoracic mechanism resulting in\ngross injury to the cerebral vasculature.\n",
        "  This is a survey article on two topics. The Energy E of knots can be obtained\nby generalizing an electrostatic energy of charged knots in order to produce\noptimal knots. It turns out to be invariant under Moebius transformations. We\nshow that it can be expressed in terms of the infinitesimal cross ratio, which\nis a conformal invariant of a pair of 1-jets, and give two kinds of\ninterpretations of the real part of the infinitesimal cross ratio.\n",
        "  We first explain the pseudogap of high-temperature superconductivity based on\nan approach of quantum optics. After introducing a damping factor for the\nlifetime $\\tau$ of quasiparticles, the superconducting dome is naturally\nproduced, and the pseudogap is the consequence of pairing with damped\ncoherence. We derive a new expression of Ginzburg-Landau free energy density,\nin which a six-order term due to decoherence damping effect is included.\nWithout invoking any microscopic pairing mechanism, this approach provides a\nsimple universal equation of second-order phase transition, which can be\nreduced to two well-known empirical scaling equations: the superconducting dome\nPresland-Tallon equation, and the normal-state pseudogap crossover temperature\n$T^{*}$ line.\n",
        "  Sr$_{2}$RuO$_{4}$ (SRO) is the prime candidate for chiral $p$-wave\nsuperconductor with critical temperature $T_{c}(SRO)\\sim$1.5 K. Chiral domains\nwith opposite chiralities $p_{x}\\pm ip_{y}$ were proposed, but yet to be\nconfirmed. We measure the field dependence of the point contact (PC) resistance\nbetween a tungsten tip and the SRO-Ru eutectic crystal, where micrometer-sized\nRu inclusions are embedded in SRO with atomic sharp interface. Ruthenium is an\n$s$-wave superconductor with $T_{c}(Ru)\\sim$0.5 K, flux pinned near the Ru\ninclusions can suppress its superconductivity as reflected from the PC\nresistance and spectra. This flux pinning effect is originated from SRO\n\\textit{underneath} the surface and is very strong. To fully remove it, one has\nto thermal cycle the sample above $T_{c}(SRO)$. This resembles the thermal\ndemagnetization for a ferromagnet, where ferromagnetic domains are randomized\nabove its Curie temperature. Another way is by applying alternating fields with\ndecreasing amplitude, resembling field demagnetization for the ferromagnet. The\nobserved hysteresis in magnetoresistance can be explained by domain dynamics,\nproviding support for the existence of chiral domains. The origin of strong\npinning \\textit{underneath} the surface is also discussed.\n",
        "  We show that the zeroth coefficient of the cables of the HOMFLY polynomial\n(colored HOMFLY polynomials) does not distinguish mutants. This makes a sharp\ncontrast with the total HOMFLY polynomial whose 3-cables can distinguish\nmutants.\n",
        "  We obtain a finite set of generators for the level 2 mapping class group of a\nclosed nonorientable surface of genus $g\\ge 3$. This set consists of isotopy\nclasses of Lickorish's Y-homeomorphisms also called crosscap slides.\n",
        "  We have obtained high-resolution mid-infrared (MIR) imaging, nuclear spectral\nenergy distributions (SEDs) and archival Spitzer spectra for 22 low-luminosity\nactive galactic nuclei (LLAGN; L_bol < 5 x 10^42 erg/s). Infrared (IR)\nobservations may advance our understanding of the accretion flows in LLAGN, the\nfate of the obscuring torus at low accretion rates, and, perhaps, the star\nformation histories of these objects. However, while comprehensively studied in\nhigher-luminosity Seyferts and quasars, the nuclear IR properties of LLAGN have\nnot yet been well-determined. In these proceedings we summarise the results for\nthe LLAGN at the relatively high-luminosity, high-Eddington ratio end of the\nsample. Strong, compact nuclear sources are visible in the MIR images of these\nobjects, with luminosities consistent with or slightly in execss of that\npredicted by the standard MIR/X-ray relation. Their broadband nuclear SEDs are\ndiverse; some resemble typical Seyfert nuclei, while others possess less of a\nwell-defined MIR ``dust bump''. Strong silicate emission is present in many of\nthese objects. We speculate that this, together with high ratios of silicate\nstrength to hydrogen column density, could suggest optically thin dust and low\ndust-to-gas ratios, in accordance with model predictions that LLAGN do not host\na Seyfert-like obscuring torus.\n",
        "  The goal of the presented work is to illustrate a method by which the data\nexchange between a standalone computer software and a shared database server\ncan be protected of unauthorized interceptation of the traffic in Internet\nnetwork, a transport network for data managed by those two systems,\ninterceptation by which an attacker could gain illegetimate access to the\ndatabase, threatening this way the data integrity and compromising the\ndatabase.\n",
        "  In this paper, we revisit the view update problem in a relational setting and\npropose a framework based on the notion of determinacy under constraints.\nWithin such a framework, we characterise when a view mapping is invertible,\nestablishing that this is the case precisely when each database symbol has an\nexact rewriting in terms of the view symbols under the given constraints, and\nwe provide a general effective criterion to understand whether the changes\nintroduced by a view update can be propagated to the underlying database\nrelations in a unique and unambiguous way.\n  Afterwards, we show how determinacy under constraints can be checked, and\nrewritings effectively found, in three different relevant scenarios in the\nabsence of view constraints. First, we settle the long-standing open issue of\nhow to solve the view update problem in a multi-relational database with views\nthat are projections of joins of relations, and we do so in a more general\nsetting where views are defined by arbitrary conjunctive queries and database\nconstraints are stratified embedded dependencies. Next, we study a setting\nbased on horizontal decompositions of a single database relation, where views\nare defined by selections on possibly interpreted attributes (e.g., arithmetic\ncomparisons) in the presence of domain constraints over the database schema.\nLastly, we look into another multi-relational database setting, where views are\ndefined in an expressive \"Type\" Relational Algebra based on the n-ary\nDescription Logic DLR and database constraints are inclusions of expressions in\nthat algebra.\n",
        "  Time Series Data Server (TSDS) is a software package for implementing a\nserver that provides fast super-setting, sub-setting, filtering, and uniform\ngridding of time series-like data. TSDS was developed to respond quickly to\nrequests for long time spans of data. Data may be served from a fast database,\ntypically created by aggregating granules (e.g., data files) from a remote data\nsource and storing them in a local cache that is optimized for serving time\nseries. The system was designed specifically for time series data, and is\noptimized for requests where the longest dimension of the requested data\nstructure is time. Scalar, vector, and spectrogram time series types are\nsupported. The user can interact with the server by requesting a time series, a\ndate range, and an optional filter to apply to the data. Available filters\ninclude strides, block average/minimum/maximum, exclude, and inequality.\nConstraint expressions are supported, which allow such operations as a request\nfor data from one time series when a different time series satisfied a\nspecified relationship. TSDS builds upon DAP (Data Access Protocol), NcML\n(netCDF Mark-up language) and related software libraries. In this work, we\ndescribe the current design of this server, as well as planned features and\npotential implementation strategies.\n",
        "  This manual accompanies the release of the particle data for 24 simulations\nof the EAGLE suite of cosmological hydrodynamical simulations of galaxy\nformation by the virgo consortium. It describes how to download these snapshots\nand how to extract datasets from them, emphasising the meaning of variables,\nand their units. We provide examples for extracting the particle data in\npython. This data release complements our earlier release of numerous\nintegrated properties of the galaxies in EAGLE through an SQL relational\ndatabase. This database has been updated to include the additional simulations\nthat are part of the present data release. Scientists wanting to use EAGLE may\nfind it useful to first investigate whether their analysis can be performed\nusing the database, before accessing the particle data. The particles in the\nsnapshot files are indexed by a peano-hilbert key. This allows for an eased\nextraction of simply connected spatial volumes, without needing to read the\nentire snapshot. This makes it possible to analyse many aspects of galaxies\nusing modest computing resources, even when using EAGLE simulations with large\nnumbers of particles. A reading routine is provided to simplify this process.\n",
        "  F-substituted ROBiS2 (R = La, Ce, Nd) superconducting single crystals with\ndifferent F concentration were grown successfully using CsCl/KCl flux. All the\nobtained single crystals had a plate-like shape with a well-developed ab-plane\nof 1-2 mm in size. The flux components of Cs, K, and Cl were not detected in\nthe obtained single crystals by electron probe microanalysis. The grown single\ncrystals of F-substituted LaOBiS2 and CeOBiS2 showed superconducting at about 3\nK while the Tc of the F-substituted NdOBiS2 exhibited approximately 5 K. The\nsuperconducting anisotropy of the single crystals of F-substituted LaOBiS2 and\nNdOBiS2 was estimated to be 30-45 according to the effective mass model whereas\nthose values were 13-21 for the F-substituted CeOBiS2 single crystals. The\nF-substituted CeOBiS2 single crystals exhibited magnetic order at about 7 K\nthat apparently coexisted with superconductivity below around 3 K.\n",
        "  This paper consists of three parts.\n  First, we generalize the Jaeger Formula to express the Kauffman-Vogel graph\npolynomial as a state sum of the Murakami-Ohtsuki-Yamada graph polynomial.\n  Then, we demonstrate that reversing the orientation and the color of a MOY\ngraph along a simple circuit does not change the sl(N) Murakami-Ohtsuki-Yamada\npolynomial or the sl(N) homology of this MOY graph. In fact, reversing the\norientation and the color of a component of a colored link only changes the\nsl(N) homology by an overall grading shift.\n  Finally, as an application of the first two parts, we prove that the so(6)\nKauffman polynomial is equal to the 2-colored sl(4) Reshetikhin-Turaev link\npolynomial, which implies that the 2-colored sl(4) link homology categorifies\nthe so(6) Kauffman polynomial.\n",
        "  This study presents the first observation of elastic shear waves generated in\nsoft solids using a dynamic electromagnetic field. The first and second\nexperiments of this 5 study showed that Lorentz force can induce a displacement\nin a soft phantom and that this displacement was detectable by an ultrasound\nscanner using speckle-tracking algorithms. For a 100 mT magnetic field and a 10\nms, 100 mA peak-to-peak electrical burst, the displacement reached a magnitude\nof 1 um. In the third experiment, we showed that Lorentz force can induce shear\nwaves in a phantom. A physical model 10 using electromagnetic and elasticity\nequations was proposed. Computer simulations were in good agreement with\nexperimental results. The shear waves induced by Lorentz force were used in the\nlast experiment to estimate the elasticity of a swine liver sample.\n",
        "  Considerable evidence for proximity-induced triplet superconductivity on the\nferromagnetic side of a superconductor-ferromagnet (S-F) interface now exists;\nhowever, the corresponding effect on the superconductor side has hardly been\naddressed. We have performed scanning tunneling spectroscopy measurements on\nNbN superconducting thin films proximity coupled to the half-metallic\nferromagnet La2/3Ca1/3MnO3 (LCMO) as a function of magnetic field. We have\nfound that at zero and low applied magnetic fields the tunneling spectra on NbN\ntypically show an anomalous gap structure with suppressed coherence peaks and,\nin some cases, a zero-bias conductance peak. As the field increases to the\nmagnetic saturation of LCMO where the magnetization is homogeneous, the spectra\nbecome more BCS-like and the critical temperature of the NbN increases,\nimplying a reduced proximity effect. Our results therefore suggest that\ntriplet-pairing correlations are also induced in the S side of an S-F bilayer.\n",
        "  In this paper we apply the twisted Alexander polynomial to study the fibering\nand genus detecting problems for oriented links. In particular we generalize a\nconjecture of Dunfield, Friedl and Jackson on the torsion polynomial of\nhyperbolic knots to hyperbolic links, and confirm it for an infinite family of\nhyperbolic 2-bridge links. Moreover we consider a similar problem for parabolic\nrepresentations of 2-bridge link groups.\n",
        "  A numerical hydrodynamical model for the evolution of spherically symmetric\ncollapsing clouds, designed for the calculation of the thermal structure of\nthese objects in both the prestellar and protostellar stages of their\nevolution, is presented. Distinctive features of the model include the\npossibility of independently describing the temperatures of the gas and dust,\nwhich is extremely important when calculating the thermal structure of\nprestellar and protostellar clouds, and the account of the radiation flux from\nthe central protostar. This model is used to compare the theoretical density\nand temperature distributions with observations for nearby sites of star\nformation obtained with the Herschel Space Observatory. Application of the\ndiffusion approximation with a flux limiter describes well the radial density\nand temperature distributions in protostellar clouds. However, significant\ndifferences between the model and observational density profiles were found for\nprestellar stages, suggesting the presence of appreciable deviations from\nequilibrium in the prestellar clouds. An approximate method for calculating the\nthermal structure of a cloud based on the adaptive $\\tau$-approximation is\npresented. Application of the $\\tau$-approximation yields good agreement with\nthe diffusion approximation for the prestellar phase, but produces appreciable\ndiscrepancies for the protostellar phase, when the thermal structure of the\naccreting envelope is determined by the radiation of the protostar.\n",
        "  Among iron chalcogenide superconductors, FeS can be viewed as a simple,\nhighly compressed relative of FeSe without nematic phase and with weaker\nelectronic correlations. Under pressure, however, the superconductivity of\nstoichiometric FeS disappears and reappears, forming two domes. We perform\nelectronic structure and spin fluctuation theory calculations for tetragonal\nFeS in order to analyze the nature of the superconducting order parameter. In\nthe random phase approximation we find a gap function with d-wave symmetry at\nambient pressure, in agreement with several reports of a nodal superconducting\norder parameter in FeS. Our calculations show that, as a function of pressure,\nthe superconducting pairing strength decreases until a Lifshitz transition\ntakes place at 4.6 GPa. As a hole pocket with a large density of states appears\nat the Lifshitz transition, the gap symmetry is altered to sign-changing\ns-wave. At the same time the pairing strength is severely enhanced and\nincreases up to a new maximum at 5.5 GPa. Therefore, our calculations naturally\nexplain the occurrence of two superconducting domes in FeS.\n",
        "  For lensed galaxy SGAS J111020.0+645950.8 at redshift z=2.481, which is\nmagnified by a factor of 28 +- 8, we analyze the morphology of star formation\nas traced by rest-frame ultraviolet emission, in both the highly-magnified\nsource plane, and in simulations of how this galaxy would appear without\nlensing magnification. Were this galaxy not lensed but drawn from an HST deep\nfield, we would conclude that almost all its star formation arises from an\nexponential disk (S\\'ersic index of 1.0 +- 0.4) with an effective radius of r_e\n= 2.7 +- 0.3 kpc measured from two-dimensional fitting to F606W using Galfit,\nand r_e=1.9 +- 0.1 kpc measured by fitting a radial profile to F606W elliptical\nisophotes. At the normal spatial resolution of the deep fields, there is no\nsign of clumpy star formation within SGAS J111020.0+645950.8 . However, the\nenhanced spatial resolution enabled by gravitational lensing tells a very\ndifferent story: much of the star formation arises in two dozen clumps with\nsizes of r=30--50 pc spread across the 7 kpc length of the galaxy. The color\nand spatial distribution of the diffuse component suggests that still smaller\nclumps are unresolved. Despite this clumpy, messy morphology, the radial\nprofile is still well-characterized by an exponential profile. In this lensed\ngalaxy, stars are forming in complexes with sizes well below 100 pc; such sizes\nare wholly unexplored by surveys of galaxy evolution at 1<z<3.\n",
        "  The usual formulation of the BCS ansatz for superconductivity in the grand\ncanonical ensemble makes the handling of the Pauli exclusion principle between\npaired electrons straightforward. It however tends to mask that many-body\neffects between Cooper pairs interacting through the reduced BCS potential are\nentirely controlled by this exclusion. To show it up, one has to work in the\ncanonical ensemble. Pauli blocking between a fixed number of composite bosons\nis however known to be difficult to handle. To do it, we here develop a\ncommutator formalism for Cooper pairs, along the line we used for excitons. We\nthen rederive, within the $N$-pair subspace, a few results of BCS\nsuperconductivity commonly derived in the grand canonical ensemble, to evidence\ntheir Pauli blocking origin. We end by discussing what should be called \"Cooper\npair wave function\".\n",
        "  The main objective of the electrical stimulation of the brain is to generate\naction potentials from the application of electromagnetic fields. Among the\navailable techniques, transcranial electrical stimulation (TES) represents a\npopular method of administration that has the advantage of being non-invasive\nand economically more affordable. This article aims to briefly introduce the\nreader into the understanding of TES in terms of the physics involved as well\nas for some of the relevant results of studies applying this technique.\n",
        "  An earthquake, Tohoku region Pacific Coast earthquake, occurred on the 11th\nof March, 2011, and subsequent Fukushima nuclear power plant accidents have\nbeen stirring natural radiation around the author's office in Fukushima Medical\nUniversity (FMU). FMU is located in Fukushima city, and is 57 km (35 miles)\naway from northwest of the Fukushima Daiichi nuclear power plant. This paper\npresents three types of radiation survey undertaken through the unprecedented\naccidents at the campus and the hospital of FMU. First, a group of interested\npeople immediately began radiation surveillance; the group members were\nassembled from the faculty members of \"Life Sciences and Social Medicine\" and\n\"Human and Natural Sciences\". Second, the present author, regardless of the\nearthquake, had serially observed natural radiations such as gamma radiation in\nair with NaI scintillation counter, atmospheric radon with Lucas cell, and\nsecond cosmic rays with NaI scintillation. Gamma radiation indicated most\ndrastic change, i.e., peak value (9.3 times usual level) appeared on March 16,\nand decreased to 1.7 times usual level after two months. A nonlinear least\nsquares regression to this decreasing data gave short half-life of 3.6 days and\nlong half-life of 181 days. These two apparent half-lives are attributed to two\ngroups of radioisotopes, i.e., short half-life one of I-131 and long half-life\nones of Cs-134, Cs-137 and Sr-90. Also, atmospheric radon concentration became\nhigh since a stop of ventilation, while second cosmic rays did not show any\nresponse. Third, late April, 2011, a team of radiation dosimetry under the\ndirect control of Dean, School of Medicine, was established for the\ncontinuation of radiation survey in the campus and the hospital of Fukushima\nMedical University.\n",
        "  Syntactic parsing is a key task in natural language processing. This task has\nbeen dominated by symbolic, grammar-based parsers. Neural networks, with their\ndistributed representations, are challenging these methods. In this article we\nshow that existing symbolic parsing algorithms can cross the border and be\nentirely formulated over distributed representations. To this end we introduce\na version of the traditional Cocke-Younger-Kasami (CYK) algorithm, called\nD-CYK, which is entirely defined over distributed representations. Our D-CYK\nuses matrix multiplication on real number matrices of size independent of the\nlength of the input string. These operations are compatible with traditional\nneural networks. Experiments show that our D-CYK approximates the original CYK\nalgorithm. By showing that CYK can be entirely performed on distributed\nrepresentations, we open the way to the definition of recurrent layers of\nCYK-informed neural networks.\n",
        "  We prove that for every P there is a bound B depending only on P so that the\nmapping torus of every P--small irreducible train-track map can be obtained by\nsurgery from one of B mapping tori. We show that given an integer P>0 there is\na bound $M$ depending only on P, so that there exists a presentation of the\nfundamental group of the mapping torus of a P--small irreducible train-track\nmap with less than M generators and M relations.\n",
        "  The purpose of the present study was to evaluate radiation dose reduction and\nimage quality during CT scanning by using a new dose reduction fiber sheet\n(DRFS) with commercially available bismuth shields. These DRFS were composed of\nnano-barium sulfate (BaSO4), filling the gaps left by the large oxide bismuth\n(Bi2O3) particle sizes. The radiation dose was measured five times at\ndirectionss of 12 o'clock from the center of the polymethyl methacrylate (PMMA)\nhead phantom to calculate an average value using a CT ionization chamber. The\nimage quality measured CT transverse images of the PMMA head phantom depending\non X-ray tube voltages and the type of shielding. Two regions of interest in CT\ntransverse images were chosen from the right and left areas under the surface\nof the PMMA head phantom and from ion chamber holes located at directions of 12\no'clock from the center of the PMMA head phantom. The results of this study\nshowed that the new DRFS shields could reduce dosages to 15.61%, 23.05%, and\n22.71% more in 90 kVp, 120 kVp, and 140 kVp, respectively, than with a\nconventional bismuth shield of the same thickness, while maintaining image\nquality. In addition, the DRFS were produced to about 25% more thinness than\nconventional bismuth. We concluded, therefore, that DRFS can replace the\nconventional bismuth and may be utilized as a new shield.\n",
        "  Objective: Proximal and whole-body vibrations are well studied in\nseismocardiography and ballistocardiography, yet distal vibrations are still\npoorly understood. In this paper we develop two methods to measure aortic valve\nopening (AVO) and closing (AVC) from distal vibrations. Methods: AVO and AVC\nwere detected for each heartbeat with accelerometers on the upper arm (A),\nwrist (W), and knee (K) of 22 consenting adults following isometric exercise.\nExercise-induced changes were recorded with impedance cardiography, and\nnine-beat ensemble averaging was applied. Our first method, FilterBCG, detects\npeaks in distal vibrations after filtering with individually-tuned bandpass\nfilters while RidgeBCG uses ridge regression to estimate AVO and AVC without\npeaks. Pseudocode is provided. Results: In agreement with recent studies, we\ndid not find peaks at AVO and AVC in distal vibrations, and the conventional\nR-J interval method from the literature also correlated poorly with AVO (r2 =\n0.22 A, 0.14 W, 0.12 K). Interestingly, distal vibrations filtered with\nFilterBCG resembled seismocardiogram signals and yielded reliable peaks at AVO\n(r2 = 0.95 A, 0.94 W, 0.77 K) and AVC (r2 = 0.92 A, 0.89 W, 0.68 K).\nConclusion: FilterBCG measures AVO and AVC accurately from arm, wrist, and knee\nvibrations, and it outperforms R-J intervals and RidgeBCG. Significance: To our\nknowledge, this study is the first to measure AVC accurately from distal\nvibrations. Finally, AVO timing is needed to assess cardiovascular disease risk\nwith pulse wave velocity (PWV) and for cuff-less diastolic blood pressure\nmeasurement via aortic pulse-transit time (PTT).\n",
        "  Automatic resolution of rumours is a challenging task that can be broken down\ninto smaller components that make up a pipeline, including rumour detection,\nrumour tracking and stance classification, leading to the final outcome of\ndetermining the veracity of a rumour. In previous work, these steps in the\nprocess of rumour verification have been developed as separate components where\nthe output of one feeds into the next. We propose a multi-task learning\napproach that allows joint training of the main and auxiliary tasks, improving\nthe performance of rumour verification. We examine the connection between the\ndataset properties and the outcomes of the multi-task learning models used.\n",
        "  This article concerns the following question arising in computational\nevolutionary biology. For a given subclass of phylogenetic networks, what is\nthe maximum value of 0 <= p <= 1 such that for every input set T of rooted\ntriplets, there exists some network N(T) from the subclass such that at least\np|T| of the triplets are consistent with N(T)? Here we prove that the set\ncontaining all triplets (the full triplet set) in some sense defines p, and\nmoreover that any network N achieving fraction p' for the full triplet set can\nbe converted in polynomial time into an isomorphic network N'(T) achieving >=\np' for an arbitrary triplet set T. We demonstrate the power of this result for\nthe field of phylogenetics by giving worst-case optimal algorithms for level-1\nphylogenetic networks (a much-studied extension of phylogenetic trees),\nimproving considerably upon the 5/12 fraction obtained recently by Jansson,\nNguyen and Sung. For level-2 phylogenetic networks we show that p >= 0.61. We\nnote that all the results in this article also apply to weighted triplet sets.\n",
        "  It is generally believed that the laws of thermodynamics govern\nsuperconductivity as an equilibrium state of matter. Here we point out that\nwithin the conventional BCS-London description of the normal-superconductor\ntransition in the presence of a magnetic field, the transition cannot be\nreversible, in contradiction with the thermodynamic description and with\nexperiments. This indicates that the conventional theory of superconductivity\nis internally inconsistent. We argue that to describe a reversible transition\nit is necessary to assume that charge transfer occurs across the\nnormal-superconductor phase boundary, as proposed in the theory of hole\nsuperconductiviy. This provides also a solution to the angular momentum puzzle\npointed out in previous work. Our explanation can only apply if the current\ncarriers in the normal state are holes. An experimental test of these ideas is\nproposed.\n",
        "  We present high-frequency Karl G. Jansky Very Large Array (VLA) continuum and\nspectral line (NH3, H64${\\alpha}$, and H63${\\alpha}$) observations of the\nGalactic Center Radio Arc region, covering the Sickle H II region, the\nQuintuplet cluster, and molecular clouds M0.20-0.033 and M0.10-0.08. These\nobservations show that the two velocity components of M0.20-0.033 (~25 & 80\nkm/s), previously thought to be separate clouds along the same line-of-sight,\nare physically connected in position-velocity space via a third southern\ncomponent around 50 km/s. Further position-velocity analysis of the surrounding\nregion, using lower-resolution survey observations taken with the Mopra and\nATCA telescopes, indicates that both molecular components in M0.20-0.033 are\nphysically connected to the M0.10-0.08 molecular cloud, which is suggested to\nbe located on stream 1 in the Kruijssen et al. (2015) orbital model. The\nmorphology and kinematics of the molecular gas in M0.20-0.033 indicate that the\ntwo velocity components in M0.20-0.033 constitute an expanding shell. Our\nobservations suggest that the M0.20-0.033 expanding shell has an expansion\nvelocity of 40 km/s, with a systemic velocity of 53 km/s, comparable to\nvelocities detected in M0.10-0.08. The origin of the expanding shell is located\nnear the Quintuplet cluster, suggesting that the energy and momentum output\nfrom this massive stellar cluster may have contributed to the expansion.\n",
        "  This paper presents results of our experiments for the next utterance ranking\non the Ubuntu Dialog Corpus -- the largest publicly available multi-turn dialog\ncorpus. First, we use an in-house implementation of previously reported models\nto do an independent evaluation using the same data. Second, we evaluate the\nperformances of various LSTMs, Bi-LSTMs and CNNs on the dataset. Third, we\ncreate an ensemble by averaging predictions of multiple models. The ensemble\nfurther improves the performance and it achieves a state-of-the-art result for\nthe next utterance ranking on this dataset. Finally, we discuss our future\nplans using this corpus.\n",
        "  We show that in multiband superconductors even small interband proximity\neffect can lead to a qualitative change in the interaction potential between\nsuperconducting vortices by producing long-range intervortex attraction. This\ntype of vortex interaction results in unusual response to low magnetic fields\nleading to phase separation into domains of a two-component Meissner states and\nvortex droplets.\n",
        "  We are giving tables of quasi-alternating knots with $8\\le n \\le 12$\ncrossings. As the obstructions for a knot to be quasialternating we used\nhomology thickness with regards to Khovanov homology, odd homology, and\nHeegaard-Floer homology $\\widehat{HFK}$. Except knots which are homology thick,\nso cannot be quasialternating, by using the results of our computations\n[JaSa1], for one of knots which are homology thin, knot $11n_{50}$, J. Greene\nproved that it is not quasi-alternating, so this is the first example of\nhomologically thin knot which is not quasi-alternating [Gr]. In this paper we\nprovide a few more candidates for homology thin knots for which the method used\nby J. Greene cannot be used to prove that they are not quasialternating. All\ncomputations were performed by A. Shumakovitch's program \\emph{KhoHo}, the\nprogram \\emph{Knotscape}, the package \\emph{Knot Atlas} by Dror Bar-Natan, and\nour program \\emph{LinKnot}.\n",
        "  Recent advances in 3D modeling provide us with real 3D datasets to answer\nqueries, such as \"What is the best position for a new billboard?\" and \"Which\nhotel room has the best view?\" in the presence of obstacles. These applications\nrequire measuring and differentiating the visibility of an object (target) from\ndifferent viewpoints in a dataspace, e.g., a billboard may be seen from two\nviewpoints but is readable only from the viewpoint closer to the target. In\nthis paper, we formulate the above problem of quantifying the visibility of\n(from) a target object from (of) the surrounding area with a visibility color\nmap (VCM). A VCM is essentially defined as a surface color map of the space,\nwhere each viewpoint of the space is assigned a color value that denotes the\nvisibility measure of the target from that viewpoint. Measuring the visibility\nof a target even from a single viewpoint is an expensive operation, as we need\nto consider factors such as distance, angle, and obstacles between the\nviewpoint and the target. Hence, a straightforward approach to construct the\nVCM that requires visibility computation for every viewpoint of the surrounding\nspace of the target, is prohibitively expensive in terms of both I/Os and\ncomputation, especially for a real dataset comprising of thousands of\nobstacles. We propose an efficient approach to compute the VCM based on a key\nproperty of the human vision that eliminates the necessity of computing the\nvisibility for a large number of viewpoints of the space. To further reduce the\ncomputational overhead, we propose two approximations; namely, minimum bounding\nrectangle and tangential approaches with guaranteed error bounds. Our extensive\nexperiments demonstrate the effectiveness and efficiency of our solutions to\nconstruct the VCM for real 2D and 3D datasets.\n",
        "  We discuss the observational evidences of the morphological transformation of\nSpirals into S0 galaxies in the cluster environment exploiting two big\ndatabases of galaxy clusters: WINGS (0.04 < z < 0.07) and EDisCS (0.4 < z <\n0.8). The most important results are: 1) the average number of S0 galaxies in\nclusters is almost a factor of $\\sim 3 - 4$ larger today than at redshift $z\n\\sim 1$; 2) the fraction of S0's to Spirals increases on average by a factor\n$\\sim$ 2 every Gyr; 3) the average rate of transformation for Spirals (not\nconsidering the infall of new galaxies from the cosmic web) is: $\\sim$ 5 Sp\ninto S0's per Gyr and $\\sim$ 2 Sp into E's per Gyr; 4) there are evidences that\nthe interstellar gas of Spirals is stripped by an hot intergalactic medium; 5)\nthere are also indirect hints that major/minor merging events have played a\nrole in the transformation of Spiral galaxies. In particular, we show that: 1)\nthe ratio between the number of S0's and Spirals (NS0/NSp) in the WINGS\nclusters is correlated with their X-ray luminosity $L_X$ ; 2) that the\nbrightest and massive S0's are always close to the cluster center; 3) that the\nmean Sersic index of S0's is always larger than that of Spirals (and lower than\nE's) for galaxy stellar masses above $10^9.5$ Msun; 4) that the number of E's\nin clusters cannot be constant; 5) that the largest difference between the mean\nmass of S0's and E's with respect to Spirals is observed in clusters with low\nvelocity dispersion. Finally, by comparing the properties of the various\nmorphological types for galaxies in clusters and in the field, we find that the\nmost significant effect of the environment is the stripping of the outer galaxy\nregions, resulting in a systematic difference in effective radius and Sersic\nindex.\n",
        "  A theory of dissipative nonlinear conductivity, $\\sigma_1(\\omega,H)$, of\ns-wave superconductors under strong electromagnetic fields at low temperatures\nis proposed. Closed-form expressions for $\\sigma_1(H)$ and the surface\nresistance $R_s(\\omega,H)$ are obtained in the nonequilibrium dirty limit for\nwhich $\\sigma_1(H)$ has a significant minimum as a function of a low-frequency\n$(\\hbar\\omega\\ll k_BT)$ magnetic field $H$. The calculated microwave\nsuppression of $R_s(H)$ is in good agreement with recent experiments on alloyed\nNb resonator cavities. It is shown that superimposed dc and ac fields,\n$H=H_0+H_a\\cos\\omega t$, can be used to reduce ac dissipation in thin film\nnanostructures by tuning $\\sigma_1(H_0)$ with the dc field.\n",
        "  Entity Resolution concerns identifying co-referent entity pairs across\ndatasets. A typical workflow comprises two steps. In the first step, a blocking\nmethod uses a one-many function called a blocking scheme to map entities to\nblocks. In the second step, entities sharing a block are paired and compared.\nCurrent DNF blocking scheme learners (DNF-BSLs) apply only to structurally\nhomogeneous tables. We present an unsupervised algorithmic pipeline for\nlearning DNF blocking schemes on RDF graph datasets, as well as structurally\nheterogeneous tables. Previous DNF-BSLs are admitted as special cases. We\nevaluate the pipeline on six real-world dataset pairs. Unsupervised results are\nshown to be competitive with supervised and semi-supervised baselines. To the\nbest of our knowledge, this is the first unsupervised DNF-BSL that admits RDF\ngraphs and structurally heterogeneous tables as inputs.\n",
        "  We present a measurement of the trigonometric parallax of IRAS 05168+3634\nwith VERA. The parallax is 0.532 +/- 0.053 mas, corresponding to a distance of\n1.88 +0.21/-0.17 kpc. This is significantly closer than the previous distance\nestimate of 6 kpc based on a kinematic distance measurement. This drastic\nchange in the source distance implies the need for revised values of not only\nthe physical parameters of IRAS 05168+3634, but it also implies a different\nlocation in the Galaxy, placing it in the Perseus arm rather than the Outer\narm. We also measured the proper motion of the source. A combination of the\ndistance and proper motion with the systemic velocity yields a rotation\nvelocity {\\Theta} = 227 +9/-11 km s^-1 at the source position, assuming\n{\\Theta}_0 = 240 km s^-1. Our result, combined with previous VLBI results for\nsix sources in the Perseus arm, indicates that the sources rotate\nsystematically more slowly than the Galactic rotation velocity at the local\nstandard of rest. In fact, we derive peculiar motions in the disk averaged over\nthe seven sources in the Perseus arm of (U_mean, V_mean) = (11 +/- 3, -17 +/-\n3) km s^-1, which indicates that these seven sources are moving systematically\ntoward the Galactic Center and lag behind the overall Galactic rotation.\n",
        "  NGC 4993 hosts a binary neutron star merger emitting gravitational waves and\nelectromagnetic waves, GW170817/GRB 170817A. The distance to this galaxy is not\nwell established. We select the globular cluster candidates from the Hubble\nSpace Telescope/ACS F606W images of NGC 4993 in the archive, using the\nstructural parameters of the detected sources. The radial number density\ndistribution of these candidates shows a significant central concentration\naround the galaxy center at the galactocentric distance $r<50''$, showing that\nthey are mostly the members of NGC 4993. Also the luminosity function of these\ncandidates is fit well by a Gaussian function. Therefore the selected\ncandidates at $r<50''$ are mostly considered to be globular clusters in NGC\n4993. We derive an extinction-corrected turnover Vega magnitude in the\nluminosity function of the globular clusters at $20''<r<50''$, F606W (max)$_0=\n25.36\\pm0.08$ ($V_0 =25.52\\pm0.11$)} mag. Adopting the calibration of the\nturnover magnitudes of the globular clusters, $M_V({\\rm max})=-7.58\\pm0.11$, we\nderive a distance to NGC 4993, $d=41.65\\pm3.00$ Mpc ($(m-M)_0=33.10\\pm0.16$).\nThe systematic error of this method can be as large as $\\pm0.3$ mag. This value\nis consistent with the previous distance estimates based on the fundamental\nplane relation and the gravitational wave method in the literature. The\ndistance in this study can be used to constrain the values of the parameters\nincluding the inclination angle of the binary system in the models of\ngravitational wave analysis.\n",
        "  Sharing real-time aggregate statistics of private data is of great value to\nthe public to perform data mining for understanding important phenomena, such\nas Influenza outbreaks and traffic congestion. However, releasing time-series\ndata with standard differential privacy mechanism has limited utility due to\nhigh correlation between data values. We propose FAST, a novel framework to\nrelease real-time aggregate statistics under differential privacy based on\nfiltering and adaptive sampling. To minimize the overall privacy cost, FAST\nadaptively samples long time-series according to the detected data dynamics. To\nimprove the accuracy of data release per time stamp, FAST predicts data values\nat non-sampling points and corrects noisy observations at sampling points. Our\nexperiments with real-world as well as synthetic data sets confirm that FAST\nimproves the accuracy of released aggregates even under small privacy cost and\ncan be used to enable a wide range of monitoring applications.\n",
        "  The length and pattern speed of the Milky Way bar are still controversial.\nPhotometric and spectroscopic surveys of the inner Galaxy, as well as gas\nkinematics, favour a long and slowly rotating bar, with corotation around a\nGalactocentric radius of 6 kpc. On the other hand, the existence of the\nHercules stream in local velocity space favours a short and fast bar with\ncorotation around 4 kpc. This follows from the fact that the Hercules stream\nlooks like a typical signature of the outer Lindblad resonance of the bar. As\nwe showed recently, reconciling this local stream with a slow bar would need to\nfind a yet unknown alternative explanation, based for instance on the effect of\nspiral arms. Here, by combining the TGAS catalogue of the Gaia DR1 with LAMOST\nradial velocities, we show that the position of Hercules in velocity space as a\nfunction of radius in the outer Galaxy indeed varies exactly as predicted by\nfast bar models with a pattern speed no less than 1.8 times the circular\nfrequency at the Sun's position.\n",
        "  We study tidal features (TFs) around galaxies in the REsolved Spectroscopy of\na Local VolumE (RESOLVE) survey. Our sample consists of 1048 RESOLVE galaxies\nthat overlap with the DECam Legacy Survey, which reaches an r-band 3$\\sigma$\ndepth of $\\sim$27.9 mag arcsec$^{-2}$ for a 100 arcsec$^{2}$ feature. Images\nwere masked, smoothed, and inspected for TFs like streams, shells, or\ntails/arms. We find TFs in 17$^{\\pm 2} \\%$ of our galaxies, setting a lower\nlimit on the true frequency. The frequency of TFs in the gas-poor\n(gas-to-stellar mass ratio $<$ 0.1) subsample is lower than in the gas-rich\nsubsample (13$^{\\pm 3} \\%$ vs. 19$^{\\pm 2} \\%$). Within the gas-poor subsample,\ngalaxies with TFs have higher stellar and halo masses, $\\sim 3\\times$ closer\ndistances to nearest neighbors (in the same group), and possibly fewer group\nmembers at fixed halo mass than galaxies without TFs, but similar specific star\nformation rates. These results suggest TFs in gas-poor galaxies are typically\nstreams/shells from dry mergers or satellite disruption. In contrast, the\npresence of TFs around gas-rich galaxies does not correlate with stellar or\nhalo mass, suggesting these TFs are often tails/arms from resonant\ninteractions. Similar to TFs in gas-poor galaxies, TFs in gas-rich galaxies\nimply 1.7x closer nearest neighbors in the same group; however, TFs in gas-rich\ngalaxies are associated with diskier morphologies, higher star formation rates,\nand higher gas content. In addition to interactions with known neighbors, we\nsuggest that TFs in gas-rich galaxies may arise from accretion of cosmic gas\nand/or gas-rich satellites below the survey limit.\n",
        "  The type-theoretic modelling of DRT that [degroote06] proposed features\ncontinuations for the management of the context in which a clause has to be\ninterpreted. This approach, while keeping the standard definitions of\nquantifier scope, translates the rules of the accessibility constraints of\ndiscourse referents inside the semantic recipes. In this paper, we deal with\nadditional rules for these accessibility constraints. In particular in the case\nof discourse referents introduced by proper nouns, that negation does not\nblock, and in the case of rhetorical relations that structure discourses. We\nshow how this continuation-based approach applies to those accessibility\nconstraints and how we can consider the parallel management of various\nprinciples.\n",
        "  From the application point of view, large critical current densities Jc (H)\nfor superconducting wires are required, preferably for magnetic fields higher\nthan 5 T. Here we show that strong c-axis textured Sr1-xKxFe2As2 tapes with\nnearly isotropic transport Jc were fabricated by an ex-situ powder-in-tube\n(PIT) process. At 4.2 K, the Jc values show extremely weak magnetic field\ndependence and reach high values of 1.7x10^4 A/cm^2 at 10 T and 1.4x10^4 A/cm^2\nat 14 T, respectively, these values are by far the highest ever reported for\niron based wires and approach the Jc level desired for practical applications.\nTransmission electron microscopy investigations revealed that amorphous oxide\nlayers at grain boundaries were significantly reduced by Sn addition which\nresulted in greatly improved intergranular connectivity. Our results\ndemonstrated the strong potential of using iron based superconductors for high\nfield applications.\n",
        "  The center of the Milky Way hosts a massive black hole. The observational\nevidence for its existence is overwhelming. The compact radio source Sgr A* has\nbeen associated with a black hole since its discovery. In the last decade,\nhigh-resolution, near-infrared measurements of individual stellar orbits in the\ninnermost region of the Galactic Center have shown that at the position of Sgr\nA* a highly concentrated mass of 4 x 10^6 M_sun is located. Assuming that\ngeneral relativity is correct, the conclusion that Sgr A* is a massive black\nhole is inevitable. Without doubt this is the most important application of\nstellar orbits in the Galactic Center. Here, we discuss the possibilities going\nbeyond the mass measurement offered by monitoring these orbits. They are an\nextremely useful tool for many scientific questions, such as a geometric\ndistance estimate to the Galactic Center or the puzzle, how these stars reached\ntheir current orbits. Future improvements in the instrumentation will open up\nthe route to testing relativistic effects in the gravitational potential of the\nblack hole, allowing to take full advantage of this unique laboratory for\ncelestial mechanics.\n",
        "  Gibson et al. (2017) argued that color naming is shaped by patterns of\ncommunicative need. In support of this claim, they showed that color naming\nsystems across languages support more precise communication about warm colors\nthan cool colors, and that the objects we talk about tend to be warm-colored\nrather than cool-colored. Here, we present new analyses that alter this\npicture. We show that greater communicative precision for warm than for cool\ncolors, and greater communicative need, may both be explained by perceptual\nstructure. However, using an information-theoretic analysis, we also show that\ncolor naming across languages bears signs of communicative need beyond what\nwould be predicted by perceptual structure alone. We conclude that color naming\nis shaped both by perceptual structure, as has traditionally been argued, and\nby patterns of communicative need, as argued by Gibson et al. - although for\nreasons other than those they advanced.\n",
        "  We report new observations of SL2SJ021737-051329, a lens system consisting of\na bright arc at z=1.84435, magnified ~17x by a massive galaxy at z=0.65.\nSL2SJ0217 is a low-mass (M <10^9 M*), low-metallicity (Z~1/20 Z*) galaxy, with\nextreme star-forming conditions that produce strong nebular UV emission lines\nin the absence of any apparent outflows. Here we present several notable\nfeatures from rest-frame UV Keck/LRIS spectroscopy: (1) Very strong narrow\nemission lines are measured for CIV 1548,1550, HeII 1640, OIII] 1661,1666,\nSiIII] 1883,1892, and CIII] 1907,1909. (2) Double-peaked LyA emission is\nobserved with a dominant blue peak and centered near the systemic velocity. (3)\nThe low- and high-ionization absorption features indicate very little or no\noutflowing gas along the sightline to the lensed galaxy. The relative emission\nline strengths can be reproduced with a very high-ionization, low-metallicity\nstarburst with binaries, with the exception of He \\ii, which indicates an\nadditional ionization source is needed. We rule out large contributions from\nAGN and shocks to the photoionization budget, suggesting that the emission\nfeatures requiring the hardest radiation field likely result from extreme\nstellar populations that are beyond the capabilities of current models.\nTherefore, SL2S0217 serves as a template for the extreme conditions that are\nimportant for reionization and thought to be more common in the early Universe.\n",
        "  A number of reconstruction methods have been proposed recently for\naccelerated functional Magnetic Resonance Imaging (fMRI) data collection.\nHowever, existing methods suffer with the challenge of greater artifacts at\nhigh acceleration factors. This paper addresses the issue of accelerating fMRI\ncollection via undersampled k-space measurements combined with the proposed\nDouble Temporal Sparsity based Reconstruction (DTSR) method with the l1 -l1\nnorm constraint. The robustness of the proposed DTSR method has been thoroughly\nevaluated both at the subject level and at the group level on real fMRI data.\nResults are presented at various acceleration factors. Quantitative analysis in\nterms of Peak Signal-to-Noise Ratio (PSNR) and other metrics, and qualitative\nanalysis in terms of reproducibility of brain Resting State Networks (RSNs)\ndemonstrate that the proposed method is accurate and robust. In addition, the\nproposed DTSR method preserves brain networks that are important for studying\nfMRI data. Compared to the existing accelerated fMRI reconstruction methods,\nthe DTSR method shows promising potential with an improvement of 10-12dB in\nPSNR with acceleration factors upto 3.5. Simulation results on real data\ndemonstrate that DTSR method can be used to acquire accelerated fMRI with\naccurate detection of RSNs.\n",
        "  In comparison with document summarization on the articles from social media\nand newswire, argumentative zoning (AZ) is an important task in scientific\npaper analysis. Traditional methodology to carry on this task relies on feature\nengineering from different levels. In this paper, three models of generating\nsentence vectors for the task of sentence classification were explored and\ncompared. The proposed approach builds sentence representations using learned\nembeddings based on neural network. The learned word embeddings formed a\nfeature space, to which the examined sentence is mapped to. Those features are\ninput into the classifiers for supervised classification. Using\n10-cross-validation scheme, evaluation was conducted on the\nArgumentative-Zoning (AZ) annotated articles. The results showed that simply\naveraging the word vectors in a sentence works better than the paragraph to\nvector algorithm and by integrating specific cuewords into the loss function of\nthe neural network can improve the classification performance. In comparison\nwith the hand-crafted features, the word2vec method won for most of the\ncategories. However, the hand-crafted features showed their strength on\nclassifying some of the categories.\n",
        "  Purpose: Different multichannel methods for film dosimetry have been proposed\nin the literature. Two of them are the weighted mean method and the method put\nforth by Micke et al and Mayer et al. The purpose of this work was to compare\ntheir results and to develop a generalized channel-independent perturbations\nframework in which both methods enter as special cases.\n  Methods: Four models of channel-independent perturbations were compared:\nweighted mean, Micke-Mayer method, uniform distribution and truncated normal\ndistribution. A closed-form formula to calculate film doses and the associated\nType B uncertainty for all four models was deduced. To evaluate the models,\nfilm dose distributions were compared with planned and measured dose\ndistributions. At the same time, several elements of the dosimetry process were\ncompared: film type EBT2 versus EBT3, different waiting-time windows,\nreflection mode versus transmission mode scanning, and planned versus measured\ndose distribution for film calibration and for gamma-index analysis. The\nmethods and the models described in this study are publicly accessible through\nIRISEU. Alpha 1.1 (http://www.iriseu.com). IRISEU. is a cloud computing web\napplication for calibration and dosimetry of radiochromic films.\n  Results: The truncated normal distribution model provided the best agreement\nbetween film and reference doses, both for calibration and gamma-index\nverification, and proved itself superior to both the weighted mean model, which\nneglects correlations between the channels, and the Micke-Mayer model, whose\naccuracy depends on the properties of the sensitometric curves.\n  Conclusions: The truncated normal distribution model of channel-independent\nperturbations was found superior to the other three models under comparison and\nwe propose its use for multichannel dosimetry.\n",
        "  Dominance and subordinate behaviours are important ingredients in the social\norganizations of group living animals. Behavioural observations on the two\neusocial species \\textit{Ropalidia marginata} and \\textit{Ropalidia\ncyathiformis} suggest varying complexities in their social systems. The queen\nof R. cyathiformis is an aggressive individual who usually holds the top\nposition in the dominance hierarchy although she does not necessarily show the\nmaximum number of acts of dominance, while the R. marginata queen rarely shows\naggression and usually does not hold the top position in the dominance\nhierarchy of her colony. These differences are reflected in the distribution of\ndominance-subordinate interactions among the hierarchically ranked individuals\nin both the species. The percentage of dominance interactions decrease\ngradually with hierarchical ranks in R. marginata while in R. cyathiformis it\nfirst increases and then decreases. We use an agent-based model to investigate\nthe underlying mechanism that could give rise to the observed patterns for both\nthe species. The model assumes, besides some non-interacting individuals, that\nthe interaction probabilities of the agents depend on their pre-differentiated\nwinning abilities. Our simulations show that if the queen takes up a strategy\nof being involved in a moderate number of dominance interactions, one could get\nthe pattern similar to R. cyathiformis, while taking up the strategy of very\nlow interactions by the queen could lead to the pattern of R. marginata. We\ninfer that both the species follow a common interaction pattern, while the\ndifferences in their social organization are due to the slight changes in queen\nas well as worker strategies. These changes in strategies are expected to\naccompany the evolution of more complex societies from simpler ones.\n",
        "  Mining medical datasets is a challenging problem before data mining\nresearchers as these datasets have several hidden challenges compared to\nconventional datasets.Starting from the collection of samples through field\nexperiments and clinical trials to performing classification,there are numerous\nchallenges at every stage in the mining process. The preprocessing phase in the\nmining process itself is a challenging issue when, we work on medical datasets.\nOne of the prime challenges in mining medical datasets is handling missing\nvalues which is part of preprocessing phase. In this paper, we address the\nissue of handling missing values in medical dataset consisting of categorical\nattribute values. The main contribution of this research is to use the proposed\nimputation measure to estimate and fix the missing values. We discuss a case\nstudy to demonstrate the working of proposed measure.\n",
        "  Building interactive tools to support data analysis is hard because it is not\nalways clear what to build and how to build it. To address this problem, we\npresent Precision Interfaces, a semi-automatic system to generate task-specific\ndata analytics interfaces. Precision Interface can turn a log of executed\nprograms into an interface, by identifying micro-variations between the\nprograms and mapping them to interface components. This paper focuses on SQL\nquery logs, but we can generalize the approach to other languages. Our system\noperates in two steps: it first build an interaction graph, which describes how\nthe queries can be transformed into each other. Then, it finds a set of UI\ncomponents that covers a maximal number of transformations. To restrict the\ndomain of changes to be detected, our system uses a domain-specific language,\nPILang. We give a full description of Precision Interface's components,\nshowcase an early prototype on real program logs and discuss future research\nopportunities.\n",
        "  1) Micro-evolutionary predictions are complicated by ecological feedbacks\nlike density dependence, while ecological predictions can be complicated by\nevolutionary change. A widely used approach in micro-evolution, quantitative\ngenetics, struggles to incorporate ecological processes into predictive models,\nwhile structured population modelling, a tool widely used in ecology, rarely\nincorporates evolution explicitly.\n  2) In this paper we develop a flexible, general framework that links\nquantitative genetics and structured population models. We use the quantitative\ngenetic approach to write down the phenotype as an additive map. We then\nconstruct integral projection models for each component of the phenotype. The\ndynamics of the distribution of the phenotype are generated by combining\ndistributions of each of its components. Population projection models can be\nformulated on per generation or on shorter time steps.\n  3) We introduce the framework before developing example models with\nparameters chosen to exhibit specific dynamics. These models reveal (i) how\nevolution of a phenotype can cause populations to move from one dynamical\nregime to another (e.g. from stationarity to cycles), (ii) how additive genetic\nvariances and covariances (the G matrix) are expected to evolve over multiple\ngenerations, (iii) how changing heritability with age can maintain additive\ngenetic variation in the face of selection and (iii) life history, population\ndynamics, phenotypic characters and parameters in ecological models will change\nas adaptation occurs.\n  4) Our approach unifies population ecology and evolutionary biology providing\na framework allowing a very wide range of questions to be addressed. The next\nstep is to apply the approach to a variety of laboratory and field systems.\nOnce this is done we will have a much deeper understanding of eco-evolutionary\ndynamics and feedbacks.\n",
        "  Time-dependent distribution of the global extinction of megafauna is compared\nwith the growth of human population. There is no correlation between the two\nprocesses. Furthermore, the size of human population and its growth rate were\nfar too small to have any significant impact on the environment and on the life\nof megafauna.\n",
        "  In this work some possible applications of negative permeability magnetic\nmetamaterial lenses for magnetic resonance imaging (MRI) are analyzed.\nMetamaterials are artificial composites designed to have a given permittivity\nand/or permeability, including negative values for these constants. It is shown\nthat using magnetic metamaterials lenses it is possible to manipulate the\nspatial distribution of the radio-frequency (RF) field used in MR systems and,\nunder some circumstances, improve the sensitivity of surface coils. Furthermore\na collimation of the RF field, phenomenon that may find application in parallel\nimaging, is presented. MR images of real tissues are shown in order to prove\nthe suitability of the theoretical analysis for practical applications.\n",
        "  Background: Data fitting approaches to modeling allow for parametric formulae\nthat may not reveal the physical quantities involved and their influence on the\nfunction being studied. In this paper the author models the approach to\nequilibrium function by a method that allows the physical quantities to be\ndefined beforehand, allows their influence to be studied, and can be used to\npredict how each physical quantity affects approach to equilibrium. Methods: An\nordinary differential equation (ODE) is used to model the approach to\nequilibrium function for the case where collimation is changed at fixed\ndetector size. A parallel model is used to study the approach to equilibrium as\na function of detector size for fixed collimation. Both models are validated by\nexperimental measurements in a 60cm body phantom. Influence of detector size is\nsimulated by using leaded sleeves of varying sizes wrapped around a 100mm\npencil chamber. This creates sleeve gaps of 10 - 60mm around the chamber. A\n100mm detector size is the pencil chamber without a leaded sleeve. Results:\nModel accurately describes the experimental data for the cases studied. Except\nfor the smallest collimation of 5mm, linear regression analysis shows good\ncorrelation between data and model according to the R^2 values at all\ncollimations. Further analysis of the model reveals minimum detector size that\nwill integrate 98% of the signal for a given collimation is, d=4.95L.\nAdditional analysis shows the minimum collimation at which a given detector\nwill collect 98% of the signal is, L=5.49d. Conclusion: Modeling by\ndifferential equations is an accurate method for the approach to equilibrium\nfunction, and has advantages over curve fitting methods.\n",
        "  Differential dependencies (DDs) capture the relationships between data\ncolumns of relations. They are more general than functional dependencies (FDs)\nand and the difference is that DDs are defined on the distances between values\nof two tuples, not directly on the values. Because of this difference, the\nalgorithms for discovering FDs from data find only special DDs, not all DDs and\ntherefore are not applicable to DD discovery. In this paper, we propose an\nalgorithm to discover DDs from data following the way of fixing the left hand\nside of a candidate DD to determine the right hand side. We also show some\nproperties of DDs and conduct a comprehensive analysis on how sampling affects\nthe DDs discovered from data.\n",
        "  Magnetic particle imaging (MPI) is a tomographic method to determine the\nspatio-temporal distribution of magnetic nanoparticles. In this document, a\nfile format for the standardized storage of MPI and magnetic particle\nspectroscopy (MPS) data is introduced. The aim of the Magnetic Particle Imaging\nData Format (MDF) is to provide a coherent way of exchanging MPI and MPS data\nacquired with different devices worldwide. The focus of the MDF is on sequence\nparameters, measurement data, calibration data, and reconstruction data. The\nformat is based on the hierarchical document format in version 5 (HDF5). This\ndocument discusses the MDF version 2.1.0, which is not backward compatible with\nversion 1.x.y.\n",
        "  We expect manifolds obtained by Dehn filling to inherit properties from the\nknot manifold. To what extent does that hold true for the Heegaard structure?\nWe study four changes to the Heegaard structure that may occur after filling:\n(1) Heegaard genus decreases, (2) a new Heegaard surface is created, (3) a\nnon-stabilized Heegaard surface destabilizes, and (4) two or more non-isotopic\nHeegaard surfaces become isotopic. We survey general results that give quite\nsatisfactory restrictions to phenomena (1) and (2) and, in a parallel thread,\ngive a complete classification of when all four phenomena occur when filling\nmost torus knot exteriors. This latter thread yields sufficient (and perhaps\nnecessary) conditions for the occurrence of phenomena (3) and (4).\n",
        "  Typical legacy information systems store data in relational databases.\nProcess mining is a research discipline that analyzes this data to obtain\ninsights into processes. Many different process mining techniques can be\napplied to data. In current techniques, an XES event log serves as a basis for\nanalysis. However, because of the static characteristic of an XES event log, we\nneed to create one XES file for each process mining question, which leads to\noverhead and inflexibility. As an alternative, people attempt to perform\nprocess mining directly on the data source using so-called intermediate\nstructures. In previous work, we investigated methods to build intermediate\nstructures on source data by executing a basic SQL query on the database.\nHowever, the nested form in the SQL query can cause performance issues on the\ndatabase side. Therefore, in this paper, we propose a native SQL operator for\ndirect process discovery on relational databases. We define a native operator\nfor the simplest form of the intermediate structure, called the \"directly\nfollows relation\". This approach has been evaluated with big event data and the\nexperimental results show that it performs faster than the state-of-the-art of\ndatabase approaches.\n",
        "  To analyze complex phenomena which involve moving objects, Trajectory Data\nWarehouse (TDW) seems to be an answer for many recent decision problems related\nto various professions (physicians, commercial representatives, transporters,\necologists ...) concerned with mobility. This work aims to make trajectories as\na first class concept in the trajectory data conceptual model and to design a\nTDW, in which data resulting from mobile information collectors' trajectory are\ngathered. These data will be analyzed, according to trajectory characteristics,\nfor decision making purposes, such as new products commercialization, new\ncommerce implementation, etc.\n",
        "  We announce a shared task on UCCA parsing in English, German and French, and\ncall for participants to submit their systems. UCCA is a cross-linguistically\napplicable framework for semantic representation, which builds on extensive\ntypological work and supports rapid annotation. UCCA poses a challenge for\nexisting parsing techniques, as it exhibits reentrancy (resulting in DAG\nstructures), discontinuous structures and non-terminal nodes corresponding to\ncomplex semantic units. Given the success of recent semantic parsing shared\ntasks (on SDP and AMR), we expect the task to have a significant contribution\nto the advancement of UCCA parsing in particular, and semantic parsing in\ngeneral. Furthermore, existing applications for semantic evaluation that are\nbased on UCCA will greatly benefit from better automatic methods for UCCA\nparsing. The competition website is\nhttps://competitions.codalab.org/competitions/19160\n",
        "  Privacy-preserving record linkage (PPRL) aims at integrating sensitive\ninformation from multiple disparate databases of different organizations. PPRL\napproaches are increasingly required in real-world application areas such as\nhealthcare, national security, and business. Previous approaches have mostly\nfocused on linking only two databases as well as the use of a dedicated linkage\nunit. Scaling PPRL to more databases (multi-party PPRL) is an open challenge\nsince privacy threats as well as the computation and communication costs for\nrecord linkage increase significantly with the number of databases. We thus\npropose the use of a new encoding method of sensitive data based on Counting\nBloom Filters (CBF) to improve privacy for multi-party PPRL. We also\ninvestigate optimizations to reduce communication and computation costs for\nCBF-based multi-party PPRL with and without the use of a dedicated linkage\nunit. Empirical evaluations conducted with real datasets show the viability of\nthe proposed approaches and demonstrate their scalability, linkage quality, and\nprivacy protection.\n",
        "  In this note we prove that for any finite quandle $X$ and any 2-cocycle\n$\\phi\\in Z^2_{Q\\pm}(X; \\mathds{Z})$, the cocycle invariant\n$\\Phi^{\\pm}_{\\phi}(K)$ is trivial for all knots $K$.\n",
        "  The superconducting and ground state samples of PrFeAsO0.8F0.2 and PrFeAsO\nhave been synthesised via easy and versatile single step solid state reaction\nroute. X-ray & Reitveld refine parameters of the synthesised samples are in\ngood agreement to the earlier reported value of the structure. The ground state\nof the pristine compound (PrFeAsO) exhibited a metallic like step in\nresistivity below 150K followed by another step at 12K. The former is\nassociated with the spin density wave (SDW) like ordering of Fe spins and later\nto the anomalous magnetic ordering for Pr moments. Both the resistivity\nanomalies are absent in case of superconducting PrFeAsO0.8F0.2 sample. Detailed\nhigh field (up to 12Tesla) electrical and magnetization measurements are\ncarried out for superconducting PrFeAsO0.8F0.2 sample. The PrFeAsO0.8F0.2\nexhibited superconducting onset (Tconset) at around 47K with Tc({\\rho} =0) at\n38K. Though the Tconset remains nearly invariant, the Tc({\\rho} =0) is\ndecreased with applied field, and the same is around 23K under applied field of\n12Tesla. The upper critical field (Hc2) is estimated from the Ginzburg Landau\nequation (GL) fitting, which is found to be ~ 182Tesla. Critical current\ndensity (Jc) being calculated from high field isothermal magnetization (MH)\nloops with the help of Beans critical state model, is found to be of the order\nof 103 A/cm2. Summarily, the superconductivity characterization of single step\nsynthesised PrFeAsO0.8F0.2 superconductor is presented.\n",
        "  Purpose: Current inverse planning methods for IMRT are limited because they\nare not designed to explore the trade-offs between the competing objectives\nbetween the tumor and normal tissues. Our goal was to develop an efficient\nmultiobjective optimization algorithm that was flexible enough to handle any\nform of objective function and that resulted in a set of Pareto optimal plans.\n  Methods: We developed a hierarchical evolutionary multiobjective algorithm\ndesigned to quickly generate a diverse Pareto optimal set of IMRT plans that\nmeet all clinical constraints and reflect the trade-offs in the plans. The top\nlevel of the hierarchical algorithm is a multiobjective evolutionary algorithm\n(MOEA). The genes of the individuals generated in the MOEA are the parameters\nthat define the penalty function minimized during an accelerated deterministic\nIMRT optimization that represents the bottom level of the hierarchy. The MOEA\nincorporates clinical criteria to restrict the search space through protocol\nobjectives and then uses Pareto optimality among the fitness objectives to\nselect individuals.\n  Results: Acceleration techniques implemented on both levels of the\nhierarchical algorithm resulted in short, practical runtimes for optimizations.\nThe MOEA improvements were evaluated for example prostate cases with one target\nand two OARs. The modified MOEA dominated 11.3% of plans using a standard\ngenetic algorithm package. By implementing domination advantage and protocol\nobjectives, small diverse populations of clinically acceptable plans that were\nonly dominated 0.2% by the Pareto front could be generated in a fraction of an\nhour.\n  Conclusions: Our MOEA produces a diverse Pareto optimal set of plans that\nmeet all dosimetric protocol criteria in a feasible amount of time. It\noptimizes not only beamlet intensities but also objective function parameters\non a patient-specific basis.\n",
        "  Much of philosophical logic and all of philosophy of language make empirical\nclaims about the vernacular natural language. They presume semantics under\nwhich `and' and `or' are related by the dually paired distributive and\nabsorption laws. However, at least one of each pair of laws fails in the\nvernacular. `Implicature'-based auxiliary theories associated with the\nprogramme of H.P. Grice do not prove remedial. Conceivable alternatives that\nmight replace the familiar logics as descriptive instruments are briefly noted:\n(i) substructural logics and (ii) meaning composition in linear algebras over\nthe reals, occasionally constrained by norms of classical logic. Alternative\n(ii) locates the problem in violations of one of the idempotent laws. Reasons\nfor a lack of curiosity about elementary and easily testable implications of\nthe received theory are considered. The concept of `reflective equilibrium' is\ncritically examined for its role in reconciling normative desiderata and\ndescriptive commitments.\n",
        "  We present Jansky Very Large Array observations of 20 - 37 GHz absorption\nlines from nearby Galactic diffuse molecular gas seen against four\ncosmologically-distant compact radio continuum sources. The main new\nobservational results are that \\linearC3H\\ and \\methCN\\ are ubiqitous in the\nlocal diffuse molecular interstellar medium at \\AV\\ $\\la 1$ while HC$_3$N was\nseen only toward B0415 at \\AV\\ $>$ 4 mag. The linear/cyclic ratio is much\nlarger in C$_3$H than in C$_3$\\HH\\ and the ratio \\methCN/HCN is enhanced\ncompared to TMC-1, although not as much as toward the Horsehead Nebula. More\nconsequentially, this work completes a long-term program assessing the\nabundances of small hydrocarbons (CH, \\cch, linear and cyclic C$_3$H and\nC$_3$\\HH, and \\cfh\\ and \\cfhm) and the CN-bearing species (CN, HCN, HNC,\nHC$_3$N, HC$_5$N and CH$_3$CN): their systematics in diffuse molecular gas are\npresented in detail here. We also observed but did not strongly constrain the\nabundances of a few oxygen-bearing species, most prominently HNCO. We set\nlimits on the column density of C\\HH CN, such that the anion C\\HH CN\\m\\ is only\nviable as a carrier of diffuse interstellar bands if the N(C\\HH CN)/N(C\\HH\nCN\\m) abundance ratio is much smaller in this species than in any others for\nwhich the anion has been observed. We argue that complex organic molecules are\nnot present in clouds meeting a reasonable definition of diffuse molecular gas,\nie \\AV\\ $\\la 1$ mag.\n",
        "  We highlight several analogies between the Finsler (infinitesimal) properties\nof Teichm\\\"uller's metric and Thurston's asymmetric metric on Teichm\\\"uller\nspace. Thurston defined his asymmetric metric in analogy with Teichm\\\"ullers'\nmetric, as a solution to an extremal problem, which consists, in the case of\nthe asymmetric metric, of finding the best Lipschitz maps in the hoomotopy\nclass of homeomorphisms between two hyperbolic surface. (In the Teichm\\\"uller\nmetric case, one searches for the best quasiconformal map between two conformal\nsurfaces.) It turns out also that some properties of Thurston's asymmetric\nmetric can be used to get new insight into Teichm\\\"uller's metric. In this\ndirection, in analogy with Thurston's formula for the Finsler norm of a vector\nfor the asymmetric metric that uses the hyperbolic length function, we give a\nnew formula for the Finsler norm of a vector for the Teichm\\\"uller metric that\nuses the extremal length function. We also describe an embedding of projective\nmeasured foliation space in the cotangent space to Teichm\\\"uller space whose\nimage is the boundary of the dual of the unit ball in the tangent space\nrepresenting vectors of norm one for the Finsler structure associated to the\nTeichm\\\"uller metric.\n",
        "  Purpose: Investigation of the feasibility of the R2* mapping techniques by\nusing latest theoretical models corrected for confounding factors and optimized\nfor signal to noise ratio. Theory and Methods: The improvement of the\nperformance of state of the art MRI relaxometry algorithms is challenging\nbecause of a non-negligible bias and still unresolved numerical instabilities.\nHere, R2* mapping reconstructions, including complex-fitting with\nmulti-spectral fat-correction by using single-decay (1D) and double-decay (2D)\nformulation, are studied in order to identify optimal configuration parameters\nand minimize numerical artifacts. The effects of echo number, echo spacing, and\nfat/water relaxation model are evaluated through simulated and in-vivo data. We\nalso explore the stability of such models by analyzing the impact of high\npercentage of fat infiltrations and local transverse relaxation differences\namong biological species. Results: The main limits of the MRI relaxometry are\nthe presence of bias and the occurrence of artifacts which affect its accuracy.\nChemical-shift complex reconstructions R2*-corrected with 1D formulation\nexhibit a large bias in presence of a significant difference in the relaxation\nrates of fat and water and with fat concentration larger than 30%. We find that\nfor fat-dominated tissues or in patients affected by iron overload, MRI\nreconstructions accounting for multi-exponential relaxation time provide\naccurate R2* measurements and are less prone to numerical artifacts.\nConclusions: Complex fitting 2D formulation outperforms the conventional 1D\napproximation in various diagnostic scenarios. Although it still lacks of\nnumerical stability which requires model enhancement and support from\nspectroscopy, it offers promising perspectives for the development of\nrelaxometry as a reliable tool to improve tissue characterization and\nmonitoring of neuromuscular disorders.\n",
        "  The ambition of a character recognition system is to transform a text\ndocument typed on paper into a digital format that can be manipulated by word\nprocessor software Unlike other languages, Arabic has unique features, while\nother language doesn't have, from this language these are seven or eight\nlanguage such as ordo, jewie and Persian writing, Arabic has twenty eight\nletters, each of which can be linked in three different ways or separated\ndepending on the case. The difficulty of the Arabic handwriting recognition is\nthat, the accuracy of the character recognition which affects on the accuracy\nof the word recognition, in additional there is also two or three from for each\ncharacter, the suggested solution by using artificial neural network can solve\nthe problem and overcome the difficulty of Arabic handwriting recognition.\n",
        "  Nuclear magnetic resonance spectroscopy (MRS) allows for the determination of\natomic structures and concentrations of different chemicals in a biochemical\nsample of interest. MRS is used in vivo clinically to aid in the diagnosis of\nseveral pathologies that affect metabolic pathways in the body. Typically, this\nexperiment produces a one dimensional (1D) 1H spectrum containing several peaks\nthat are well associated with biochemicals, or metabolites. However, since many\nof these peaks overlap, distinguishing chemicals with similar atomic structures\nbecomes much more challenging. One technique capable of overcoming this issue\nis the localized correlated spectroscopy (L-COSY) experiment, which acquires a\nsecond spectral dimension and spreads overlapping signal across this second\ndimension. Unfortunately, the acquisition of a two dimensional (2D)\nspectroscopy experiment is extremely time consuming. Furthermore, quantitation\nof a 2D spectrum is more complex. Recently, artificial intelligence has emerged\nin the field of medicine as a powerful force capable of diagnosing disease,\naiding in treatment, and even predicting treatment outcome. In this study, we\nutilize deep learning to: 1) accelerate the L-COSY experiment and 2) quantify\nL-COSY spectra. We demonstrate that our deep learning model greatly outperforms\ncompressed sensing based reconstruction of L-COSY spectra at higher\nacceleration factors. Specifically, at four-fold acceleration, our method has\nless than 5% normalized mean squared error, whereas compressed sensing yields\n20% normalized mean squared error. We also show that at low SNR (25% noise\ncompared to maximum signal), our deep learning model has less than 8%\nnormalized mean squared error for quantitation of L-COSY spectra. These pilot\nsimulation results appear promising and may help improve the efficiency and\naccuracy of L-COSY experiments in the future.\n",
        "  We present new large field observations of molecular clouds with NANTEN2\ntoward the super star cluster NGC3603 in the transitions 12CO(J=2-1, J=1-0) and\n13CO(J=2-1, J=1-0). We suggest that two molecular clouds at 13 km s-1 and 28 km\ns-1 are associated with NGC3603 as evidenced by higher temperatures toward the\nH II region as well as morphological correspondence. The mass of the clouds is\ntoo small to gravitationally bind them, given their relative motion of ~20 km\ns-1. We suggest that the two clouds collided with each other a Myr ago to\ntrigger the formation of the super star cluster. This scenario is able to\nexplain the origin of the highest mass stellar population in the cluster which\nis as young as a Myr and is segregated within the central sub-pc of the\ncluster. This is the second super star cluster along side Westerlund2 where\nformation may have been triggered by a cloud-cloud collision.\n",
        "  We derive distance, density and metallicity distribution of the stellar\nMonoceros Overdensity (MO) in the outer Milky Way, based on deep imaging with\nthe Subaru Telescope. We applied CMD fitting techniques in three stripes at\ngalactic longitudes: l=130 deg, 150 deg, 170 deg; and galactic latitudes: +15 <\nb [deg] < +25 . The MO appears as a wall of stars at a heliocentric distance of\n~ 10.1\\pm0.5 kpc across the observed longitude range with no distance change.\nThe MO stars are more metal rich ([Fe/H] ~ -1.0) than the nearby stars at the\nsame latitude. These data are used to test three different models for the\norigin of the MO: a perturbed disc model, which predicts a significant drop in\ndensity adjacent to the MO that is not seen; a basic flared disc model, which\ncan give a good match to the density profile but the MO metallicity implies the\ndisc is too metal rich to source the MO stars; and a tidal stream model, which\nbracket the distances and densities we derive for the MO, suggesting that a\nmodel can be found that would fully fit the MO data. Further data and modeling\nwill be required to confirm or rule out the MO feature as a stream or as a\nflaring of the disc.\n",
        "  The rapid expansion of human activities threatens ocean-wide biodiversity\nloss. Numerous marine animal populations have declined, yet it remains unclear\nwhether these trends are symptomatic of a chronic accumulation of global marine\nextinction risk. We present the first systematic analysis of threat for a\nglobally-distributed lineage of 1,041 chondrichthyan fishes - sharks, rays, and\nchimaeras. We estimate that one-quarter are threatened according to IUCN Red\nList criteria due to overfishing (targeted and incidental). Large-bodied,\nshallow-water species are at greatest risk and five out of the seven most\nthreatened families are rays. Overall chondrichthyan extinction risk is\nsubstantially higher than for most other vertebrates, and only one-third of\nspecies are considered safe. Population depletion has occurred throughout the\nworld's ice-free waters, but is particularly prevalent in the Indo-Pacific\nBiodiversity Triangle and Mediterranean Sea. Improved management of fisheries\nand trade is urgently needed to avoid extinctions and promote population\nrecovery.\n",
        "  This paper aims to determine the images of the braid group under\nrepresentations afforded by the Yang Baxter equation when the solution is a\nnontrivial $4 \\times 4$ matrix. Making the assumption that all the eigenvalues\nof the Yang Baxter solution are roots of unity, leads to the conclusion that\nall the images are finite.\n  Using results of Turaev, we have also identified cases in which one would get\na link invariant. Finally, by observing the group algebra generated by the\nimage of the braid group sometimes factor through known algebras, in certain\ninstances we can identify the invariant as particular specializations of a\nknown invariant.\n",
        "  It is well-known that the monoid of long virtual knots is not commutative.\nThis contrasts with the case of classical long knots, where $A \\# B\n\\leftrightharpoons B \\# A$ for all $A,B$. In the present paper, we present a\nnew proof that two inequivalent non-classical prime long virtual knots never\ncommute. The original result is due to Manturov. The techniques used here are\nmostly geometric. First, a slightly strengthened version of Kuperberg's theorem\nis established. We then show that a well-defined concatenation of two long\nknots in a thickened surface is preserved by stabilization when both long knots\nare non-classical. Finally, it is proved that if $A,B,C,D$ are prime\nnon-classical long virtual knots such that $A \\# B$ is non-classical and $A \\#\nB \\leftrightharpoons C \\# D$, then $A \\leftrightharpoons C$ and $B\n\\leftrightharpoons D$.\n",
        "  We present a systematic study of the properties of TiN films by varying the\ndeposition conditions in an ultra-high-vacuum reactive magnetron sputtering\nchamber. By increasing the deposition pressure from 2 to 9 mTorr while keeping\na nearly stoichiometric composition of Ti(1-x)N(x) (x=0.5), the film\nresistivity increases, the dominant crystal orientation changes from (100) to\n(111), grain boundaries become clearer, and the strong compressive strain\nchanges to weak tensile strain. The TiN films absorb a high concentration of\ncontaminants including hydrogen, carbon, and oxygen when they are exposed to\nair after deposition. With the target-substrate distance set to 88 mm the\ncontaminant levels increase from ~0.1% to ~10% as the pressure is increased\nfrom 2 to 9 mTorr. The contaminant concentrations also correlate with in-plane\ndistance from the center of the substrate and increase by roughly two orders of\nmagnitude as the target-substrate distance is increased from 88 mm to 266 mm.\nThese contaminants are found to strongly influence the properties of TiN films.\nFor instance, the resistivity of stoichiometric films increases by around a\nfactor of 5 as the oxygen content increases from 0.1% to 11%. These results\nsuggest that the sputtered TiN particle energy is critical in determining the\nTiN film properties, and that it is important to control this energy to obtain\nhigh-quality TiN films. Superconducting coplanar waveguide resonators made from\na series of nearly stoichiometric films grown at pressures from 2 mTorr to 7\nmTorr show an increase in intrinsic quality factor from ~10^4 to ~10^6 as the\nmagnitude of the compressive strain decreases from nearly 3800 MPa to\napproximately 150 MPa and the oxygen content increases from 0.1% to 8%. The\nfilms with a higher oxygen content exhibit lower loss, but the nonuniformity of\nthe oxygen incorporation hinders the use of sputtered TiN in larger circuits.\n",
        "  About 20% of low-redshift galaxies are late-type spirals with a small or no\nbulge component. Although they are the simplest disk galaxies in terms of\nstructure and dynamics, the role of the different physical processes driving\ntheir formation and evolution is not yet fully understood. We investigated\nwhether small bulges of late-type spirals follow the same scaling relations\ntraced by ellipticals and large bulges and if they are disk-like or classical\nbulges. We derived the photometric and kinematic properties of 9 nearby\nlate-type spirals. To this aim, we analyzed the surface brightness distribution\nfrom the i-band images of the Sloan Digital Sky Survey and obtained the\nstructural parameters of the galaxies from a two-dimensional photometric\ndecomposition. We measured the line-of-sight stellar velocity distribution\nwithin the bulge effective radius from the long-slit spectra taken with high\nspectral resolution at the Telescopio Nazionale Galileo. We used the\nphotometric and kinematic properties of the sample bulges to study their\nlocation in the Fundamental Plane, Kormendy, and Faber-Jackson relations\ndefined for ellipticals and large bulges. We found that our sample bulges\nsatisfy some of the photometric and kinematic prescriptions for being\nconsidered disk-like bulges such as small sizes and masses with nearly\nexponential light profiles, small bulge-to-total luminosity ratios, low stellar\nvelocity dispersions, and ongoing star formation. However, each of them follows\nthe same scaling relations of ellipticals, massive bulges, and compact\nearly-type galaxies so they cannot be classified as disk-like systems. We find\na single population of galaxy spheroids that follow the same scaling relations,\nwhere the mass seems to lead to a smooth transition in the photometric and\nkinematic properties from less massive bulges to more massive bulges and\nellipticals.\n",
        "  Given a connect sum of link diagrams, there is an isomorphism which\ndecomposes unnormalized Khovanov chain groups for the product in terms of\nnormalized chain groups for the factors; this isomorphism is straightforward to\nsee on the level of chains. Similarly, any plumbing $x*y$ of Kauffman states\ncarries an isomorphism of the chain subgroups generated by the enhancements of\n$x*y$, $x$, $y$:\n  \\[\n  \\mathcal{C}_R(x*y)\\to\n  \\left(\\mathcal{C}_{R,p\\to1}(x)\\otimes\n\\mathcal{C}_{R,p\\to1}(y)\\right)\\oplus\\left(\\mathcal{C}_{R,p\\to0}(x)\\otimes\n\\mathcal{C}_{R,p\\to0}(y)\\right). \\] We apply this plumbing of chains to to\nprove that every homogeneously adequate state has enhancements $X^\\pm$ in\ndistinct $j$-gradings whose $A$-traces (which we define) represent nonzero\nKhovanov homology classes over $\\mathbb{F}_2$, and that this is also true over\n$\\mathbb{Z}$ when all $A$-blocks' state surfaces are two-sided. We construct\n$X^\\pm$ explicitly.\n",
        "  The isiZulu verb is known for its morphological complexity, which is a\nsubject for on-going linguistics research, as well as for prospects of\ncomputational use, such as controlled natural language interfaces, machine\ntranslation, and spellcheckers. To this end, we seek to answer the question as\nto what the precise grammar rules for the isiZulu complex verb are (and, by\nextension, the Bantu verb morphology). To this end, we iteratively specify the\ngrammar as a Context Free Grammar, and evaluate it computationally. The grammar\npresented in this paper covers the subject and object concords, negation,\npresent tense, aspect, mood, and the causative, applicative, stative, and the\nreciprocal verbal extensions, politeness, the wh-question modifiers, and aspect\ndoubling, ensuring their correct order as they appear in verbs. The grammar\nconforms to specification.\n",
        "  Treatment planning system calculations in inhomogeneous regions may present\nsignificant inaccuracies due to loss of electronic equilibrium. In this study,\nthree different dose calculation algorithms, pencil beam, collapsed cone, and\nMonte-Carlo, provided by our planning system were compared to assess their\nimpact on the three-dimensional planning of lung and breast cases. A total of\nfive breast and five lung cases were calculated using the PB, CC, and MC\nalgorithms. Planning treatment volume and organs at risk delineation was\nperformed according to our institutions protocols on the Oncentra MasterPlan\nimage registration module, on 0.3 to 0.5 cm computed tomography slices taken\nunder normal respiration conditions. Four intensity-modulated radiation therapy\nplans were calculated according to each algorithm for each patient. The plans\nwere conducted on the Oncentra MasterPlan and CMS Monaco treatment planning\nsystems, for 6 MV. The plans were compared in terms of the dose distribution in\ntarget, OAR volumes, and monitor units. Furthermore, absolute dosimetry was\nmeasured using a three-dimensional diode array detector to evaluate the dose\ndifferences in a homogeneous phantom. Comparing the PB, CC, and MC algorithms\nplanned dose distribution, the PB algorithm provided adequate coverage of the\nPTV. The MUs calculated using the PB algorithm was less than those of the other\nalgorithms. The MC algorithm showed the highest accuracy in terms of the\nabsolute dosimetry. Differences were found when comparing the calculation\nalgorithms. The PB algorithm estimated higher doses for the target than the CC\nand MC algorithms. The PB algorithm actually overestimated the dose compared\nwith those calculated by the CC and MC algorithms. The MC algorithm showed\nbetter accuracy than the other algorithms.\n",
        "  The online analytical processing (OLAP) does not provide any explanation of\ncorrelations discovered between data. Thus, the coupling of OLAP and data\nmining, especially association rules, is considered as an efficient solution to\nthis problem. In this context, we mainly focus on a particular class of\nassociation rules which is the cyclic association rules. These rules aimed to\ndiscover patterns that display regular variation over user-defined intervals.\nGenerally,the generated patterns do not take an advantage from the\nspecificities of the multidimensional context namely, the consideration of the\nmeasures and their aggregations. In this paper, we introduce a novel method for\nextracting cyclic association rules from measures, and we redefine the\nevaluation metrics of association rules quality inspired of the temporal\nsummarizability of measures concept through the integration of appropriate\naggregation functions. To prove the usefulness of our approach, we conduct an\nempirical study on a real data warehouse.\n",
        "  Quantifying material mass and electron density from computed tomography (CT)\nreconstructions can be highly valuable in certain medical practices, such as\nradiation therapy planning. However, uniquely parameterising the X-ray\nattenuation in terms of mass or electron density is an ill-posed problem when a\nsingle polyenergetic source is used with a spectrally indiscriminate detector.\nExisting approaches to single source polyenergetic modelling often impose\nconsistency with a physical model, such as water--bone or\nphotoelectric--Compton decompositions, which will either require detailed prior\nsegmentation or restrictive energy dependencies, and may require further\ncalibration to the quantity of interest. In this work, we introduce a data\ncentric approach to fitting the attenuation with piecewise-linear functions\ndirectly to mass or electron density, and present a segmentation-free\nstatistical reconstruction algorithm for exploiting it, with the same order of\ncomplexity as other iterative methods. We show how this allows both higher\naccuracy in attenuation modelling, and demonstrate its superior quantitative\nimaging, with numerical chest and metal implant data, and validate it with real\ncone-beam CT measurements.\n",
        "  Soft and pliable conductive polymer composites hold promise for application\nas bioelectronic interfaces such as for electroencephalography (EEG). In\nclinical, laboratory, and real-world EEG there is a desire for dry, soft, and\ncomfortable interfaces to the scalp that are capable of relaying the\nmicrovolt-level scalp potentials to signal processing electronics. A key\nchallenge is that most material approaches are sensitive to deformation-induced\nshifts in electrical impedance associated with decreased signal-to-noise ratio.\nThis is a particular concern in real-world environments where human motion is\npresent. The entire set of brain information outside of tightly controlled\nlaboratory or clinical settings are currently unobtainable due to this\nchallenge. Here we explore the performance of an elastomeric material solution\npurposefully designed for dry, soft, comfortable scalp contact electrodes for\nEEG that is specifically targeted to have flat electrical impedance response to\ndeformation to enable utilization in real world environments. A conductive\ncarbon nanofiber filled polydimethylsiloxane (CNF-PDMS) elastomer was evaluated\nat three fill ratios (3, 4 and 7 volume percent). To evaluate usability for\nEEG, pre-recorded human EEG signals were replayed through the contact\nelectrodes subjected to quasi-static compressive strains between zero and 35%.\nThese tests show that conductive filler ratios well above the electrical\npercolation threshold are desirable in order to maximize signal-to-noise ratio\nand signal correlation with an ideal baseline. Increasing fill ratios yield\nincreasingly flat electrical impedance response to large applied compressive\ndeformations with a trade in increased material stiffness, and with nominal\nelectrical impedance tunable over greater than 4 orders of magnitude.\n",
        "  Among other things, we prove the following two topologcal statements about\nclosed hyperbolic 3-manifolds. First, every rational second homology class of a\nclosed hyperbolic 3-manifold has a positve integral multiple represented by an\noriented connected closed $\\pi_1$-injectively immersed quasi-Fuchsian\nsubsurface. Second, every rationally null-homologous, $\\pi_1$-injectively\nimmersed oriented closed 1-submanifold in a closed hyperbolic 3-manifold has an\nequidegree finite cover which bounds an oriented compact $\\pi_1$-injective\nimmersed quasi-Fuchsian subsurface. In part, we exploit techniques developed\nearlier by Kahn and Markovic about good pants constructions, but we only\ndistill geometric and topological ingredients from their papers so no hard\nanalysis is involved in this paper.\n",
        "  The variation of the kinematical properties of the Galactic thick disk with\nGalactic height Z are studied by means of 412 red giants observed in the\ndirection of the south Galactic pole up to 4.5 kpc from the plane. We confirm\nthe non-null mean radial motion toward the Galactic anticenter found by other\nauthors, but we find that it changes sign at |Z|=3 kpc, and the proposed inward\nmotion of the LSR alone cannot explain these observations. The rotational\nvelocity decreases with |Z| by -30 km/s/kpc, but the data are better\nrepresented by a power-law with index 1.25, similar to that proposed from the\nanalysis of SDSS data. All the velocity dispersions increase with |Z|, but the\nvertical gradients are small. The dispersions grow proportionally, with no\nsignificant variation of the anisotropy. The ratio sigma_U/sigma_W=2 suggests\nthat the thick disk could have formed from a low-latitude merging event. The\nvertex deviation increases with Galactic height, reaching ~20 degrees at\n|Z|=3.5 kpc. The tilt angle also increases, and the orientation of the\nellipsoid in the radial-vertical plane is constantly intermediate between the\nalignment with the cylindrical and the spherical coordinate systems. The tilt\nangle at |Z|=2 kpc coincides with the expectations of MOND, but an extension of\nthe calculations to higher |Z| is required to perform a conclusive test.\nFinally, between 2.5 and 3.5 kpc we detect deviations from the linear trend of\nmany kinematical quantities, suggesting that some kinematical substructure\ncould be present.\n",
        "  Main-memory database management systems (DBMS) can achieve excellent\nperformance when processing massive volume of on-line transactions on modern\nmulti-core machines. But existing durability schemes, namely, tuple-level and\ntransaction-level logging-and-recovery mechanisms, either degrade the\nperformance of transaction processing or slow down the process of failure\nrecovery. In this paper, we show that, by exploiting application semantics, it\nis possible to achieve speedy failure recovery without introducing any costly\nlogging overhead to the execution of concurrent transactions. We propose\nPACMAN, a parallel database recovery mechanism that is specifically designed\nfor lightweight, coarse-grained transaction-level logging. PACMAN leverages a\ncombination of static and dynamic analyses to parallelize the log recovery: at\ncompile time, PACMAN decomposes stored procedures by carefully analyzing\ndependencies within and across programs; at recovery time, PACMAN exploits the\navailability of the runtime parameter values to attain an execution schedule\nwith a high degree of parallelism. As such, recovery performance is remarkably\nincreased. We evaluated PACMAN in a fully-fledged main-memory DBMS running on a\n40-core machine. Compared to several state-of-the-art database recovery\nmechanisms, PACMAN can significantly reduce recovery time without compromising\nthe efficiency of transaction processing.\n",
        "  In a recent article, Desai and Fisher (2007) proposed that the speed of\nadaptation in an asexual population is determined by the dynamics of the\nstochastic edge of the population, that is, by the emergence and subsequent\nestablishment of rare mutants that exceed the fitness of all sequences\ncurrently present in the population. Desai and Fisher perform an elaborate\nstochastic calculation of the mean time $\\tau$ until a new class of mutants has\nbeen established, and interpret $1/\\tau$ as the speed of adaptation. As they\nnote, however, their calculations are valid only for moderate speeds. This\nlimitation arises from their method to determine $\\tau$: Desai and Fisher\nback-extrapolate the value of $\\tau$ from the best-fit class' exponential\ngrowth at infinite time. This approach is not valid when the population adapts\nrapidly, because in this case the best-fit class grows non-exponentially during\nthe relevant time interval. Here, we substantially extend Desai and Fisher's\nanalysis of the stochastic edge. We show that we can apply Desai and Fisher's\nmethod to high speeds by either exponentially back-extrapolating from finite\ntime or using a non-exponential back-extrapolation. Our results are compatible\nwith predictions made using a different analytical approach (Rouzine et al.\n2003, 2007), and agree well with numerical simulations.\n",
        "  We conduct experiments and numerical simulations of the dynamics of bubble\nclouds nucleated on the surface of an epoxy cylindrical stone model during\nburst wave lithotripsy (BWL). In the experiment, the bubble clouds are\nvisualized and bubble-scattered acoustics are measured. In the numerical\nsimulation, we combine methods for modeling compressible multicomponent flows\nto capture complex interactions among cavitation bubbles, the stone, and the\nburst wave. Quantitative agreement is confirmed between results of the\nexperiment and the simulation. We observe and quantify a significant shielding\nof incident wave energy by the bubble clouds. The magnitude of shielding\nreaches up to 80% of the total acoustic energy of the incoming burst wave,\nsuggesting a potential loss of efficacy of stone comminution. We further\ndiscovered a strong linear correlation between the magnitude of the energy\nshielding and the amplitude of the bubble-scattered acoustics, independent of\nthe initial size and the void fraction of the bubble cloud within a range\naddressed in the simulation. This correlation could provide for real-time\nmonitoring of cavitation activity in BWL.\n",
        "  In this work a mu=-1 metamaterial (MM) lens for magnetic resonance imaging\n(MRI) is demonstrated. MRI uses surface coils to detect the radiofrequency(RF)\nenergy absorbed and emitted by the nuclear spins in the imaged object. The\nproposed MM lens manipulates the RF field detected by these surface coils, so\nthat the coil sensitivity and spatial localization is substantially improved.\nBeyond this specific application, we feel that the reported results are the\nexperimental confirmation of a new concept for the manipulation of RF field in\nMRI, which paves the way to many other interesting applications.\n",
        "  Data validation is becoming more and more important with the ever-growing\namount of data being consumed and transmitted by systems over the Internet. It\nis important to ensure that the data being sent is valid as it may contain\nentry errors, which may be consumed by different systems causing further\nerrors. XML has become the defacto standard for data transfer. The XML Schema\nDefinition language (XSD) was created to help XML structural validation and\nprovide a schema for data type restrictions, however it does not allow for more\ncomplex situations. In this article we introduce a way to provide rule based\nXML validation and correction through the extension and improvement of our SRML\nmetalanguage. We also explore the option of applying it in a database as a\ntrigger for CRUD operations allowing more granular dataset validation on an\natomic level allowing for more complex dataset record validation rules.\n",
        "  We propose and analyze a structure with which to organize the difference\nbetween a knot in the 3-sphere bounding a topologically embedded 2-disk in the\n4-ball and it bounding a smoothly embedded disk. The n-solvable filtration of\nthe topological knot concordance group, due to Cochran-Orr-Teichner, may be\ncomplete in the sense that any knot in the intersection of its terms may well\nbe topologically slice. However, the natural extension of this filtration to\nwhat is called the n-solvable filtration of the smooth knot concordance group,\nis unsatisfactory because any topologically slice knot lies in every term of\nthe filtration. To ameliorate this we investigate a new filtration, {B_n}, that\nis simultaneously a refinement of the n-solvable filtration and a\ngeneralization of notions of positivity studied by Gompf and Cochran. We show\nthat each B_n/B_{n+1} has infinite rank. But our primary interest is in the\ninduced filtration, {T_n}, on the subgroup, T, of knots that are topologically\nslice. We prove that T/T_0 is large, detected by gauge-theoretic invariants and\nthe tau, s, and epsilon-invariants; while the non-triviliality of T_0/T_1 can\nbe detected by certain d-invariants. All of these concordance obstructions\nvanish for knots in T_1. Nonetheless, going beyond this, our main result is\nthat T_1/T_2 has positive rank. Moreover under a \"weak homotopy-ribbon\"\ncondition, we show that each T_n/T_{n+1} has positive rank. These results\nsuggest that, even among topologically slice knots, the fundamental group is\nresponsible for a wide range of complexity.\n",
        "  The value of a locally frozen magnetic field in a region with a diameter of\n0.5 mm in a 0.5-mm-thick YBa2Cu3O7-x plate was investigated as a function of\nthe excitation field (to 2x10^4 A/m), plate cooling mode (in the absence or\npresence of a field; i.e., zero-field cooling (ZFC) or field coupling (FC)),\nand local demagnetizing field. Analysis of the measurement results in the noted\nrange of excitation fields showed the following: (i) the dependence on the\nexcitation field for the ZFC mode is explained by the local inhomogeneity of\ncritical currents of weak links in the ceramic Josephson medium and is limited\nby their maximum value at the temperature of the experiment (77 K); (ii) the\ndependence on the excitation field for the FC mode contains a portion of the\nmagnetic phase transition from the frozen current structure, typical of the\ninitial portion of the dependence, to the current structure characteristic of\nthe ZFC freezing mode, and is limited by this transition; and (iii) the\ndependence on the demagnetizing field for the ZFC mode can be explained by the\nstable coexistence (without annihilation) of microscopic current loops with\nopposite current directions in the ceramics.\n",
        "  The multi-criteria decision making, which is possible with the advent of\nskyline queries, has been applied in many areas. Though most of the existing\nresearch is concerned with only a single relation, several real world\napplications require finding the skyline set of records over multiple\nrelations. Consequently, the join operation over skylines where the preferences\nare local to each relation, has been proposed. In many of those cases, however,\nthe join often involves performing aggregate operations among some of the\nattributes from the different relations. In this paper, we introduce such\nqueries as \"aggregate skyline join queries\". Since the naive algorithm is\nimpractical, we propose three algorithms to efficiently process such queries.\nThe algorithms utilize certain properties of skyline sets, and processes the\nskylines as much as possible locally before computing the join. Experiments\nwith real and synthetic datasets exhibit the practicality and scalability of\nthe algorithms with respect to the cardinality and dimensionality of the\nrelations.\n",
        "  Zipf's law has been found in many human-related fields, including language,\nwhere the frequency of a word is persistently found as a power law function of\nits frequency rank, known as Zipf's law. However, there is much dispute whether\nit is a universal law or a statistical artifact, and little is known about what\nmechanisms may have shaped it. To answer these questions, this study conducted\na large scale cross language investigation into Zipf's law. The statistical\nresults show that Zipf's laws in 50 languages all share a 3-segment structural\npattern, with each segment demonstrating distinctive linguistic properties and\nthe lower segment invariably bending downwards to deviate from theoretical\nexpectation. This finding indicates that this deviation is a fundamental and\nuniversal feature of word frequency distributions in natural languages, not the\nstatistical error of low frequency words. A computer simulation based on the\ndual-process theory yields Zipf's law with the same structural pattern,\nsuggesting that Zipf's law of natural languages are motivated by common\ncognitive mechanisms. These results show that Zipf's law in languages is\nmotivated by cognitive mechanisms like dual-processing that govern human verbal\nbehaviors.\n",
        "  Dropped Pronouns (DP) in which pronouns are frequently dropped in the source\nlanguage but should be retained in the target language are challenge in machine\ntranslation. In response to this problem, we propose a semi-supervised approach\nto recall possibly missing pronouns in the translation. Firstly, we build\ntraining data for DP generation in which the DPs are automatically labelled\naccording to the alignment information from a parallel corpus. Secondly, we\nbuild a deep learning-based DP generator for input sentences in decoding when\nno corresponding references exist. More specifically, the generation is\ntwo-phase: (1) DP position detection, which is modeled as a sequential\nlabelling task with recurrent neural networks; and (2) DP prediction, which\nemploys a multilayer perceptron with rich features. Finally, we integrate the\nabove outputs into our translation system to recall missing pronouns by both\nextracting rules from the DP-labelled training data and translating the\nDP-generated input sentences. Experimental results show that our approach\nachieves a significant improvement of 1.58 BLEU points in translation\nperformance with 66% F-score for DP generation accuracy.\n",
        "  Odd-parity, spin-triplet superconductor Sr2RuO4 has been found to feature\nexotic vortex physics including half-flux quanta trapped in a doubly connected\nsample and the formation of vortex lattices at low fields. The consequences of\nthese vortex states on the low-temperature magnetoresistive behavior of\nmesoscopic samples of Sr2RuO4 were investigated in this work using ring device\nfabricated on mechanically exfoliated single crystals of Sr2RuO4 by\nphotolithography and focused ion beam. With the magnetic field applied\nperpendicular to the in-plane direction, thin-wall rings of Sr2RuO4 were found\nto exhibit pronounced quantum oscillations with a conventional period of the\nfull-flux quantum even though the unexpectedly large amplitude and the number\nof oscillations suggest the observation of vortex-flow-dominated\nmagnetoresistance oscillations rather than a conventional Little-Parks effect.\nFor rings with a thick wall, two distinct periods of quantum oscillations were\nfound in high and low field regimes, respectively, which we argue to be\nassociated with the \"lock-in\" of a vortex lattice in these thick-wall rings. No\nevidence for half-flux-quantum resistance oscillations were identified in any\nsample measured so far without the presence of an in-plane field.\n",
        "  In today's scenario, imagining a world without negativity is something very\nunrealistic, as bad NEWS spreads more virally than good ones. Though it seems\nimpractical in real life, this could be implemented by building a system using\nMachine Learning and Natural Language Processing techniques in identifying the\nnews datum with negative shade and filter them by taking only the news with\npositive shade (good news) to the end user. In this work, around two lakhs\ndatum have been trained and tested using a combination of rule-based and data\ndriven approaches. VADER along with a filtration method has been used as an\nannotating tool followed by statistical Machine Learning approach that have\nused Document Term Matrix (representation) and Support Vector Machine\n(classification). Deep Learning algorithms then came into picture to make this\nsystem reliable (Doc2Vec) which finally ended up with Convolutional Neural\nNetwork(CNN) that yielded better results than the other experimented modules.\nIt showed up a training accuracy of 96%, while a test accuracy of (internal and\nexternal news datum) above 85% was obtained.\n",
        "  Biological systems are modular, and this modularity evolves over time and in\ndifferent environments. A number of observations have been made of increased\nmodularity in biological systems under increased environmental pressure. We\nhere develop a quasispecies theory for the dynamics of modularity in\npopulations of these systems. We show how the steady-state fitness in a\nrandomly changing environment can be computed. We derive a fluctuation\ndissipation relation for the rate of change of modularity and use it to derive\na relationship between rate of environmental changes and rate of growth of\nmodularity. We also find a principle of least action for the evolved modularity\nat steady state. Finally, we compare our predictions to simulations of protein\nevolution and find them to be consistent.\n",
        "  The N2 index ([N II]6584/Ha) is used to determine emission line galaxy\nmetallicities at all redshifts, including high redshift, where galaxies tend to\nbe metal-poor. The initial aim of the work was to improve the calibrations used\nto infer oxygen abundance from N2 employing updated low-metallicity galaxy\ndatabases. We compare N2 and the metallicity determined using the direct method\nfor the set of extremely metal-poor galaxies compiled by Morales-Luis et al.\n(2011). To our surprise, the oxygen abundance presents a tendency to be\nconstant with N2, with a very large scatter. Consequently, we find that the\nexisting N2 calibrators overstimate the oxygen abundance for most low\nmetallicity galaxies, and then they can be used only to set upper limits to the\ntrue metallicity in low-metallicity galaxies. An explicit expression for this\nlimit is given. In addition, we try to explain the observed scatter using\nphotoionization models. It is mostly due to the different evolutionary state of\nthe HII regions producing the emission lines, but it also arises due to\ndifferences of N/O among the galaxies.\n",
        "  Among non-alternating knots with $n\\le 12$ crossings given in \\cite{1} Turaev\ngenus is not known for 191 knot. For 154 of them we show that they are almost\nalternating, so their Turaev genus is 1.\n",
        "  In this paper we present calculations of the UV luminosity function from the\nDark-ages Reionization And Galaxy-formation Observables from Numerical\nSimulations (DRAGONS) project, which combines N-body, semi-analytic and\nsemi-numerical modelling designed to study galaxy formation during the Epoch of\nReionization. Using galaxy formation physics including supernova feedback, the\nmodel naturally reproduces the UV LFs for high-redshift star-forming galaxies\nfrom $z{\\sim}5$ through to $z{\\sim}10$. We investigate the luminosity--star\nformation rate (SFR) relation, finding that variable SFR histories of galaxies\nresult in a scatter around the median relation of $0.1$--$0.3$ dex depending on\nUV luminosity. We find close agreement between the model and observationally\nderived SFR functions. We use our calculated luminosities to investigate the\nluminosity function below current detection limits, and the ionizing photon\nbudget for reionization. We predict that the slope of the UV LF remains steep\nbelow current detection limits and becomes flat at\n$M_\\mathrm{UV}{\\gtrsim}{-14}$. We find that $48$ ($17$) per cent of the total\nUV flux at $z{\\sim}6$ ($10$) has been detected above an observational limit of\n$M_\\mathrm{UV}{\\sim}{-17}$, and that galaxies fainter than\n$M_\\mathrm{UV}{\\sim}{-17}$ are the main source of ionizing photons for\nreionization. We investigate the luminosity--stellar mass relation, and find a\ncorrelation for galaxies with $M_\\mathrm{UV}{<}{-14}$ that has the form\n$M_*{\\propto}10^{-0.47M_\\mathrm{UV}}$, in good agreement with observations, but\nwhich flattens for fainter galaxies. We determine the luminosity--halo mass\nrelation to be $M_\\mathrm{vir}{\\propto}10^{-0.35M_\\mathrm{UV}}$, finding that\ngalaxies with $M_\\mathrm{UV}{=}{-20}$ reside in host dark matter haloes of\n$10^{11.0\\pm 0.1}\\mathrm{M_\\odot}$ at $z{\\sim}6$, and that this mass decreases\ntowards high redshift.\n",
        "  We use deep images acquired with the Advanced Camera for Surveys (ACS) on\nboard of the Hubble Space Telescope (HST) in the filters F555W and F814W to\ncharacterize the properties of NGC 376, a young star cluster located in the\nwing of the Small Magellanic Cloud (SMC). Using isochrone fitting we derive for\nNGC 376 an age of 28+/-7 Myr, in good agreement with previous studies. The high\nspatial resolution ACS data allow us to determine the center of gravity of the\ncluster and to construct extended surface brightness and radial density\nprofiles. Neither of these profiles can be fitted with a theoretical model,\nsuggesting that the cluster is not in virial equilibrium. Considering the young\nage of the cluster, we speculate that the distortion of the radial profiles may\nbe the result of the rapid gas dispersal that follows the initial phase of star\nformation. The cluster shows clear evidence of dynamical mass segregation. From\nthe properties of the radial profiles and the present day mass function (PDMF)\nwe conclude that NGC 376 appears to have already lost nearly 90% of its initial\nstellar mass, probably as a consequence of the sudden gas dispersal that\nfollows the early phase of star formation (SF).\n",
        "  The Andreev reflection spectra were studied for the first time in the\nmagnetic superconductor Dy0.6Y0.4Rh3.85Ru0.15B4. It is found that an external\nmagnetic field (up to 0.7 Hc2) is a strong stimulator of superconductivity in\ncontrast with traditional superconductors with a singlet pairing. The ratio\n2\\Delta/kBTc \\approx 4.0 obtained for some contacts is higher than the value\n3.52 which is typical of conventional superconductors with a weak\nelectron-phonon interaction. It has been proposed that in\nDy0.6Y0.4Rh3.85Ru0.15B4 a triplet mechanism of superconducting pairing is\nrealized.\n",
        "  Cables made of MgB2 superconductors are currently explored as a viable\nsolution for transporting high electrical power in the AC regime. In order to\nbe competitive against the DC solution, the cables need to have an acceptable\nlevel of AC losses. In this contribution, we discuss the main aspects relevant\nfor designing a cable with a sufficiently low AC loss level. To this end, we\nperform finite-element-method (FEM) simulations to determine the current and\nfield distributions and calculate the AC losses of such cable configuration.\nFor current capacities of 2-5 kA (peak), power cables are assembled from a\nrelatively small number of MgB2 strands. The performance of such cables\nstrongly depends on the current and field distributions, which are in turn\ninfluenced by the number and the arrangement of the superconducting components\nand also by the magnetic properties of supporting materials. Numerical\nsimulations can help to test different cable configurations and provide\nimportant insights for optimizing the cable's design. The numerical model\nincludes the field dependence of the superconductor's critical current density\nJc(B) as well as the non-linear properties of magnetic materials.\n",
        "  The analytical generalization of the classical dynamical friction formula\n(derived under the assumption that all the field particles have the same mass)\nto the case in which the masses of the field particles are distributed with a\nmass spectrum is presented. Two extreme cases are considered: in the first,\nenergy equipartition is assumed, in the second all the field particles have the\nsame (Maxwellian) velocity distribution. Three different mass spectra are\nstudied in detail, namely the exponential, discrete (two components), and\npower-law cases. It is found that the dynamical friction deceleration can be\nsignificantly stronger than in the equivalent classical case, with the largest\ndifferences (up to a factor of 10 or more in extreme cases) arising for test\nparticle velocities comparable to the mass-averaged velocity dispersion of the\nfield particles. The present results are relevant to our understanding of the\ndynamical evolution of globular clusters, in particular in the modelization of\nmass segregation and sedimentation of Blue Straggler stars and Neutron stars,\nand for the study of binary black holes in galactic nuclei.\n",
        "  In this study, the pairing mechanism for layered HTS materials based on\nattraction between electrons from adjacent layers is proposed. Initially, each\nlayer has expanded Fermi sphere owing to ridged geometry. When the two layers\nare close enough for tunneling, it becomes energetically advantageous to form\ncorrelated quantum states (CQS), reducing the Fermi sphere volume. Cooper\npairs, comprising inter-tunneling electrons, occupy the CQS. The image force is\nresponsible for the electron-electron attraction. Pair-binding energy and the\ncorresponding effective mass vary in a wide range. At T>0, some heavy pairs do\nnot condense. Such pairs are responsible for pseudogap. Light pairs get Bose\ncondensed and are responsible for superconductivity. The proposed mechanism\nprovides clarification of superconductivity in cuprates, iron based\nsuperconductors and LSCO/LCO interfaces. It provides explanation of two energy\ngaps and two characteristic temperatures in layered superconducting materials.\nIt also provides clarification on the Fermi surface pockets, anisotropy of\ncharge transport in pseudogap state, and other properties of HTS materials. The\npseudogap, estimated within the model, fits the experimental values for the\ntwo-layer cuprates, such as YBCO, Bi2212, Tl2212, and Hg1212.\n",
        "  Cosmological structure formation predicts that our galactic halo contains an\nenormous hierarchy of substructures and streams, the remnants of the merging\nhierarchy that began with tiny Earth mass microhalos. If these structures\npersist until the present time, they could influence dramatically the detection\nsignatures of weakly interacting elementary particle dark matter (WIMP). Using\nnumerical simulations that follow the tidal disruption within the Galactic\npotential and heating from stellar encounters, we find that neither microhalos\nnor streams have significant impact on direct detection, implying that dark\nmatter constraints derived using simple smooth halo models are relatively\nrobust. We also find that many dense central cusps survive, yielding a small\nenhancement in the signal for indirect detection experiments.\n",
        "  Mutualistic networks have been shown to involve complex patterns of\ninteractions among animal and plant species. The architecture of these webs\nseems to pervade some of their robust and fragile behaviour. Recent work\nindicates that there is a strong correlation between the patterning of\nanimal-plant interactions and their phylogenetic organisation. Here we show\nthat such pattern and other reported regularities from mutualistic webs can be\nproperly explained by means of a very simple model of speciation and\ndivergence. This model also predicts a co-extinction dynamics under species\nloss consistent with the presence of an evolutionary signal. The agreement\nbetween observed and model networks suggests that some patterns displayed by\nreal mutualistic webs might actually represent evolutionary spandrels.\n",
        "  The computation of relatedness between two fragments of text in an automated\nmanner requires taking into account a wide range of factors pertaining to the\nmeaning the two fragments convey, and the pairwise relations between their\nwords. Without doubt, a measure of relatedness between text segments must take\ninto account both the lexical and the semantic relatedness between words. Such\na measure that captures well both aspects of text relatedness may help in many\ntasks, such as text retrieval, classification and clustering. In this paper we\npresent a new approach for measuring the semantic relatedness between words\nbased on their implicit semantic links. The approach exploits only a word\nthesaurus in order to devise implicit semantic links between words. Based on\nthis approach, we introduce Omiotis, a new measure of semantic relatedness\nbetween texts which capitalizes on the word-to-word semantic relatedness\nmeasure (SR) and extends it to measure the relatedness between texts. We\ngradually validate our method: we first evaluate the performance of the\nsemantic relatedness measure between individual words, covering word-to-word\nsimilarity and relatedness, synonym identification and word analogy; then, we\nproceed with evaluating the performance of our method in measuring text-to-text\nsemantic relatedness in two tasks, namely sentence-to-sentence similarity and\nparaphrase recognition. Experimental evaluation shows that the proposed method\noutperforms every lexicon-based method of semantic relatedness in the selected\ntasks and the used data sets, and competes well against corpus-based and hybrid\napproaches.\n",
        "  In this working paper, we present a simple theoretical framework based on\nnetwork theory to study how speciation, the process by which new species\nappear, shapes spatial patterns of diversity. We show that this framework can\nbe expanded to account for different types of networks and interactions, and\nincorporates different modes of speciation.\n",
        "  We derive the response of the magnetic superconductors in the vortex state to\nthe ac Lorentz force, $F_L(t)=F_{{\\rm ac}}\\sin(\\omega t)$, taking into account\nthe interaction of vortices with the magnetic moments described by the\nrelaxation dynamics (polaronic effect). At low amplitudes of the driving force\n$F_{{\\rm ac}}$ the dissipation in the system is suppressed due to the\nenhancement of the effective viscosity at low frequencies and due to formation\nof the magnetic pinning at high frequencies $\\omega$. In the adiabatic limit\nwith low frequencies $\\omega$ and high amplitude of the driving force $F_{ac}$,\nthe vortex and magnetic polarization form a vortex polaron when $F_L(t)$ is\nsmall. When $F_L$ increases, the vortex polaron accelerates and at a threshold\ndriving force, the vortex polaron dissociates and the motion of vortex and the\nrelaxation of magnetization are decoupled. When $F_L$ decreases, the vortex is\nretrapped by the background of remnant magnetization and they again form vortex\npolaron. This process repeats when $F_L(t)$ increases in the opposite\ndirection. Remarkably, after dissociation, decoupled vortices move in the\nperiodic potential induced by magnetization which remains for some periods of\ntime due to retardation after the decoupling. At this stage vortices oscillate\nwith high frequencies determined by the Lorentz force at the moment of\ndissociation. We derive also the creep rate of vortices and show that magnetic\nmoments suppress creep rate.\n",
        "  We introduce the notion of quasi-triviality of quandles and define homology\nof quasi-trivial quandles. Quandle cocycle invariants are invariant under\nlink-homotopy if they are associated with 2-cocycles of quasi-trivial quandles.\nWe thus obtain a lot of numerical link-homotopy invariants.\n",
        "  This report describes an NLP assistant for the collaborative development\nenvironment Clide, that supports the development of NLP applications by\nproviding easy access to some common NLP data structures. The assistant\nvisualizes text fragments and their dependencies by displaying the semantic\ngraph of a sentence, the coreference chain of a paragraph and mined triples\nthat are extracted from a paragraph's semantic graphs and linked using its\ncoreference chain. Using this information and a logic programming library, we\ncreate an NLP database which is used by a series of queries to mine the\ntriples. The algorithm is tested by translating a natural language text\ndescribing a graph to an actual graph that is shown as an annotation in the\ntext editor.\n",
        "  What makes some types of languages more probable than others? For instance,\nwe know that almost all spoken languages contain the vowel phoneme /i/; why\nshould that be? The field of linguistic typology seeks to answer these\nquestions and, thereby, divine the mechanisms that underlie human language. In\nour work, we tackle the problem of vowel system typology, i.e., we propose a\ngenerative probability model of which vowels a language contains. In contrast\nto previous work, we work directly with the acoustic information -- the first\ntwo formant values -- rather than modeling discrete sets of phonemic symbols\n(IPA). We develop a novel generative probability model and report results based\non a corpus of 233 languages.\n",
        "  We report the results of a search for molecular oxygen (O2) toward the Orion\nBar, a prominent photodissociation region at the southern edge of the HII\nregion created by the luminous Trapezium stars. We observed the spectral region\naround the frequency of the O2 N_J = 3_3 - 1_2 transition at 487 GHz and the\n5_4 - 3_4 transition at 774 GHz using the Heterodyne Instrument for the Far\nInfrared on the Herschel Space Observatory. Neither line was detected, but the\n3sigma upper limits established here translate to a total line-of-sight O2\ncolumn density < 1.5 10^16 cm^-2 for an emitting region whose temperature is\nbetween 30K and 250 K, or < 1 10^16 cm^-2 if the O2 emitting region is\nprimarily at a temperature of ~< 100 K. Because the Orion Bar is oriented\nnearly edge-on relative to our line of sight, the observed column density is\nenhanced by a factor estimated to be between 4 and 20 relative to the face-on\nvalue. Our upper limits imply that the face-on O2 column density is less than 4\n10^15 cm^-2, a value that is below, and possibly well below, model predictions\nfor gas with a density of 10^4 - 10^5 cm^-3 exposed to a far ultraviolet flux\n10^4 times the local value, conditions inferred from previous observations of\nthe Orion Bar. The discrepancy might be resolved if: (1) the adsorption energy\nof O atoms to ice is greater than 800 K; (2) the total face-on Av of the Bar is\nless than required for O2 to reach peak abundance; (3) the O2 emission arises\nwithin dense clumps with a small beam filling factor; or, (4) the face-on depth\ninto the Bar where O2 reaches its peak abundance, which is density dependent,\ncorresponds to a sky position different from that sampled by our Herschel\nbeams.\n",
        "  This paper explores if the mean properties of Early-Type Galaxies (ETG) can\nbe reconstructed from \"genetic\" information stored in their GCs (i.e., in their\nchemical abundances, spatial distributions and ages). This approach implies\nthat the formation of each globular occurs in very massive stellar\nenvironments, as suggested by some models that aim at explaining the presence\nof multi-populations in these systems. The assumption that the relative number\nof globular clusters to diffuse stellar mass depends exponentially on chemical\nabundance, [Z/H], and the presence of two dominant GC sub-populations blue and\nred, allows the mapping of low metallicity halos and of higher metallicity (and\nmore heterogeneous) bulges. In particular, the masses of the low-metallicity\nhalos seem to scale up with dark matter mass through a constant. We also find a\ndependence of the globular cluster formation efficiency with the mean projected\nstellar mass density of the galaxies within their effective radii. The analysis\nis based on a selected sub-sample of galaxies observed within the ACS Virgo\nCluster Survey of the {\\it Hubble Space Telescope}. These systems were grouped,\naccording to their absolute magnitudes, in order to define composite fiducial\ngalaxies and look for a quantitative connection with their (also composite)\nglobular clusters systems. The results strengthen the idea that globular\nclusters are good quantitative tracers of both baryonic and dark matter in\nETGs.\n",
        "  Neutral hydrogen represents the major observable baryonic constituent of\ngalaxies that fuels the formation of stars through the transformation in\nmolecular hydrogen. The emission of the hydrogen recombination line Halpha is\nthe most direct tracer of the process that transforms gas (fuel) into stars. We\ncontinue to present Halpha3 (acronym for Halpha-alpha-alpha), an extensive\nHalpha+[NII] narrow-band imaging campaign of galaxies selected from the HI\nArecibo Legacy Fast ALFA Survey (ALFALFA), using the instrumentation available\nat the San Pedro Martir observatory (Mexico). In only four years since 2011 we\nwere able to complete in 48 nights the Halpha imaging observations of 724\ngalaxies in the region of the Coma supercluster 10^h < R.A. <16^h; 24^o < Dec.\n<28^o and 3900<cz<9000 kms^{-1}. Of these, 603 are selected from the HI Arecibo\nLegacy Fast ALFA Survey (ALFALFA) and constitute a 97% complete sample. They\nprovide for the first time a complete census of the massive star formation\nproperties of local gas-rich galaxies belonging to different environments\n(cluster vs filaments), morphological type (spirals vs dwarf Irr), over a wide\nrange of stellar mass (10^{8}-10^{11.5} Modot) in the Coma Supercluster. The\npresent Paper V provides the Halpha data and the derived star formation rates\nfor the observed galaxies.\n",
        "  We show that the distortion of the (2,q)-torus knot is not bounded linearly\nfrom below.\n",
        "  Traditional Neural machine translation (NMT) involves a fixed training\nprocedure where each sentence is sampled once during each epoch. In reality,\nsome sentences are well-learned during the initial few epochs; however, using\nthis approach, the well-learned sentences would continue to be trained along\nwith those sentences that were not well learned for 10-30 epochs, which results\nin a wastage of time. Here, we propose an efficient method to dynamically\nsample the sentences in order to accelerate the NMT training. In this approach,\na weight is assigned to each sentence based on the measured difference between\nthe training costs of two iterations. Further, in each epoch, a certain\npercentage of sentences are dynamically sampled according to their weights.\nEmpirical results based on the NIST Chinese-to-English and the WMT\nEnglish-to-German tasks depict that the proposed method can significantly\naccelerate the NMT training and improve the NMT performance.\n",
        "  It was asked by J.Birman, Williams, and L.Rudolph whether nontrivial Lorentz\nknots have always positive signature. Lorentz knots are examples of positive\nbraids (in our convention they have all crossings negative so they are negative\nlinks). It was shown by L.Rudolph that positive braids have positive signature\n(if they represent nontrivial links). K.Murasugi has shown that nontrivial,\nalternating, positive links have negative signature. Here we prove in general\nthe old folklore conjecture that nontrivial positive links have negative\nsignature.\n",
        "  We show that there exist infinitely many examples of pairs of knots, K_1 and\nK_2, that have no epimorphism $\\pi_1(S^3\\setminus K_1) \\to \\pi_1(S^3\\setminus\nK_2)$ preserving peripheral structure although their A-polynomials have the\nfactorization $A_{K_2}(L,M) \\mid A_{K_1}(L,M)$. Our construction accounts for\nmost of the known factorizations of this form for knots with 10 or fewer\ncrossings. In particular, we conclude that while an epimorphism will lead to a\nfactorization of A-polynomials, the converse generally fails.\n",
        "  We present a method to leverage radical for learning Chinese character\nembedding. Radical is a semantic and phonetic component of Chinese character.\nIt plays an important role as characters with the same radical usually have\nsimilar semantic meaning and grammatical usage. However, existing Chinese\nprocessing algorithms typically regard word or character as the basic unit but\nignore the crucial radical information. In this paper, we fill this gap by\nleveraging radical for learning continuous representation of Chinese character.\nWe develop a dedicated neural architecture to effectively learn character\nembedding and apply it on Chinese character similarity judgement and Chinese\nword segmentation. Experiment results show that our radical-enhanced method\noutperforms existing embedding learning algorithms on both tasks.\n",
        "  Purpose: Regional lung volume change as a function of lung inflation serves\nas an index of parenchymal and airway status as well as an index of regional\nventilation and can be used to detect pathologic changes over time. In this\narticle, we propose a new regional measure of lung mechanics --- the specific\nair volume change by corrected Jacobian.\n  Methods: 4DCT and Xe-CT data sets from four adult sheep are used in this\nstudy. Nonlinear, 3D image registration is applied to register an image\nacquired near end inspiration to an image acquired near end expiration.\nApproximately 200 annotated anatomical points are used as landmarks to evaluate\nregistration accuracy. Three different registration-based measures of regional\nlung mechanics are derived and compared: the specific air volume change\ncalculated from the Jacobian (SAJ); the specific air volume change calculated\nby the corrected Jacobian (SACJ); and the specific air volume change by\nintensity change (SAI).\n  Results: After registration, the mean registration error is on the order of 1\nmm. For cubical ROIs in cubes with size 20 mm $\\times$ 20 mm $\\times$ 20 mm,\nthe SAJ and SACJ measures show significantly higher correlation (linear\nregression, average $r^2=0.75$ and $r^2=0.82$) with the Xe-CT based measure of\nspecific ventilation (sV) than the SAI measure. For ROIs in slabs along the\nventral-dorsal vertical direction with size of 150 mm $\\times$ 8 mm $\\times$ 40\nmm, the SAJ, SACJ, and SAI all show high correlation (linear regression,\naverage $r^2=0.88$, $r^2=0.92$ and $r^2=0.87$) with the Xe-CT based sV without\nsignificant differences when comparing between the three methods.\n  Conclusion: Given a deformation field by an image registration algorithm,\nsignificant differences between the SAJ, SACJ, and SAI measures were found at a\nregional level compared to the Xe-CT sV in four sheep that were studied.\n",
        "  For any closed surface $S$ of genus $g \\geq 2$, we show that the deformation\nspace of marked hyperbolic 3-manifolds homotopy equivalent to $S$, $AH(S \\times\nI)$, is not locally connected. This proves a conjecture of Bromberg who\nrecently proved that the space of Kleinian punctured torus groups is not\nlocally connected. Playing an essential role in our proof is a new version of\nthe filling theorem that is based on the theory of cone-manifold deformations\ndeveloped by Hodgson, Kerckhoff, and Bromberg.\n",
        "  Selectivity estimation is important in query optimization, however accurate\nestimation is difficult when predicates are complex. Instead of existing\ndatabase synopses and statistics not helpful for such cases, we introduce a new\napproach to compute the exact selectivity by running an aggregate query during\nthe optimization phase. Exact selectivity can be achieved without significant\noverhead for in-memory and GPU-accelerated databases by adding extra query\nexecution calls. We implement a selection push-down extension based on the\nnovel selectivity estimation strategy in the MapD database system. Our approach\nrecords constant and less than 30 millisecond overheads in any circumstances\nwhile running on GPU. The novel strategy successfully generates better query\nexecution plans which result in performance improvement up to 4.8 times from\nTPC-H benchmark SF-50 queries and 7.3 times from star schema benchmark SF-80\nqueries.\n",
        "  Complex Event Processing (CEP) is an emerging field with important\napplications in many areas. CEP systems collect events arriving from input data\nstreams and use them to infer more complex events according to predefined\npatterns. The Non-deterministic Finite Automaton (NFA) is one of the most\npopular mechanisms on which such systems are based. During the event detection\nprocess, NFAs incrementally extend previously observed partial matches until a\nfull match for the query is found. As a result, each arriving event needs to be\nprocessed to determine whether a new partial match is to be initiated or an\nexisting one extended. This method may be highly inefficient when many of the\nevents do not result in output matches. We present a lazy evaluation mechanism\nthat defers processing of frequent event types and stores them internally upon\narrival. Events are then matched in ascending order of frequency, thus\nminimizing potentially redundant computations. We introduce a Lazy Chain NFA,\nwhich utilizes the above principle, and does not depend on the underlying\npattern structure. An algorithm for constructing a Lazy Chain NFA for common\npattern types is presented, including conjunction, negation and iteration.\nFinally, we experimentally evaluate our mechanism on real-world stock trading\ndata. The results demonstrate a performance gain of two orders of magnitude\nover traditional NFA-based approaches, with significantly reduced memory\nresource requirements.\n",
        "  In neural machine translation (NMT), researchers face the challenge of\nun-seen (or out-of-vocabulary OOV) words translation. To solve this, some\nresearchers propose the splitting of western languages such as English and\nGerman into sub-words or compounds. In this paper, we try to address this OOV\nissue and improve the NMT adequacy with a harder language Chinese whose\ncharacters are even more sophisticated in composition. We integrate the Chinese\nradicals into the NMT model with different settings to address the unseen words\nchallenge in Chinese to English translation. On the other hand, this also can\nbe considered as semantic part of the MT system since the Chinese radicals\nusually carry the essential meaning of the words they are constructed in.\nMeaningful radicals and new characters can be integrated into the NMT systems\nwith our models. We use an attention-based NMT system as a strong baseline\nsystem. The experiments on standard Chinese-to-English NIST translation shared\ntask data 2006 and 2008 show that our designed models outperform the baseline\nmodel in a wide range of state-of-the-art evaluation metrics including LEPOR,\nBEER, and CharacTER, in addition to BLEU and NIST scores, especially on the\nadequacy-level translation. We also have some interesting findings from the\nresults of our various experiment settings about the performance of words and\ncharacters in Chinese NMT, which is different with other languages. For\ninstance, the fully character level NMT may perform well or the state of the\nart in some other languages as researchers demonstrated recently, however, in\nthe Chinese NMT model, word boundary knowledge is important for the model\nlearning.\n",
        "  Nowadays, more and more RDF data is becoming available on the Semantic Web.\nWhile the Semantic Web is generally incomplete by nature, on certain topics, it\nalready contains complete information and thus, queries may return all answers\nthat exist in reality. In this paper we develop a technique to check query\ncompleteness based on RDF data annotated with completeness information, taking\ninto account data-specific inferences that lead to an inference problem which\nis $\\Pi^P_2$-complete. We then identify a practically relevant fragment of\ncompleteness information, suitable for crowdsourced, entity-centric RDF data\nsources such as Wikidata, for which we develop an indexing technique that\nallows to scale completeness reasoning to Wikidata-scale data sources. We\nverify the applicability of our framework using Wikidata and develop COOL-WD, a\ncompleteness tool for Wikidata, used to annotate Wikidata with completeness\nstatements and reason about the completeness of query answers over Wikidata.\nThe tool is available at http://cool-wd.inf.unibz.it/.\n",
        "  Iterative image reconstruction (IIR) with sparsity-exploiting methods, such\nas total variation (TV) minimization, investigated in compressive sensing (CS)\nclaim potentially large reductions in sampling requirements. Quantifying this\nclaim for computed tomography (CT) is non-trivial, because both full sampling\nin the discrete-to-discrete imaging model and the reduction in sampling\nadmitted by sparsity-exploiting methods are ill-defined. The present article\nproposes definitions of full sampling by introducing four sufficient-sampling\nconditions (SSCs). The SSCs are based on the condition number of the system\nmatrix of a linear imaging model and address invertibility and stability. In\nthe example application of breast CT, the SSCs are used as reference points of\nfull sampling for quantifying the undersampling admitted by reconstruction\nthrough TV-minimization. In numerical simulations, factors affecting admissible\nundersampling are studied. Differences between few-view and few-detector bin\nreconstruction as well as a relation between object sparsity and admitted\nundersampling are quantified.\n",
        "  Relatedness and synergy affect the selection pressure on cooperation and\naltruism. Although early work investigated the effect of these factors\nindependently of each other, recent efforts have been aimed at exploring their\ninterplay. Here, we contribute to this ongoing synthesis in two distinct but\ncomplementary ways. First, we integrate models of $n$-player matrix games into\nthe direct fitness approach of inclusive fitness theory, hence providing a\nframework to consider synergistic social interactions between relatives in\nfamily and spatially structured populations. Second, we illustrate the\nusefulness of this framework by delineating three distinct types of helping\ntraits (\"whole-group\", \"nonexpresser-only\" and \"expresser-only\"), which are\ncharacterized by different synergies of kind (arising from differential fitness\neffects on individuals expressing or not expressing helping) and can be\nsubjected to different synergies of scale (arising from economies or\ndiseconomies of scale). We find that relatedness and synergies of kind and\nscale can interact to generate nontrivial evolutionary dynamics, such as cases\nof bistable coexistence featuring both a stable equilibrium with a positive\nlevel of helping and an unstable helping threshold. This broadens the\nqualitative effects of relatedness (or spatial structure) on the evolution of\nhelping.\n",
        "  Transparent conductive indium tin oxide (ITO) thin films, electrochemically\nintercalated with sodium or other cations, show tunable superconducting\ntransitions with a maximum $T_c$ at 5 K. The transition temperature and the\ndensity of states, $D(E_F)$ (extracted from the measured Pauli susceptibility\n$\\chi_p$ exhibit the same dome shaped behavior as a function of electron\ndensity. Optimally intercalated samples have an upper critical field $\\approx\n4$ T and $\\Delta/{k_BT_c} \\approx 2.0$. Accompanying the development of\nsuperconductivity, the films show a reversible electrochromic change from\ntransparent to colored and are partially transparent (orange) at the peak of\nthe superconducting dome. This reversible intercalation of alkali and alkali\nearth ions into thin ITO films opens diverse opportunities for tunable,\noptically transparent superconductors.\n",
        "  We describe some regular techniques of calculating finite degree invariants\nof triple points free smooth plane curves $S^1 \\to R^2$. They are a direct\nanalog of similar techniques for knot invariants and are based on the calculus\nof {\\em triangular diagrams} and {\\em connected hypergraphs} in the same way as\nthe calculation of knot invariants is based on the study of chord diagrams and\nconnected graphs.\n  E.g., the simplest such invariant is of degree 4 and corresponds to the\ndiagram consisting of two triangles with alternating vertices in a circle in\nthe same way as the simplest knot invariant (of degree 2) corresponds to the\n2-chord diagram $\\bigoplus$. Also, following V.I.Arnold and other authors we\nconsider invariants of {\\em immersed} triple points free curves and describe\nsimilar techniques also for this problem, and, more generally, for the\ncalculation of homology groups of the space of immersed plane curves without\npoints of multiplicity $\\ge k$ for any $k \\ge 3.$\n",
        "  An important part of textual inference is making deductions involving\nmonotonicity, that is, determining whether a given assertion entails\nrestrictions or relaxations of that assertion. For instance, the statement 'We\nknow the epidemic spread quickly' does not entail 'We know the epidemic spread\nquickly via fleas', but 'We doubt the epidemic spread quickly' entails 'We\ndoubt the epidemic spread quickly via fleas'. Here, we present the first\nalgorithm for the challenging lexical-semantics problem of learning linguistic\nconstructions that, like 'doubt', are downward entailing (DE). Our algorithm is\nunsupervised, resource-lean, and effective, accurately recovering many DE\noperators that are missing from the hand-constructed lists that\ntextual-inference systems currently use.\n",
        "  Purpose: 3D rotational setup errors in radiotherapy are often ignored by most\nclinics due to inability to correct or simulate them accurately and\nefficiently. There are two types of rotation-related problems in a clinical\nsetting. One is to assess the affected dose distribution in real-time if\ncorrection is not applied and the other one is to correct the rotational setup\nerrors prior to the initiation of the treatment. Here, we present our\nanalytical solutions to both problems.\n",
        "  (Abridged) We present palaeoeconomy reconstructions for pre-modern\nagriculture, with the Cucuteni-Trypillia Cultural unity (5,400-2,700 BC, modern\nUkraine, Moldova and Romania) as example. The starting point of our analysis is\nthe palaeodiet structure suggested by archaeological data, stable isotope\nanalyses of human remains, and palynology. We allow for the archeologically\nattested contributions of domesticated and wild animal products to the diet,\ndevelop plausible estimates of the yield of ancient cereal varieties cultivated\nwith ancient techniques, and quantify the yield dependence on the time after\ninitial planting and on rainfall (as a climate proxy). Our conclusions involve\nanalysis of the labour costs of the agricultural cycle of both an individual\nand a farmer's family. Finally, we put our results into the context of the\nexploitation territory and catchment analysis. The simplest economic complex\nbased on cereals, domestic and wild animal products, with fallow cropping,\nappears to be capable of supporting an isolated, relatively small farming\ncommunity of 50-300 people (2-10 ha). Our results strongly suggest that dairy\nproducts played a significant role in the dietary and labour balance. The\nsmaller settlements are typical of the earliest Trypillia A but remain\npredominant at the later stages. A larger settlement of several hundred people\ncould function in isolation only with technological innovations, such as manure\nfertiliser and ard tillage. Very large settlements of a few hundred hectares\ncould function only if supported by satellite farming villages and stable\nexchange networks. We also discuss, quantify and assess some strategies to\nmitigate the risks of arable agriculture associated with strong temporal\nfluctuations in the cereal yield, such as manure fertilisation, increased\nfraction of cereals in the diet combined with producing grain surplus for\nemergency storage.\n",
        "  Based on first-principles FLAPW-GGA calculations, we have investigated the\nelectronic structure of newly synthesized novel superconductors LaONiBi and\nLaOCuBi, the first bismuth-containing compounds from the family of quaternary\noxypnictides which attract now a great deal of interest in search for novel\n26-55K superconductors. The band structure, density of states and Fermi\nsurfaces are discussed. Our results indicate that the bonding inside of the\n(La-O) and (Ni(Cu)-Bi) layers is covalent whereas the bonding between the\n(La-O) and (Ni(Cu)- Bi) blocks is mostly ionic. For both oxybismuthides, the\nDOSs at the Fermi level are formed mainly by the states of the (Ni(Cu)-Bi)\nlayers, the corresponding Fermi surfaces have a twodimensional character and\nthe conduction should be strongly anisotropic andhappen only on the (Ni(Cu)-Bi)\nlayers. As a whole, the new oxybismuthides may be described as low-TC\nsuperconducting non-magnetic ionic metals.\n",
        "  This paper outlines the results of sentence level linguistics based rules for\nimproving part-of-speech tagging. It is well known that the performance of\ncomplex NLP systems is negatively affected if one of the preliminary stages is\nless than perfect. Errors in the initial stages in the pipeline have a\nsnowballing effect on the pipeline's end performance. We have created a set of\nlinguistics based rules at the sentence level which adjust part-of-speech tags\nfrom state-of-the-art taggers. Comparison with state-of-the-art taggers on\nwidely used benchmarks demonstrate significant improvements in tagging accuracy\nand consequently in the quality and accuracy of NLP systems.\n",
        "  Locomotion is a natural task that has been assessed since decades and used as\na proxy to highlight impairments of various origins. Most studies adopted\nclassical linear analyses of spatio-temporal gait parameters. Here, we use more\nadvanced, yet not less practical, non-linear techniques to analyse gait time\nseries of healthy subjects. We aimed at finding more sensitive indexes related\nto spatio-temporal gait parameters than those previously used, with the hope to\nbetter identify abnormal locomotion. We analysed large-scale stride interval\ntime series and mean step width in 34 participants while altering walking\ndirection (forward vs. backward walking) and with or without galvanic\nvestibular stimulation. The Hurst exponent $\\alpha$ and the Minkowski fractal\ndimension $D$ were computed and interpreted as indexes expressing\npredictability and complexity of stride interval time series, respectively. We\nshow that $\\alpha$ and $D$ accurately capture stride interval changes in\nfunction of the experimental condition. Walking forward exhibited maximal\ncomplexity ($D$) and hence, adaptability. In contrast, any perturbation\n(walking backward and/or stimulation of the vestibular system) decreased it.\nFurthermore, walking backward increased predictability ($\\alpha$) through a\nmore stereotyped pattern of the stride interval and galvanic vestibular\nstimulation reduced predictability. The present study demonstrates the\ncomplementary power of the Hurst exponent and the fractal dimension to improve\nwalking classification. These holistic indexes can easily be interpreted in the\nframework of optimal movement complexity. Our developments may have immediate\napplications in rehabilitation, diagnosis, and classification procedures.\n",
        "  We present self-consistent triaxial stellar systems that have analytic\ndistribution functions (DFs) expressed in terms of the actions. These provide\ntriaxial density profiles with cores or cusps at the centre. They are the first\nself-consistent triaxial models with analytic DFs suitable for modelling giant\nellipticals and dark haloes. Specifically, we study triaxial models that\nreproduce the Hernquist profile from Williams & Evans (2015), as well as\nflattened isochrones of the form proposed by Binney (2014). We explore the\nkinematics and orbital structure of these models in some detail. The models\ntypically become more radially anisotropic on moving outwards, have velocity\nellipsoids aligned in Cartesian coordinates in the centre and aligned in\nspherical polar coordinates in the outer parts.\n  In projection, the ellipticity of the isophotes and the position angle of the\nmajor axis of our models generally changes with radius. So, a natural\napplication is to elliptical galaxies that exhibit isophote twisting. As\ntriaxial St\\\"ackel models do not show isophote twists, our DFs are the first to\ngenerate mass density distributions that do exhibit this phenomenon, typically\nwith a gradient of $\\approx 10^\\circ$/effective radius, which is comparable to\nthe data.\n  Triaxiality is a natural consequence of models that are susceptible to the\nradial orbit instability. We show how a family of spherical models with\nanisotropy profiles that transition from isotropic at the centre to radially\nanisotropic becomes unstable when the outer anisotropy is made sufficiently\nradial. Models with a larger outer anisotropy can be constructed but are found\nto be triaxial. We argue that the onset of the radial orbit instability can be\nidentified with the transition point when adiabatic relaxation yields strongly\ntriaxial rather than weakly spherical endpoints.\n",
        "  In biology, information flows from the environment to the genome by the\nprocess of natural selection. But it has not been clear precisely what sort of\ninformation metric properly describes natural selection. Here, I show that\nFisher information arises as the intrinsic metric of natural selection and\nevolutionary dynamics. Maximizing the amount of Fisher information about the\nenvironment captured by the population leads to Fisher's fundamental theorem of\nnatural selection, the most profound statement about how natural selection\ninfluences evolutionary dynamics. I also show a relation between Fisher\ninformation and Shannon information (entropy) that may help to unify the\ncorrespondence between information and dynamics. Finally, I discuss possible\nconnections between the fundamental role of Fisher information in statistics,\nbiology, and other fields of science.\n",
        "  Understanding the detailed behaviour of superconducting pair breaking photon\ndetectors such as Kinetic Inductance Detectors requires knowledge of the\nnonequilibrium quasiparticle energy distributions. We have previously\ncalculated the steady state distributions resulting from uniform absorption of\nmonochromatic sub gap and above gap frequency radiation by thin films. In this\nwork, we use the same methods to calculate the effect of illumination by\nbroadband sources, such as thermal radiation from astrophysical phenomena or\nfrom the readout system. Absorption of photons at multiple above gap\nfrequencies is shown to not change the structure of the quasiparticle energy\ndistribution close to the superconducting gap. Hence for typical absorbed\npowers, we find the effects of absorption of broadband pair breaking radiation\ncan simply be considered as the sum of the effects of absorption of many\nmonochromatic sources. Distribution averaged quantities, like quasiparticle\ngeneration effciency $\\eta$, match exactly a weighted average over the\nbandwidth of the source of calculations assuming a monochromatic source. For\nsub gap frequencies, however, distributing the absorbed power across multiple\nfrequencies does change the low energy quasiparticle distribution. For moderate\nand high absorbed powers, this results in a significantly larger $\\eta$ - a\nhigher number of excess quasiparticles for a broadband source compared to a\nmonochromatic source of equal total absorbed power. Typically in KIDs the\nmicrowave power absorbed has a very narrow bandwidth, but in devices with broad\nresonance characteristics (low quality factors), this increase in $\\eta$ may be\nmeasurable.\n",
        "  NGC 2264-C is a high-mass protocluster where several star-formation events\nare known to have occurred. To investigate whether past protostellar activity\nhas left a chemical imprint in this region, we mapped it in SiO($J = 2-1$), a\nshock tracer, and several other molecular lines with the Nobeyama 45 m\ntelescope. Our observations show the presence of a complex network of\nprotostellar outflows. The strongest SiO emission lies beyond a radius of $\\sim\n0.1$ pc with respect to the center of the clump, and is characterized by broad\n($> 10$ km s$^{-1}$) lines and abundances of $\\sim 1.4 \\times 10^{-8}$ with\nrespect to H$_2$. Interestingly, SiO appears relatively depleted\n($\\chi_\\mathrm{SiO} \\sim 4 \\times 10^{-9}$) within this radius, despite it\nbeing affected by molecular outflow activity. We attribute this to fast\ncondensation of SiO back onto dust grains and/or rapid gas-phase destruction of\nSiO, favored by the high density present in this area ($> 10^6$ cm$^{-3}$).\nFinally, we identify a peripheral, narrow-line ($\\sim 2$ km s$^{-1}$)\ncomponent, where SiO has an abundance of a few times 10$^{-11}$. After\nconsidering different options, we conclude that this weak emission may be\ntracing protostellar shocks from the star formation episode that preceded the\ncurrent one, which have decelerated over time and eventually resulted in SiO\nbeing largely depleted/destroyed. Alternatively, a population of unresolved\nlow-mass protostars may be responsible for the narrow SiO emission.\nHigh-angular resolution observations are necessary to distinguish between these\ntwo possibilities and thus understand the role of SiO as a chemical tracer of\npast star-formation episodes in massive protoclusters.\n",
        "  Random tunneling two-level systems (TLSs) in dielectrics have been of\ninterest recently because they adversely affect the performance of\nsuperconducting qubits. The coupling of TLSs to qubits has allowed individual\nTLS characterization, which has previously been limited to TLSs within (thin)\nJosephson tunneling barriers made from aluminum oxide. Here we report on the\nmeasurement of an individual TLS within the capacitor of a lumped-element LC\nmicrowave resonator, which forms a cavity quantum electrodynamics (CQED) system\nand allows for individual TLS characterization in a different structure and\nmaterial than demonstrated with qubits. Due to the reduced volume of the\ndielectric (80 $\\mu$m$^{3}$), even with a moderate dielectric thickness (250\nnm), we achieve the strong coupling regime as evidenced by the vacuum Rabi\nsplitting observed in the cavity spectrum. A TLS with a coherence time of 3.2\n$\\mu$s was observed in a film of silicon nitride as analyzed with a\nJaynes-Cummings spectral model, which is larger than seen from superconducting\nqubits. As the drive power is increased, we observe an unusual but explicable\nset of continuous and discrete crossovers from the vacuum Rabi split\ntransitions to the Glauber (coherent) state.\n",
        "  Huge amount of data with both space and text information, e.g., geo-tagged\ntweets, is flooding on the Internet. Such spatio-textual data stream contains\nvaluable information for millions of users with various interests on different\nkeywords and locations. Publish/subscribe systems enable efficient and\neffective information distribution by allowing users to register continuous\nqueries with both spatial and textual constraints. However, the explosive\ngrowth of data scale and user base has posed challenges to the existing\ncentralized publish/subscribe systems for spatio-textual data streams.\n  In this paper, we propose our distributed publish/subscribe system, called\nPS2Stream, which digests a massive spatio-textual data stream and directs the\nstream to target users with registered interests. Compared with existing\nsystems, PS2Stream achieves a better workload distribution in terms of both\nminimizing the total amount of workload and balancing the load of workers. To\nachieve this, we propose a new workload distribution algorithm considering both\nspace and text properties of the data. Additionally, PS2Stream supports dynamic\nload adjustments to adapt to the change of the workload, which makes PS2Stream\nadaptive. Extensive empirical evaluation, on commercial cloud computing\nplatform with real data, validates the superiority of our system design and\nadvantages of our techniques on system performance improvement.\n",
        "  A substantial fraction of the baryons of disk galaxies like the Milky Way is\nexpected to reside in coronae of gas at the virial temperature. This is the\nonly realistic reservoir of gas available to feed star formation in the disks.\nThe way this feeding occurs depends crucially on whether galactic coronae can\nfragment into cool clouds via thermal instability. Here I summarize arguments\nsuggesting that galactic coronae are not prone to thermal instability, and I\nbriefly discuss the implications for galaxy-formation models and for the origin\nof the high-velocity clouds of the Milky Way.\n",
        "  Peer to peer marketplaces such as AirBnB enable transactional exchange of\nservices directly between people. In such platforms, those providing a service\n(hosts in AirBnB) are faced with various choices. For example in AirBnB,\nalthough some amenities in a property (attributes of the property) are fixed,\nothers are relatively flexible and can be provided without significant effort.\nProviding an amenity is usually associated with a cost. Naturally different\nsets of amenities may have a different \"gains\" for a host. Consequently, given\na limited budget, deciding which amenities (attributes) to offer is\nchallenging.\n  In this paper, we formally introduce and define the problem of Gain\nMaximization over Flexible Attributes (GMFA). We first prove that the problem\nis NP-hard and show that identifying an approximate algorithm with a constant\napproximate ratio is unlikely. We then provide a practically efficient exact\nalgorithm to the GMFA problem for the general class of monotonic gain\nfunctions, which quantify the benefit of sets of attributes. As the next part\nof our contribution, we focus on the design of a practical gain function for\nGMFA. We introduce the notion of frequent-item based count (FBC), which\nutilizes the existing tuples in the database to define the notion of gain, and\npropose an efficient algorithm for computing it. We present the results of a\ncomprehensive experimental evaluation of the proposed techniques on real\ndataset from AirBnB and demonstrate the practical relevance and utility of our\nproposal.\n",
        "  We examine the effect of momentum-driven OB-star stellar winds on a parameter\nspace of simulated turbulent Giant Molecular Clouds using SPH hydrodynamical\nsimulations. By comparison with identical simulations in which ionizing\nradiation was included instead of winds, we show that momentum-driven winds are\nconsiderably less effective in disrupting their host clouds than are HII\nregions. The wind bubbles produced are smaller and generally smoother than the\ncorresponding ionization-driven bubbles. Winds are roughly as effective in\ndestroying the very dense gas in which the O-stars are embedded, and thus\nshutting down the main regions of star-forming activity in the model clouds.\nHowever, their influence falls off rapidly with distance from the sources, so\nthey are not as good at sweeping up dense gas and triggering star formation\nfurther out in the clouds. As a result, their effect on the star formation rate\nand efficiency is generally more negative than that of ionization, if they\nexert any effect at all.\n",
        "  A novel rf-SQUID flux qubit that is robust against fabrication variations in\nJosephson junction critical currents and device inductance has been\nimplemented. Measurements of the persistent current and of the tunneling energy\nbetween the two lowest lying states, both in the coherent and incoherent\nregime, are presented. These experimental results are shown to be in agreement\nwith predictions of a quantum mechanical Hamiltonian whose parameters were\nindependently calibrated, thus justifying the identification of this device as\na flux qubit. In addition, measurements of the flux and critical current noise\nspectral densities are presented that indicate that these devices with Nb\nwiring are comparable to the best Al wiring rf-SQUIDs reported in the\nliterature thusfar, with a $1/f$ flux noise spectral density at $1 $Hz of\n$1.3^{+0.7}_{-0.5} \\mu\\Phi_0/\\sqrt{\\text{Hz}}$. An explicit formula for\nconverting the observed flux noise spectral density into a free induction decay\ntime for a flux qubit biased to its optimal point and operated in the energy\neigenbasis is presented.\n",
        "  Variant Stochastic cracking is a significantly more resilient approach to\nadaptive indexing. It showed [1]that Stochastic cracking uses each query as a\nhint on how to reorganize data, but not blindly so; it gains resilience and\navoids performance bottlenecks by deliberately applying certain arbitrary\nchoices in its decision making. Therefore bring, adaptive indexing forward to a\nmature formulation that confers the workload-robustness that previous\napproaches lacked. Original cracking relies on the randomness of the workloads\nto converge well. [2][3] However, where the workload is non-random, cracking\nneeds to introduce randomness on its own. Stochastic Cracking clearly improves\nover original cracking by being robust in workload changes while maintaining\nall original cracking features when it comes to adaptation. But looking at both\ntypes of cracking, it conveyed an incomplete picture as at some point of time\nit is must to know whether the workload is random or sequential. In this paper\nour focus is on optimization of variant stochastic cracking, that could be\nachieved in two ways either by reducing the initialization cost to make\nstochastic cracking even more transparent to the user, especially for queries\nthat initiate a workload change and hence incur a higher cost or by combining\nthe strengths of the various stochastic cracking algorithms via a dynamic\ncomponent that decides which algorithm to choose for a query on the fly. The\nefforts have been put in to make an algorithm that reduces the initialization\ncost by using the main notion of both cracking, while considering the\nrequirements of adaptive indexing [2].\n",
        "  Scientific discoveries are increasingly driven by analyzing large volumes of\nimage data. Many new libraries and specialized database management systems\n(DBMSs) have emerged to support such tasks. It is unclear, however, how well\nthese systems support real-world image analysis use cases, and how performant\nare the image analytics tasks implemented on top of such systems. In this\npaper, we present the first comprehensive evaluation of large-scale image\nanalysis systems using two real-world scientific image data processing use\ncases. We evaluate five representative systems (SciDB, Myria, Spark, Dask, and\nTensorFlow) and find that each of them has shortcomings that complicate\nimplementation or hurt performance. Such shortcomings lead to new research\nopportunities in making large-scale image analysis both efficient and easy to\nuse.\n",
        "  Purpose: To identify the most informative methods for reporting results of\ntreatment planning comparisons.\n  Methods: Seven papers from the past year of International Journal of\nRadiation Oncology Biology Physics reported on comparisons of treatment plans\nfor IMRT and IMAT. The papers were reviewed to identify methods of comparisons.\nDecision theoretical concepts were used to evaluate the study methods and\nhighlight those that provide the most information.\n  Results: None of the studies examined the correlation between objectives.\nStatistical comparisons provided some information but not enough to make\nprovide support for a robust decision analysis.\n  Conclusion: The increased use of treatment planning studies to evaluate\ndifferent methods in radiation therapy requires improved standards for\ndesigning the studies and reporting the results.\n",
        "  Polycrystalline NdFeAsO0.88F0.12 superconductors prepared by high pressure\n(HP) and ambient pressure (AP) method were comparatively studied by\nmagnetization and transport measurements. Upper critical field (Hc2),\nirreversibility field (Hirr) and the anisotropy parameter were estimated from\nresistance transition curves. The broadening of transition width was observed,\nand ascribed to both Hc2 anisotropy and superconductivity inhomogeneity of\nsamples. Magnetic hysteresis loops (MHLs) in low fields were measured to detect\nthe trace of weak-link behavior. The reclosed hysteresis loops in low fields\nmanifest that there are weak-links in both samples. Magnetization critical\ncurrent density Jcm were derived from MHLs. High-pressure synthesized sample\nshows higher Jcm. However, by means of direct transport I-V measurements,\ntransport critical current density Jct was very low. The Jct values for two\nsamples are comparable. Large discrepancies between Jcm and Jct also indicate\nthat there are weak-links in both samples. The relative mechanism is discussed\nin detail.\n",
        "  A link diagram is said to be lune-free if, when viewed as a 4-regular plane\ngraph it does not have multiple edges between any pair of nodes. We prove that\nany colored link diagram is equivalent to a colored lune-free diagram with the\nsame number of colors. Thus any colored link diagram with a minimum number of\ncolors (known as a minimal diagram) is equivalent to a colored lune-free\ndiagram with that same number of colors. We call the passage from a link\ndiagram to an equivalent lune-free diagram its delunification process.\n  We then introduce a notion of grey sets in order to obtain higher lower\nbounds for minimum number of colors. We calculate these higher lower bounds for\na number of prime moduli with the help of computer programs.\n  For each number of crossings through 16, we list the lune-free diagrams and\nwe color them. If the number of colors equals the corresponding higher lower\nbound we know we have a minimum number of colors. We also introduce and list\nthe lune-free crossing number of a link i.e., the minimum number of crossings\nneeded for a lune-free diagram of this link, and other related link invariants.\n",
        "  Pareatic snakes possess outstanding asymmetry in the mandibular tooth number,\nwhich has probably been caused by its evolution to improve the feeding on the\npredominant dextral snails. Gene mutation can generate chiral inversion on the\nsnail body. A sinistral snail population can thrive in this ecological context.\nThe interactions between dextral/sinistral snails and Pareas snakes are modeled\nin this paper by using a new generalized functional response of Holling type\nII. Distinct Pareas species show different bilateral asymmetry degrees. This\nparameter plays an essential role in our model and determines the evolution of\nthe populations. Stability of the solutions is also analyzed for different\nregimes in the space of parameters.\n",
        "  We investigate the evolution of star formation rates (SFRs), stellar masses,\nand M/L$_{3.4 \\mu m}$ ratios of brightest cluster galaxies (BCGs) in the COSMOS\nsurvey since z ~ 1 to determine the contribution of star formation to the\ngrowth-rate of BCG stellar mass over time. Through the spectral energy\ndistribution (SED) fitting of the GALEX, CFHT, Subaru, Vista, Spitzer, and\nHerschel photometric data available in the COSMOS2015 catalog, we estimate the\nstellar mass and SFR of each BCG. We use a modified version of the iSEDfit\npackage to fit the SEDs of our sample with both stellar and dust emission\nmodels, as well as constrain the impact of star formation history assumptions\non our results. We find that in our sample of COSMOS BCGs, star formation\nevolves similarly to that in BCGs in samples of more massive galaxy clusters.\nHowever, compared to the latter, the magnitude of star formation in our sample\nis lower by ~ 1 dex. Additionally, we find an evolution of BCG baryonic\nmass-to-light ratio (M/L$_{3.4 \\mu m}$) with redshift which is consistent with\na passively aging stellar population. We use this to build upon Wen et al.'s\n(2013) low-redshift $\\nu$L$_{3.4 \\mu m}$-M$_{Stellar}$ relation, quantifying a\ncorrelation between $\\nu$L$_{3.4 \\mu m}$ and M$_{Stellar}$ to z ~ 1. By\ncomparing our results to BCGs in Sunyaev-Zel'dovich and X-ray-selected samples\nof galaxy clusters, we find evidence that the normalization of star formation\nevolution in a cluster sample is driven by the mass range of the sample and may\nbe biased upwards by cool cores.\n",
        "  We show that the subsurface projection of a train track splitting sequence is\nan unparameterized quasi-geodesic in the curve complex of the subsurface. For\nthe proof we introduce induced tracks, efficient position, and wide curves.\n  This result is an important step in the proof that the disk complex is Gromov\nhyperbolic. As another application we show that train track sliding and\nsplitting sequences give quasi-geodesics in the train track graph, generalizing\na result of Hamenstaedt [Invent. Math.].\n",
        "  We present Atacama Large Millimeter Array (ALMA) detections of atomic carbon\nline and dust continuum emission in two UV-luminous galaxies at redshift 6. The\nfar-infrared (FIR) luminosities of these galaxies are substantially lower than\nsimilar starbursts at later cosmic epochs, indicating an evolution in the dust\nproperties with redshift, in agreement with the evolution seen in ultraviolet\n(UV) attenuation by dust. The [CII] to FIR ratios are found to be higher than\nat low redshift showing that [CII] should be readily detectable by ALMA within\nthe reionization epoch. One of the two galaxies shows a complex merger nature\nwith the less massive component dominating the UV emission and the more massive\ncomponent dominating the FIR line and continuum. Using the interstellar atomic\ncarbon line to derive the systemic redshifts we investigate the velocity of\nLyman alpha emission emerging from high-z galaxies. In contrast to previous\nwork, we find no evidence for decreasing Lyman alpha velocity shifts at\nhigh-redshift. We observe an increase in velocity shifts from z$\\sim$2 to\nz$\\sim$6, consistent with the effects of increased IGM absorption.\n",
        "  We present a novel approach for determining learners' second language\nproficiency which utilizes behavioral traces of eye movements during reading.\nOur approach provides stand-alone eyetracking based English proficiency scores\nwhich reflect the extent to which the learner's gaze patterns in reading are\nsimilar to those of native English speakers. We show that our scores correlate\nstrongly with standardized English proficiency tests. We also demonstrate that\ngaze information can be used to accurately predict the outcomes of such tests.\nOur approach yields the strongest performance when the test taker is presented\nwith a suite of sentences for which we have eyetracking data from other\nreaders. However, it remains effective even using eyetracking with sentences\nfor which eye movement data have not been previously collected. By deriving\nproficiency as an automatic byproduct of eye movements during ordinary reading,\nour approach offers a potentially valuable new tool for second language\nproficiency assessment. More broadly, our results open the door to future\nmethods for inferring reader characteristics from the behavioral traces of\nreading.\n",
        "  J. Davis showed that the topological concordance class of a link in the\n3-sphere is uniquely determined by its Alexander polynomial for 2-component\nlinks with Alexander polynomial one. A similar result for knots with Alexander\npolynomial one was shown earlier by M. Freedman. We prove that these two cases\nare the only exceptional cases, by showing that the link concordance class is\nnot determined by the Alexander invariants in any other case.\n",
        "  X-ray energy spectrum plays an essential role in computed tomography (CT)\nimaging and related tasks. Due to the high photon flux of clinical CT scanners,\nmost of spectrum estimation methods are indirect and usually suffered from\nvarious limitations. In this study, we aim to provide a segmentation-free\nindirect transmission measurement-based energy spectrum estimation method using\ndual-energy material decomposition. The general principle of the method is to\nminimize the quadratic error between the polychromatic forward projection and\nthe raw projection to calibrate a set of unknown weights which are used to\nexpress the unknown spectrum together with a set of model spectra. The\npolychromatic forward projection is performed using material-specific images\nwhich are obtained using dual-energy material decomposition. The algorithm has\nbeen evaluated using numerical simulations, experimental phantom data as well\nas realistic patient data. The results show the estimated spectrum matches the\nreference spectrum quite well and the method is robust. Extensive studies\nsuggest the method provides accurate estimate of the CT spectrum without\ndedicated physical phantom and prolonged work flow. This paper may be\nattractive for CT dose calculations, artifacts reduction, polychromatic image\nreconstruction and other spectrum-involved CT applications.\n",
        "  We describe the methodology required for estimation of photometric estimates\nof metallicity based on the SDSS gri passbands, which can be used to probe the\nproperties of main-sequence stars beyond ~ 10 kpc, complementing studies of\nnearby stars from more metallicity-sensitive color indices that involve the u\npassband. As a first application of this approach, we determine photometric\nmetal abundance estimates for individual main-sequence stars in the Virgo\nOverdensity, which covers almost 1000 square degrees on the sky, based on a\ncalibration of the metallicity sensitivity of stellar isochrones in the gri\nfilter passbands using field stars with well-determined spectroscopic metal\nabundances. Despite the low precision of the method for individual stars,\ninternal errors of in [Fe/H] ~ +/- 0.1 dex can be achieved for bulk stellar\npopulations. The global metal abundance of the Virgo Overdensity determined in\nthis way is <[Fe/H]> = -2.0 +/- 0.1 (internal) +/- 0.5 (systematic), from\nphotometric measurements of 0.7 million stars with heliocentric distances from\n~ 10 kpc to ~ 20 kpc. A preliminary metallicity map, based on results for 2.9\nmillion stars in the northern SDSS DR-7 footprint, exhibits a shift to lower\nmetallicities as one proceeds from the inner- to the outer-halo population,\nconsistent with recent interpretation of the kinematics of local samples of\nstars with spectroscopically available metallicity estimates and full space\nmotions.\n",
        "  Graph database systems are increasingly adapted for storing and processing\nheterogeneous network-like datasets. However, due to the novelty of such\nsystems, no standard data model or query language has yet emerged.\nConsequently, migrating datasets or applications even between related\ntechnologies often requires a large amount of manual work or ad-hoc solutions,\nthus subjecting the users to the possibility of vendor lock-in. To avoid this\nthreat, vendors are working on supporting existing standard languages (e.g.\nSQL) or creating standardised languages.\n  In this paper, we present a formal specification for openCypher, a high-level\ndeclarative graph query language with an ongoing standardisation effort. We\nintroduce relational graph algebra, which extends relational operators by\nadapting graph-specific operators and define a mapping from core openCypher\nconstructs to this algebra. We propose an algorithm that allows systematic\ncompilation of openCypher queries.\n",
        "  Increased noise is a general concern for dual-energy material decomposition.\nHere, we develop an image-domain material decomposition algorithm for\ndual-energy CT (DECT) by incorporating an edge-preserving filter into the Local\nHighlY constrained backPRojection Reconstruction (HYPR-LR) framework. With\neffective use of the non-local mean, the proposed algorithm, which is referred\nto as HYPR-NLM, reduces the noise in dual energy decomposition while preserving\nthe accuracy of quantitative measurement and spatial resolution of the\nmaterial-specific dual energy images. We demonstrate the noise reduction and\nresolution preservation of the algorithm with iodine concentrate numerical\nphantom by comparing the HYPR-NLM algorithm to the direct matrix inversion,\nHYPR-LR and iterative image-domain material decomposition (Iter-DECT). We also\nshow the superior performance of the HYPR-NLM over the existing methods by\nusing two sets of cardiac perfusing imaging data. The reference drawn from the\ncomparison study includes: (1) HYPR-NLM significantly reduces the DECT material\ndecomposition noise while preserving quantitative measurements and\nhigh-frequency edge information, and (2) HYPR-NLM is robust with respect to\nparameter selection.\n",
        "  In this paper, we study surfaces embedded in $4$-manifolds. We give a\ncomplete set of moves relating banded unlink diagrams of isotopic surfaces in\nan arbitrary $4$-manifold. This extends work of Swenton and Kearton-Kurlin in\n$S^4$. As an application, we show that bridge trisections of isotopic surfaces\nin a trisected $4$-manifold are related by a sequence of perturbations and\ndeperturbations, affirmatively proving a conjecture of Meier and Zupan. We also\nexhibit several isotopies of unit surfaces in $\\mathbb{C}P^2$ (i.e. spheres in\nthe generating homology class), proving that many explicit unit surfaces are\nisotopic to the standard $\\mathbb{C}P^1$. This strengthens some previously\nknown results about the Gluck twist in $S^4$, related to Kirby problem 4.23.\n",
        "  In the present article, we use an axially symmetric galactic gravitational\nmodel with a disk-halo and a spherical nucleus, in order to investigate the\ntransition from regular to chaotic motion for stars moving in the meridian\n(r,z) plane. We study in detail the transition from regular to chaotic motion,\nin two different cases: the time independent model and the time evolving model.\nIn the time dependent model, we follow the evolution of orbits as the galaxy\ndevelops a dense and massive nucleus in its core, as mass is transported\nexponentially from the disk to the galactic center. In addition, we construct\nsome numerical diagrams in which we present the correlations between the main\nparameters of our galactic model. Our numerical calculations indicate, that\nstars with values of angular momentum Lz less than or equal to a critical value\nLzc, moving near to the galactic plane, are scattered to the halo upon\nencountering the nuclear region and subsequently display chaotic motion. A\nlinear relationship exists between the critical value of the angular momentum\nLzc and the mass of the nucleus Mn. Furthermore, the extent of the chaotic\nregion increases as the value of the mass of the nucleus increases. Moreover,\nour simulations indicate that the degree of chaos increases linearly, as the\nmass of the nucleus increases. These results strongly indicate that the ordered\nor chaotic nature of orbits, depends on the presence of massive objects in the\ngalactic cores of the galaxies. Our results suggest, that for disk galaxies\nwith massive and prominent nuclei, the low angular momentum stars in the\nassociated central regions of the galaxy, must be in predominantly chaotic\norbits. Some theoretical arguments to support the numerically derived outcomes\nare presented. Comparison with similar previous works is also made.\n",
        "  The performance of Niobium-based Superconducting Radio Frequency (SRF)\nparticle accelerator cavities can be sensitive to localized defects that give\nrise to quenches at high accelerating gradients. In order to identify these\nmaterial defects on bulk Nb surfaces at their operating frequency and\ntemperature, a wide bandwidth microwave microscope with localized and strong RF\nmagnetic fields is developed by integrating a magnetic write head into the\nnear-field microwave microscope to enable mapping of the local electrodynamic\nresponse in the multi-GHz frequency regime at cryogenic temperatures. This\nmagnetic writer demonstrates a localized and strong RF magnetic field on bulk\nNb surface with $B_{surface}$ > 102 mT and sub-micron resolution. By measuring\nthe nonlinear response of the superconductor, nonlinearity coming from the\nnano-scale weak link Josephson junctions due to the contaminated surface in the\ncavity fabrication process is demonstrated.\n",
        "  The article consists of a survey on analytic and topological torsion.\nAnalytic torsion is defined in terms of the spectrum of the analytic Laplace\noperator on a Riemannian manifold, whereas topological torsion is defined in\nterms of a triangulation. The celebrated theorem of Cheeger and M\\\"uller\nidentifies these two notions for closed Riemannian manifolds. We also deal with\nmanifolds with boundary and with isometric actions of finite groups. The basic\ntheme is to extract topological invariants from the spectrum of the analytic\nLaplace operator on a Riemannian manifold.\n",
        "  This note shows that if two elements of equal trace (e.g., conjugate\nelements) generate an arithmetic two-bridge knot or link group, then the\nelements are parabolic. This includes the figure-eight knot and Whitehead link\ngroups. Similarly, if two conjugate elements generate the trefoil knot group,\nthen the elements are peripheral.\n",
        "  We use polarization-resolved Raman scattering to study lattice dynamics in\nNaFe$_{0.53}$Cu$_{0.47}$As single crystals. We identify 4 $A_{1g}$ phonon modes\nat 125, 172, 183 and 197 cm$^{-1}$, and 4 $B_{3g}$ phonon modes at 101, 138,\n173, 226 cm$^{-1}$. The phonon spectra are consistent with the $Ibam$ group,\nwhich confirms that the Cu and Fe atoms form a stripe order. The temperature\ndependence of the phonon spectra suggests weak electron-phonon and\nmagneto-elastic interactions.\n",
        "  A muon-spin relaxation (muSR) investigation is presented for the molecular\nsuperconductor kappa-(BEDT-TTF)2Cu[N(CN)2Br]. Evidence is found for\nlow-temperature phase-separation, with only a fraction of the sample showing a\nsuperconducting signal, even for slow cooling. Rapid cooling reduces the\nsuperconducting fraction still further. For the superconducting phase, the\nin-plane penetration depth is measured to be lambda_{parallel} = 0.47(1) mu m\nand evidence is seen for a vortex decoupling transition in applied fields above\n40 mT. The magnetic fluctuations in the normal state produce Korringa behavior\nof the muon spin relaxation rate below 100 K, a precipitous drop in relaxation\nrate is seen at higher temperatures and an enhanced local spin susceptibility\noccurs just above T_c.\n",
        "  Recently Convolutional Neural Networks (CNNs) models have proven remarkable\nresults for text classification and sentiment analysis. In this paper, we\npresent our approach on the task of classifying business reviews using word\nembeddings on a large-scale dataset provided by Yelp: Yelp 2017 challenge\ndataset. We compare word-based CNN using several pre-trained word embeddings\nand end-to-end vector representations for text reviews classification. We\nconduct several experiments to capture the semantic relationship between\nbusiness reviews and we use deep learning techniques that prove that the\nobtained results are competitive with traditional methods.\n",
        "  We investigated the parameter optimization of ridge filter thickness using a\nMonte Carlo simulation for carbon ion therapy. For this study, a ridge filter\nwas designed for the Spread-Out Bragg Peak (SOBP) by considering the relative\nbiological effect (RBE). The thickness, height, and width of the ridge filter\nwere designed by using the FLUKA and GEANT4 code, and we analyzed and compared\nthe results of the physical dose distribution for the FLUKA and GEANT4 coding.\nThe results show that the minimum width of the groove for the ridge filter\nshould be at least 0.4cm for the appropriate biological dose. The SOBP sections\nare 8cm, 9cm, and 10cm, respectively, when heights are 3.5cm, 4.0cm, and 4.5cm.\nThe height of the ridge filter is designed to be associated with the SOBP\nwidth. Also, the results for the FLUKA and GEANT4 code show that an average\nvalue of difference is 3% and a maximum error is 5%; however, its trend was\nsimilar. Therefore, the height and width of the groove for the ridge filter are\nused for important parameters to decide the length and plateau of SOBP.\n",
        "  We present a detailed study of the complex ionization structure in a small\n(~250 pc) extended narrow line region (ENLR) cloud near Centaurus A using the\nMulti Unit Spectroscopic Explorer. This cloud is located in the so-called outer\nfilament of ionized gas (about 15 kpc from the nucleus) where jet-induced star\nformation has been suggested to occur by different studies. We find that,\ndespite the small size, a mixture of ionization mechanisms is operating,\nresulting in considerable complexity in the spatial ionization structure. The\narea includes two H II regions where star formation is occurring and another\nlocation where star formation must have ceased very recently. Interestingly,\nthe extreme Balmer decrement of one of the star forming regions\n(H_alpha/H_beta~6) indicates that it is still heavily embedded in its natal\ncocoon of gas and dust. At all three locations a continuum counterpart is found\nwith spectra matching those of O/B stars local to Centaurus A. The H II regions\nare embedded in a larger gas complex which is photoionized by the radiation of\nthe central active galactic nucleus (AGN), but the O/B stars affect the spatial\nionization pattern in the ENLR cloud very locally. In particular, in the\nsurroundings of the youngest star forming region, we can isolate a tight mixing\nsequence in the diagnostic diagram going from gas with ionization due to a pure\nstellar continuum to gas only photoionized by the AGN. These results emphasize\nthe complexity and the mixture of processes occurring in star forming regions\nunder the influence of an AGN radiation. This is relevant for our understanding\nof AGN-induced star formation suggested to occur in a number of objects,\nincluding this region of Centaurus A. They also illustrate that these young\nstars influence the gas over only a limited region.\n",
        "  The model of a Local Hot Bubble has been widely accepted as providing a\nframework that can explain the ubiquitous presence of the soft X-ray background\ndiffuse emission. We summarize the current knowledge on this local interstellar\nregion, paying particular reference to observations that sample emission from\nthe presumed local million degree K hot plasma. However, we have listed\nnumerous observations that are seemingly in conflict with the concept of a hot\nLocal Bubble. In particular, the discovery of solar wind charge exchange that\ncan generate an appreciable soft X-ray background signal within the\nheliosphere, has led to a reassessment of the generally accepted model that\nrequires a hot local plasma. In order to explain the majority of observations\nof the local plasma, we forward two new speculative models that describe the\nphysical state of the local interstellar gas. One possible scenario is similar\nto the present widely accepted model of the Local Hot Bubble, except that it\naccounts for only 50% of the soft X-ray emission currently detected in the\ngalactic plane, has a lower thermal pressure than previously thought, and its\nhot plasma is not as hot as previously believed. Although such a model can\nsolve several difficulties with the traditional hot Local Bubble model, a\nheating mechanism for the dimmer and cooler gas remains to be found. The second\npossible explanation is that of the Hot Top model, in which the Local Cavity is\nan old supernova remnant in which no (or very little) million degree local\nplasma is presently required. Instead, the cavity is now thought to be filled\nwith partially ionized cloudlets of temperature 7000 K that are surrounded by\nlower density envelopes of photoionized gas of temperature 20,000 K.\n",
        "  A fundamental requirement of any task-oriented dialogue system is the ability\nto generate object descriptions that refer to objects in the task domain. The\nsubproblem of content selection for object descriptions in task-oriented\ndialogue has been the focus of much previous work and a large number of models\nhave been proposed. In this paper, we use the annotated COCONUT corpus of\ntask-oriented design dialogues to develop feature sets based on Dale and\nReiters (1995) incremental model, Brennan and Clarks (1996) conceptual pact\nmodel, and Jordans (2000b) intentional influences model, and use these feature\nsets in a machine learning experiment to automatically learn a model of content\nselection for object descriptions. Since Dale and Reiters model requires a\nrepresentation of discourse structure, the corpus annotations are used to\nderive a representation based on Grosz and Sidners (1986) theory of the\nintentional structure of discourse, as well as two very simple representations\nof discourse structure based purely on recency. We then apply the\nrule-induction program RIPPER to train and test the content selection component\nof an object description generator on a set of 393 object descriptions from the\ncorpus. To our knowledge, this is the first reported experiment of a trainable\ncontent selection component for object description generation in dialogue.\nThree separate content selection models that are based on the three theoretical\nmodels, all independently achieve accuracies significantly above the majority\nclass baseline (17%) on unseen test data, with the intentional influences model\n(42.4%) performing significantly better than either the incremental model\n(30.4%) or the conceptual pact model (28.9%). But the best performing models\ncombine all the feature sets, achieving accuracies near 60%. Surprisingly, a\nsimple recency-based representation of discourse structure does as well as one\nbased on intentional structure. To our knowledge, this is also the first\nempirical comparison of a representation of Grosz and Sidners model of\ndiscourse structure with a simpler model for any generation task.\n",
        "  We investigate the behavior of the SL(2,C) Casson invariant for 3-manifolds\nobtained by Dehn surgery along two-bridge knots. Using the results of Hatcher\nand Thurston, and also results of Ohtsuki, we outline how to compute the\nCuller--Shalen seminorms, and we illustrate this approach by providing explicit\ncomputations for double twist knots. We then apply the surgery formula of\nCurtis to deduce the SL(2,C) Casson invariant for the 3-manifolds obtained by\np/q-Dehn surgery on such knots. These results are applied to prove\nnontriviality of the SL(2,C) Casson invariant for nearly all 3-manifolds\nobtained by nontrivial Dehn surgery on a hyperbolic two-bridge knot. We relate\nthe formulas derived to degrees of A-polynomials and use this information to\nidentify factors of higher multiplicity in the $\\hat{A}$-polynomial, which is\nthe A-polynomial with multiplicities as defined by Boyer-Zhang.\n",
        "  In the framework of the Herschel-WISH key program, several ortho-H2O and\npara-H2O emission lines, in the frequency range from 500 to 1700 GHz, were\nobserved with the HIFI instrument in two bow-shock regions (B2 and R) of the\nL1157 cloud. The primary aim is to analyse water emission lines as a diagnostic\nof the physical conditions in the blue (B2) and red-shifted (R) lobes to\ncompare the excitation conditions. A total of 5 ortho- and para-H216O plus one\no-H218O transitions were observed in B2 and R with a wide range of excitation\nenergies (27 K<=Eu<=215 K). The H2O spectra, observed in the two shocked\nregions, show that the H2O profiles are markedly different in the two regions.\nIn particular, at the bow-shock R, we observed broad (~30 km s-1 with respect\nto the ambient velocity) red-shifted wings where lines at different excitation\npeak at different red-shifted velocities. The B2 spectra are associated with a\nnarrower velocity range (~6 km s-1), peaking at the systemic velocity. The\nexcitation analysis suggests, for B2, low values of column density NH2O\n<=5{\\times}1013 cm-2, a density range of 105 <=nH2 <=107 cm-3, and warm\ntemperatures (>=300 K). The presence of the broad red-shifted wings and\nmultiple peaks in the spectra of the R region, prompted the modelling of two\ncomponents. High velocities are associated with relatively low temperatures\n(~100K),NH2O{\\simeq}5{\\times}1012-5{\\times}1013 cm-2 and densities\nnH2{\\simeq}106-108 cm-3.Lower velocities are associated with higher excitation\nconditions with Tkin>=300 K, very dense gas (nH2 ~108 cm-3) and low column\ndensity (NH2O<5{\\times}1013 cm-2).\n",
        "  Using the conjugation symmetry on Heegaard Floer complexes, we define a\nthree-manifold invariant called involutive Heegaard Floer homology, which is\nmeant to correspond to $\\mathbb{Z}_4$-equivariant Seiberg-Witten Floer\nhomology. Further, we obtain two new invariants of homology cobordism,\n$\\underline{d}$ and $\\bar{d}$, and two invariants of smooth knot concordance,\n$\\underline{V}_0$ and $\\overline{V}_0$. We also develop a formula for the\ninvolutive Heegaard Floer homology of large integral surgeries on knots. We\ngive explicit calculations in the case of L-space knots and thin knots. In\nparticular, we show that $\\underline{V}_0$ detects the non-sliceness of the\nfigure-eight knot. Other applications include constraints on which large\nsurgeries on alternating knots can be homology cobordant to other large\nsurgeries on alternating knots.\n",
        "  This paper presents the preliminary works to put online a French oral corpus\nand its transcription. This corpus is the Socio-Linguistic Survey in Orleans,\nrealized in 1968. First, we numerized the corpus, then we handwritten\ntranscribed it with the Transcriber software adding different tags about\nspeakers, time, noise, etc. Each document (audio file and XML file of the\ntranscription) was described by a set of metadata stored in an XML format to\nallow an easy consultation. Second, we added different levels of annotations,\nrecognition of named entities and annotation of personal information about\nspeakers. This two annotation tasks used the CasSys system of transducer\ncascades. We used and modified a first cascade to recognize named entities.\nThen we built a second cascade to annote the designating entities, i.e.\ninformation about the speaker. These second cascade parsed the named entity\nannotated corpus. The objective is to locate information about the speaker and,\nalso, what kind of information can designate him/her. These two cascades was\nevaluated with precision and recall measures.\n",
        "  The lattice stick number of a knot type is defined to be the minimal number\nof straight line segments required to construct a polygon presentation of the\nknot type in the cubic lattice. In this paper, we mathematically prove that the\ntrefoil knot $3_1$ and the figure-8 knot $4_1$ are the only knot types of\nlattice stick number less than 15, which verifies the result from previous\nnumerical estimations on this quantity.\n",
        "  State of the art benchmarks for Twitter Sentiment Analysis do not consider\nthe fact that for more than half of the tweets from the public stream a\ndistinct sentiment cannot be chosen. This paper provides a new perspective on\nTwitter Sentiment Analysis by highlighting the necessity of explicitly\nincorporating uncertainty. Moreover, a dataset of high quality to evaluate\nsolutions for this new problem is introduced and made publicly available.\n",
        "  Evolution of gene regulation is crucial for our understanding of the\nphenotypic differences between species, populations and individuals.\nSequence-specific binding of transcription factors to the regulatory regions on\nthe DNA is a key regulatory mechanism that determines gene expression and hence\nheritable phenotypic variation. We use a biophysical model for directional\nselection on gene expression to estimate the rates of gain and loss of\ntranscription factor binding sites (TFBS) in finite populations under both\npoint and insertion/deletion mutations. Our results show that these rates are\ntypically slow for a single TFBS in an isolated DNA region, unless the\nselection is extremely strong. These rates decrease drastically with increasing\nTFBS length or increasingly specific protein-DNA interactions, making the\nevolution of sites longer than ~10 bp unlikely on typical eukaryotic speciation\ntimescales. Similarly, evolution converges to the stationary distribution of\nbinding sequences very slowly, making the equilibrium assumption questionable.\nThe availability of longer regulatory sequences in which multiple binding sites\ncan evolve simultaneously, the presence of \"pre-sites\" or partially decayed old\nsites in the initial sequence, and biophysical cooperativity between\ntranscription factors, can all facilitate gain of TFBS and reconcile\ntheoretical calculations with timescales inferred from comparative genetics.\n",
        "  The theoretical existence of non-classical Schottky groups is due to Marden.\nExplicit examples of such kind of groups are only known in rank two, the first\none by by Yamamoto in 1991 and later by Williams in 2009. In 2006, Maskit and\nthe author provided a theoretical method to obtain examples of non-classical\nSchottky groups in any rank. The method assumes the knowledge of some algebraic\nlimits of Schottky groups, called sufficiently complicated noded Schottky\ngroups, whose existence was stated. In this paper we provide an explicit\nconstruction of a sufficiently complicated noded Schottky group of rank three\nand it is explained how to construct explicit non-classical Schottky groups of\nrank three.\n",
        "  We have measured the strength of the UV upturn for red sequence galaxies in\nthe Abell~1689 cluster at $z=0.18$, reaching to or below the $L^*$ level and\ntherefore probing the general evolution of the upturn phenomenon. We find that\nthe range of UV upturn strengths in the population as a whole has not declined\nover the past 2.2 Gyrs. This is consistent with a model where hot horizontal\nbranch stars, produced by a Helium-enriched population, provide the required UV\nflux. Based on local counterparts, this interpretation of the result implies\nHelium abundances of at least 1.5 times the primordial value for this HB\npopulation, along with high formation and assembly redshifts for the galaxies\nand at least a subset of their stellar populations.\n",
        "  Suppose that $f$ is a homomorphism from the mapping class group\n$\\mathcal{M}(N_{g,n})$ of a nonorientable surface of genus $g$ with $n$\nboundary components, to $\\mathrm{GL}(m,\\mathbb{C})$. We prove that if $g\\ge 5$,\n$n\\le 1$ and $m\\le g-2$, then $f$ factors through the abelianization of\n$\\mathcal{M}(N_{g,n})$, which is $\\mathbb{Z}_2\\times\\mathbb{Z}_2$ for\n$g\\in\\{5,6\\}$ and $\\mathbb{Z}_2$ for $g\\ge 7$. If $g\\ge 7$, $n=0$ and $m=g-1$,\nthen either $f$ has finite image (of order at most two if $g\\ne 8$), or it is\nconjugate to one of four \"homological representations\". As an application we\nprove that for $g\\ge 5$ and $h<g$, every homomorphism\n$\\mathcal{M}(N_{g,0})\\to\\mathcal{M}(N_{h,0})$ factors through the\nabelianization of $\\mathcal{M}(N_{g,0})$.\n",
        "  Within the BCS theory of superconductivity we calculate the superconducting\ngap at zero temperature for metallic hydrogen-graphene system in order to\nestimate the superconducting critical temperature of quasi two dimensional\nhighly oriented pyrolytic graphite. The obtained results are given as a\nfunction of the hydrogen-induced density of carriers $n$ and their effective\nmass $m^\\star$. The obtained gap shows a Maxwell-like distribution with a\nmaximum of $\\sim 60 $K at $n \\sim 3 \\times 10^{14} $cm$^{-2}$ and $m^\\star/m =\n1$. The theoretical results are discussed taking into account recent\nexperimental evidence for granular superconductivity in graphite.\n",
        "  The aim of this study was to evaluate and compare temperature distributions\nfor different tissues being treated at the time of interstitial microwave\nhyperthermia. A coaxial-slot antenna implemented into the tissue is the source\nof microwave radiation. The described model takes into account the wave\nequation for the TM mode and the Pennes equation determining the temperature\ndistribution within the tissue in the stationary case. The simulation results\nfor the three fundamental microwave frequencies of tissue heating devices are\npresented.\n",
        "  The Guard-Guardee model for plant immunity describes how resistance proteins\n(guards) in host cells monitor host target proteins (guardees) that are\nmanipulated by pathogen effector proteins. A recently suggested extension of\nthis model includes decoys, which are duplicated copies of guardee proteins,\nand which have the sole function to attract the effector and, when modified by\nthe effector, trigger the plant immune response. Here we present a\nproof-of-principle model for the functioning of decoys in plant immunity,\nquantitatively developing this experimentally-derived concept. Our model links\nthe basic cellular chemistry to the outcomes of pathogen infection and\nresulting fitness costs for the host. In particular, the model allows\nidentification of conditions under which it is optimal for decoys to act as\ntriggers for the plant immune response, and of conditions under which it is\noptimal for decoys to act as sinks that bind the pathogen effectors but do not\ntrigger an immune response.\n",
        "  Sz\\H ucs proved in 2000 that the $r$-tuple-point manifold of a generic\nimmersion is cobordant to the $\\Sigma^{1_{r-1}}$-point manifold of its generic\nprojection. Here we slightly extend this by showing that the natural mappings\nof these manifolds are bordant to each other. The main novelty of our approach\nis that we construct the bordism explicitly.\n",
        "  The aim of this article is to present an overview of the major families of\nstate-of-the-art data-base benchmarks, namely: relational benchmarks, object\nand object-relational benchmarks, XML benchmarks, and decision-support\nbenchmarks, and to discuss the issues, tradeoffs and future trends in database\nbenchmarking. We particularly focus on XML and decision-support benchmarks,\nwhich are currently the most innovative tools that are developed in this area.\n",
        "  We study decoherence effects in qubits coupled to environments that exhibit\nresonant frequencies in their spectral function. We model the coupling of the\nqubit to its environment via the Caldeira-Leggett formulation of quantum\ndissipation/coherence, and study the simplest example of decoherence effects in\ncircuits with resonances such as a dc SQUID phase qubit in the presence of an\nisolation circuit, which is designed to enhance the coherence time. We\nemphasize that the spectral density of the environment is strongly dependent on\nthe circuit design, and can be engineered to produce longer decoherence times.\nWe begin with a general discussion of superconducting qubits such as the flux\nqubit, the Cooper pair box and the phase qubit and show that in these kinds of\nsystems appropriate circuit design can greatly modify the spectral density of\nthe environment and lead to enhancement of decoherence times. In the particular\ncase of the phase qubit, for instance, we show that when the frequency of the\nqubit is at least two times larger than the resonance frequency of the\nenvironmental spectral density, the decoherence time of the qubit is a few\norders of magnitude larger than that of the typical ohmic regime, where the\nfrequency of the qubit is much smaller than the resonance frequency of the\nspectral density. In addition, we demonstrate that the environment does not\nonly affect the decoherence time, but also the frequency of the transition\nitself, which is shifted from its environment-free value. Second, we show that\nwhen the qubit frequency is nearly the same as the resonant frequency of the\nenvironmental spectral density, an oscillatory non-Markovian decay emerges, as\nthe qubit and its environment self-generate Rabi oscillations of characteristic\ntime scales shorter than the decoherence time.\n",
        "  We have carried out near-infrared polarimetry toward the boundary of the\nCentral Molecular Zone, in the field of (-1.4 deg $\\lesssim l \\lesssim$ -0.3\ndeg and 1.0 deg $\\lesssim l \\lesssim$ 2.9 deg, $|b|\\lesssim$ 0.1 deg), using\nthe near-infrared polarimetric camera SIRPOL on the 1.4 m Infrared Survey\nFacility telescope. We have selected 112 intrinsically polarized sources on the\nbasis of the estimate of interstellar polarization on Stokes $Q/I-U/I$ planes.\nThe selected sources are brighter than $K_S=14.5$ mag and have polarimetric\nuncertainty $\\delta P<1\\,%$. Ten of these distinctive polarized sources are fit\nwell with spectral energy distributions of young stellar objects when using the\nphotometry in the archive of the Spitzer Space Telescope mid-infrared data.\nHowever, many sources have spectral energy distributions of normal stars\nsuffering heavy interstellar extinction; these might be stars behind dark\nclouds. Due to the small number of distinctive polarized sources and candidates\nof young stellar object, we cannot judge if there is a decline of them outside\nthe Central Molecular Zone. Many of massive candidates of young stellar object\nin the literature have only small intrinsic polarization. This might suggest\nthat their masses are 4-15 M$_{{\\rm sun}}$, whose intrinsic polarization has\nbeen expected to be small.\n",
        "  Protogalactic environments are typically identified using quasar absorption\nlines, and these galactic building blocks can manifest as Damped Lyman-Alpha\nAbsorbers (DLAs) and Lyman Limit Systems (LLSs). We use radio observations of\nFaraday effects to test whether DLAs and LLSs host a magnetised medium, by\ncombining DLA and LLS detections throughout the literature with 1.4 GHz\npolarization data from the NRAO VLA Sky Survey (NVSS). We obtain a control, a\nDLA, and a LLS sample consisting of 114, 19, and 27 lines-of-sight respectively\n- all of which are polarized at $\\ge8\\sigma$ to ensure Rician bias is\nnegligible. Using a Bayesian framework, we are unable to detect either coherent\nor random magnetic fields in DLAs: the regular coherent magnetic fields within\nthe DLAs must be $\\le2.8$ $\\mu$G, and the lack of depolarization is consistent\nwith the weakly magnetised gas in DLAs being non-turbulent and quiescent.\nHowever, we find mild suggestive evidence that LLSs have coherent magnetic\nfields: after controlling for the redshift-distribution of our data, we find a\n71.5% probability that LLSs have a higher RM than a control sample. We also\nfind strong evidence that LLSs host random magnetic fields, with a 95.5%\nprobability that LLS lines-of-sight have lower polarized fractions than a\ncontrol sample. The regular coherent magnetic fields within the LLSs must be\n$\\le2.4$ $\\mu$G, and the magnetised gas must be highly turbulent with a typical\nscale on the order of $\\approx5$-20 pc, which is similar to that of the Milky\nWay. This is consistent with the standard dynamo pedagogy, whereby magnetic\nfields in protogalaxies increase in coherence and strength as a function of\ncosmic time. Our results are consistent with a hierarchical galaxy formation\nscenario, with the DLAs, LLSs, and strong magnesium II (MgII) systems exploring\nthree different stages of magnetic field evolution in galaxies.\n",
        "  Neutral evolution assumes that there are no selective forces distinguishing\ndifferent variants in a population. Despite this striking assumption, many\nrecent studies have sought to assess whether neutrality can provide a good\ndescription of different episodes of cultural change. One approach has been to\ntest whether neutral predictions are consistent with observed progeny\ndistributions, recording the number of variants that have produced a given\nnumber of new instances within a specified time interval: a classic example is\nthe distribution of baby names. Using an overlapping generations model we show\nthat these distributions consist of two phases: a power law phase with a\nconstant exponent of -3/2, followed by an exponential cut-off for variants with\nvery large numbers of progeny. Maximum likelihood estimations of the model\nparameters provide a direct way to establish whether observed empirical\npatterns are consistent with neutral evolution. We apply our approach to a\ncomplete data set of baby names from Australia. Crucially we show that analyses\nbased on only the most popular variants, as is often the case in studies of\ncultural evolution, can provide misleading evidence for underlying transmission\nhypotheses. While neutrality provides a plausible description of progeny\ndistributions of abundant variants, rare variants deviate from neutrality.\nFurther, we develop a simulation framework that allows for the detection of\nalternative cultural transmission processes. We show that anti-novelty bias is\nable to replicate the complete progeny distribution of the Australian data set.\n",
        "  Data compression schemes have exhibited their importance in column databases\nby contributing to the high-performance OLAP (Online Analytical Processing)\nquery processing. Existing works mainly concentrate on evaluating compression\nschemes for disk-resident databases as data is mostly stored on disks. With the\ncontinuously decreasing of the price/capacity ratio of main memory, it is the\ntendencies of the times to reside data in main memory. But the discussion of\ndata compression on in-memory databases is very vague in the literature. In\nthis work, we present an updated discussion about whether it is valuable to use\ndata compression techniques in memory databases. If yes, how should memory\ndatabases apply data compression schemes to maximize performance?\n",
        "  We present a combination of theoretical and simulation-based examinations of\nthe role of two-fluid ambipolar drift on molecular line widths. The dissipation\nprovided by ion-neutral interactions can produce a significant difference\nbetween the widths of neutral molecules and the widths of ionic species,\ncomparable to the sound speed. We demonstrate that Alfven waves and certain\nfamilies of magnetosonic waves become strongly damped on scales comparable to\nthe ambipolar diffusion scale. Using the RIEMANN code, we simulate two-fluid\nturbulence with ionization fractions ranging from 10^{-2} to 10^{-6}. We show\nthat the wave damping causes the power spectrum of the ion velocity to drop\nbelow that of the neutral velocity when measured on a relative basis. Following\na set of motivational observations by Li & Houde (2008), we produce synthetic\nline width-size relations that shows a difference between the ion and neutral\nline widths, illustrating that two-fluid effects can have an observationally\ndetectable role in modifying the MHD turbulence in the clouds.\n",
        "  Post-annealing effects on the crystal structure and superconductivity of the\nlithium- and hexamethylenediamine (HMDA)-intercalated superconductor\nLix(C6H16N2)yFe2-zSe2 have been investigated. Through the post-annealing, a\ntwo-step reduction of the interlayer spacing between neighboring Fe layers, d,\nhas been observed. It has been found that a new phase of Lix(C6H16N2)yFe2-zSe2\nwith d= 10.30(2) {\\AA} and Tc = 41 K different from the as-intercalated phase\nis stabilized owing to the possible stable inclination of HMDA intercalated\nbetween FeSe layers. This result supports the domic relation between Tc and d\nin the FeSe-based intercalation superconductors. The reason why Tc increases\nwith a decrease in d through the post-annealing is discussed.\n",
        "  The alternating knots, links and twists projected on the $S_2$ sphere were\nidentified with the phase space of a Hamiltonian dynamic system of one degree\nof freedom. The saddles of the system correspond to the crossings, the edges\ncorrespond to the stable and unstable manifolds connecting the saddles. Each\nface is then oriented in one of two different senses determined by the\ndirection of these manifolds. This correspondence can be also realized between\nthe knot and the Poincar\\'e section of a two degrees of freedom integrable\ndynamical system. The crossings corresponding to unstable orbits, and the faces\nto foliated torus, around a stable orbit.\n  The associated matrix to that connected graph was decomposed in two\npermutations. The separation was shown unique for knots not for links. The\ncharacteristic polynomial corresponding to some knot, link or twist families\nwas explicitly computed in terms of Chebyschev polynomials. A classification of\nrational knots was formulated in terms of the first derivative of the\npolynomial of a knot computed in $x=2$, equal to the number of crossings of the\nknot multiplying the same number used previously by Conway for tabulation of\nknot properties. This leads to a classification of knots exemplified for the\nfamilies having up to five ribbons. We subdivide the families of $N$ ribbons in\nsubfamilies related to the prime knots of $N$ crossings.\n",
        "  The icy mantles of interstellar dust grains are the birthplaces of the\nprimordial prebiotic molecular inventory that may eventually seed nascent solar\nsystems and the planets and planetesimals that form therein. Here, we present a\nstudy of two of the most abundant species in these ices after water: carbon\ndioxide (CO2) and methanol (CH3OH) using TeraHertz (THz) time-domain\nspectroscopy and mid-infrared spectroscopy. We study pure and mixed-ices of\nthese species, and demonstrate the power of the THz region of the spectrum to\nelucidate the long-range structure (i.e. crystalline versus amorphous) of the\nice, the degree of segregation of these species within the ice, and the thermal\nhistory of the species within the ice. Finally, we comment on the utility of\nthe THz transitions arising from these ices for use in astronomical\nobservations of interstellar ices.\n",
        "  The evolution of the antiferromagnetic order parameter in CeFeAsO_{1-x}F_{x}\nas a function of the fluorine content x was investigated primarily via\nzero-field muon-spin spectroscopy. The long-range magnetic order observed in\nthe undoped compound gradually turns into a short-range order at x=0.04,\nseemingly accompanied/induced by a drastic reduction of the magnetic moment of\nthe iron ions. Superconductivity appears upon a further increase in doping\n(x>0.04) when, unlike in the cuprates, the Fe magnetic moments become even\nweaker. The resulting phase diagram evidences the presence of a crossover\nregion, where the superconducting and the magnetic order parameters coexist on\na nanoscopic range.\n",
        "  We use single-epoch spectroscopy of three gravitationally lensed quasars,\nHE0435-1223, WFI2033-4723, and HE2149-2745, to study their inner structure (BLR\nand continuum source). We detect microlensing-induced magnification in the\nwings of the broad emission lines of two of the systems (HE0435-1223 and\nWFI2033-4723). In the case of WFI2033-4723, microlensing affects two \"bumps\" in\nthe spectra which are almost symmetrically arranged on the blue (coincident\nwith an AlIII emission line) and red wings of CIII]. These match the typical\ndouble-peaked profile that follows from disk kinematics. The presence of\nmicrolensing in the wings of the emission lines indicates the existence of two\ndifferent regions in the BLR: a relatively small one with kinematics possibly\nrelated to an accretion disk, and another one that is substantially more\nextended and insensitive to microlensing. There is good agreement between the\nestimated size of the region affected by microlensing in the emission lines,\n$r_s=10^{+15}_{-7} \\sqrt{M/M_{\\odot}}$ light-days (red wing of CIV in\nHE0435-1223) and $r_s=11^{+28}_{-7} \\sqrt{M/M_{\\odot}}$ light-days (CIII] bumps\nin WFI2033-4723) with the sizes inferred from the continuum emission,\n$r_s=13^{+5}_{-4} \\sqrt{M/M_{\\odot}}$ light-days (HE0435-1223) and\n$r_s=10^{+3}_{-2} \\sqrt{M/M_{\\odot}}$ light-days (WFI2033-4723). For\nHE2149-2745 we measure an accretion disk size $r_s=8^{+11}_{-5}\n\\sqrt{M/M_{\\odot}}$ light-days. The estimates of $p$, the exponent of the size\nvs. wavelength ($r_s\\propto\\lambda^p$), are $1.2\\pm0.6$, $0.8\\pm0.2$, and\n$0.4\\pm0.3$ for HE0435-1223, WFI2033-4723, and HE2149-2745, respectively. In\nconclusion, the continuum microlensing amplitude in the three quasars and\nchromaticity in WFI2033-4723 and HE2149-2745 are below expectations for the\nthin disk model. The disks are larger and their temperature gradients are\nflatter than predicted by this model.\n",
        "  For embedded 2-spheres in a 4-manifold sharing the same embedded transverse\nsphere homotopy implies isotopy, provided the ambient 4-manifold has no\n$\\BZ_2$-torsion in the fundamental group. This gives a generalization of the\nclassical light bulb trick to 4-dimensions, the uniqueness of spanning discs\nfor a simple closed curve in $S^4$ and $\\pi_0(\\Diff_0(S^2\\times\nD^2)/\\Diff_0(B^4))=1$. In manifolds with $\\BZ_2$-torsion, one surface can be\nput into a normal form relative to the other.\n",
        "  DGCC protocol has been shown to achieve good performance on multi-core\nin-memory system. However, distributed transactions complicate the dependency\nresolution, and therefore, an effective transaction partitioning strategy is\nessential to reduce expensive multi-node distributed transactions. During\nfailure recovery, log must be examined from the last checkpoint onwards and the\naffected transactions are re-executed based on the way they are partitioned and\nexecuted. Existing approaches treat both transaction management and recovery as\ntwo separate problems, even though recovery is dependent on the sequence in\nwhich transactions are executed.\n  In this paper, we propose to treat the transaction management and recovery\nproblems as one. We first propose an efficient Distributed Dependency Graph\nbased Concurrency Control (DistDGCC) protocol for handling transactions\nspanning multiple nodes, and propose a new novel and efficient logging protocol\ncalled Dependency Logging that also makes use of dependency graphs for\nefficient logging and recovery. DistDGCC optimizes the average cost for each\ndistributed transaction by processing transactions in batch. Moreover, it also\nreduces the effects of thread blocking caused by distributed transactions and\nconsequently improves the runtime performance. Further, dependency logging\nexploits the same data structure that is used by DistDGCC to reduce the logging\noverhead, as well as the logical dependency information to improve the recovery\nparallelism. Extensive experiments are conducted to evaluate the performance of\nour proposed technique against state-of-the-art techniques. Experimental\nresults show that DistDGCC is efficient and scalable, and dependency logging\nsupports fast recovery with marginal runtime overhead. Hence, the overall\nsystem performance is significantly improved as a result.\n",
        "  We describe which knots can be obtained as cycles in the canonical book\nrepresentation of K_n, the complete graph on n vertices. We show that the\ncanonical book representation of K_n contains a Hamiltonian cycle that is a\ncomposite knot if and only if n>11 and we show that when p and q are relatively\nprime, the (p,q) torus knot is a Hamiltonian cycle in the canonical book\nrepresentation of K_{2p+q}. Finally, we list the number and type of all\nnon-trivial knots that occur as cycles in the canonical book representation of\nK_n for n<12. We conjecture that the canonical book representation of K_n\nattains the least possible number of knotted cycles for any embedding of K_n.\n",
        "  In earlier work, we constructed invariants of irreducible representations of\nthe Kauffman skein algebra of a surface. We introduce here an inverse\nconstruction, which to a set of possible invariants associates an irreducible\nrepresentation that realizes these invariants. The current article is\nrestricted to surfaces with at least one puncture, a condition that will be\nlifted in subsequent work of the authors that relies on this one. A step in the\nproof is of independent interest, and describes the algebraic structure of the\nThurston intersection form on the space of integer weight systems for a train\ntrack.\n",
        "  Recent H5N1 influenza research has revived the debate on the storage and\nmanipulation of potentially harmful pathogens. In the last two decades, new\nhigh biosafety (BSL-4) laboratories entered into operation, raising strong\nconcerns from the public. The probability of an accidental release of a\npathogen from a BSL-4 laboratory is extremely low, but the corresponding risk\n-- defined as the probability of occurrence multiplied by its impact -- could\nbe significant depending on the pathogen specificities and the population\npotentially affected. A list of BSL-4 laboratories throughout the world, with\ntheir location and date of first activity, was established from publicly\navailable sources. This database was used to estimate the total population\nliving within a daily commuting distance of BSL-4 laboratories, and to quantify\nhow this figure changed over time. We show that from 1990 to present, the\npopulation living within the commuting belt of BSL-4 laboratories increased by\na factor of 4 to reach up to 1.8% of the world population, owing to an increase\nin the number of facilities and their installation in cities. Europe is\ncurrently hosting the largest population living in the direct vicinity of BSL-4\nlaboratories, while the recent building of new facilities in Asia suggests that\nan important increase of the population living close to BSL-4 laboratories will\nbe observed in the next decades. We discuss the potential implications in term\nof global risk, and call for better pathogen-specific quantitative assessment\nof the risk of outbreaks resulting from the accidental release of potentially\npandemic pathogens\n",
        "  In this paper we develop two mathematical models to predict the release\nkinetics of a water soluble drug from a polymer/excipient matrix tablet. The\nfirst of our models consists of a random walk on a weighted graph, where the\nvertices of the graph represent particles of drug, excipient and polymer,\nrespectively. The graph itself is the contact graph of a multidisperse random\nsphere packing. The second model describes the dissolution and the subsequent\ndiffusion of the active drug out of a porous matrix using a system of partial\ndifferential equations. The predictions of both models show good qualitative\nagreement with experimental release curves. The models will provide tools for\ndesigning better controlled release devices.\n",
        "  Human language emerged abruptly. Diverse body forms evolved suddenly.\nSeed-bearing plants spread rapidly. How do complex evolutionary innovations\narise so quickly? Resolving alternative claims remains difficult. The great\nevents of the past happened a long time ago. Cancer provides a model to study\nevolutionary innovation. A tumor must evolve many novel traits to become an\naggressive cancer. I use what we know or could study about cancer to describe\nthe key processes of innovation. In general, evolutionary systems form a\nhierarchy of recursive processes. Those recursive processes determine the rates\nat which innovations are generated, spread and transmitted. I relate the\nrecursive processes to abrupt evolutionary innovation.\n",
        "  Taking advantage of the Sloan Digital Sky Survey Stripe82 data, we have\nexplored the spatial distribution of ultra-diffuse galaxies (UDGs) within an\narea of 8$\\times$8 Mpc$^2$ centred around the galaxy cluster Abell 168 ($z$ =\n0.045). This intermediate massive cluster ($\\sigma$ = 550 km s$^{-1}$) is\nsurrounded by a complex large-scale structure. Our work confirms the presence\nof UDGs in the cluster and in the large-scale structure that surrounds it, and\nit is the first detection of UDGs outside clusters. Approximately 50 per cent\nof the UDGs analysed in the selected area inhabit the cluster region ($\\sim$11\n$\\pm$ 5 per cent in the core and $\\sim$39 $\\pm$ 9 per cent in the outskirts),\nwhereas the remaining UDGs are found outside the main cluster structure\n($\\sim$50 $\\pm$ 11 per cent). The colours and the spatial distribution of the\nUDGs within this large-scale structure are more similar to the dwarf galaxies\nthan to L$_\\star$ galaxies, suggesting that most of UDGs could be bona fide\ndwarf galaxies.\n",
        "  The automatic identification of discourse relations is still a challenging\ntask in natural language processing. Discourse connectives, such as \"since\" or\n\"but\", are the most informative cues to identify explicit relations; however\ndiscourse parsers typically use a closed inventory of such connectives. As a\nresult, discourse relations signaled by markers outside these inventories (i.e.\nAltLexes) are not detected as effectively. In this paper, we propose a novel\nmethod to leverage parallel corpora in text simplification and lexical\nresources to automatically identify alternative lexicalizations that signal\ndiscourse relation. When applied to the Simple Wikipedia and Newsela corpora\nalong with WordNet and the PPDB, the method allowed the automatic discovery of\n91 AltLexes.\n",
        "  Superconductivity has continued to be a fascinating phenomenon ever since its\ndiscovery in 1911. The magnitude of the transition temperature, Tc, provides\nvaluable insight into the underlying physics. Here we provide select examples\nof the extensive research that has been done towards understanding Tc, and some\ncases where further investigation is called for. We believe that searching for\nnew and enhanced Tc's remains a fertile frontier.\n",
        "  In high intensity focused ultrasound (HIFU) systems using non-ionizing\nmethods in cancer treatment, if the device is applied to the body externally,\nthe HIFU beam can damage nearby healthy tissues and burn skin due to lack of\nknowledge about the viscoelastic properties of patient tissue and failure to\nconsider the physical properties of tissue in treatment planning. Addressing\nthis problem by using various methods, such as MRI or ultrasound, elastography\ncan effectively measure visco-elastic properties of tissue and fits within the\npattern of stimulation and total treatment planning. In this paper, in a linear\npath of HIFU propagation, and by considering the smallest part of the path,\nincluding voxel with three mechanical elements of mass, spring and damper,\nwhich represents the properties of viscoelasticity of tissue, by creating waves\nof HIFU in the wire environment of MATLAB mechanics and stimulating these\nelements, pressure and heat transfer due to stimulation in the hypothetical\nvoxel was obtained. Through the repeatability of these three-dimensional\nelements, tissue is created. The measurement was performed on three layers. The\nvalues of these elements for liver tissue and kidney of sheep in a practical\nexample and outside the body are measured, and pressure and heat for three\nlayers of liver and kidney tissue of an organism were obtained by applying\nultrasound signals with a designed model. This action is repeated in three\ndifferent directions, and the results are then compared with simulation\nsoftware for ultrasound, as a reference to U.S. Food and Drug Administration\n(FDA) measures for HIFU, as well as comparisons of results with an operational\nmethod for an HIFU cell.\n",
        "  Mathematical models of cholera and waterborne disease vary widely in their\nstructures, in terms of transmission pathways, loss of immunity, and other\nfeatures. These differences may yield different predictions and parameter\nestimates from the same data. Given the increasing use of models to inform\npublic health decision-making, it is important to assess distinguishability\n(whether models can be distinguished based on fit to data) and inference\nrobustness (whether model inferences are robust to realistic variations in\nmodel structure). We examined the effects of uncertainty in model structure in\nepidemic cholera, testing a range of models based on known features of cholera\nepidemiology. We fit to simulated epidemic and long-term data, as well as data\nfrom the 2006 Angola epidemic. We evaluated model distinguishability based on\ndata fit, and whether parameter values and forecasts can accurately be inferred\nfrom incidence data. In general, all models were able to successfully fit to\nall data sets, even if misspecified. However, in the long-term data, the best\nmodel fits were achieved when the loss of immunity form matched those of the\nmodel that simulated the data. Two transmission and reporting parameters were\naccurately estimated across all models, while the remaining showed broad\nvariation across the different models and data sets. Forecasting efforts were\nnot successful early, but once the epidemic peak had been achieved, most models\nshowed similar accuracy. Our results suggest that we are unlikely to be able to\ninfer mechanistic details from epidemic case data alone, underscoring the need\nfor broader data collection. Nonetheless, with sufficient data, conclusions\nfrom forecasting and some parameter estimates were robust to variations in the\nmodel structure, and comparative modeling can help determine how variations in\nmodel structure affect conclusions drawn from models and data.\n",
        "  We use four dimensional techniques to derive general bounds on the $\\tau$\ninvariant of a satellite knot in $S^{3}$.\n",
        "  This paper considers the problem of efficiently answering reachability\nqueries over views of provenance graphs, derived from executions of workflows\nthat may include recursion. Such views include composite modules and model\nfine-grained dependencies between module inputs and outputs. A novel\nview-adaptive dynamic labeling scheme is developed for efficient query\nevaluation, in which view specifications are labeled statically (i.e. as they\nare created) and data items are labeled dynamically as they are produced during\na workflow execution. Although the combination of fine-grained dependencies and\nrecursive workflows entail, in general, long (linear-size) data labels, we show\nthat for a large natural class of workflows and views, labels are compact\n(logarithmic-size) and reachability queries can be evaluated in constant time.\nExperimental results demonstrate the benefit of this approach over the\nstate-of-the-art technique when applied for labeling multiple views.\n",
        "  In parallel magnetic resonance imaging (pMRI), to find a joint solution for\nthe image and coil sensitivity functions is a nonlinear and nonconvex problem.\nA class of algorithms reconstruct sensitivity encoded images of the coils first\nfollowed by the magnitude only image reconstruction, e.g. GRAPPA. It is shown\nin this paper that, if only the magnitude image is reconstructed, there exists\na convex solution space for the magnitude image and sensitivity encoded images.\nThis solution space enables formulation of a regularized convex optimization\nproblem and leads to a globally optimal and unique solution for the magnitude\nimage reconstruction. Its applications to in-vivo MRI data sets result in\nsuperior reconstruction performance compared with other algorithms.\n",
        "  We present a new, efficient frame-semantic parser that labels semantic\narguments to FrameNet predicates. Built using an extension to the segmental RNN\nthat emphasizes recall, our basic system achieves competitive performance\nwithout any calls to a syntactic parser. We then introduce a method that uses\nphrase-syntactic annotations from the Penn Treebank during training only,\nthrough a multitask objective; no parsing is required at training or test time.\nThis \"syntactic scaffold\" offers a cheaper alternative to traditional syntactic\npipelining, and achieves state-of-the-art performance.\n",
        "  In the context of star and planet formation, understanding the formation of\ndisks is of fundamental importance. Previous studies found that the magnetic\nfield has a very strong impact on the collapse of a prestellar cloud,\nparticularly in possibly suppressing the formation of a disk even for\nrelatively modest values of the magnetic intensity. Since observations infer\nthat cores have a substantial level of magnetization, this raises the question\nof how disks form. However, most studies have been restricted to the case in\nwhich the initial angle, $\\alpha$, between the magnetic field and the rotation\naxis equals 0$^\\circ$. We explore and analyse the influence of non aligned\nconfigurations on disk formation. We perform 3D ideal MHD, AMR numerical\nsimulations for various values of $\\mu$, the ratio of the mass-to-flux to the\ncritical mass-to-flux, and various values of $\\alpha$. We find that disks form\nmore easily as $\\alpha$ increases from 0 to 90$^\\circ$. We propose that as the\nmagnetized pseudo-disks become thicker with increasing $\\alpha$, the magnetic\nbraking efficiency is lowered. We also find that even small values of $\\alpha$\n($\\simeq$ 10-20$^\\circ$) show significant differences with the alligned case.\nWithin the framework of ideal MHD and for our choice of initial conditions,\ncentrifugally supported disks cannot form for values of $\\mu$ smaller than\n$\\simeq$3, if the magnetic field and the rotation axis are perpendicular, and\nsmaller than about $\\simeq$5-10 when they are perfectly aligned.\n",
        "  Purpose: To introduce a novel method for the recovery of multi-shot diffusion\nweighted (MS-DW) images from echo-planar imaging (EPI) acquisitions.\n  Methods: Current EPI-based MS-DW reconstruction methods rely on the explicit\nestimation of the motion- induced phase maps to recover the unaliased images.\nIn the new formulation, the k-space data of the unaliased DWI is recovered\nusing a structured low-rank matrix completion scheme, which does not require\nexplicit estimation of the phase maps. The structured matrix is obtained as the\nlifting of the multi-shot data. The smooth phase-modulations between shots\nmanifest as null-space vectors of this matrix, which implies that the\nstructured matrix is low-rank. The missing entries of the structured matrix are\nfilled in using a nuclear-norm minimization algorithm subject to the\ndata-consistency. The formulation enables the natural introduction of\nsmoothness regularization, thus enabling implicit motion-compensated recovery\nof fully-sampled as well as under-sampled MS-DW data.\n  Results: Our experiments on in-vivo data show effective removal of the\nghosting artifacts arising from intershot motion in MS-DW data using the\nproposed method. The performance is comparable and better in certain cases than\nconventional phase-based methods.\n  Conclusion: The proposed method can achieve effective unaliasing of\nfully/under-sampled MS-DW images without using explicit phase estimates.\n",
        "  The paper deals with using descriptive mark-up to emphasize translation\nmistakes. The author postulates the necessity to develop a standard and formal\nXML-based way of describing translation mistakes. It is considered to be\nimportant for achieving impersonal translation quality assessment. Marked-up\ntranslations can be used in corpus translation studies; moreover, automatic\ntranslation assessment based on marked-up mistakes is possible. The paper\nconcludes with setting up guidelines for further activity within the described\nfield.\n",
        "  In this paper we describe a functorial data migration scenario about the\nmanufacturing service capability of a distributed supply chain. The scenario is\na category-theoretic analog of an OWL ontology-based semantic enrichment\nscenario developed at the National Institute of Standards and Technology\n(NIST). The scenario is presented using, and is included with, the open-source\nFQL tool, available for download at categoricaldata.net/fql.html.\n",
        "  The increasing interest in Semantic Web technologies has led not only to a\nrapid growth of semantic data on the Web but also to an increasing number of\nbackend applications with already more than a trillion triples in some cases.\nConfronted with such huge amounts of data and the future growth, existing\nstate-of-the-art systems for storing RDF and processing SPARQL queries are no\nlonger sufficient. In this paper, we introduce Partout, a distributed engine\nfor efficient RDF processing in a cluster of machines. We propose an effective\napproach for fragmenting RDF data sets based on a query log, allocating the\nfragments to nodes in a cluster, and finding the optimal configuration. Partout\ncan efficiently handle updates and its query optimizer produces efficient query\nexecution plans for ad-hoc SPARQL queries. Our experiments show the superiority\nof our approach to state-of-the-art approaches for partitioning and distributed\nSPARQL query processing.\n",
        "  The hexabasic book is the cone of the 1-dimensional skeleton of the union of\ntwo tetrahedra glued along a common face. The universal 3-dimensional\npolyhedron UP is the product of a segment and the hexabasic book. We show that\nany 2-dimensional link in 4-space is isotopic to a surface in UP. The proof is\nbased on a representation of surfaces in 4-space by marked graphs, links with\ndouble intersections in 3-space. We construct a finitely presented semigroup\nwhose central elements uniquely encode all isotopy classes of 2-dimensional\nlinks.\n",
        "  We present the results of the one year long observational campaign of the\ntype II-plateau SN 2005cs, which exploded in the nearby spiral galaxy M51 (the\nWhirlpool Galaxy). This extensive dataset makes SN 2005cs the best observed\nlow-luminosity, 56Ni-poor type II-plateau event so far and one of the best\ncore-collapse supernovae ever. The optical and near-infrared spectra show\nnarrow P-Cygni lines characteristic of this SN family, which are indicative of\na very low expansion velocity (about 1000 km/s) of the ejected material. The\noptical light curves cover both the plateau phase and the late-time radioactive\ntail, until about 380 days after core-collapse. Numerous unfiltered\nobservations obtained by amateur astronomers give us the rare opportunity to\nmonitor the fast rise to maximum light, lasting about 2 days. In addition to\noptical observations, we also present near-infrared light curves that (together\nwith already published UV observations) allow us to construct for the first\ntime a reliable bolometric light curve for an object of this class. Finally,\ncomparing the observed data with those derived from a semi-analytic model, we\ninfer for SN 2005cs a 56Ni mass of about 0.003 solar masses, a total ejected\nmass of 8-13 solar masses and an explosion energy of about 3 x 10^50 erg.\n",
        "  We investigate the transient dynamics of a short overdamped Josephson\njunction with a periodic driving signal in the presence of colored noise. We\nanalyze noise induced henomena, specifically resonant activation and noise\nenhanced stability. We find that the positions both of the minimum of RA and\nmaximum of NES depend on the value of the noise correlation time tau_c.\nMoreover, in the range where RA is observed, we find a non-monotonic behavior\nof the mean switching time as a function of the correlation time tau_c.\n",
        "  NGC 4203 is a nearby early-type galaxy surrounded by a very large,\nlow-column-density HI disc. In this paper we study the star formation\nefficiency in the gas disc of NGC 4203 by using the UV, deep optical imaging\nand infrared data. We confirm that the HI disc consists of two distinct\ncomponents: an inner star forming ring with radius from $\\sim$ 1 to $\\sim$ 3\nR$_{eff}$, and an outer disc. The outer HI disc is 9 times more massive than\nthe inner HI ring. At the location of the inner HI ring we detect spiral-like\nstructure both in the deep $g'-r'$ image and in the 8 $\\mu$m $Spitzer$-IRAC\nimage, extending in radius up to $\\sim$ 3 R$_{eff}$. These two gas components\nhave a different star formation efficiency likely due to the different\nmetallicity and dust content. The inner component has a star formation\nefficiency very similar to the inner regions of late-type galaxies. Although\nthe outer component has a very low star formation efficiency, it is similar to\nthat of the outer regions of spiral galaxies and dwarfs. We suggest that these\ndifferences can be explained with different gas origins for the two components\nsuch as stellar mass loss for the inner HI ring and accretion from the inter\ngalactic medium (IGM) for the outer HI disc. The low level star formation\nefficiency in the outer HI disc is not enough to change the morphology of NGC\n4203, making the depletion time of the HI gas much too long.\n",
        "  Thermal instability in an electron-ion magnetized plasma, which is relevant\nin the intragalactic medium (IGM) of galaxy clusters, solar corona, and other\ntwo-component plasma objects is investigated. We apply the multicomponent\nplasma approach when the dynamics of all species is considered separately\nthrough the electric field perturbations. General expressions for the dynamical\nvariables obtained in this paper can be applied for a wide range of\nastrophysical and laboratory plasmas also containing neutrals and dust grains.\nWe assume that background temperatures of electrons and ions are different and\ninclude the energy exchange in the thermal equations for the electrons and ions\nalong with the collisional momentum exchange in the equations of motion. We\ntake into account the dependence of collision frequency on the density and\ntemperature perturbations. The cooling-heating functions are taken for both\nelectrons and ions. A condensation mode of thermal instability has been studied\nin the fast sound speed limit. A new dispersion relation including the\ndifferent electron and ion cooling-heating functions and other effects\nmentioned above has been derived and its simple solutions for growth rates in\nthe limiting cases have been found. We have shown that the perturbations have\nan electromagnetic nature. The crucial role of the electric field perturbation\nalong the background magnetic field in the fast sound speed limit has been\ndemonstrated. We have found that at conditions under consideration, the\ncondensation must occur along the magnetic field while the transverse scale\nsizes can be both larger and smaller than the longitudinal ones. The results\nobtained can be useful for interpretation of observations of dense cold regions\nin astrophysical objects.\n",
        "  Recently, an ultra-low-cost linear scan based tomography architecture was\nproposed by our team. Similar to linear tomosynthesis, the source and detector\nare translated in opposite directions and the data acquisition system targets\non a region-of-interest (ROI) to acquire data for image reconstruction. This\nkind of tomography architecture was named parallel translational computed\ntomography (PTCT). In our previous studies, filtered backprojection (FBP)-type\nalgorithms were developed to reconstruct images from PTCT. However, the\nreconstructed ROI images from truncated projections have severe truncation\nartifacts. In this paper, we propose two backprojection filtering (BPF)-type\nalgorithms named MP-BPF and MZ-BPF to reconstruct ROI images from truncated\nPTCT data. A weight function is constructed to deal with data redundancy for\nmulti-linear translations modes. Extensive numerical simulations are performed\nto evaluate the proposed MP-BPF and MZ-BPF algorithms for PTCT in fan-beam\ngeometry. Qualitative and quantitative results demonstrate that the proposed\nBPF-type algorithms can not only accurately reconstruct ROI images from\ntruncated projections but also provide high-quality images for the entire image\nsupport in some circumstances.\n",
        "  We construct the first examples of asymmetric L-space knots in $S^3$. More\nspecifically, we exhibit a construction of hyperbolic knots in $S^3$ with both\n(i) a surgery that may be realized as a surgery on a strongly invertible link\nsuch that the result of the surgery is the double branched cover of an\nalternating link and (ii) trivial isometry group. In particular, this produces\nL-space knots in $S^3$ which are not strongly invertible. The construction also\nimmediately extends to produce asymmetric L-space knots in any lens space,\nincluding $S^1 \\times S^2$.\n",
        "  This work proposes V-SMART-Join, a scalable MapReduce-based framework for\ndiscovering all pairs of similar entities. The V-SMART-Join framework is\napplicable to sets, multisets, and vectors. V-SMART-Join is motivated by the\nobserved skew in the underlying distributions of Internet traffic, and is a\nfamily of 2-stage algorithms, where the first stage computes and joins the\npartial results, and the second stage computes the similarity exactly for all\ncandidate pairs. The V-SMART-Join algorithms are very efficient and scalable in\nthe number of entities, as well as their cardinalities. They were up to 30\ntimes faster than the state of the art algorithm, VCL, when compared on a real\ndataset of a small size. We also established the scalability of the proposed\nalgorithms by running them on a dataset of a realistic size, on which VCL never\nsucceeded to finish. Experiments were run using real datasets of IPs and\ncookies, where each IP is represented as a multiset of cookies, and the goal is\nto discover similar IPs to identify Internet proxies.\n",
        "  Super-spreading events for infectious diseases occur when some infected\nindividuals infect more than the average number of secondary cases. Several\nsuper-spreading individuals have been identified for the 2003 outbreak of\nsevere acute respiratory syndrome (SARS). We develop a model for\nsuper-spreading events of infectious diseases, which is based on the outbreak\nof SARS. Using this model we describe two methods for estimating the parameters\nof the model, which we demonstrate with the small-scale SARS outbreak at the\nAmoy Gardens, Hong Kong, and the large-scale outbreak in the entire Hong Kong\nSpecial Administrative Region. One method is based on parameters calculated for\nthe classical susceptible - infected - removed (SIR) disease model. The second\nis based on parameter estimates found in the literature. Using the parameters\ncalculated for the SIR model, our model predicts an outcome similar to that for\nthe SIR model. On the other hand, using parameter estimates from SARS\nliterature our model predicts a much more serious epidemic.\n",
        "  We study the effects of quenched disorder on the two-dimensional d-wave\nsuperconductors (SC's) at zero temperature by Monte-Carlo simulations. The\nmodel is defined on the three-dimesional (3D) lattice and the SC pair field is\nput on each spatial link as motivated in the resonating-valence-bond theory of\nthe high-$T_{\\rm c}$ SC's. For the nonrandom case, the model exhibits a\nsecond-order phase transition to a SC state as density of charge carriers is\nincreased. It belongs to the universality class {\\it different from} that of\nthe 3D XY model. Quenched disorders (impurities) are introduced both in the\nhopping amplitude and the plaquette term of pair fields. Then the second-order\ntransition disappears at a critical concentration of quenched disorder,\n$p_c\\simeq 15%$. Implication of the results to cold atomic systems in optical\nlattices is also discussed.\n",
        "  Online petitions are a cost-effective way for citizens to collectively engage\nwith policy-makers in a democracy. Predicting the popularity of a petition ---\ncommonly measured by its signature count --- based on its textual content has\nutility for policy-makers as well as those posting the petition. In this work,\nwe model this task using CNN regression with an auxiliary ordinal regression\nobjective. We demonstrate the effectiveness of our proposed approach using UK\nand US government petition datasets.\n",
        "  The dusty, ionized gas cloud G2 is currently passing the massive black hole\nin the Galactic Center at a distance of roughly 2400 Schwarzschild radii. We\nexplore the possibility of a starting point of the cloud within the disks of\nyoung stars. We make use of the large amount of new observations in order to\nput constraints on G2's origin. Interpreting the observations as a diffuse\ncloud of gas, we employ three-dimensional hydrodynamical adaptive mesh\nrefinement (AMR) simulations with the PLUTO code and do a detailed comparison\nwith observational data. The simulations presented in this work update our\npreviously obtained results in multiple ways: (1) high resolution\nthree-dimensional hydrodynamical AMR simulations are used, (2) the cloud\nfollows the updated orbit based on the Brackett-$\\gamma$ data, (3) a detailed\ncomparison to the observed high-quality position-velocity diagrams and the\nevolution of the total Brackett-$\\gamma$ luminosity is done. We concentrate on\ntwo unsolved problems of the diffuse cloud scenario: the unphysical formation\nepoch only shortly before the first detection and the too steep\nBrackett-$\\gamma$ light curve obtained in simulations, whereas the observations\nindicate a constant Brackett-$\\gamma$ luminosity between 2004 and 2013. For a\ngiven atmosphere and cloud mass, we find a consistent model that can explain\nboth, the observed Brackett-$\\gamma$ light curve and the position-velocity\ndiagrams of all epochs. Assuming initial pressure equilibrium with the\natmosphere, this can be reached for a starting date earlier than roughly 1900,\nwhich is close to apo-center and well within the disks of young stars.\n",
        "  We report a novel phenomenon intimately related to the spin-triplet\nsuperconductivity. It is well known that the spin susceptibility decreases\nbelow the superconducting transition temperature in almost all superconductors\nbecause of spin-singlet pair formation, while it may remain unchanged in a\nhandful of spin-triplet exceptions. Here we report the observation in\nSr$_2$RuO$_4$ with nuclear magnetic resonance (NMR) that the spin\nsusceptibility originating from the Ru-4$d$ electron slightly $increases$ by\n$\\sim 2 $\\% of total and becomes inhomogeneous in the superconducting state.\nThese are reasonably explained if the electron pairs form the\nequal-spin-pairing (ESP) in the mixed state. A similar phenomenon was predicted\nfor superfluid $^3$He forty years ago, but had never been demonstrated in any\nsuperconductor.\n",
        "  When there is conspicuous under-exploitation of a limited resource, it is\nworth asking what mechanisms allow presumably valuable resources to be left\nunused? Evolutionary biologists have generated a wide range of hypotheses to\nexplain this, ranging from interdemic group selection to selfishly prudent\nindividual restraint. We consider a situation in which, in spite of high\nintraspecific competition, individuals leave most of a key resource\nunexploited. The parasitic wasp that does this finds virtually all host egg\nclusters in a landscape, but parasitizes only about a third of the eggs in\neach, and then leaves a deterrent mark around the cluster. We first test, and\nreject, a series of system-specific simple constraints that might limit full\nhost exploitation, such as asynchronous maturation of host eggs. We then\nconsider classical hypotheses for the evolution of restraint. Prudent predation\nand bet-hedging fail as explanations because the wasp lives as a large\nwell-mixed population. Also, we find no individual benefits to the parasitoid\nof developing in a sparsely parasitized host nest. However an optimal foraging\nmodel, including empirically measured costs of superparasitism and\nhyperparasitism, can explain through individual selection, both the\nconsistently low rate of parasitism and marking.\n",
        "  Purpose: 4D CBCT is a beneficial tool for the treatment of movable tumors,\nbecause it can help us to understand where the tumors are actually located and\nhave a precise treatment plan. However, there is a limitation that general CBCT\nimages cannot perfectly help the sophisticated registration. On the other hand,\nSymmetryTM 4D IGRT system of Elekta can offer the 4D CBCT registration option.\nIn this study, we intend to evaluate the usefulness of SymmetryTM. Method and\nMaterials: Planning CT images of the CIRS moving lung phantom were acquired\nfrom 4D MDCT. And they are sorted as 10 phases from 0% phase to 90% phase. The\nthickness of CT images was 1 mm. Acquired MDCT images were transferred to the\ncontouring software and a virtual target was generated. An one arc VMAT plan\nwas performed by using the treatment planning system on the virtual target.\nFinally, the movement of the phantom was verified through XVI SymmetryTM\nsystem. Results: The physical movement of CIRS moving lung phantom was +/- 10.0\nmm in superior-inferior direction, +/- 1.0 mm in lateral direction, and +/- 2.5\nmm in anterior-posterior direction. The movement of the phantom was measured\nfrom 4D MDCT registration as +/- 10.2 mm in superior-inferior direction, +/-\n0.9 mm in lateral direction, and +/- 2.45 mm in anterior-posterior direction.\nThe movement of the phantom was measured from SymmetryTM registration as +/-\n10.1 mm in superior-inferior direction, +/- 0.9 mm in lateral direction, and\n+/- 2.4 mm in anterior/posterior direction. Conclusion: It is confirmed that 4D\nCBCT is a beneficial tool for the treatment of movable tumors. Therefore, 4D\nregistration of SymmetryTM can increase the precision of the registration when\na movable tumor is a target of radiation treatment.\n",
        "  Sentence pair modeling is critical for many NLP tasks, such as paraphrase\nidentification, semantic textual similarity, and natural language inference.\nMost state-of-the-art neural models for these tasks rely on pretrained word\nembedding and compose sentence-level semantics in varied ways; however, few\nworks have attempted to verify whether we really need pretrained embeddings in\nthese tasks. In this paper, we study how effective subword-level (character and\ncharacter n-gram) representations are in sentence pair modeling. Though it is\nwell-known that subword models are effective in tasks with single sentence\ninput, including language modeling and machine translation, they have not been\nsystematically studied in sentence pair modeling tasks where the semantic and\nstring similarities between texts matter. Our experiments show that subword\nmodels without any pretrained word embedding can achieve new state-of-the-art\nresults on two social media datasets and competitive results on news data for\nparaphrase identification.\n",
        "  It is widely accepted that stars do not form in isolation but result from the\nfragmentation of molecular clouds, which in turn leads to star cluster\nformation. Over time, clusters dissolve or are destroyed by interactions with\nmolecular clouds or tidal stripping, and their members become part of the\ngeneral field population. Star clusters are thus among the basic building\nblocks of galaxies. In turn, star cluster populations, from young associations\nand open clusters to old globulars, are powerful tracers of the formation,\nassembly, and evolutionary history of their parent galaxies. Although their\nimportance had been recognised for decades, major progress in this area has\nonly become possible in recent years, both for Galactic and extragalactic\ncluster populations. Star clusters are the observational foundation for stellar\nastrophysics and evolution, provide essential tracers of galactic structure,\nand are unique stellar dynamical environments. Star formation, stellar\nstructure, stellar evolution, and stellar nucleosynthesis continue to benefit\nand improve tremendously from the study of these systems. Additionally,\nfundamental quantities such as the initial mass function can be successfully\nderived from modelling either the H-R diagrams or the integrated velocity\nstructures of, respectively, resolved and unresolved clusters and cluster\npopulations. Star cluster studies thus span the fields of Galactic and\nextragalactic astrophysics, while heavily affecting our detailed understanding\nof the process of star formation in dense environments.This report highlights\nscience results of the last decade in the major fields covered by IAU\nCommission 37: Star clusters and associations.\n",
        "  According to Pixton, there are Morse-Smale diffeomorphisms of the 3-sphere\nwhich have no energy function, that is a Lyapunov function whose critical\npoints are all periodic points of the diffeomorphism. We introduce the concept\nof quasi-energy function for a Morse-Smale diffeomorphism as a Lyapunov\nfunction with the least number of critical points and construct a quasi-energy\nfunction for any diffeomorphism from some class of Morse-Smale diffeomorphisms\non the 3-sphere.\n",
        "  Helium line observations towards 11 Galactic positions using Westerbork\nSynthesis Radio Telescope(WSRT) have been reported. These observations were\nmade towards nearby positions where already hydrogen lines were detected at\nsufficiently high intensity($\\geq$50mK) at 1.4 GHz. This approach gave a fair\nchance for the detection of helium line as well, keeping in mind the relative\nabundance(10%) of helium with respect to hydrogen. Care was also taken to avoid\nthe presence of HII regions along the line of sight so that the line emission\noriginates from the extended diffuse low density ionized component, ELDWIM of\nthe Galaxy. The observations have resulted in the detection of helium line\ntowards 5 positions out of 11 with signal to noise ratio(snr) $>$ 4$\\sigma$. An\nattempt has been made to associate detection/non-detection of helium line to\nthe presence of surrounding HII regions. A weighting scheme that accounts for\nnearby($<$ 500pc) HII regions, their distances and other factors produces\nfavourable results. It is seen from this weighting scheme that a higher weight\nfavours the detection of helium line while lower weight is associated with\nnon-detection. The idea is to correlate ionization of ELDWIM with the\nsurrounding HII regions.\n",
        "  Data Security is a major issue in any web-based application. There have been\napproaches to handle intruders in any system, however, these approaches are not\nfully trustable; evidently data is not totally protected. Real world databases\nhave information that needs to be securely stored. The approach of generating\nnegative database could help solve such problem. A Negative Database can be\ndefined as a database that contains huge amount of data consisting of\ncounterfeit data along with the real data. Intruders may be able to get access\nto such databases, but, as they try to extract information, they will retrieve\ndata sets that would include both the actual and the negative data. In this\npaper we present our approach towards implementing the concept of negative\ndatabase to help prevent data theft from malicious users and provide efficient\ndata retrieval for all valid users.\n",
        "  Recently we have developed an algorithm for reconstructing volumetric images\nand extracting 3D tumor motion information from a single x-ray projection. We\nhave demonstrated its feasibility using a digital respiratory phantom with\nregular breathing patterns. In this work, we present a detailed description and\na comprehensive evaluation of the improved algorithm. The algorithm was\nimproved by incorporating respiratory motion prediction. The accuracy and\nefficiency were then evaluated on 1) a digital respiratory phantom, 2) a\nphysical respiratory phantom, and 3) five lung cancer patients. These\nevaluation cases include both regular and irregular breathing patterns that are\ndifferent from the training dataset. For the digital respiratory phantom with\nregular and irregular breathing, the average 3D tumor localization error is\nless than 1 mm. On an NVIDIA Tesla C1060 GPU card, the average computation time\nfor 3D tumor localization from each projection ranges between 0.19 and 0.26\nseconds, for both regular and irregular breathing, which is about a 10%\nimprovement over previously reported results. For the physical respiratory\nphantom, an average tumor localization error below 1 mm was achieved with an\naverage computation time of 0.13 and 0.16 seconds on the same GPU card, for\nregular and irregular breathing, respectively. For the five lung cancer\npatients, the average tumor localization error is below 2 mm in both the axial\nand tangential directions. The average computation time on the same GPU card\nranges between 0.26 and 0.34 seconds.\n",
        "  Evolutionary dynamics on graphs can lead to many interesting and\ncounterintuitive findings. We study the Moran process, a discrete time\nbirth-death process, that describes the invasion of a mutant type into a\npopulation of wild-type individuals. Remarkably, the fixation probability of a\nsingle mutant is the same on all regular networks. But non-regular networks can\nincrease or decrease the fixation probability. While the time until fixation\nformally depends on the same transition probabilities as the fixation\nprobabilities, there is no obvious relation between them. For example, an\namplifier of selection, which increases the fixation probability and thus\ndecreases the number of mutations needed until one of them is successful, can\nat the same time slow down the process of fixation. Based on small networks, we\nshow analytically that (i) the time to fixation can decrease when links are\nremoved from the network and (ii) the node providing the best starting\nconditions in terms of the shortest fixation time depends on the fitness of the\nmutant. Our results are obtained analytically on small networks, but numerical\nsimulations show that they are qualitatively valid even in much larger\npopulations.\n",
        "  Cancer poses danger because of its unregulated growth, development of\nresistant subclones, and metastatic spread to vital organs. Although the major\ntransitions in cancer development are increasingly well understood, we lack\nquantitative theory for how chemoprevention is predicted to affect survival. We\nemploy master equations and probability generating functions, the latter well\nknown in statistical physics, to derive the dynamics of tumor growth as a\nmean-field approximation. We also study numerically the associated stochastic\nbirth-death process. Our findings predict exponential tumor growth when a\ncancer is in its early stages of development and hyper-exponential growth\nthereafter. Numerical simulations are in general agreement with our analytical\napproach. We evaluate how constant, low impact treatments affect both\nneoplastic growth and the frequency of chemoresistant clones. We show that\ntherapeutic outcomes are highly predictable for treatments starting either\nsufficiently early or late in terms of initial tumor size and the initial\nnumber of chemoresistant cells, whereas stochastic dynamics dominate therapies\nstarting at intermediate neoplasm sizes, with high outcome sensitivity both in\nterms of tumor control and the emergence of resistant subclones. The outcome of\nchemoprevention can be understood in terms of both minimal physiological\nimpacts resulting in long-term control and either preventing or slowing the\nemergence of resistant subclones. We argue that our model and results can also\nbe applied to the management of early, clinically detected cancers after tumor\nexcision.\n",
        "  Stability of evolutionary dynamics of non-repeated Prisoner's Dilemma game\nwith non-uniform interaction rates [1], via benefit and cost dilemma is studied\n. Moreover, the stability condition (b+c/b-c)2 < r1r3 is derived in case of\ncoexistence between cooperators and defectors .If r1,r3 -> infinity cooperation\nis the dominant strategy and defectors can no longer exploit cooperators.\n",
        "  Context. Void population consists mainly of late-type and low surface\nbrightness (LSB) dwarf galaxies whose atomic hydrogen is the main component of\ntheir baryonic matter. Therefore, observations of void galaxy HI are mandatory\nin order to understand their evolution and dynamics.\n  Aims. Our aim was to obtain integrated HI parameters for a fainter part of\nthe nearby Lynx-Cancer void galaxy sample (total of 45 objects) with the Nancay\nRadio Telescope (NRT) and to conduct the comparative analysis of all the 103\nvoid galaxies with known HI data with a sample of similar galaxies residing in\ndenser environments of the Local Volume.\n  Methods. For HI observations we used the NRT with its sensitive\nantenna/receiver system FORT and standard processing. The comparison of the\nvoid and control samples on the parameter M(HI)/L_B is conducted with the\nnon-parametric method `The 2x2 Contingency Table test'.\n  Results. We obtained new HI data for about 40% of the Lynx-Cancer galaxy\nsample. Along with data from the literature, we use these new data for further\nanalysis of 103 void objects. The proxy of the evolutional parameter M(HI)/L_B\nof the void sample is compared with that of 82 galaxies of morphological types\n8--10 residing in the Local Volume (LV) groups and aggregates.\n  Conclusions. At the confidence level of P = 0.988, we conclude that for the\nsame luminosity, these void galaxies are systematically gas-richer, on average\nby ~39%. This result is consistent with the authors' earlier conclusion on the\nsmaller gas metallicities and evidence for the slower low-mass galaxy evolution\nin voids.\n",
        "  A non-trivial slope $r$ on a knot $K$ in $S^3$ is called a characterizing\nslope if whenever the result of $r$-surgery on a knot $K'$ is orientation\npreservingly homeomorphic to the result of $r$-surgery on $K$, then $K'$ is\nisotopic to $K$. Ni and Zhang ask: for any hyperbolic knot $K$, is a slope $r =\np/q$ with $|p| + |q|$ sufficiently large a characterizing slope? In this\narticle we answer this question in the negative by demonstrating that there is\na hyperbolic knot $K$ in $S^3$ which has infinitely many non-characterizing\nslopes. As the simplest known example, the hyperbolic knot $8_6$ has no\nintegral characterizing slopes.\n",
        "  MADlib is a free, open source library of in-database analytic methods. It\nprovides an evolving suite of SQL-based algorithms for machine learning, data\nmining and statistics that run at scale within a database engine, with no need\nfor data import/export to other tools. The goal is for MADlib to eventually\nserve a role for scalable database systems that is similar to the CRAN library\nfor R: a community repository of statistical methods, this time written with\nscale and parallelism in mind. In this paper we introduce the MADlib project,\nincluding the background that led to its beginnings, and the motivation for its\nopen source nature. We provide an overview of the library's architecture and\ndesign patterns, and provide a description of various statistical methods in\nthat context. We include performance and speedup results of a core design\npattern from one of those methods over the Greenplum parallel DBMS on a\nmodest-sized test cluster. We then report on two initial efforts at\nincorporating academic research into MADlib, which is one of the project's\ngoals. MADlib is freely available at http://madlib.net, and the project is open\nfor contributions of both new methods, and ports to additional database\nplatforms.\n",
        "  Smart meters are increasingly used worldwide. Smart meters are the advanced\nmeters capable of measuring energy consumption at a fine-grained time interval,\ne.g., every 15 minutes. Smart meter data are typically bundled with social\neconomic data in analytics, such as meter geographic locations, weather\nconditions and user information, which makes the data sets very sizable and the\nanalytics complex. Data mining and emerging cloud computing technologies make\ncollecting, processing, and analyzing the so-called big data possible. This\npaper proposes an innovative ICT-solution to streamline smart meter data\nanalytics. The proposed solution offers an information integration pipeline for\ningesting data from smart meters, a scalable platform for processing and mining\nbig data sets, and a web portal for visualizing analytics results. The\nimplemented system has a hybrid architecture of using Spark or Hive for big\ndata processing, and using the machine learning toolkit, MADlib, for doing\nin-database data analytics in PostgreSQL database. This paper evaluates the key\ntechnologies of the proposed ICT-solution, and the results show the\neffectiveness and efficiency of using the system for both batch and online\nanalytics.\n",
        "  We report the magnetic field-amplitude and field-angle dependence of the\nsuperconducting onset temperature Tc_onset of the organic superconductor\n(TMTSF)2ClO4 in magnetic fields H accurately aligned to the conductive ab'\nplane. We revealed that the rapid increase of the onset fields at low\ntemperatures occurs both for H // b' and H // a, irrespective of the carrier\nconfinement. Moreover, in the vicinity of the Pauli limiting field, we report a\nshift of a principal axis of the in-plane field-angle dependence of Tc_onset\naway from the b' axis. This feature may be related to an occurrence of\nFulde-Ferrell-Larkin-Ovchinnikov phases.\n",
        "  We present a new distance determination to the Galactic globular cluster 47\nTucanae by fitting the spectral energy distributions of its white dwarfs to\npure hydrogen atmosphere white dwarf models. Our photometric dataset is\nobtained from a 121 orbit Hubble Space Telescope program using the Wide Field\nCamera 3 UVIS/IR channels, capturing F390W, F606W, F110W, and F160W images.\nThese images cover more than 60 square arcmins and extend over a radial range\nof 5-13.7 arcmin (6.5-17.9 pc) within the globular cluster. Using a likelihood\nanalysis, we obtain a best fitting unreddened distance modulus of (m -\nM)o=13.36+/-0.02+/-0.06 corresponding to a distance of 4.70+/-0.04+/-0.13 kpc,\nwhere the first error is random and the second is systematic. We also search\nthe white dwarf photometry for infrared excess in the F160W filter, indicative\nof debris disks or low mass companions, and find no convincing cases within our\nsample.\n",
        "  As the information available to lay users through autonomous data sources\ncontinues to increase, mediators become important to ensure that the wealth of\ninformation available is tapped effectively. A key challenge that these\ninformation mediators need to handle is the varying levels of incompleteness in\nthe underlying databases in terms of missing attribute values. Existing\napproaches such as QPIAD aim to mine and use Approximate Functional\nDependencies (AFDs) to predict and retrieve relevant incomplete tuples. These\napproaches make independence assumptions about missing values---which\ncritically hobbles their performance when there are tuples containing missing\nvalues for multiple correlated attributes. In this paper, we present a\nprincipled probabilistic alternative that views an incomplete tuple as defining\na distribution over the complete tuples that it stands for. We learn this\ndistribution in terms of Bayes networks. Our approach involves\nmining/\"learning\" Bayes networks from a sample of the database, and using it to\ndo both imputation (predict a missing value) and query rewriting (retrieve\nrelevant results with incompleteness on the query-constrained attributes, when\nthe data sources are autonomous). We present empirical studies to demonstrate\nthat (i) at higher levels of incompleteness, when multiple attribute values are\nmissing, Bayes networks do provide a significantly higher classification\naccuracy and (ii) the relevant possible answers retrieved by the queries\nreformulated using Bayes networks provide higher precision and recall than AFDs\nwhile keeping query processing costs manageable.\n",
        "  We present observations of the $^{12}$CO(6-5) line and 686GHz continuum\nemission in NGC253 with the Submillimeter Array at an angular resolution of\n~4arcsec. The $^{12}$CO(6-5) emission is clearly detected along the disk and\nfollows the distribution of the lower $^{12}$CO line transitions with little\nvariations of the line ratios in it. A large-velocity gradient analysis\nsuggests a two-temperature model of the molecular gas in the disk, likely\ndominated by a combination of low-velocity shocks and the disk wide PDRs. Only\nmarginal $^{12}$CO(6-5) emission is detected in the vicinity of the expanding\nshells at the eastern and western edges of the disk. While the eastern shell\ncontains gas even warmer (T$_{\\rm kin}$>300~K) than the hot gas component\n(T$_{\\rm kin}$=300K) of the disk, the western shell is surrounded by gas much\ncooler (T$_{\\rm kin}$=60K) than the eastern shell but somewhat hotter than the\ncold gas component of the disk (for similar H$_2$ and CO column densities),\nindicative of different (or differently efficient) heating mechansisms. The\ncontinuum emission at 686GHz in the disk agrees well in shape and size with\nthat at lower (sub-)millimeter frequencies, exhibiting a spectral index\nconsistent with thermal dust emission. We find dust temperatures of ~10-30K and\nlargely optically thin emission. However, our fits suggest a second (more\noptically thick) dust component at higher temperatures (T$_{\\rm d}$>60K),\nsimilar to the molecular gas. We estimate a global dust mass of ~10$^6$Msun for\nthe disk translating into a gas-to-dust mass ratio of a few hundred consistent\nwith other nearby active galaxies.\n",
        "  With today's public data sets containing billions of data items, more and\nmore companies are looking to integrate external data with their traditional\nenterprise data to improve business intelligence analysis. These distributed\ndata sources however exhibit heterogeneous data formats and terminologies and\nmay contain noisy data. In this paper, we present a novel framework that\nenables business users to semi-automatically perform data integration on\npotentially noisy tabular data. This framework offers an extension to Google\nRefine with novel schema matching algorithms leveraging Freebase rich types.\nFirst experiments show that using Linked Data to map cell values with instances\nand column headers with types improves significantly the quality of the\nmatching results and therefore should lead to more informed decisions.\n",
        "  The thermal conductivity $\\kappa$ of the iron-arsenide superconductor\nBa$_{1-x}$K$_x$Fe$_2$As$_2$ ($T_c \\simeq$ 30 K) was measured in single crystals\nat temperatures down to $T \\simeq 50$ mK ($\\simeq T_c$/600) and in magnetic\nfields up to $H = 15$ T ($\\simeq H_{c2}$/4). A negligible residual linear term\nin $\\kappa/T$ as $T \\to 0$ shows that there are no zero-energy quasiparticles\nin the superconducting state. This rules out the existence of line and in-plane\npoint nodes in the superconducting gap, imposing strong constraints on the\nsymmetry of the order parameter. It excludes d-wave symmetry, drawing a clear\ndistinction between these superconductors and the high-$T_c$ cuprates. However,\nthe fact that a magnetic field much smaller than $H_{c2}$ can induce a residual\nlinear term indicates that the gap must be very small on part of the Fermi\nsurface, whether from strong anisotropy or band dependence, or both.\n",
        "  We developed a method for measuring blood coagulation using superparamagnetic\niron oxide nanoparticles (SPIONs) and an alternating magnetic field (AMF). The\n3rd and 5th harmonic signals from SPIONs mixed with blood induced by AMF were\ndetected using a gradiometer coil. Blood coagulation was induced artificially\nby adding CaCl2 solution to whole blood of sheep at various temperatures and\nhematocrits. We calculated the coagulation rate (k) and normalized signal\nintensity at infinite time (Sinf) by fitting the time course of the normalized\n3rd harmonic signal to S(t)=(1-Sinf)exp(-kt)+Sinf. The k values increased\nsignificantly with increasing temperature and decreased significantly with\nincreasing hematocrit. The Sinf values decreased significantly with increasing\ntemperature and tended to increase with increasing hematocrit. Blood\nanticoagulation was induced by adding heparin to the whole blood sampled from\nmice. There were significant differences in both the 3rd and 5th harmonic\nsignals between groups with and without heparin at 25 min or more after adding\nheparin. We also calculated the 3rd and 5th harmonic signals for viscosities\nranging from 0.001 to 1 kg/m/s, with an assumption that the magnetization and\nparticle size distribution of SPIONs obey the Langevin theory of paramagnetism\nand log-normal distribution, respectively. The 3rd and 5th harmonic signals\nincreased slowly with increasing viscosity and had peaks at approximately 0.015\nand 0.025 kg/m/s, respectively. After these peaks, they decreased monotonically\nwith increasing viscosity. These results confirm the rationale of our method.\nIn conclusion, our method will be useful for measuring blood coagulation and\nanticoagulation and for studying their processes.\n",
        "  Let $u(K)$ and $g(K)$ denote the unknotting number and the genus of a knot\n$K$, respectively. For a 3-braid knot $K$, we show that $u(K)\\le g(K)$ holds,\nand that if $u(K)=g(K)$ then $K$ is either a 2-braid knot, a connected sum of\ntwo 2-braid knots, the figure-eight knot, a strongly quasipositive knot or its\nmirror image.\n",
        "  A new variant of the pencil-beam (PB) algorithm for dose distribution\ncalculation for radiotherapy with protons and heavier ions, the grid-dose\nspreading (GDS) algorithm, is proposed. The GDS algorithm is intrinsically\nfaster than conventional PB algorithms due to approximations in convolution\nintegral, where physical calculations are decoupled from simple grid-to-grid\nenergy transfer. It was effortlessly implemented to a carbon-ion radiotherapy\ntreatment planning system to enable realistic beam blurring in the field, which\nwas absent with the broad-beam (BB) algorithm. For a typical prostate\ntreatment, the slowing factor of the GDS algorithm relative to the BB algorithm\nwas 1.4, which is a great improvement over the conventional PB algorithms with\na typical slowing factor of several tens. The GDS algorithm is mathematically\nequivalent to the PB algorithm for horizontal and vertical coplanar beams\ncommonly used in carbon-ion radiotherapy while dose deformation within the size\nof the pristine spread occurs for angled beams, which was within 3 mm for a\nsingle proton pencil beam of $30^\\circ$ incidence, and needs to be assessed\nagainst the clinical requirements and tolerances in practical situations.\n",
        "  We propose a simple yet effective text- based user geolocation model based on\na neural network with one hidden layer, which achieves state of the art\nperformance over three Twitter benchmark geolocation datasets, in addition to\nproducing word and phrase embeddings in the hidden layer that we show to be\nuseful for detecting dialectal terms. As part of our analysis of dialectal\nterms, we release DAREDS, a dataset for evaluating dialect term detection\nmethods.\n",
        "  This paper presents a synthesis of different studies willing to bring a\nscientific insight into leg compression, which is the process of applying\nexternal compression forces onto the human leg with stockings or socks, for\nenhancing the venous flow. It seems obvious that the pressure distribution on\nthe leg affects the blood flow in the veins. However, the pressure distribution\nthat leads to the optimal blood flow is not trivial, and it is different with\nregard to the application: medical treatment or recovering after an effort in\nsports. In order to improve the scientific knowledge about this topic, a\nnumerical 2D model was set up for computing stress fields inside the leg,\naccounting for the actual material properties of both the compression stocking\nand of the leg biological tissues. Suitable identification methods, based\neither on model updating from digital image correlation (for the compression\nstockings), or on image warping and model updating from MRI scans (for the\ninternal leg tissues), were developed for retrieving these material properties.\n",
        "  The vast increase in our ability to obtain and store trajectory data\nnecessitates trajectory analytics techniques to extract useful information from\nthis data. Pair-wise distance functions are a foundation building block for\ncommon operations on trajectory datasets including constrained SELECT queries,\nk-nearest neighbors, and similarity and diversity algorithms. The accuracy and\nperformance of these operations depend heavily on the speed and accuracy of the\nunderlying trajectory distance function, which is in turn affected by\ntrajectory calibration. Current methods either require calibrated data, or\nperform calibration of the entire relevant dataset first, which is expensive\nand time consuming for large datasets. We present TRAJEDI, a calibrationaware\npair-wise distance calculation scheme that outperforms naive approaches while\npreserving accuracy. We also provide analyses of parameter tuning to trade-off\nbetween speed and accuracy. Our scheme is usable with any diversity, similarity\nor k-nearest neighbor algorithm.\n",
        "  We found that the transport Jc of the ex-situ PIT processed (Ba,K)Fe2As2\n(Ba-122) wire with single Ag sheath can be significantly enhanced by repeating\nthe combined process of rolling and heat treatment. The transport Jc (4.2 K and\n10 T) of 4.4 x 103 A/cm2 (Ic =15.7 A) was obtained for a thin tape of 0.3 mm in\nthickness, which is the highest value reported so far for the PIT processed 122\nwires and tapes with a Ag sheath and processed by a conventional route. The\nmeasurement by a hybrid magnet showed that Jc-H curve maintains very small\nfield dependence up to the strong magnetic field of 28 T as expected from the\npreviously reported high Hc2 value. The core of the thin tape shows dense grain\nstructure with less cracks and voids and indicates the development of c-axis\nalignment, although it is still incomplete. The researches to elucidate the\norigin of Jc enhancement and to have further improvement of transport Jc are\nnow ongoing. The process is simple using a Ag single sheath and, therefore,\nmore realistic technique for long length wire production.\n",
        "  Context. In the HI line profiles in the Leiden-Argentina-Bonn (LAB) all-sky\ndatabase, we have found a population of very cold HI clouds. So far, the role\nof these clouds in the interstellar medium (ISM) has remained unclear. Aims. In\nthis paper, we attempt to confirm the existence of the narrow-line HI emission\n(NHIE) clouds by using the data from the Parkes Galactic all-sky survey (GASS)\nand try to find their place among other coldest constituents of the ISM.\nMethods. We repeat the search of NHIE with the GASS data and derive or compile\nsome preliminary estimates for the distribution, temperatures, distances,\nlinear sizes, column and number densities, masses, and the composition of NHIE\nclouds, and compare these data with corresponding estimates for HI\nself-absorption (HISA) features, the Planck cold clumps (CC), and infrared dark\nclouds (IRDC). Results. We demonstrate that from LAB and GASS we can separate\ncomparable NHIE complexes, and the properties of the obtained NHIE clouds are\nvery similar to those of HISA features, but both of these types of clouds are\nsomewhat warmer and more extended and have lower densities than the cores in\nthe Planck CC and IRDC. Conclusions. We conclude that NHIE may be the same type\nof clouds as HISA, but in different observing conditions, in the same way as\nthe Planck CC and IRDC are most likely similar ISM structures in different\nobserving conditions and probably in slightly different evolutionary stages.\nBoth NHIE and HISA may be an intermediate phase between the diffuse cold\nneutral medium and star-forming molecular clumps represented by the Planck CC\nand IRDC.\n",
        "  We use neutron scattering to show that replacing the larger arsenic with\nsmaller phosphorus in CeFeAs(1-x)P(x)O simultaneously suppresses the AF order\nand orthorhombic distortion near x = 0.4, providing evidence for a magnetic\nquantum critical point. Furthermore, we find that the pnictogen height in iron\narsenide is an important controlling parameter for their electronic and\nmagnetic properties, and may play an important role in electron pairing and\nsuperconductivity.\n",
        "  Digital computing currently uses irreversible logic gates whose energy\ndissipation is fundamentally limited. Reversible logic gates can provide an\nenergy-efficient alternative since they can operate with reversible processes\nthat have no dissipation, such as with scattering processes involving elastic\nparticles. The presented logic uses fluxons, topological solitons in Long\nJosephson Junctions (LJJs), as inputs into and outputs from logic gates. An\nadvantage of using LJJs for connections is that they restrict scattering to 1D\npaths, in contrast to previous ballistic logic which is based on 2D scattering.\nFurthermore, we find through simulation that there is almost no energy loss in\nthe scattering of fluxons between LJJs of designed unpowered circuit gates. To\nswitch bit states, the fluxons are made to change polarity during operations --\nfluxons in an input LJJ freely propagate into the gate circuit, excite a\nnonlinear oscillatory interface mode, and quickly scatter deterministically to\nan output LJJ as a fluxon or antifluxon. The numerically simulated soliton\ndynamics shows that over $97\\%$ of the total energy is preserved as fluxon\nenergy after gate operations. These phenomena are further analyzed with a\ncollective coordinate ansatz, reducing the dynamics of many degrees of freedom\nto two coordinates which characterize fluxon and antifluxon type excitations in\nthe input and the output LJJs. The solutions of the reduced model accurately\ndescribe the four possible energy-conserving scattering processes found in\nsimulations of one-bit gate circuits (with two scattering directions and two\noutput polarities). Calculated parameter tolerances indicate that the gates can\nbe manufactured and tested. Results are shown for 1-bit gates as well as a\nfundamental 2-bit gate.\n",
        "  Photoacoustic computed tomography (PACT) is a rapidly emerging bioimaging\nmodality that seeks to reconstruct an estimate of the absorbed optical energy\ndensity within an object. Conventional PACT image reconstruction methods assume\na constant speed-of-sound (SOS), which can result in image artifacts when\nacoustic aberrations are significant. It has been demonstrated that\nincorporating knowledge of an object's SOS distribution into a PACT image\nreconstruction method can improve image quality. However, in many cases, the\nSOS distribution cannot be accurately and/or conveniently estimated prior to\nthe PACT experiment. Because variations in the SOS distribution induce\naberrations in the measured photoacoustic wavefields, certain information\nregarding an object's SOS distribution is encoded in the PACT measurement data.\nBased on this observation, a joint reconstruction (JR) problem has been\nproposed in which the SOS distribution is concurrently estimated along with the\nsought-after absorbed optical energy density from the photoacoustic measurement\ndata. A broad understanding of the extent to which the JR problem can be\naccurately and reliably solved has not been reported. In this work, a series of\nnumerical experiments is described that elucidate some important properties of\nthe JR problem that pertain to its practical feasibility. To accomplish this,\nan optimization-based formulation of the JR problem is developed that yields a\nnon-linear iterative algorithm that alternatingly updates the two image\nestimates. Heuristic analytic insights into the reconstruction problem are also\nprovided. These results confirm the ill-conditioned nature of the joint\nreconstruction problem that will present significant challenges for practical\napplications.\n",
        "  Friction incorporates the close connection between classical mechanics in\nirreversible thermodynamics. The translation to a quantum mechanical foundation\nis not trivial and requires a generalization of the Lagrange function. A change\nto electromagnetic circuits appears to more adequate, since the electric\nanalogue (Ohms law) is related to scatter of electrons at lattice vibrations.\n",
        "  This work is concerned with applying iterative image reconstruction, based on\nconstrained total-variation minimization, to low-intensity X-ray CT systems\nthat have a high sampling rate. Such systems pose a challenge for iterative\nimage reconstruction, because a very fine image grid is needed to realize the\nresolution inherent in such scanners. These image arrays lead to\nunder-determined imaging models whose inversion is unstable and can result in\nundesirable artifacts and noise patterns. There are many possibilities to\nstabilize the imaging model, and this work proposes a method which may have an\nadvantage in terms of algorithm efficiency. The proposed method introduces\nadditional constraints in the optimization problem; these constraints set to\nzero high spatial frequency components which are beyond the sensing capability\nof the detector. The method is demonstrated with an actual CT data set and\ncompared with another method based on projection up-sampling.\n",
        "  Previous studies on Chinese semantic role labeling (SRL) have concentrated on\nsingle semantically annotated corpus. But the training data of single corpus is\noften limited. Meanwhile, there usually exists other semantically annotated\ncorpora for Chinese SRL scattered across different annotation frameworks. Data\nsparsity remains a bottleneck. This situation calls for larger training\ndatasets, or effective approaches which can take advantage of highly\nheterogeneous data. In these papers, we focus mainly on the latter, that is, to\nimprove Chinese SRL by using heterogeneous corpora together. We propose a novel\nprogressive learning model which augments the Progressive Neural Network with\nGated Recurrent Adapters. The model can accommodate heterogeneous inputs and\neffectively transfer knowledge between them. We also release a new corpus,\nChinese SemBank, for Chinese SRL. Experiments on CPB 1.0 show that ours model\noutperforms state-of-the-art methods.\n",
        "  This paper describes the Arabic MGB-3 Challenge - Arabic Speech Recognition\nin the Wild. Unlike last year's Arabic MGB-2 Challenge, for which the\nrecognition task was based on more than 1,200 hours broadcast TV news\nrecordings from Aljazeera Arabic TV programs, MGB-3 emphasises dialectal Arabic\nusing a multi-genre collection of Egyptian YouTube videos. Seven genres were\nused for the data collection: comedy, cooking, family/kids, fashion, drama,\nsports, and science (TEDx). A total of 16 hours of videos, split evenly across\nthe different genres, were divided into adaptation, development and evaluation\ndata sets. The Arabic MGB-Challenge comprised two tasks: A) Speech\ntranscription, evaluated on the MGB-3 test set, along with the 10 hour MGB-2\ntest set to report progress on the MGB-2 evaluation; B) Arabic dialect\nidentification, introduced this year in order to distinguish between four major\nArabic dialects - Egyptian, Levantine, North African, Gulf, as well as Modern\nStandard Arabic. Two hours of audio per dialect were released for development\nand a further two hours were used for evaluation. For dialect identification,\nboth lexical features and i-vector bottleneck features were shared with\nparticipants in addition to the raw audio recordings. Overall, thirteen teams\nsubmitted ten systems to the challenge. We outline the approaches adopted in\neach system, and summarise the evaluation results.\n",
        "  It is shown that many published models for the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) lack robustness, suffering an over 50%\ndecrease in F1 score during adversarial evaluation based on the AddSent (Jia\nand Liang, 2017) algorithm. It has also been shown that retraining models on\ndata generated by AddSent has limited effect on their robustness. We propose a\nnovel alternative adversary-generation algorithm, AddSentDiverse, that\nsignificantly increases the variance within the adversarial training data by\nproviding effective examples that punish the model for making certain\nsuperficial assumptions. Further, in order to improve robustness to AddSent's\nsemantic perturbations (e.g., antonyms), we jointly improve the model's\nsemantic-relationship learning capabilities in addition to our\nAddSentDiverse-based adversarial training data augmentation. With these\nadditions, we show that we can make a state-of-the-art model significantly more\nrobust, achieving a 36.5% increase in F1 score under many different types of\nadversarial evaluation while maintaining performance on the regular SQuAD task.\n",
        "  We propose the formation of massive pristine dark-matter (DM) halos with\nmasses of $\\sim 10^8~M_\\odot$, due to the dynamical effects of frequent mergers\nin rare regions of the Universe with high baryonic streaming velocity relative\nto DM. Since the streaming motion prevents gas collapse into DM halos and\ndelays prior star formation episodes, the gas remains metal-free until the halo\nvirial temperatures $\\gtrsim 2\\times 10^4~{\\rm K}$. The minimum cooling mass of\nDM halos is boosted by a factor of $\\sim 10-30$ because frequent major mergers\nof halos further inhibit gas collapse. We use Monte Carlo merger trees to\nsimulate the DM assembly history under a streaming velocity of twice the\nroot-mean-square value, and estimate the number density of massive DM halos\ncontaining pristine gas as $\\simeq 10^{-4}~{\\rm cMpc}^{-3}$. When the gas\ninfall begins, efficient Ly$\\alpha$ cooling drives cold streams penetrating\ninside the halo and feeding a central galactic disk. When one stream collides\nwith the disk, strong shock forms a dense and hot gas cloud, where the gas\nnever forms H$_2$ molecules due to effective collisional dissociation. As a\nresult, a massive gas cloud forms by gravitational instability and collapses\ndirectly into a massive black hole (BH) with $M_\\bullet \\sim 10^5~M_\\odot$.\nAlmost simultaneously, a galaxy with $M_{\\star, \\rm tot}\\sim 10^6~M_\\odot$\ncomposed of Population III stars forms in the nuclear region. If the typical\nstellar mass is as high as $\\sim 100~M_\\odot$, the galaxy could be detected\nwith the James Webb Space Telescope even at $z\\gtrsim 15$. These massive seed\nBHs would be fed by continuous gas accretion from the host galaxy, and grow to\nbe bright quasars observed at $z\\gtrsim 6$.\n",
        "  Measurements of the high-frequency complex resistivity in superconductors are\na tool often used to obtain the vortex parameters, such as the vortex\nviscosity, the pinning constant and the depinning frequency. In anisotropic\nsuperconductors, the extraction of these quantities from the measurements faces\nnew difficulties due to the tensor nature of the electromagnetic problem. The\nproblem is specifically intricate when the magnetic field is tilted with\nrespect to the crystallographic axes. Partial solutions exist in the\nfree-flux-flow (no pinning) and Campbell (pinning dominated) regimes. In this\npaper we develop a full tensor model for the vortex motion complex resistivity,\nincluding flux-flow, pinning, and creep. We give explicit expressions for the\ntensors involved. We obtain that, despite the complexity of the physics, some\nparameters remain scalar in nature. We show that under specific circumstances\nthe directly measured quantities do not reflect the true vortex parameters, and\nwe give procedures to derive the true vortex parameters from measurements taken\nwith arbitrary field orientations. Finally, we discuss the applicability of the\nangular scaling properties to the measured and transformed vortex parameters\nand we exploit these properties as a tool to unveil the existence of\ndirectional pinning.\n",
        "  Assuming a hydrostatic equilibrium in an HI cloud, the joint Poisson's\nequation is set up and numerically solved to calculate the expected HI\ndistribution. Unlike previous studies, the cloud is considered to be\nnon-isothermal, and an {\\it iterative} method is employed to iteratively\nestimate the intrinsic velocity dispersion profile using the observed\nsecond-moment of the HI data. We apply our {\\it iterative} method to a recently\ndiscovered dwarf galaxy Leo T and find that its observed HI distribution does\nnot comply with the expected one if one assumes no dark matter in it. To model\nthe mass distribution in Leo T, we solve the Poisson's equation using a large\nnumber of trial dark matter halos and compare the model HI surface density\n($\\Sigma_{HI}$) profiles to the observed one to identify the best dark matter\nhalo parameters. For Leo T, we find a pseudo-isothermal halo with core density,\n$\\rho_0 \\sim 0.67$ $\\rm M_{\\odot} \\thinspace pc^{-3}$ and core radius, $r_s\n\\sim 37$ parsec explains the observation best. The resulting dark matter halo\nmass within the central 300 pc, $M_{300}$, found to be $\\sim 2.7 \\times 10^6$\n$\\rm M_{\\odot}$. We also find that a set of dark matter halos with similar\n$M_{300} \\sim 3.7 \\times 10^6$ $\\rm M_{\\odot}$ but very different $\\rho_0$ and\n$r_s$ values, can produce equally good $\\Sigma_{HI}$ profile within the\nobservational uncertainties. This, in turn, indicates a strong degeneracy\nbetween the halo parameters and the best fit values are not unique.\nInterestingly, it also implies that the mass of a dark matter halo, rather than\nits structure primarily directs the expected HI distribution under hydrostatic\nequilibrium.\n",
        "  We have studied the effect of tensile strain on the superconductivity in FeSe\nfilms. 50 nm, 100 nm, and 200 nm FeSe films were grown on MgO, SrTiO$_3$, and\nLaAlO$_3$ substrates by using a pulsed laser deposition technique. X-ray\ndiffraction analysis showed that the tetragonal phase is dominant in all of our\nFeSe films. The 50 nm FeSe films on MgO and SrTiO$_3$ are under tensile strain,\nwhile the 50 nm FeSe film on LaAlO$_3$ and the other thick FeSe films are\nunstrained. Superconducting transitions have been observed in unstrained FeSe\nfilms with T$_{onset}$ $\\approx$ 8 K, which is close to the bulk value.\nHowever, no sign of superconductivity has been observed in FeSe films under\ntensile strain down to 5 K. There is evidence to show that tensile strain\nsuppresses superconductivity in FeSe films.\n",
        "  A recurrent neural network that has been trained to separately model the\nlanguage of several documents by unknown authors is used to measure similarity\nbetween the documents. It is able to find clues of common authorship even when\nthe documents are very short and about disparate topics. While it is easy to\nmake statistically significant predictions regarding authorship, it is\ndifficult to group documents into definite clusters with high accuracy.\n",
        "  We train one multilingual model for dependency parsing and use it to parse\nsentences in several languages. The parsing model uses (i) multilingual word\nclusters and embeddings; (ii) token-level language information; and (iii)\nlanguage-specific features (fine-grained POS tags). This input representation\nenables the parser not only to parse effectively in multiple languages, but\nalso to generalize across languages based on linguistic universals and\ntypological similarities, making it more effective to learn from limited\nannotations. Our parser's performance compares favorably to strong baselines in\na range of data scenarios, including when the target language has a large\ntreebank, a small treebank, or no treebank for training.\n",
        "  We present a backward approach for the interpretation of the evolution of the\nnear-infrared and the far-infrared luminosity functions across the redshift\nrange 0<z<3. In our method, late-type galaxies are treated by means of a\nparametric phenomenological method based on PEP/HerMES data up to z~4, whereas\nspheroids are described by means of a physically motivated backward model. The\nspectral evolution of spheroids is modelled by means of a single-mass model,\nassociated to a present-day elliptical with K-band luminosity comparable to the\nbreak of the local early-type luminosity function. The formation of\nproto-spheroids is assumed to occurr across the redshift range 1< z < 5. The\nkey parameter is represented by the redshift z_0.5 at which half\nproto-spheroids are already formed. A statistical study indicates for this\nparameter values between z_0.5=1.5 and z_0.5=3. We assume as fiducial value\nz_0.5~2, and show that this assumption allows us to describe accourately the\nredshift distributions and the source counts. By assuming z_0.5 ~ 2 at the\nfar-IR flux limit of the PEP-COSMOS survey, the PEP-selected sources observed\nat z>2 can be explained as progenitors of local spheroids caught during their\nformation. We also test the effects of mass downsizing by dividing the\nspheroids into three populations of different present-day stellar masses. The\nresults obtained in this case confirm the validity of our approach, i.e. that\nthe bulk of proto-spheroids can be modelled by means of a single model which\ndescribes the evolution of galaxies at the break of the present-day early type\nK-band LF.\n",
        "  We examine the two-dimensional extension of the model of Kessler and Sander\nof competition between two species identical except for dispersion rates. In\nthis class of models, the spatial inhomogeneity of reproduction rates gives\nrise to an implicit cost of dispersal, due to the tendency to leave favorable\nlocations. Then, as in the Hamilton-May model with its explicit dispersal cost,\nthe tradeoff between dispersal case and the beneficial role of dispersal in\nlimiting fluctuations, leads to an advantage of one dispersal rate over\nanother, and the eventual extinction of the disadvantaged species. In two\ndimensions we find that while the competition leads to the elimination of one\nspecies at high and low population density, at intermediate densities the two\nspecies can coexist essentially indefinitely. This is a new phenomenon not\npresent in either the one-dimensional form of the Kessler-Sander model nor in\nthe totally connected Hamilton-May model, and points to the importance of\ngeometry in the question of dispersal.\n",
        "  Online content analysis employs algorithmic methods to identify entities in\nunstructured text. Both machine learning and knowledge-base approaches lie at\nthe foundation of contemporary named entities extraction systems. However, the\nprogress in deploying these approaches on web-scale has been been hampered by\nthe computational cost of NLP over massive text corpora. We present SpeedRead\n(SR), a named entity recognition pipeline that runs at least 10 times faster\nthan Stanford NLP pipeline. This pipeline consists of a high performance Penn\nTreebank- compliant tokenizer, close to state-of-art part-of-speech (POS)\ntagger and knowledge-based named entity recognizer.\n",
        "  Evolutionary branching points are a paradigmatic feature of adaptive\ndynamics, because they are potential starting points for adaptive\ndiversification. The antithesis to evolutionary branching points are\nContinuously stable strategies (CSS's), which are convergent stable and\nevolutionarily stable equilibrium points of the adaptive dynamics and hence are\nthought to represent endpoints of adaptive processes. However, this assessment\nis based on situations in which the invasion fitness function determining the\nadaptive dynamics have non-zero second derivatives at a CSS. Here we show that\nthe scope of evolutionary branching can increase if the invasion fitness\nfunction vanishes to higher than first order at a CSS. Using a class of\nclassical models for frequency-dependent competition, we show that if the\ninvasion fitness vanishes to higher orders, a CSS may be the starting point for\nevolutionary branching, with the only additional requirement that mutant types\nneed to reach a certain threshold frequency, which can happen e.g. due to\ndemographic stochasticity. Thus, when invasion fitness functions vanish to\nhigher than first order at equilibrium points of the adaptive dynamics,\nevolutionary diversification can occur even after convergence to an\nevolutionarily stable strategy.\n",
        "  Providing an appropriate level of accessibility and traceability to data or\nprocess elements (so-called Items) in large volumes of data, often\nCloud-resident, is an essential requirement in the Big Data era.\nEnterprise-wide data systems need to be designed from the outset to support\nusage of such Items across the spectrum of business use rather than from any\nspecific application view. The design philosophy advocated in this paper is to\ndrive the design process using a so-called description-driven approach which\nenriches models with meta-data and description and focuses the design process\non Item re-use, thereby promoting traceability. Details are given of the\ndescription-driven design of big data systems at CERN, in health informatics\nand in business process management. Evidence is presented that the approach\nleads to design simplicity and consequent ease of management thanks to loose\ntyping and the adoption of a unified approach to Item management and usage.\n",
        "  Neural network based models commonly regard event detection as a word-wise\nclassification task, which suffer from the mismatch problem between words and\nevent triggers, especially in languages without natural word delimiters such as\nChinese. In this paper, we propose Nugget Proposal Networks (NPNs), which can\nsolve the word-trigger mismatch problem by directly proposing entire trigger\nnuggets centered at each character regardless of word boundaries. Specifically,\nNPNs perform event detection in a character-wise paradigm, where a hybrid\nrepresentation for each character is first learned to capture both structural\nand semantic information from both characters and words. Then based on learned\nrepresentations, trigger nuggets are proposed and categorized by exploiting\ncharacter compositional structures of Chinese event triggers. Experiments on\nboth ACE2005 and TAC KBP 2017 datasets show that NPNs significantly outperform\nthe state-of-the-art methods.\n",
        "  We compute the effect of concordance surgery, a generalization of knot\nsurgery defined using a self-concordance of a knot, on the Ozsv\\'ath-Szab\\'o\n4-manifold invariant. The formula involves the graded Lefschetz number of the\nconcordance map on knot Floer homology. The proof uses the sutured Floer TQFT,\nand a version of sutured Floer homology perturbed by a 2-form.\n",
        "  Destination prediction is an essential task in a variety of mobile\napplications. In this paper, we optimize the matrix operation and adapt a\nsemi-lazy framework to improve the prediction accuracy and efficiency of a\nstate-of-the-art approach. To this end, we employ efficient dynamic-programming\nby devising several data constructs including Efficient Transition Probability\nand Transition Probabilities with Detours that are capable of pinpointing the\nminimum amount of computation. We prove that our method achieves one order of\ncut in both time and space complexity. The experimental results on real-world\nand synthetic datasets have shown that our solution consistently outperforms\nits state-of-the-art counterparts in terms of both efficiency (approximately\nover 100 times faster) and accuracy (above 30% increase).\n",
        "  Most ectotherms show an inverse relationship between developmental\ntemperature and body size, a phenomenon known as the temperature size rule\n(TSR). Several competing hypotheses have been proposed to explain its\noccurrence. According to one set of views, the TSR results from inevitable\nbiophysical effects of temperature on the rates of growth and differentiation,\nwhereas other views suggest the TSR is an adaptation that can be achieved by a\ndiversity of mechanisms in different taxa. Our data reveal that the fruit fly,\nDrosophila melanogaster, obeys the TSR using a novel mechanism: reduction of\ncritical size at higher temperatures. In holometabolous insects, attainment of\ncritical size initiates the hormonal cascade that terminates growth, and hence,\nDrosophila larvae appear to instigate the signal to stop growth at a smaller\nsize at higher temperatures. This is in contrast to findings from another\nholometabolous insect, Manduca sexta, in which the TSR results from the effect\nof temperature on the rate and duration of growth. This contrast suggests that\nthere is no single mechanism that accounts for the TSR. Instead, the TSR\nappears to be an adaptation that is achieved at a proximate level through\ndifferent mechanisms in different taxa.\n",
        "  A Dialogue System is a system which interacts with human in natural language.\nAt present many universities are developing the dialogue system in their\nregional language. This paper will discuss about dialogue system, its\ncomponents, challenges and its evaluation. This paper helps the researchers for\ngetting info regarding dialogues system.\n",
        "  The number of linked data sources and the size of the linked open data graph\nkeep growing every day. As a consequence, semantic RDF services are more and\nmore confronted to various \"big data\" problems. Query processing is one of them\nand needs to be efficiently addressed with executions over scalable, highly\navailable and fault tolerant frameworks. Data management systems requiring\nthese properties are rarely built from scratch but are rather designed on top\nof an existing cluster computing engine. In this work, we consider the\nprocessing of SPARQL queries with Apache Spark. We propose and compare five\ndifferent query processing approaches based on different join execution models\nand Spark components. A detailed experimentation, on real-world and synthetic\ndata sets, emphasizes that two approaches tailored for the RDF data model\noutperform the other ones on all major query shapes, i.e., star, snowflake,\nchain and hybrid.\n",
        "  While neural networks have been shown to achieve impressive results for\nsentence-level sentiment analysis, targeted aspect-based sentiment analysis\n(TABSA) --- extraction of fine-grained opinion polarity w.r.t. a pre-defined\nset of aspects --- remains a difficult task. Motivated by recent advances in\nmemory-augmented models for machine reading, we propose a novel architecture,\nutilising external \"memory chains\" with a delayed memory update mechanism to\ntrack entities. On a TABSA task, the proposed model demonstrates substantial\nimprovements over state-of-the-art approaches, including those using external\nknowledge bases.\n",
        "  We use the optimal foraging theory to study coexistence between two plant\nspecies and a generalist pollinator. We compare conditions for plant\ncoexistence for non-adaptive vs. adaptive pollinators that adjust their\nforaging strategy to maximize fitness. When pollinators have fixed preferences,\nwe show that plant coexistence typically requires both weak competition between\nplants for resources (e.g., space or nutrients) and pollinator preferences that\nare not too biased in favour of either plant. We also show how plant\ncoexistence is promoted by indirect facilitation via the pollinator. When\npollinators are adaptive foragers, pollinator's diet maximizes pollinator's\nfitness measured as the per capita population growth rate. Simulations show\nthat this has two conflicting consequences for plant coexistence. On the one\nhand, when competition between pollinators is weak, adaptation favours\npollinator specialization on the more profitable plant which increases\nasymmetries in plant competition and makes their coexistence less likely. On\nthe other hand, when competition between pollinators is strong, adaptation\npromotes generalism, which facilitates plant coexistence. In addition, adaptive\nforaging allows pollinators to survive sudden loss of the preferred plant host,\nthus preventing further collapse of the entire community.\n  Keywords: mutualism, competition, optimal foraging, evolutionarily stable\nstrategy, coexistence, adaptation rate\n",
        "  We report the kinematic discovery of the Pisces Stellar Stream (PSS), at\nGalactic longitude of roughly l=135 degrees and longitude between -39 < b < -36\ndeg. We originally identified this halo substructure from velocities of red\ngiant branch stars in Sloan Digital Sky Survey (SDSS) Data Release 8, and\nconfirmed its presence in turnoff stars from SDSS photometric data. The PSS is\na narrow, kinematically cold tidal stream, with a velocity dispersion of 8\nkm/s. Its metallicity is [Fe/H]=-2.2, with ~0.3 dex dispersion. The\ncolor-magnitude signature of the stream turnoff, combined with our measured\nmetallicity, places the PSS at a distance of 35+/-3 kpc. The Pisces Stellar\nStream is the same as the previously announced \"Triangulum stream\" and part of\nthe proposed \"stream a\". We rule out an association of the PSS with other\npreviously known Milky Way substructures in the same region of the sky.\n",
        "  Machine translation is evolving quite rapidly in terms of quality. Nowadays,\nwe have several machine translation systems available in the web, which provide\nreasonable translations. However, these systems are not perfect, and their\nquality may decrease in some specific domains. This paper examines the effects\nof different training methods when it comes to Polish - English Statistical\nMachine Translation system used for the medical data. Numerous elements of the\nEMEA parallel text corpora and not related OPUS Open Subtitles project were\nused as the ground for creation of phrase tables and different language models\nincluding the development, tuning and testing of these translation systems. The\nBLEU, NIST, METEOR, and TER metrics have been used in order to evaluate the\nresults of various systems. Our experiments deal with the systems that include\nPOS tagging, factored phrase models, hierarchical models, syntactic taggers,\nand other alignment methods. We also executed a deep analysis of Polish data as\npreparatory work before automatized data processing such as true casing or\npunctuation normalization phase. Normalized metrics was used to compare\nresults. Scores lower than 15% mean that Machine Translation engine is unable\nto provide satisfying quality, scores greater than 30% mean that translations\nshould be understandable without problems and scores over 50 reflect adequate\ntranslations. The average results of Polish to English translations scores for\nBLEU, NIST, METEOR, and TER were relatively high and ranged from 70,58 to\n82,72. The lowest score was 64,38. The average results ranges for English to\nPolish translations were little lower (67,58 - 78,97). The real-life\nimplementations of presented high quality Machine Translation Systems are\nanticipated in general medical practice and telemedicine.\n",
        "  Chiral $p$-wave superconductors in applied magnetic field can exhibit more\ncomplex topological defects than just conventional superconducting vortices,\ndue to the two-component order parameter (OP) and the broken time-reversal\nsymmetry. We investigate the electronic properties of those exotic states, some\nof which contain clusters of one-component vortices in chiral components of the\nOP and/or exhibit skyrmionic character in the \\textit{relative} OP space, all\nobtained as a self-consistent solution of the microscopic Bogoliubov-de Gennes\nequations. We reveal the link between the local density of states (LDOS) of the\nnovel topological states and the behavior of the chiral domain wall between the\nOP components, enabling direct identification of those states in scanning\ntunneling microscopy. For example, a skyrmion always contains a closed chiral\ndomain wall, which is found to be mapped exactly by zero-bias peaks in LDOS.\nMoreover, the LDOS exhibits electron-hole asymmetry, which is different from\nthe LDOS of conventional vortex states with the same vorticity. Finally, we\npresent the magnetic field and temperature dependence of the properties of a\nskyrmion, indicating that this topological defect can be surprisingly large in\nsize, and can be pinned by an artificially indented non-superconducting closed\npath in the sample. These features are expected to facilitate the experimental\nobservation of skyrmionic states, thereby enabling experimental verification of\nchirality in emerging superconducting materials.\n",
        "  We address the problem of efficiently evaluating target functional\ndependencies (fds) in the Data Exchange (DE) process. Target fds naturally\noccur in many DE scenarios, including the ones in Life Sciences in which\nmultiple source relations need to be structured under a constrained target\nschema. However, despite their wide use, target fds' evaluation is still a\nbottleneck in the state-of-the-art DE engines. Systems relying on an all-SQL\napproach typically do not support target fds unless additional information is\nprovided. Alternatively, DE engines that do include these dependencies\ntypically pay the price of a significant drop in performance and scalability.\nIn this paper, we present a novel chase-based algorithm that can efficiently\nhandle arbitrary fds on the target. Our approach essentially relies on\nexploiting the interactions between source-to-target (s-t) tuple-generating\ndependencies (tgds) and target fds. This allows us to tame the size of the\nintermediate chase results, by playing on a careful ordering of chase steps\ninterleaving fds and (chosen) tgds. As a direct consequence, we importantly\ndiminish the fd application scope, often a central cause of the dramatic\noverhead induced by target fds. Moreover, reasoning on dependency interaction\nfurther leads us to interesting parallelization opportunities, yielding\nadditional scalability gains. We provide a proof-of-concept implementation of\nour chase-based algorithm and an experimental study aiming at gauging its\nscalability with respect to a number of parameters, among which the size of\nsource instances and the number of dependencies of each tested scenario.\nFinally, we empirically compare with the latest DE engines, and show that our\nalgorithm outperforms them.\n",
        "  We study the set G of growth rates of of ideal Coxeter groups in hyperbolic\n3-space which consists of real algebraic integers greater than 1. We show that\n(1) G is unbounded above while it has the minimum, (2) any element of G is a\nPerron number, and (3) growth rates of of ideal Coxeter groups with $n$\ngenerators are located in the closed interval $[n-3, n-1]$.\n",
        "  The problem of determining a current density confined to a volume from\nmeasurements of the magnetic field it produces exterior to that volume is known\nto have non-unique solutions. To uniquely determine the current density, or the\nnon-silent components of it, additional spatial encoding of the electric\ncurrent is needed. In biological systems such as the brain and heart, which\ngenerate electric current associated with normal function, a reliable means of\ngenerating such additional encoding, on a spatial and temporal scale meaningful\nto the study of such systems, would be a boon for research. This paper explores\na speculative method by which the required additional encoding might be\naccomplished, on the time scale associated with the propagation of sound across\nthe volume of interest, by means of the application of a radially encoding\npulsed acoustic spherical wave.\n",
        "  Dynamic models - often deterministic in nature - were used to estimate the\nbasic reproductive number, R_0, of the 2014-5 Ebola virus disease (EVD)\nepidemic outbreak in West Africa. Estimates of R_0 were then used to project\nthe likelihood for large outbreak sizes, e.g., exceeding hundreds of thousands\nof cases. Yet fitting deterministic models can lead to over-confidence in the\nconfidence intervals of the fitted R_0, and, in turn, the type and scope of\nnecessary interventions. In this manuscript we propose a hybrid\nstochastic-deterministic method to estimate R_0 and associated confidence\nintervals (CIs). The core idea is that stochastic realizations of an underlying\ndeterministic model can be used to evaluate the compatibility of candidate\nvalues of R_0 with observed epidemic curves. The compatibility is based on\ncomparing the distribution of expected epidemic growth rates with the observed\nepidemic growth rate given \"process noise\", i.e., arising due to stochastic\ntransmission, recovery and death events. By applying our method to reported EVD\ncase counts from Guinea, Liberia and Sierra Leone, we show that prior estimates\nof R_0 based on deterministic fits appear to be more confident than analysis of\nstochastic trajectories suggests should be possible. Moving forward, we\nrecommend including a hybrid stochastic-deterministic fitting procedure when\nquantifying the full R_0 CI at the onset of an epidemic due to multiple sources\nof noise.\n",
        "  A double pants decomposition of a 2-dimensional surface is a collection of\ntwo pants decomposition of this surface introduced in arXiv:1005.0073v2. There\nare two natural operations acting on double pants decompositions: flips and\nhandle twists. It is shown in arXiv:1005.0073v2 that the groupoid generated by\nflips and handle twists acts transitively on admissible double pants\ndecompositions where the class of admissible decompositions has a natural\ntopological and combinatorial description. In this paper, we label the curves\nof double pants decompositions and show that for all but one surfaces the same\ngroupoid acts transitively on all labeled admissible double pants\ndecompositions. The only exclusion is a sphere with two handles, where the\ngroupoid has 15 orbits.\n",
        "  We search for the existence of chemically-distinct stellar components in the\nsolar neighbourhood using published data. Extending earlier work, we show that\nwhen the abundances of Fe, alpha elements, and the r-process element Eu are\nconsidered, stars separate neatly into two groups that delineate the\ntraditional thin and thick disk of the Milky Way. The group akin to the thin\ndisk is traced by stars with [Fe/H]>-0.7 and alpha/Fe<0.2. The thick disk-like\ngroup overlaps the thin disk in [Fe/H] but has higher abundances of alpha\nelements and Eu. Stars in the range -1.5<[Fe/H]<-0.7 with low [alpha/Fe]\nratios, however, seem to belong to a separate, dynamically-cold, non-rotating\ncomponent that we associate with tidal debris. The kinematically-hot stellar\nhalo dominates the sample for [Fe/H]<-1.5. These results suggest that it may be\npossible to define the main dynamical components of the solar neighbourhood\nusing only their chemistry, an approach with a number of interesting\nconsequences. The kinematics of thin disk stars is then independent of\nmetallicity: their average rotation speed remains roughly constant in the range\n-0.7<[Fe/H]<+0.4, a result that argues against radial migration having played a\nsubstantial role in the evolution of the thin disk. The velocity dispersion of\nstars assigned to the thin disk is also independent of [Fe/H], implying that\nthe familiar increase in velocity dispersion with decreasing metallicity is the\nresult of the increased prevalence of the thick disk at lower metallicities,\nrather than of the sustained operation of a dynamical heating mechanism. The\nsubstantial overlap in [Fe/H] and, probably, stellar age, of the various\ncomponents might affect other reported trends in the properties of stars in the\nsolar neighbourhood.\n",
        "  Generative Adversarial Networks (GANs) have gained a lot of attention from\nmachine learning community due to their ability to learn and mimic an input\ndata distribution. GANs consist of a discriminator and a generator working in\ntandem playing a min-max game to learn a target underlying data distribution;\nwhen fed with data-points sampled from a simpler distribution (like uniform or\nGaussian distribution). Once trained, they allow synthetic generation of\nexamples sampled from the target distribution. We investigate the application\nof GANs to generate synthetic feature vectors used for speech emotion\nrecognition. Specifically, we investigate two set ups: (i) a vanilla GAN that\nlearns the distribution of a lower dimensional representation of the actual\nhigher dimensional feature vector and, (ii) a conditional GAN that learns the\ndistribution of the higher dimensional feature vectors conditioned on the\nlabels or the emotional class to which it belongs. As a potential practical\napplication of these synthetically generated samples, we measure any\nimprovement in a classifier's performance when the synthetic data is used along\nwith real data for training. We perform cross-validation analyses followed by a\ncross-corpus study.\n",
        "  Measurements performed on superconductive networks shaped in the form of\nplanar graphs display anomalously large currents when specific branches are\nbiased. The temperature dependencies of these currents evidence that their\norigin is due to Cooper pair hopping through the Josephson junctions connecting\nthe superconductive islands of the array. The experimental data are discussed\nin terms of a theoretical model which predicts, for the system under\nconsideration, an inhomogeneous Cooper pair distribution on the superconductive\nislands of the network.\n",
        "  Point contact tunneling (PCT) and low energy muon spin rotation (LE-muSR) are\nused to probe, on the same samples, the surface superconducting properties of\nmicrometer thick niobium films deposited onto copper substrates using different\nsputtereing techniques: diode, dc magnetron (dcMS) and HIPIMS. The combined\nresults are compared to radio-frequency tests performances of RF cavities made\nwith the same processes. Degraded surface superconducting properties are found\nto yield lower quality factors and stronger Q slope. In addition, both\ntechniques find evidence for surface paramagnetism on all samples and\nparticularly on Nb films prepared by HIPIMS.\n",
        "  The design and operation of an electronic cooler based on a combination of\nsuperconducting tunnel junctions is described. The cascade extraction of\nhot-quasiparticles, which stems from the energy gaps of two different\nsuperconductors, allows for a normal metal to be cooled down to about 100 mK\nstarting from a bath temperature of 0.5 K. We discuss the practical\nimplementation, potential performance and limitations of such a device.\n",
        "  Recently, MapReduce based spatial query systems have emerged as a cost\neffective and scalable solution to large scale spatial data processing and\nanalytics. MapReduce based systems achieve massive scalability by partitioning\nthe data and running query tasks on those partitions in parallel. Therefore,\neffective data partitioning is critical for task parallelization, load\nbalancing, and directly affects system performance. However, several pitfalls\nof spatial data partitioning make this task particularly challenging. First,\ndata skew is very common in spatial applications. To achieve best query\nperformance, data skew need to be reduced. Second, spatial partitioning\napproaches generate boundary objects that cross multiple partitions, and add\nextra query processing overhead. Consequently, boundary objects need to be\nminimized. Third, the high computational complexity of spatial partitioning\nalgorithms combined with massive amounts of data require an efficient approach\nfor partitioning to achieve overall fast query response. In this paper, we\nprovide a systematic evaluation of multiple spatial partitioning methods with a\nset of different partitioning strategies, and study their implications on the\nperformance of MapReduce based spatial queries. We also study sampling based\npartitioning methods and their impact on queries, and propose several MapReduce\nbased high performance spatial partitioning methods. The main objective of our\nwork is to provide a comprehensive guidance for optimal spatial data\npartitioning to support scalable and fast spatial data processing in massively\nparallel data processing frameworks such as MapReduce. The algorithms developed\nin this work are open source and can be easily integrated into different high\nperformance spatial data processing systems.\n",
        "  Adding manually annotated prosodic information, specifically pitch accents\nand phrasing, to the typical text-based feature set for coreference resolution\nhas previously been shown to have a positive effect on German data. Practical\napplications on spoken language, however, would rely on automatically predicted\nprosodic information. In this paper we predict pitch accents (and phrase\nboundaries) using a convolutional neural network (CNN) model from acoustic\nfeatures extracted from the speech signal. After an assessment of the quality\nof these automatic prosodic annotations, we show that they also significantly\nimprove coreference resolution.\n",
        "  The current-voltage characteristics of superconductors are commonly\ndetermined by means of a standard 4-probe DC measurement technique. In coated\nconductors, however, one often encounters problems due to weak or no\nstabilization and in such case, when the risk of burning the tape is high, the\n4-probe approach can fail. In other cases such as in HTS cable structures,\nbesides vulnerability and high risk of damage, it may also be difficult to\nachieve a high critical current level with a power supply that is normally used\nto characterize single strands. We therefore investigated an alternative method\nto determine the DC I-V curve, based on measuring the AC transport loss as a\nfunction of frequency in overcritical current range. In the AC regime, both the\nhysteretic loss component and the joule dissipation are present for applied\ntransport currents exceeding the critical current Ic. In this work, AC loss\nmeasurements at different frequencies are related to different electric fields\nin DC and a curve similar to the DC I-V is obtained. From this curve, the power\nindex N and Ic values are estimated. AC regime operation can be more beneficial\nfor two reasons: firstly, because very high currents can be achieved easily by\nusing a transformer; secondly, because the effective joule heat dissipation\nwith overcritical AC currents is much lower than that with DC of the same\namplitudes. In the experiment, we used a 4 mm wide YBCO reference tape with 20\n{\\mu}m of Cu-stabilization. For the comparison with AC transport loss, the\nsample was measured with the standard 4-probe technique and remained stable\neven in overload condition. To support our study, numerical simulations,\nreproducing both transient and stationary operation conditions, were performed.\nA similar relation between the AC current transport loss and the I-V curves was\nalso observed in the simulations.\n",
        "  Phenotypic evolution implies sequential fixations of new genomic sequences.\nThe speed at which these mutations fixate depends, in part, on the relative\nfitness (selection coefficient) of the mutant vs. the ancestor. Using a simple\npopulation dynamics model we show that the relative fitness in dynamical\nenvironments is not equal to the fitness averaged over individual environments.\nInstead it includes a term that explicitly depends on the sequence of the\nenvironments. This term is geometric in nature and depends only on the oriented\narea enclosed by the trajectory taken by the system in the environment state\nspace. It is related to the well-studied geometric phases in classical and\nquantum physical systems. We discuss possible biological implications of these\nobservations, focusing on evolution of novel metabolic or stress-resistant\nfunctions.\n",
        "  It is well known that the collection of uniformizations of a closed Riemann\nsurface $S$ is partially ordered; the lowest ones are the Schottky\nunformizations, that is, tuples $(\\Omega,\\Gamma,P:\\Omega \\to S)$, where\n$\\Gamma$ is a Schottky group with region of discontinuity $\\Omega$ and\n$P:\\Omega \\to S$ is a regular holomorphic cover map with $\\Gamma$ as its deck\ngroup.\n  Let $\\tau:S \\to S$ be a conformal (respectively, anticonformal) automorphism\nof $S$ of finite order $n$, and let $(\\Omega,\\Gamma,P:\\Omega \\to S)$ be a\nSchottky uniformization of $S$. Assume that $\\tau$ lifts with respect to the\nprevious Schottky uniformization, that is, there exists a M\\\"obius\n(respectively, extended M\\\"obius) transformation $\\kappa$, keeping $\\Omega$\ninvariant, with $P \\circ \\kappa=\\tau \\circ P$. The Kleinian (respectively,\nextended Kleinian) group $K=< \\Gamma, \\kappa >$ contains $\\Gamma$ as a finite\nindex normal subgroup and $K/\\Gamma \\cong {\\mathbb Z}_{n}$. We provide a\nstructural picture of $K$ in terms of the Klein-Maskit's combination theorems\nand some basic groups. Some consequences are (i) the determination of the\nnumber of topologically different types of such groups (fixed $n$ and the rank\nof the Schottky normal subgroup) and (ii) for $n$ prime, the number of normal\nSchottky normal subgroups, up to conjugacy, that $K$ has.\n",
        "  We address the problem of performing semantic transformations on strings,\nwhich may represent a variety of data types (or their combination) such as a\ncolumn in a relational table, time, date, currency, etc. Unlike syntactic\ntransformations, which are based on regular expressions and which interpret a\nstring as a sequence of characters, semantic transformations additionally\nrequire exploiting the semantics of the data type represented by the string,\nwhich may be encoded as a database of relational tables. Manually performing\nsuch transformations on a large collection of strings is error prone and\ncumbersome, while programmatic solutions are beyond the skill-set of end-users.\nWe present a programming by example technology that allows end-users to\nautomate such repetitive tasks. We describe an expressive transformation\nlanguage for semantic manipulation that combines table lookup operations and\nsyntactic manipulations. We then present a synthesis algorithm that can learn\nall transformations in the language that are consistent with the user-provided\nset of input-output examples. We have implemented this technology as an add-in\nfor the Microsoft Excel Spreadsheet system and have evaluated it successfully\nover several benchmarks picked from various Excel help-forums.\n",
        "  A gap in a paper of Rubinstein-Scharlemann is explored: new examples are\nfound of closed orientable 3-manifolds with possibly multiple genus 2 Heegaard\nsplittings. Properties common to all the examples in the original paper are not\nuniversally shared by the new examples: some of the new examples have Hempel\ndistance 3, and it is not clear that a single stabilization always makes the\nmultiple splittings isotopic.\n",
        "  Large databases such as aflowlib.org provide valuable data sources for\ndiscovering material trends through machine learning. Although a REST API and\nquery language are available, there is a learning curve associated with the\nAFLUX language that acts as a barrier for new users. Additionally, the data is\nstored using non-standard serialization formats. Here we present a high-level\nAPI that allows immediate access to the aflowlib data using standard python\noperators and language features. It provides an easy way to integrate aflowlib\ndata with other python materials packages such as ase and quippy, and provides\nautomatic deserialization into numpy arrays and python objects. This package is\navailable via \"pip install aflow\".\n",
        "  Standard approaches in entity identification hard-code boundary detection and\ntype prediction into labels (e.g., John/B-PER Smith/I-PER) and then perform\nViterbi. This has two disadvantages: 1. the runtime complexity grows\nquadratically in the number of types, and 2. there is no natural segment-level\nrepresentation. In this paper, we propose a novel neural architecture that\naddresses these disadvantages. We frame the problem as multitasking, separating\nboundary detection and type prediction but optimizing them jointly. Despite its\nsimplicity, this architecture performs competitively with fully structured\nmodels such as BiLSTM-CRFs while scaling linearly in the number of types.\nFurthermore, by construction, the model induces type-disambiguating embeddings\nof predicted mentions.\n",
        "  In the framework of four-band model of superconductivity in iron arsenides\nproposed by Barzykin and Gor'kov we analyze the gap ratios on hole - like and\nelectron - like Fermi - surface cylinders. It is shown that experimentally\nobserved (ARPES) gap ratios can be obtained only within rather strict limits on\nthe values of pairing coupling constants. The difference of T_c values in 1111\nand 122 systems is reasonably explained by the relative values of partial\ndensities of states. The multiple bands electronic structure of these systems\nleads to a significant enhancement of effective pairing coupling constant\ndetermining T_c, so that high enough T_c values can be achieved even for the\ncase of rather small intraband and interband pairing interactions.\n",
        "  Motivated by recent experiments on liquid $^3$He reporting emergence of novel\nsuperfluid phases in globally anisotropic aerogels, our previous theory on\nsuperfluid $^3$He in globally anisotropic aerogels is extended to incorporate\neffects of an anisotropy of the quasiparticle scattering cross section on the\nstrong coupling (SC) contributions to the Ginzburg-Landau (GL) free energy on\nthe basis of the spin-fluctuation (paramagnon) approach to the SC contributions\ndeveloped by Brinkman et al. [Phys. Rev. A {\\bf 10}, 2386 (1974)]. In the\nglobally isotropic case, impurity effects on the SC correction destabilize the\nA-phase even at higher pressures of about 30 (bar) and make the B-phase the\nonly state in equilibrium, while SC contributions accompanied by a global\nstretched anisotropy to the GL quartic terms generally tend to broaden the\nstability region of the A-phase compared with that of the B-phase. In\nparticular, in contrast to the cases in bulk and in the isotropic aerogel, the\nSC corrections to the GL {\\it quadratic} terms are not negligible in the\nglobally anisotropic case but may change the sign of the apparent anisotropy\ndepending on the magnitude of the frequency cutoff of the normal paramagnon\npropagator. Based on this sign change of the apparent anisotropy, we discuss\ndifferent strange observations on superfluid $^3$He in porous media such as\ndisappearance of the polar superfluid phase at higher pressures seen in\nnematically-ordered aerogels and absence of the B-phase and of the A-phase with\nplanar ${\\hat {\\bf l}}$-vector in a stretched aerogel.\n",
        "  The standard Penna ageing model with sexual reproduction is enlarged by\nadding additional bit-strings for love: Marriage happens only if the male love\nstrings are sufficiently different from the female ones. We simulate at what\nlevel of required difference the population dies out.\n",
        "  Purpose: The development of a calibrationless parallel imaging method for\naccelerated simultaneous multi-slice (SMS) MRI based on Regularized Nonlinear\nInversion (NLINV), evaluated using Cartesian and radial FLASH. Theory and\nMethods: NLINV is a parallel imaging method that jointly estimates image\ncontent and coil sensitivities using a Newton-type method with regularization.\nHere, NLINV is extended to SMS-NLINV for reconstruction and separation of all\nsimultaneously acquired slices. The performance of the extended method is\nevaluated for different sampling schemes using phantom and in-vivo experiments\nbased on Cartesian and radial SMS-FLASH sequences. Results: The basic algorithm\nwas validated in Cartesian experiments by comparison with ESPIRiT. For\nCartesian and radial sampling, improved results are demonstrated compared to\nsingle-slice experiments, and it is further shown that sampling schemes using\ncomplementary samples outperform schemes with the same samples in each\npartition. Conclusion: The extension of the NLINV algorithm for SMS data was\nimplemented and successfully demonstrated in combination with a Cartesian and\nradial SMS-FLASH sequence.\n",
        "  Big Data architectures allow to flexibly store and process heterogeneous\ndata, from multiple sources, in their original format. The structure of those\ndata, commonly supplied by means of REST APIs, is continuously evolving. Thus\ndata analysts need to adapt their analytical processes after each API release.\nThis gets more challenging when performing an integrated or historical\nanalysis. To cope with such complexity, in this paper, we present the Big Data\nIntegration ontology, the core construct to govern the data integration process\nunder schema evolution by systematically annotating it with information\nregarding the schema of the sources. We present a query rewriting algorithm\nthat, using the annotated ontology, converts queries posed over the ontology to\nqueries over the sources. To cope with syntactic evolution in the sources, we\npresent an algorithm that semi-automatically adapts the ontology upon new\nreleases. This guarantees ontology-mediated queries to correctly retrieve data\nfrom the most recent schema version as well as correctness in historical\nqueries. A functional and performance evaluation on real-world APIs is\nperformed to validate our approach.\n",
        "  One important feature of the mammalian immune system is the highly specific\nbinding of antigens to antibodies. Antibodies generated in response to one\ninfection may also provide some level of cross immunity to other infections.\nOne model to describe this cross immunity is the notion of antigenic space,\nwhich assigns each antibody and each virus a point in $\\mathbb{R}^n$. Past\nstudies have suggested the dimensionality of antigenic space, $n$, may be\nsmall. In this study we show that data from hemagglutination assays suggest a\nhigh dimensional random walk (or self avoiding random random walk). The\ndiscrepancy between our result and prior studies is due to the fact that random\nwalks can appear low dimensional according to a variety of analyses. including\nprincipal component analysis (PCA) and multidimensional scaling (MDS).\n",
        "  Genetic studies have identified associations between gene mutations and clear\ncell renal cell carcinoma (ccRCC). Because the complete gene mutational\nlandscape cannot be characterized through biopsy and sequencing assays for each\npatient, non-invasive tools are needed to determine the mutation status for\ntumors. Radiogenomics may be an attractive alternative tool to identify disease\ngenomics by analyzing amounts of features extracted from medical images. Most\ncurrent radiogenomics predictive models are built based on a single classifier\nand trained through a single objective. However, since many classifiers are\navailable, selecting an optimal model is difficult. On the other hand, a single\nobjective may not be a good measure to guide model training. We proposed a new\nmulti-classifier multi-objective (MCMO) radiogenomics predictive model. To\nobtain more reliable prediction results, similarity-based sensitivity and\nspecificity were defined and considered as the two objective functions\nsimultaneously during training. To take advantage of different classifiers, the\nevidential reasoning (ER) approach was used for fusing the output of each\nclassifier. Additionally, a new similarity-based multi-objective optimization\nalgorithm (SMO) was developed for training the MCMO to predict ccRCC related\ngene mutations (VHL, PBRM1 and BAP1). Preliminary results revealed an\nassociation between some quantitative computed tomography (CT) features and\nunderlying mutations. Using the proposed MCMO model, we achieved a predictive\narea under the receiver operating characteristic curve (AUC) over 0.86 for VHL,\nPBRM1, and BAP1 genes with balanced sensitivity and specificity. Furthermore,\nMCMO outperformed all the individual classifiers and yielded more reliable\nresults than other optimization algorithms and commonly used fusion strategies.\n",
        "  Hypercycles are information integration systems which are thought to overcome\nthe information crisis of prebiotic evolution by ensuring the coexistence of\nseveral short templates. For imperfect template replication, we derive a simple\nexpression for the maximum number of distinct templates $n_m$ that can coexist\nin a hypercycle and show that it is a decreasing function of the length $L$ of\nthe templates. In the case of high replication accuracy we find that the\nproduct $n_m L$ tends to a constant value, limiting thus the information\ncontent of the hypercycle. Template coexistence is achieved either as a\nstationary equilibrium (stable fixed point) or a stable periodic orbit in which\nthe total concentration of functional templates is nonzero. For the hypercycle\nsystem studied here we find numerical evidence that the existence of an\nunstable fixed point is a necessary condition for the presence of periodic\norbits.\n",
        "  The iron chalcogenides FeSe and FeS are superconductors composed of\ntwo-dimensional sheets held together by van der Waals interactions, which makes\nthem prime candidates for the intercalation of various guest species. We review\nthe intercalation chemistry of FeSe and FeS superconductors and discuss their\nsynthesis, structure, and physical properties. Before we review the latest work\nin this area, we provide a brief background on the intercalation chemistry of\nother inorganic materials that exhibit enhanced superconducting properties upon\nintercalation, which include the transition metal dichalcogenides, fullerenes,\nand layered cobalt oxides. From past studies of these intercalated\nsuperconductors, we discuss the role of the intercalates in terms of charge\ndoping, structural distortions, and Fermi surface reconstruction. We also\nbriefly review the physical and chemical properties of the host\nmaterials---mackinawite-type FeS and $\\beta$-FeSe. The three types of\nintercalates for the iron chalcogenides can be placed in three categories: 1.)\nalkali and alkaline earth cations intercalated through the liquid ammonia\ntechnique; 2.) cations intercalated with organic amines such as\nethylenediamine; and 3.) layered hydroxides intercalated during hydrothermal\nconditions. A recurring theme in these studies is the role of the intercalated\nguest in electron doping the chalcogenide host and in enhancing the\ntwo-dimensionality of the electronic structure by spacing the FeSe layers\napart. We end this review discussing possible new avenues in the intercalation\nchemistry of transition metal monochalcogenides, and the promise of these\nmaterials as a unique set of new inorganic two-dimensional systems.\n",
        "  The diffuse extended outer regions of galaxies are hard to study because they\nare faint, with typical surface brightness of 1% of the dark night sky. We can\ntackle this problem by using resolved star tracers which remain visible at\nlarge distances from the galaxy centres. This article describes the use of\nPlanetary Nebulae as tracers and the calibration of their properties as\nindicators of the star formation history, mean age and metallicity of the\nparent stars in the Milky Way and Local Group galaxies . We then report on the\nresults from a deep, extended, planetary nebulae survey in a 0.5 sqdeg region\ncentred on the brightest cluster galaxy NGC 4486 (M87) in the Virgo cluster\ncore, carried out with SuprimeCam@Subaru and FLAMES-GIRAFFE@VLT. Two PN\npopulations are identified out to 150 kpc distance from the centre of M87. One\npopulation is associated with the M87 halo and the second one with the\nintracluster light in the Virgo cluster core. They have different line-of-sight\nvelocity and spatial distributions, as well as different planetary nebulae\nspecific frequencies and luminosity functions. The intracluster planetary\nnebulae in the surveyed region correspond to a luminosity of four times the\nluminosity of the Large Magellanic Cloud. The M87 halo planetary nebulae trace\nan older, more metal-rich, parent stellar population. A substructure detected\nin the projected phase-space of the line-of-sight velocity vs. major axis\ndistance for the M87 halo planetary nebulae provides evidence for the recent\naccretion event of a satellite galaxy with luminosity twice that of M33. The\nsatellite stars were tidally stripped about 1 Gyr ago, and reached apocenter at\na major axis distance of 60-90 kpc from the centre of M87. The M87 halo is\nstill growing significantly at the distances where the substructure is\ndetected.\n",
        "  Using a broadband, high spectral resolution survey toward Orion KL acquired\nwith Herschel/HIFI as part of the HEXOS key program, we derive the abundances\nof H$_2$O and HDO in the different spatial/velocity components associated with\nthis massive star-forming region: the Hot Core, Compact Ridge, and Plateau. A\ntotal of 20 transitions of H$_2$$^{18}$O, 14 of H$_2$$^{17}$O, 37 of\nHD$^{16}$O, 6 of HD$^{18}$O, and 6 of D$_2$O are used in the analysis, spanning\nfrom ground state transitions to over 1200 K in upper-state energy.\nLow-excitation lines are detected in multiple components, but the\nhighest-excitation lines ($E_u >$ 500 K) are well modeled as emitting from a\nsmall ($\\sim 2\"$) clump with a high abundance of H$_2$O ($\\chi = 6.5 \\times\n10^{-4}$ relative to H$_2$) and a HDO/H$_2$O ratio of 0.003. Using high spatial\nresolution ($1.5\" \\times 1.1\"$) images of two transitions of HDO measured by\nALMA as part of its science verification phase, we identify this component as\nlocated near, but not directly coincident with, known continuum sources in the\nHot Core region. Significant HDO/H$_2$O fractionation is also seen in the\nCompact Ridge and Plateau components. The outflowing gas, observed with both\nemission and absorption components, has a lower HDO/H$_2$O ratio than the\ncompact components in Orion KL, which we propose could be due to modification\nby gas-phase shock chemistry.\n",
        "  Evolutionary dynamics can be studied in well-mixed or structured populations.\nPopulation structure typically arises from the heterogeneous distribution of\nindividuals in physical space or on social networks. Here we introduce a new\ntype of space to evolutionary game dynamics: phenotype space. The population is\nwell-mixed in the sense that everyone is equally likely to interact with\neveryone else, but the behavioral strategies depend on distance in phenotype\nspace. Individuals might behave differently towards those who look similar or\ndissimilar. Individuals mutate to nearby phenotypes. We study the `phenotypic\nspace walk' of populations. We present analytic calculations that bring\ntogether ideas from coalescence theory and evolutionary game dynamics. As a\nparticular example, we investigate the evolution of cooperation in phenotype\nspace. We obtain a precise condition for natural selection to favor cooperators\nover defectors: for a one-dimensional phenotype space and large population size\nthe critical benefit-to-cost ratio is given by b/c=1+2/sqrt{3}. We derive the\nfundamental condition for any evolutionary game and explore higher dimensional\nphenotype spaces.\n",
        "  The upper critical field was measured upto 12 T for three BaFe2-xCoxAs2\nsingle crystals with estimated Co concentrations of x = 0.082, x = 0.117 and x\n= 0.143. HC2 versus temperature was measured from temperature dependent\nresistivity, for various applied magnetic fields, H || ab and H || c. The\n[dHC2/dT]T=Tc, normalized with the corresponding TC, decreases with increasing\nCo content, for both directions. The anisotropy {\\gamma} defined as HC2 || ab /\nHC2 || c shows a distinct increase with Co content, and its temperature\ndependence shows a peak close to the TC. Magneto transport measurements, in the\nspin density wave regions, showed significant negative MR for H || ab and\npositive MR of H || c in the x = 0.082 sample. The implications of these\nresults are discussed.\n",
        "  We present rest-frame ultraviolet and optical spectroscopy of the brightest\nlensed galaxy yet discovered, at redshift z = 2.4. This source reveals a\ncharacteristic, triple-peaked Lyman {\\alpha} profile which has been predicted\nby various theoretical works but to our knowledge has not been unambiguously\nobserved previously. The feature is well fit by a superposition of two\ncomponents: a double-peak profile emerging from substantial radiative transfer,\nand a narrow, central component resulting from directly escaping Lyman {\\alpha}\nphotons; but is poorly fit by either component alone. We demonstrate that the\nfeature is unlikely to contain contamination from nearby sources, and that the\ncentral peak is unaffected by radiative transfer effects apart from very slight\nabsorption. The feature is detected at signal-to-noise ratios exceeding 80 per\npixel at line center, and bears strong resemblance to synthetic profiles\npredicted by numerical models.\n",
        "  Most antigenically novel and evolutionarily successful strains of seasonal\ninfluenza A (H3N2) originate in East, South, and Southeast Asia. To understand\nthis pattern, we simulated the ecological and evolutionary dynamics of\ninfluenza in a host metapopulation representing the temperate north, tropics,\nand temperate south. Although seasonality and air traffic are frequently used\nto explain global migratory patterns of influenza, we find that other factors\nmay have a comparable or greater impact. Notably, a region's basic reproductive\nnumber ($R_0$) strongly affects the antigenic evolution of its viral population\nand the probability that its strains will spread and fix globally: a 17-28%\nhigher $R_0$ in one region can explain the observed patterns. Seasonality, in\ncontrast, increases the probability that a tropical (less seasonal) population\nwill export evolutionarily successful strains but alone does not predict that\nthese strains will be antigenically advanced. The relative sizes of different\nhost populations, their birth and death rates, and the region in which H3N2\nfirst appears affect influenza's phylogeography in different but relatively\nminor ways. These results suggest general principles that dictate the spatial\ndynamics of antigenically evolving pathogens and offer predictions for how\nchanges in human ecology might affect influenza evolution.\n",
        "  An-ever increasing number of social media websites, electronic newspapers and\nInternet forums allow visitors to leave comments for others to read and\ninteract. This exchange is not free from participants with malicious\nintentions, which do not contribute with the written conversation. Among\ndifferent communities users adopt strategies to handle such users. In this\npaper we present a comprehensive categorization of the trolling phenomena\nresource, inspired by politeness research and propose a model that jointly\npredicts four crucial aspects of trolling: intention, interpretation, intention\ndisclosure and response strategy. Finally, we present a new annotated dataset\ncontaining excerpts of conversations involving trolls and the interactions with\nother users that we hope will be a useful resource for the research community.\n",
        "  Multi-echo Chemical Shift Encoded methods for Fat-Water quantification are\ngrowing in clinical use due to their ability to estimate and correct some\nconfounding effects. State of the art CSE water-fat separation approaches rely\non a multi-peak fat spectrum with peak frequencies and relative amplitudes kept\nconstant over the entire MRI dataset. However, the latter approximation\nintroduces a systematic error in fat percentage quantification in patients\nwhere the differences in lipid chemical composition are significant, such as\nfor neuromuscular disorders, because of the spatial dependence of the peak\namplitudes. The present work aims to overcome this limitation by taking\nadvantage of an unsupervised clusterization-based approach offering a reliable\ncriterion to carry out a data-driven segmentation of the input MRI dataset into\nmultiple regions. The idea is to apply the clusterization for partitioning the\nmulti-echo MRI dataset into a finite number of clusters whose internal voxels\nexhibit similar distance metrics. For each cluster, the estimation of the fat\nspectral properties are evaluated with a self-calibration technique and finally\nthe fat-water percentages are computed via a non-linear fitting. The method is\ntested in ad-hoc and public datasets. The overall performance and results in\nterms of fitting accuracy, robustness and reproducibility are compared with\nother state-of-the-art CSE algorithms. This approach provides a more accurate\nand reproducible identification of chemical species, hence fat-water\nseparation, when compared with other calibrated and non-calibrated approaches.\n",
        "  The monarch butterfly annually migrates from central Mexico to southern\nCanada. During recent decades, its population has been reduced due to human\ninteraction with their habitat. We examine the effect of herbicide usage on the\nmonarch butterfly's population by creating a system of linear and non-linear\nordinary differential equations that describe the interaction between the\nmonarch's population and its environment at various stages of migration: spring\nmigration, summer loitering, and fall migration. The model has various stages\nthat are used to describe the dynamics of the monarch butterfly population over\nmultiple generations. In Stage 1, we propose a system of coupled ordinary\ndifferential equations that model the populations of the monarch butterflies\nand larvae during spring migration. In Stage 2, we propose a predator-prey\nmodel with age structure to model the population dynamics at the summer\nbreeding site. In Stages 3 and 4, we propose exponential decay functions to\nmodel the monarch butterfly's fall migration to central Mexico and their time\nat the overwintering site. The model is used to analyze the long-term behavior\nof the monarch butterflies through numerical analysis, given data available in\nthe research literature.\n",
        "  In this paper, we propose methods of handling attributive values of object\nclasses in object oriented database with fuzzy information and uncertainty\nbased on quantitatively semantics based hedge algebraic. In this approach we\nconsider to attributive values (as well as methods) object class is interval\nvalues and the interval values are converted into sub interval in [0, 1]\nrespectively. That its the fuzziness of the elements in the hedge algebra is\nalso sub interval in [0,1]. So, we present an algorithm allows the comparison\nof two sub interval [0,1] helping the requirements of the query data\n",
        "  We explore inflectional morphology as an example of the relationship of the\ndiscrete and the continuous in linguistics. The grammar requests a form of a\nlexeme by specifying a set of feature values, which corresponds to a corner M\nof a hypercube in feature value space. The morphology responds to that request\nby providing a morpheme, or a set of morphemes, whose vector sum is\ngeometrically closest to the corner M. In short, the chosen morpheme $\\mu$ is\nthe morpheme (or set of morphemes) that maximizes the inner product of $\\mu$\nand M.\n",
        "  We develop a novel technique to parse English sentences into Abstract Meaning\nRepresentation (AMR) using SEARN, a Learning to Search approach, by modeling\nthe concept and the relation learning in a unified framework. We evaluate our\nparser on multiple datasets from varied domains and show an absolute\nimprovement of 2% to 6% over the state-of-the-art. Additionally we show that\nusing the most frequent concept gives us a baseline that is stronger than the\nstate-of-the-art for concept prediction. We plan to release our parser for\npublic use.\n",
        "  We present the discovery of 3 quasar lenses in the Sloan Digital Sky Survey\n(SDSS), selected using two novel photometry-based selection techniques. The\nJ0941+0518 system, with two point sources separated by 5.46\" on either side of\na galaxy, has source and lens redshifts $z_s = 1.54$ and $z_l = 0.343$. The\nAO-assisted images of J2211+1929 show two point sources separated by 1.04\",\ncorresponding to the same quasar at $z_s = 1.07,$ besides the lens galaxy and\nEinstein ring. Images of J2257+2349 show two point sources separated by 1.67\"\non either side of an E/S0 galaxy. The extracted spectra show two images of the\nsame quasar at redshift $z_s = 2.10$. In total, the two selection techniques\nidentified 309 lens candidates, including 47 known lenses, and 6 previously\nruled out candidates. 55 of the remaining candidates were observed using NIRC2\nand ESI at Keck Observatory, EFOSC2 at the ESO-NTT (La Silla), and SAM and the\nGoodman spectrograph at SOAR. Of the candidates observed, 3 were confirmed as\nlenses, 36 were ruled out, and 16 remain inconclusive. Taking into account that\nwe recovered known lenses, this gives us a success rate of at least 50/309\n(16%). This initial campaign demonstrates the power of purely photometric\nselection techniques in finding lensed quasars. Developing and refining these\ntechniques is essential for efficient identification of these rare lenses in\nongoing and future photometric surveys.\n",
        "  We present a comprehensive study of how superconducting fluctuations in the\nnormal state contribute to the conductivity tensor in a thin (119 $\\AA$) film\nof NbN. It is shown how these fluctuations drive a sign change in the Hall\ncoefficient $R_\\mathrm{H}$ for low magnetic fields near the superconducting\ntransition. The scaling behaviours as a function of distance to the transition\n$\\epsilon=\\ln(T/T_\\mathrm{c})$ of the longitudinal ($\\sigma_\\mathrm{xx}$) and\ntransverse ($\\sigma_\\mathrm{xy}$) conductivity is found to be consistent with\nGaussian fluctuation theory. Moreover, excellent quantitative agreement between\ntheory and experiment is obtained without any adjustable parameters. Our\nexperimental results thus provide a case study of the conductivity tensor\noriginating from short-lived Cooper pairs.\n",
        "  Energy is a growing component of the operational cost for many \"big data\"\ndeployments, and hence has become increasingly important for practitioners of\nlarge-scale data analysis who require scale-out clusters or parallel DBMS\nappliances. Although a number of recent studies have investigated the energy\nefficiency of DBMSs, none of these studies have looked at the architectural\ndesign space of energy-efficient parallel DBMS clusters. There are many\nchallenges to increasing the energy efficiency of a DBMS cluster, including\ndealing with the inherent scaling inefficiency of parallel data processing, and\nchoosing the appropriate energy-efficient hardware. In this paper, we\nexperimentally examine and analyze a number of key parameters related to these\nchallenges for designing energy-efficient database clusters. We explore the\ncluster design space using empirical results and propose a model that considers\nthe key bottlenecks to energy efficiency in a parallel DBMS. This paper\nrepresents a key first step in designing energy-efficient database clusters,\nwhich is increasingly important given the trend toward parallel database\nappliances.\n",
        "  Context. In the Milky Way, most globular clusters are highly conspicuous\nobjects that were found centuries ago. However, a few dozen of them are faint,\nsparsely populated systems that were identified largely during the second half\nof the past century. One of the faintest is ESO 37-1 (E 3) and as such it\nremains poorly studied, with no spectroscopic observations published so far,\nalthough it was discovered in 1976.\n  Aims. We investigate the globular cluster E 3 in an attempt to better\nconstrain its fundamental parameters. Spectroscopy of stars in the field of E 3\nis shown here for the first time.\n  Methods. Deep, precise VI CCD photometry of E 3 down to V=26 mag is presented\nand analysed. Low-resolution, medium signal-to-noise ratio spectra of nine\ncandidate members are studied to derive radial velocity and metallicity. Proper\nmotions from the UCAC4 catalogue are used to explore the kinematics of the\nbright members of E 3.\n  Results. Isochrone fitting indicates that E 3 is probably very old, with an\nage of about 13 Gyr; its distance from the Sun is nearly 10 kpc. It is also\nsomewhat metal rich with [Fe/H]=-0.7. Regarding its kinematics, our tentative\nestimate for the proper motions is (-7.0+/-0.8, 3.5+/-0.3) mas/yr (or a\ntangential velocity of 382+/-79 km/s) and for the radial velocity is 45+/-5\nkm/s, in the solar rest frame.\n  Conclusions. E 3 is one of the most intriguing globular clusters in the\nGalaxy. Having an old age and being metal rich is clearly a peculiar\ncombination, only seen in a handful of objects like the far more conspicuous\nNGC 104 (47 Tucanae). In addition, its low luminosity and sparse population\nmake it a unique template for the study of the final evolutionary phases in the\nlife of a star cluster. Unfortunately, E 3 is among the most elusive and\nchallenging known globular clusters because field contamination severely\nhampers spectroscopic studies.\n",
        "  Radiation damping (RD) has been shown to affect T1 measurement in inversion\nrecovery experiments. In this work, we demonstrate that the extent of RD\ndepends upon the T1 of the sample. RD difference spectroscopy (RADDSY) is used\nto characterize the severity of RD, while gradient inversion recovery (GIR) is\nused for RD suppression in T1 measurements. At 9.4 T, for the radiation damping\ncharacteristic time (Trd) of 50 ms, these investigations show non-negligible RD\neffects for T1 values greater than Trd, with severe distortions for T1 longer\nthan about 150 ms, showing reasonable agreement with the predicted Trd. We also\nreport a discrepancy between published expressions for the characteristic RD\ntime.\n",
        "  The growth in variety and volume of OLTP (Online Transaction Processing)\napplications poses a challenge to OLTP systems to meet performance and cost\ndemands in the existing hardware landscape. These applications are highly\ninteractive (latency sensitive) and require update consistency. They target\ncommodity hardware for deployment and demand scalability in throughput with\nincreasing clients and data. Currently, OLTP systems used by these applications\nprovide trade-offs in performance and ease of development over a variety of\napplications. In order to bridge the gap between performance and ease of\ndevelopment, we propose an intuitive, high-level programming model which allows\nOLTP applications to be modeled as a cluster of application logic units. By\nextending transactions guaranteeing full ACID semantics to provide the proposed\nmodel, we maintain ease of application development. The model allows the\napplication developer to reason about program performance, and to influence it\nwithout the involvement of OLTP system designers (database designers) and/or\nDBAs. As a result, the database designer is free to focus on efficient running\nof programs to ensure optimal cluster resource utilization.\n",
        "  Recently advancements in deep learning allowed the development of end-to-end\ntrained goal-oriented dialog systems. Although these systems already achieve\ngood performance, some simplifications limit their usage in real-life\nscenarios.\n  In this work, we address two of these limitations: ignoring positional\ninformation and a fixed number of possible response candidates. We propose to\nuse positional encodings in the input to model the word order of the user\nutterances. Furthermore, by using a feedforward neural network, we are able to\ngenerate the output word by word and are no longer restricted to a fixed number\nof possible response candidates. Using the positional encoding, we were able to\nachieve better accuracies in the Dialog bAbI Tasks and using the feedforward\nneural network for generating the response, we were able to save computation\ntime and space consumption.\n",
        "  Graph similarity search algorithms usually leverage the structural properties\nof a database. Hence, these algorithms are effective only on some structural\nvariations of the data and are ineffective on other forms, which makes them\nhard to use. Ideally, one would like to design a data analytics algorithm that\nis structurally robust, i.e., it returns essentially the same accurate results\nover all possible structural variations of a dataset. We propose a novel\napproach to create a structurally robust similarity search algorithm over graph\ndatabases. We leverage the classic insight in the database literature that\nschematic variations are caused by having constraints in the database. We then\npresent RelSim algorithm which is provably structurally robust under these\nvariations. Our empirical studies show that our proposed algorithms are\nstructurally robust while being efficient and as effective as or more effective\nthan the state-of-the-art similarity search algorithms.\n",
        "  In a recent work \"Arc-presentation of links: Monotonic simplification\" Ivan\nDynnikov showed that each rectangular diagram of the unknot, composite link, or\nsplit link can be monotonically simplified into a trivial, composite, or split\ndiagram, respectively. The following natural question arises: Is it always\npossible to simplify monotonically a rectangular diagram of a satellite knot or\nlink into one where the satellite structure is seen? Here we give a negative\nanswer to that question.\n",
        "  Whether or not biodiversity dynamics tend toward stable equilibria remains an\nunsolved question in ecology and evolution with important implications for our\nunderstanding of diversity and its conservation. Phylo/population genetic\nmodels and macroecological theory represent two primary lenses through which we\nview biodiversity. While phylo/population genetics provide an averaged view of\nchanges in demography and diversity over timescales of generations to\ngeological epochs, macroecology provides an ahistorical description of\ncommonness and rarity across contemporary co-occurring species. Our goal is to\ncombine these two approaches to gain novel insights into the non-equilibrium\nnature of biodiversity. We help guide near future research with a call for\nbioinformatic advances and an outline of quantitative predictions made possible\nby our approach.\n",
        "  In this study, the concept of dual Lorentzian homotetic exponential motions\nin is discussed and their velocities, accelerations obtained. Also, some\ngeometric results between velocity and acceleration vectors of a point in a\nspatial motion are obtained. Finally, the theorems related to acceleration and\nacceleration centres is given.\n",
        "  The problem of optimizing distributed database includes: fragmentation and\npositioning data. Several different approaches and algorithms have been\nproposed to solve this problem. In this paper, we propose an algorithm that\nbuilds the initial equivalence relation based on the distance threshold. This\nthreshold is also based on knowledge- oriented clustering techniques for both\nof horizontal and vertical fragmentation. Similarity measures used in the\nalgorithms are the measures developed from the classical measures. Experimental\nresults carrying on the small data set match fragmented results based on the\nclassical algorithm. Execution time and data fragmentation significantly\nreduced while the complexity of our algorithm in the general case is stable.\n",
        "  This paper presents Aicyber's system for NLPCC 2017 shared task 2. It is\nformed by a voting of three deep learning based system trained on\ncharacter-enhanced word vectors and a well known bag-of-word model.\n",
        "  We explore the use of the recently proposed \"total nuclear variation\" (TNV)\nas a regularizer for reconstructing multi-channel, spectral CT images. This\nconvex penalty is a natural extension of the total variation (TV) to\nvector-valued images and has the advantage of encouraging common edge locations\nand a shared gradient direction among image channels. We show how it can be\nincorporated into a general, data-constrained reconstruction framework and\nderive update equations based on the first-order, primal-dual algorithm of\nChambolle and Pock. Early simulation studies based on the numerical XCAT\nphantom indicate that the inter-channel coupling introduced by the TNV leads to\nbetter preservation of image features at high levels of regularization,\ncompared to independent, channel-by-channel TV reconstructions.\n",
        "  The famous \"two-fold cost of sex\" is really the cost of anisogamy -- why\nshould females mate with males who do not contribute resources to offspring,\nrather than isogamous partners who contribute equally? In typical anisogamous\npopulations, a single very fit male can have an enormous number of offspring,\nfar larger than is possible for any female or isogamous individual. If the\nsexual selection on males aligns with the natural selection on females,\nanisogamy thus allows much more rapid adaptation via super-successful males. We\nshow via simulations that this effect can be sufficient to overcome the\ntwo-fold cost and maintain anisogamy against isogamy in populations adapting to\nenvironmental change. The key quantity is the variance in male fitness -- if\nthis exceeds what is possible in an isogamous population, anisogamous\npopulations can win out in direct competition by adapting faster.\n",
        "  We introduce a neural method for transfer learning between two (source and\ntarget) classification tasks or aspects over the same domain. Rather than\ntraining on target labels, we use a few keywords pertaining to source and\ntarget aspects indicating sentence relevance instead of document class labels.\nDocuments are encoded by learning to embed and softly select relevant sentences\nin an aspect-dependent manner. A shared classifier is trained on the source\nencoded documents and labels, and applied to target encoded documents. We\nensure transfer through aspect-adversarial training so that encoded documents\nare, as sets, aspect-invariant. Experimental results demonstrate that our\napproach outperforms different baselines and model variants on two datasets,\nyielding an improvement of 27% on a pathology dataset and 5% on a review\ndataset.\n",
        "  In the literature of the study of knot group epimorphisms, the existence of\nan epimorphism between two given knot groups is mostly (if not always) shown by\ngiving an epimorphism which preserves meridians. A natural question arises: is\nthere an epimorphism preserving meridians whenever a knot group is a\nhomomorphic image of another? We answer in the negative by presenting\ninfinitely many pairs of prime knot groups (G,G') such that G' is a homomorphic\nimage of G but no epimorphism of G onto G' preserves meridians.\n",
        "  Adaptive Radiation Therapy (ART) was developed based on Image-guided\nRadiation Therapy (IGRT) and it is the trend of photon radiation therapy. To\nget a better use of Cone Beam CT (CBCT) images for ART, the CBCT system model\nwas established based on Monte Carlo program and validated against the\nmeasurement. The BEAMnrc program was adopted to the KV x-ray tube. Both\nIOURCE-13 and ISOURCE-24 were chosen to simulate the path of beam particles.\nThe measured Percentage Depth Dose (PDD) and lateral dose profiles under 1cm\nwater were compared with the dose calculated by DOSXYZnrc program. The\ncalculated PDD was better than 1% within the depth of 10cm. More than 85%\npoints of calculated lateral dose profiles was within 2%. The correct CBCT\nsystem model helps to improve CBCT image quality for dose verification in ART\nand assess the CBCT image concomitant dose risk.\n",
        "  This paper is an extended version of a report from a student-developed study\nto compare Microsoft SQL Server and PostgreSQL, two widely-used\nenterprise-class relational database management systems (RDBMSs). The study\nfollowed an introductory undergraduate course in relational systems and was\ndesigned to help gain practical understanding of specific DBMSs. During this\nstudy, we implemented three non-trivial schemas in each system, identified 26\ncommon database design, development, and administration activities while\nimplementing the schemas, and compared the support each system offers to carry\nout the identified activities. Where relevant, we also compared each system\nagainst the SQL standard.\n  In this report, we present a summary of the similarities and differences we\nfound between the two systems, and we provide a quantitative measure ranking\nboth systems' implementations of the 26 activities. We also briefly discuss the\n\"technical suitability\" of PostgreSQL to enterprise applications. Although this\nreport is not comprehensive and is too general to comment on the suitability of\neither system to a specific enterprise application, it can nevertheless provide\nan initial set of considerations and criteria to choose a system for most\nenterprise applications.\n",
        "  We introduce a new measure of distance between languages based on word\nembedding, called word embedding language divergence (WELD). WELD is defined as\ndivergence between unified similarity distribution of words between languages.\nUsing such a measure, we perform language comparison for fifty natural\nlanguages and twelve genetic languages. Our natural language dataset is a\ncollection of sentence-aligned parallel corpora from bible translations for\nfifty languages spanning a variety of language families. Although we use\nparallel corpora, which guarantees having the same content in all languages,\ninterestingly in many cases languages within the same family cluster together.\nIn addition to natural languages, we perform language comparison for the coding\nregions in the genomes of 12 different organisms (4 plants, 6 animals, and two\nhuman subjects). Our result confirms a significant high-level difference in the\ngenetic language model of humans/animals versus plants. The proposed method is\na step toward defining a quantitative measure of similarity between languages,\nwith applications in languages classification, genre identification, dialect\nidentification, and evaluation of translations.\n",
        "  Data mining has been widely recognized as a powerful tool to explore added\nvalue from large-scale databases. Finding frequent item sets in databases is a\ncrucial in data mining process of extracting association rules. Many algorithms\nwere developed to find the frequent item sets. This paper presents a summary\nand a comparative study of the available FP-growth algorithm variations\nproduced for mining frequent item sets showing their capabilities and\nefficiency in terms of time and memory consumption on association rule mining\nby taking application of specific information into account. It proposes pattern\ngrowth mining paradigm based FP-tree growth algorithm, which employs a tree\nstructure to compress the database. The performance study shows that the anti-\nFP-growth method is efficient and scalable for mining both long and short\nfrequent patterns and is about an order of magnitude faster than the Apriority\nalgorithm and also faster than some recently reported new frequent-pattern\nmining.\n",
        "  In intensity-modulated radiation therapy, optimal intensity distributions of\nincoming beams are decomposed into linear combinations of leaf openings of a\nmultileaf collimator (segments). In order to avoid inefficient dose delivery,\nthe decomposition should satisfy a number of dosimetric constraints due to\nsuboptimal dose characteristics of small segments. However, exact decomposition\nwith dosimetric constraints is only in limited cases possible. The present work\nintroduces new heuristic segmentation algorithms for the following optimization\nproblem: Find a segmentation of an approximated matrix using only allowed\nfields and minimize the approximation error. Finally, the decomposition\nalgorithms were implemented into an optimization programme in order to examine\nthe assumptions of the algorithms for a clinical example. As a result,\nidentical dose distributions with much fewer segments and a significantly\nsmaller number of monitor units could be achieved using dosimetric constraints.\nConsequently, the dose delivery is more efficient and less time consuming.\n",
        "  We consider quantum oscillation experiments in\n$\\mathrm{YBa_{2}Cu_{3}O_{6+\\delta}}$ from the perspective of an incommensurate\nFermi surface reconstruction using an exact transfer matrix method and the\nPichard-Landauer formula for the conductivity. The specific density wave order\nconsidered is a period-8 $d$-density wave in which the current density is\nunidirectionally modulated. The current modulation is also naturally\naccompanied by a period-4 site charge modulation in the same direction, which\nis consistent with recent magnetic resonance measurements. In principle Landau\ntheory also allows for a period-4 bond charge modulation, which is not\ndiscussed, but should be simple to incorporate in the future. This scenario\nleads to a natural, but not a unique, explanation of why only oscillations from\na single electron pocket is observed, and a hole pocket of roughly twice the\nfrequency as dictated by two-fold commensurate order, and the corresponding\nLuttinger sum rule, is not observed. However, it is possible that even higher\nmagnetic fields will reveal a hole pocket of half the frequency of the electron\npocket or smaller. This may be at the borderline of achievable high field\nmeasurements because at least a few complete oscillations have to be clearly\nresolved.\n",
        "  OBJECTIVES: The aim of this paper is to introduce the principles of\ncomputer-assisted access to the kidney. The system provides the surgeon with a\npre-operative 3D planning on computed tomography (CT) images. After a rigid\nregistration with space-localized ultrasound (US) data, preoperative planning\ncan be transferred to the intra-operative conditions and an intuitive\nman-machine interface allows the user to perform a puncture. MATERIAL AND\nMETHODS: Both CT and US images of informed normal volunteer were obtained to\nperform calculation on the accuracy of registration and punctures were carried\nout on a kidney phantom to measure the precision of the whole of the system.\nRESULTS: We carried out millimetric registrations on real data and guidance\nexperiments on a kidney phantom showed encouraging results of 4.7 mm between\nplanned and reached targets. We noticed that the most significant error was\nrelated to the needle deflection during the puncture. CONCLUSION: Preliminary\nresults are encouraging. Further work will be undertaken to improve efficiency\nand accuracy, and to take breathing into account.\n",
        "  In this work we investigate the effect of each different heat treatment stage\nin the fabrication of Bi2Sr2CaCu2O8+d superconducting wires on intra-grain and\ninter-grain superconducting properties. We measure magnetic critical\ntemperature Tc values and transport critical current density Jc at temperatures\nfrom 4 K to 40 K and in fields up to 7 T. From an analysis of the temperature\ndependence of the self-field critical current density Jc(T) that takes into\naccount weak link behavior and proximity effect, we study the grain boundaries\n(GB) transparency to supercurrents and we establish a relationship between GB\noxygenation in the different steps of the fabrication process and the GB\ntransparency to supercurrents. We find that grain boundary oxygenation starts\nin the first crystallization stage, but it becomes complete in the plateau at\n836 {\\deg}C and in slow cooling stages, and is further enhanced in the\nprolonged post annealing step. Such oxygenation makes GBs more conducting, thus\nimproving the inter-grain Jc value and temperature dependence. On the other\nhand, from the inspection of the Tc values in the framework of the phase\ndiagram dome, we find that grains are oxygenated already in the crystallization\nstep up to the optimal doping, while successive slow cooling and post annealing\ntreatments further enhance the degree of overdoping, especially if carried out\nin oxygen atmosphere rather than in air.\n",
        "  We demonstrate a laparoscopic applicator probe and a method thereof for\nreal-time en-face topographic mapping of near-surface heterogeneity for\npotential use in intraoperative margin assessment during minimally invasive\noncological procedures. The probe fits in a 12mm port and houses at its maximum\n128 copper-coated 750um fibers that form radially alternating illumination (70\nfibers) and detection (58 fibers) channels. By simultaneously illuminating the\n70 source channels of the probe that is in contact with a scattering medium and\nconcurrently measuring the light diffusely propagated to the 58 detector\nchannels, the presence of near-surface optical heterogeneities can be resolved\nin an en-face 9.5mm field-of-view in real-time. Visualization of a subsurface\nmargin of strong attenuation contrast at a depth up to 3mm is demonstrated at\none wavelength at a frame rate of 1.25Hz.\n",
        "  The SAGE-LMC, SAGE-SMC and HERITAGE surveys have mapped the Magellanic Clouds\nin the infrared using the Spitzer and Herschel Space Telescopes. Over 8.5\nmillion point sources were detected and catalogued in the LMC alone. Staring\nmode observations using the InfraRed Spectrograph (IRS) on board Spitzer have\nbeen obtained for 1,000 positions in the LMC and ~250 in the SMC. From the\ninfrared spectroscopy we have identified the nature of the sources for which\nspectroscopy is available. These IRS staring mode targets represent an\nimportant contribution to the SED of these dwarf galaxies. Here we report on\nour latest results.\n",
        "  In this work, we generalize the theory of localized surface plasmons to the\ncase of high-Tc cuprate superconductors, spatially confined in the form of\nsmall spherical particles. At variance from ordinary metals, cuprate\nsuperconductors are characterized by a low-energy bulk excitation known as the\nJosephson plasma wave (JPW), arising from interlayer tunneling of the\ncondensate along the c-axis. The effect of the JPW is revealed in a\ncharacteristic spectrum of surface excitations, which we call Josephson surface\nplasmons. Our results, which apply to any material with a strongly anisotropic\nelectromagnetic response, are worked out in detail for the case of multilayered\nsuperconductors supporting both low-frequency (acoustic) and transverse-optical\nJPW. Spatial confinement of the Josephson plasma waves may represent a new\ndegree of freedom to engineer their frequencies and to explore the link between\ninterlayer tunnelling and high-Tc superconductivity.\n",
        "  Since the late `60s, various genome evolutionary models have been proposed to\npredict the evolution of a DNA sequence as the generations pass. Most of these\nmodels are based on nucleotides evolution, so they use a mutation matrix of\nsize 4x4. They encompass for instance the well-known models of Jukes and\nCantor, Kimura, and Tamura. By essence, all of these models relate the\nevolution of DNA sequences to the computation of the successive powers of a\nmutation matrix. To make this computation possible, particular forms for the\nmutation matrix are assumed, which are not compatible with mutation rates that\nhave been recently obtained experimentally on gene ura3 of the Yeast\nSaccharomyces cerevisiae. Using this experimental study, authors of this paper\nhave deduced a simple mutation matrice, compute the future evolution of the\nrate purine/pyrimidine for ura3, investigate the particular behavior of\ncytosines and thymines compared to purines, and simulate the evolution of each\nnucleotide.\n",
        "  We present an approach to combining distributional semantic representations\ninduced from text corpora with manually constructed lexical-semantic networks.\nWhile both kinds of semantic resources are available with high lexical\ncoverage, our aligned resource combines the domain specificity and availability\nof contextual information from distributional models with the conciseness and\nhigh quality of manually crafted lexical networks. We start with a\ndistributional representation of induced senses of vocabulary terms, which are\naccompanied with rich context information given by related lexical items. We\nthen automatically disambiguate such representations to obtain a full-fledged\nproto-conceptualization, i.e. a typed graph of induced word senses. In a final\nstep, this proto-conceptualization is aligned to a lexical ontology, resulting\nin a hybrid aligned resource. Moreover, unmapped induced senses are associated\nwith a semantic type in order to connect them to the core resource. Manual\nevaluations against ground-truth judgments for different stages of our method\nas well as an extrinsic evaluation on a knowledge-based Word Sense\nDisambiguation benchmark all indicate the high quality of the new hybrid\nresource. Additionally, we show the benefits of enriching top-down lexical\nknowledge resources with bottom-up distributional information from text for\naddressing high-end knowledge acquisition tasks such as cleaning hypernym\ngraphs and learning taxonomies from scratch.\n",
        "  We consider finite population size effects for Crow-Kimura and Eigen\nquasispecies models with single peak fitness landscape. We formulate accurately\nthe iteration procedure for the finite population models, then derive\nHamilton-Jacobi equation (HJE) to describe the dynamic of the probability\ndistribution. The steady state solution of HJE gives the variance of the mean\nfitness. Our results are useful for understanding population sizes of virus in\nwhich the infinite population models can give reliable results for the\nbiological evolution problems.\n",
        "  An increasing sequence of integers is said to be universal for knots if every\nknot has a reduced regular projection on the sphere such that the number of\nedges of each complementary face of the projection comes from the given\nsequence. Adams, Shinjo, and Tanaka have, in a work, shown that (2,4,5) and\n(3,4,n) (where n is a positive integer greater than 4), among others, are\nuniversal. In a forthcoming paper, Adams introduces the notion of a\nmulti-crossing projection of a knot. An n-crossing projection} is a projection\nof a knot in which each crossing has n strands, rather than 2 strands as in a\nregular projection. We then extend the notion of universality to such knots.\nThese results allow us to prove that (1,2,3,4) is a universal sequence for both\nn-crossing knot projections, for all n>2. Adams further proves that all knots\nhave an n-crossing projection for all positive n. Another proof of this fact is\nincluded in this paper. This is achieved by constructing n-crossing template\nknots, which enable us to construct multi-crossing projections with crossings\nof any multiplicity.\n",
        "  Cilibrasi and Vitanyi have demonstrated that it is possible to extract the\nmeaning of words from the world-wide web. To achieve this, they rely on the\nnumber of webpages that are found through a Google search containing a given\nword and they associate the page count to the probability that the word appears\non a webpage. Thus, conditional probabilities allow them to correlate one word\nwith another word's meaning. Furthermore, they have developed a similarity\ndistance function that gauges how closely related a pair of words is. We\npresent a specific counterexample to the triangle inequality for this\nsimilarity distance function.\n",
        "  Neural models with minimal feature engineering have achieved competitive\nperformance against traditional methods for the task of Chinese word\nsegmentation. However, both training and working procedures of the current\nneural models are computationally inefficient. This paper presents a greedy\nneural word segmenter with balanced word and character embedding inputs to\nalleviate the existing drawbacks. Our segmenter is truly end-to-end, capable of\nperforming segmentation much faster and even more accurate than\nstate-of-the-art neural models on Chinese benchmark datasets.\n",
        "  In this article, we reformulate the cobordism map of embedded contact\nhomology, which is induced by exact symplectic cobordism and defined as direct\nlimit of homomorphisms called filtered ECH cobordism map. The filtered ECH\ncobordism map is defined by counting embedded holomorphic curves with zero ECH\nindex and we prove that it is independent on almost complex structure by\nSeiberg Witten theory. Moreover, our definition in fact is equivalent to the\nexisting definition.\n",
        "  In aspect-based sentiment analysis, most existing methods either focus on\naspect/opinion terms extraction or aspect terms categorization. However, each\ntask by itself only provides partial information to end users. To generate more\ndetailed and structured opinion analysis, we propose a finer-grained problem,\nwhich we call category-specific aspect and opinion terms extraction. This\nproblem involves the identification of aspect and opinion terms within each\nsentence, as well as the categorization of the identified terms. To this end,\nwe propose an end-to-end multi-task attention model, where each task\ncorresponds to aspect/opinion terms extraction for a specific category. Our\nmodel benefits from exploring the commonalities and relationships among\ndifferent tasks to address the data sparsity issue. We demonstrate its\nstate-of-the-art performance on three benchmark datasets.\n",
        "  Purpose\n  The proposed reconstruction framework addresses the reconstruction accuracy,\nnoise propagation and computation time for Magnetic Resonance Fingerprinting\n(MRF).\n  Methods\n  Based on a singular value decomposition (SVD) of the signal evolution, MRF is\nformulated as a low rank inverse problem in which one image is reconstructed\nfor each singular value under consideration. This low rank approximation of the\nsignal evolution reduces the computational burden by reducing the number of\nFourier transformations. Also, the low rank approximation improves the\nconditioning of the problem, which is further improved by extending the low\nrank inverse problem to an augmented Lagrangian that is solved by the\nalternating direction method of multipliers (ADMM). The root mean square error\nand the noise propagation are analyzed in simulations. For verification, in\nvivo examples are provided.\n  Results\n  The proposed low rank ADMM approach shows a reduced root mean square error\ncompared to the original fingerprinting reconstruction, to a low rank\napproximation alone and to an ADMM approach without a low rank approximation.\nIncorporating sensitivity encoding allows for further artifact reduction.\n  Conclusion\n  The proposed reconstruction provides robust convergence, reduced\ncomputational burden and improved image quality compared to other MRF\nreconstruction approaches evaluated in this study.\n",
        "  The technique of building of networks of hierarchies of terms based on the\nanalysis of chosen text corpora is offered. The technique is based on the\nmethodology of horizontal visibility graphs. Constructed and investigated\nlanguage network, formed on the basis of electronic preprints arXiv on topics\nof information retrieval.\n",
        "  We present a generative model to map natural language questions into SQL\nqueries. Existing neural network based approaches typically generate a SQL\nquery word-by-word, however, a large portion of the generated results are\nincorrect or not executable due to the mismatch between question words and\ntable contents. Our approach addresses this problem by considering the\nstructure of table and the syntax of SQL language. The quality of the generated\nSQL query is significantly improved through (1) learning to replicate content\nfrom column names, cells or SQL keywords; and (2) improving the generation of\nWHERE clause by leveraging the column-cell relation. Experiments are conducted\non WikiSQL, a recently released dataset with the largest question-SQL pairs.\nOur approach significantly improves the state-of-the-art execution accuracy\nfrom 69.0% to 74.4%.\n",
        "  A lot of research has been carried out to find out the influence of low\nfrequency noise on human behavior. Based on currently available results, some\ncountries have been developing National Standard to limit the low frequency\nnoise to protect people's health and working performance. However, after\ninvestigating 28 National Standards about noise limitation of working machine\nand other noise emission standards of China, it is found that the stipulation\nof low frequency noise covers limited aspects. Most of the limitation used\nA-weight sound level as the evaluation index, which only presents the loudness\nperception of sound, not its influence. In this paper, two connected topics are\npresented. One is to investigate the Chinese National Standard and\nInternational Standard, in order to summarize all the noise limitation on low\nfrequency noise. The second topic is the application of online psychoacoustic\ntest to evaluate the influence of low frequency noise on human performance,\nwhich can enlarge the subjective testing sample size in order to facilitate the\nestablishment of related National Standard. The online Hass effect experiment\nis repeated to verify the accuracy of online psychoacoustic test. The\nvolunteers are asked to conduct several sessions of the colour identification\nproblems under different kind of controlled noise. The accuracy and finishing\ntime are taken into consideration to evaluate their performance under different\nkinds of noise. The results show that low frequency noise does affect humans'\nworking performance and the existing Chinese Noise limitation Standards are\ninadequate in low frequency noise limitation.\n",
        "  The superconducting properties of Lu2Fe3Si5 with Tc = 6.1 K have been\ninvestigated using low-temperature transverse-field muon spin rotation\n(${\\mu}$SR) and specific heat measurements. The magnetic penetration depth at\nzero temperature is 353(1) nm. However, the temperature dependence of the\nmagnetic penetration depth is consistent with a two gap s+s-wave model.\nLow-temperature specific heat measurements on the same sample also show\nevidence of two distinct superconducting gaps.\n",
        "  We re-analyse photometric near-infrared data in order to investigate why it\nis so hard to get a consensus for the shape and density law of the bulge, as\nseen from the literature. To solve the problem we use the Besancon Galaxy Model\nto provide a scheme for parameter fitting of the structural characteristics of\nthe bulge region. The fitting process allows the determination of the global\nshape of the bulge main structure.\n  We explore various parameters and shape for the bulge/bar structure based on\nFerrer's ellipsoids and fit the shape of the inner disc in the same process.\nThe results show that the main structure is a quite standard triaxial boxy\nbar/bulge with an orientation of about 13 degree with respect to the Sun-centre\ndirection. But the fit is greatly improved when we add a second structure,\nwhich is a longer and thicker ellipsoid. We emphasize that our first ellipsoid\nrepresent the main boxy bar of the Galaxy, and that the thick bulge could be\neither a classical bulge slightly flattened by the effect of the bar potential,\nor a inner thick disc counterpart. We show that the double clump seen at\nintermediate latitudes can be reproduced by adding a slight flare to the bar.\nIn order to better characterize the populations, we further simulate several\nfields which have been surveyed in spectroscopy and for which metallicity\ndistribution function (MDF) are available. The model is in good agreement with\nthese MDF along the minor axis if we assume that the main bar has a mean solar\nmetallicity and the second thicker population has a lower metallicity. It then\ncreates naturally a vertical metallicity gradient by the mixing of the two\npoulations. (abridged)\n",
        "  Various fossil evidences show that hunting is one of major means of ancient\nhuman to get foods. But the running speed of our ancestors was much slower than\nquadruped animals, and they did not have sharp claws and canines. So, they have\nto rely heavily on stone and wooden tools when they hunting or fighting against\nother predators, which are very different from the hunting behaviors of other\ncarnivores. There are mainly two types of attack and defense action during\nhuman hunting, front or side hit with a wooden stick in hands and stone or\nwooden spears throwing, and throwing had play an important role in human\nevolution process. But there is almost no work to study the why only human\nchose to hunting by this way. Here we suppose that ancient human chose two-arm\nbrachiation as main arboreal locomotion mode because of their suitable body\nweight. Human body traits include slim body, parallel arranged scapulas, long\nthumb and powerful grip ability are all evolved as results of two arm\nbrachiation. The relevant adaptive evolution of the shoulder bone structure\nmake human arms with a large range of movement and the long thumb makes human\nactivities to be more accurate and controllable. These are two important body\nstructure advantages of ancient human which makes them could get from arboreal\nlife into a whole new hunting and fighting stage.\n",
        "  Hypothetical topologically nontrivial superconducting state of\ntwo-dimensional electron system is discussed in connection with the problem of\npairing with large center-of-mass pair momentum under predominant repulsive\nscreened Coulomb interaction. Direct numerical solution of the self-consistency\nequation exhibits two nearly degenerate order parameters which can be formally\nreferred to $d^{}_{x^2-y^2}$ and $d^{}_{xy}$ orbital symmetry. Spontaneous\nbreaking of the time-reversal symmetry can mix these states and form fully\ngapped chiral $d+id$ superconducting state.\n",
        "  We propose a series of simulations about the potential use of Boron isotopes\nto trigger neutron-free (aneutronic) nuclear reactions in cancer cells through\nthe interaction with an incoming energetic proton beam, thus resulting in the\nemission of characteristic prompt gamma radiation (429 keV, 718 keV and 1435\nkeV). Furthermore assuming that the Boron isotopes are absorbed in cancer\ncells, the three alpha-particles produced in each p-11B aneutronic nuclear\nfusion reactions can potentially result in the enhancement of the biological\ndose absorbed in the tumor region since these multi-MeV alpha-particles are\nstopped inside the single cancer cell, thus allowing to spare the surrounding\ntissues. Although a similar approach based on the use of 11B nuclei has been\nproposed in [1], our work demonstrate, using Monte Carlo simulations, the\ncrucial importance of the use of 10B nuclei (in a solution containing also 11B)\nfor the generation of prompt gamma-rays, which can be applied to medical\nimaging. In fact, we demonstrate that the use of 10B nuclei can enhance the\nintensity of the 718 keV gamma-ray peak more than 30 times compared to the\nsolution containing only 11B nuclei. A detailed explanation of the origin of\nthe different prompt gamma-rays, as well as of their application as real-time\ndiagnostics during a potential cancer treatment, is here discussed.\n",
        "  Chiral superconductivity is a striking quantum phenomenon in which an\nunconventional superconductor spontaneously develops an angular momentum and\nlowers its free energy by eliminating nodes in the gap. It is a topologically\nnon-trivial state and, as such, exhibits distinctive topological modes at\nsurfaces and defects. In this paper we discuss the current theory and\nexperimental results on chiral superconductors, focusing on two of the\nbest-studied systems, Sr2RuO4, which is thought to be a chiral triplet p-wave\nsuperconductor, and UPt3, which has two low-temperature superconducting phases\n(in zero magnetic field), the lower of which is believed to be chiral triplet\nf-wave. Other systems that may exhibit chiral superconductivity are also\ndiscussed. Key signatures of chiral superconductivity are surface currents and\nchiral Majorana modes, Majorana states in vortex cores, and the possibility of\nhalf-flux quantum vortices in the case of triplet pairing. Experimental\nevidence for chiral superconductivity from muSR, NMR, strain, polar Kerr effect\nand Josephson tunneling experiments are discussed.\n",
        "  The use of fossil evidence to calibrate divergence time estimation has a long\nhistory. More recently Bayesian MCMC has become the dominant method of\ndivergence time estimation and fossil evidence has been re-interpreted as the\nspecification of prior distributions on the divergence times of calibration\nnodes. These so-called \"soft calibrations\" have become widely used but the\nstatistical properties of calibrated tree priors in a Bayesian setting has not\nbeen carefully investigated. Here we clarify that calibration densities, such\nas those defined in BEAST 1.5, do not represent the marginal prior distribution\nof the calibration node. We illustrate this with a number of analytical results\non small trees. We also describe an alternative construction for a calibrated\nYule prior on trees that allows direct specification of the marginal prior\ndistribution of the calibrated divergence time, with or without the restriction\nof monophyly. This method requires the computation of the Yule prior\nconditional on the height of the divergence being calibrated. Unfortunately, a\npractical solution for multiple calibrations remains elusive. Our results\nsuggest that direct estimation of the prior induced by specifying multiple\ncalibration densities should be a prerequisite of any divergence time dating\nanalysis.\n",
        "  The characteristics of an x-ray spectrum can greatly influence imaging and\nrelated tasks. In practice, due to the pile-up effect of the detector, it's\ndifficult to directly measure the spectrum of a CT scanner using an energy\nresolved detector. An alternative solution is to estimate the spectrum using\ntransmission measurements with a step phantom or other CT phantom. In this\nwork, we present a new spectrum estimation method based on indirect\ntransmission measurement and model spectra mixture approach. The estimated\nx-ray spectrum was expressed as weighted summation of a set of model spectra,\nwhich can significantly reduce the degrees of freedom (DOF) of the spectrum\nestimation problem. Next, an estimated projection can be calculated with the\nassumed spectrum. By iteratively updating the unknown weights, we minimized the\ndifference between the estimated projection data and the raw projection data.\nThe final spectrum was calculated with these calibrated weights and the model\nspectra. Both simulation and experimental data were used to evaluate the\nproposed method. In the simulation study, the estimated spectra were compared\nto the raw spectra which were used to generate the raw projection data. For the\nexperimental study, the ground truth measurement of the raw x-ray spectrum was\nnot available. Therefore, the estimated spectrum was compared against spectra\ngenerated using the SpekCalc software with tube configurations provided by the\nscanner manufacturer. The results show the proposed method has potential to\naccurately estimate x-ray spectra using the raw projection data. The difference\nbetween the mean energy of the raw spectra and the mean energy of the estimated\nspectra were smaller than 0.5 keV for both simulation and experimental data.\nFurther tests show the method was robust with respect to the model spectra\ngenerator.\n",
        "  We conduct simulations of turbulent mixing in the presence of a magnetic\nfield, grown by the small-scale dynamo. We show that the scalar gradient field,\n$\\nabla C$, which must be large for diffusion to operate, is strongly biased\nperpendicular to the magnetic field, ${\\mathbf B}$. This is true both early-on,\nwhen the magnetic field is negligible, and at late times, when the field is\nstrong enough to back react on the flow. This occurs because $\\nabla C$\nincreases within the plane of a compressive motion, but ${\\mathbf B}$ increases\nperpendicular to it. At late times the magnetic field resists compression,\nmaking it harder for scalar gradients to grow and likely slowing mixing.\n",
        "  Taking advantage of the Gaia DR1, we combined TGAS parallaxes with the\nTycho-2 and APASS photometry to calculate the star formation history (SFH) of\nthe solar neighbourhood within 250 pc using the colour-magnitude diagram\nfitting technique. Our dynamically-evolved SFH is in excellent agreement with\nthat calculated from the Hipparcos catalogue within 80 pc of the Sun, showing\nan enhanced star formation rate (SFR) in the past ~4 Gyr. We then correct the\nSFR for the disc thickening with age to obtain a SFR that is representative of\nthe whole solar cylinder, and show that even with an extreme correction our\nresults are not consistent with an exponentially decreasing SFR as found by\nrecent studies. Finally, we discuss how this technique can be applied out to ~5\nkpc thanks to the next Gaia data releases, which will allow us to quantify the\nSFH of the thin disc, thick disc and halo in situ.\n",
        "  We introduce subgroups ${\\mathcal{B}}_g< {\\mathcal H}_g$ of the mapping class\ngroup $Mod(\\Sigma_g)$ of a closed surface of genus $g \\ge 0$ with a Cantor set\nremoved, which are extensions of Thompson's group $V$ by a direct limit of\nmapping class groups of compact surfaces of genus $g$.\n  We first show that both ${\\mathcal{B}}_g$ and ${\\mathcal H}_g$ are finitely\npresented, and that ${\\mathcal H}_g$ is dense in $Mod(\\Sigma_g)$. We then\nexploit the relation with Thompson's groups to study properties ${\\mathcal\nB}_g$ and ${\\mathcal H}_g$ in analogy with known facts about finite-type\nmapping class groups. For instance, their homology coincides with the stable\nhomology of the mapping class group of genus $g$, every automorphism is\ngeometric, and every homomorphism from a higher-rank lattice has finite image.\n  In addition, the same connection with Thompson's groups will also prove that\n${\\mathcal B}_g$ and ${\\mathcal H}_g$ are not linear and do not have Kazhdan's\nProperty (T), which represents a departure from the current knowledge about\nfinite-type mapping class groups.\n",
        "  This study is the third of a series that investigates the degeneracy and\nstochasticity problems present in the determination of physical parameters such\nas age, mass, extinction, and metallicity of partially resolved or unresolved\nstar cluster populations situated in external galaxies when using broad-band\nphotometry. This work tests the derivation of parameters of artificial star\nclusters using models with fixed and free metallicity for the WFC3+ACS\nphotometric system. Then the method is applied to derive parameters of a sample\nof 203 star clusters in the Andromeda galaxy observed with the HST. Following\nPapers I \\& II, the star cluster parameters are derived using a large grid of\nstochastic models that are compared to the observed cluster broad-band\nintegrated WFC3+ACS magnitudes. We derive the age, mass, and extinction of the\nsample of M31 star clusters with one fixed metallicity in agreement with\nprevious studies. Using artificial tests we demonstrate the ability of the\nWFC3+ACS photometric system to derive the metallicity of star clusters. We show\nthat the metallicity derived using photometry of 36 massive M31 star clusters\nis in a good agreement with the metallicity previously derived using\nspectroscopy taken from literature.\n",
        "  Purpose: Stimulated echo acquisition mode (STEAM) diffusion MRI can be\nadvantageous over pulsed-gradient spin-echo (PGSE) for diffusion times that are\nlong compared to $\\ttwo$. It is important therefore for biomedical diffusion\nimaging applications at 7T and above where $\\ttwo$ is short. However, imaging\ngradients in the STEAM sequence contribute much greater diffusion weighting\nthan in PGSE, but are often ignored during post-processing. We demonstrate here\nthat this can severely bias parameter estimates.\n  Method: We present models for the STEAM signal for free and restricted\ndiffusion that account for crusher and slice-select (butterfly) gradients to\navoid such bias. The butterfly gradients also disrupt experiment design,\ntypically by skewing gradient-vectors towards the slice direction. We propose a\nsimple compensation to the diffusion gradient vector specified to the scanner\nthat counterbalances the butterfly gradients to preserve the intended\nexperiment design.\n  Results: High-field data fixed from a monkey brain experiments demonstrate\nthe need for both the compensation during acquisition and correct modelling\nduring post-processing for both diffusion tensor imaging and ActiveAx\naxon-diameter index mapping. Simulations support the results and indicate a\nsimilar need in in-vivo human applications.\n  Conclusion: Correct modelling and compensation are important for practical\napplications of STEAM diffusion MRI.\n",
        "  We prove that the classical set of moves for standard spines of 3-manifolds\n(i.e. the MP-move and the V-move) does not suffice to relate to each other any\ntwo standard skeleta of a 3-manifold with marked boundary. We also describe a\ncondition on the 3-manifold with marked boundary that tells whether the\ngeneralised set of moves, made up of the MP-move and the L-move, suffices to\nrelate to each other any two standard skeleta of the 3-manifold with marked\nboundary.\n  For the 3-manifolds with marked boundary that do not fulfil this condition,\nwe give three other moves: the CR-move, the T1-move and the T2-move. The first\none is local and, with the MP-move and the L-move, suffices to relate to each\nother any two standard skeleta of a 3-manifold with marked boundary fulfilling\nanother condition. For the universal case, we prove that the non-local T1-move\nand T2-move, with the MP-move and the L-move, suffice to relate to each other\nany two standard skeleta of a generic 3-manifold with marked boundary.\n  As a corollary, we get that disc-replacements suffice to relate to each other\nany two standard skeleta of a 3-manifold with marked boundary.\n",
        "  Understanding how an extinction event affects ecosystem is fundamental to\nbiodiversity conservation. For this reason, food web response to species loss\nhas been investigated in several ways in the last years. Several studies\nfocused on secondary extinction due to biodiversity loss in a bottom-up\nperspective using in-silico extinction experiments in which a single species is\nremoved at each step and the number of secondary extinctions is recorded. In\nthese binary simulations a species goes secondarily extinct if it loses all its\nresource species, that is, when the energy intake is zero. This pure\ntopological statement represents the best case scenario. In fact a consumer\nspecies could go extinct losing a certain fraction of the energy intake and the\nresponse of quantitative food webs to node loss could be very different with\nrespect to simple binary predictions. The goal of this paper is to analyze how\npatterns of secondary extinctions change when higher species sensitivity are\nincluded in the analyses. In particular, we explored how food web secondary\nextinction, triggered by the removal of most connected nodes, varies as a\nfunction of the energy intake threshold assumed as the minimum needed for\nspecies persistence. As we will show, a very low increase of energy intake\nthreshold stimulates a disproportionate growth of secondary extinction.\n",
        "  We present observations of three Class 0/I protostars (L1157-mm, CB230 IRS1,\nand L1165-SMM1) using the Karl G. Jansky Very Large Array (VLA) and\nobservations of two (L1165-SMM1 and CB230 IRS1) with the Combined Array for\nResearch in Millimeter-wave Astronomy (CARMA). The VLA observations were taken\nat wavelengths of $\\lambda = 7.3$ mm, 1.4 cm, 3.3 cm, 4.0 cm, and 6.5 cm with a\nbest resolution of $\\sim$0\\farcs06 (18 AU) at 7.3 mm. The L1165-SMM1 CARMA\nobservations were taken at $\\lambda = 1.3$ mm with a best resolution of\n$\\sim0\\farcs3$ (100 AU), and the CB230 IRS1 observations were taken at $\\lambda\n= 3.4$ mm with a best resolution of $\\sim$3\\arcsec\\ (900 AU). We find that\nL1165-SMM1 and CB230 IRS1 have probable binary companions at separations of\n$\\sim$0\\farcs3 (100 AU) from detections of secondary peaks at multiple\nwavelengths. The position angles of these companions are nearly orthogonal to\nthe direction of the observed bipolar outflows, consistent with the expected\nprotostellar disk orientations. We suggest that these companions may have\nformed from disk fragmentation; turbulent fragmentation would not\npreferentially arrange the binary companions to be orthogonal to the outflow\ndirection. For L1165-SMM1, both the 7.3 mm and 1.3 mm emission show evidence of\na large (R $>$ 100 AU) disk. For the L1165-SMM1 primary protostar and the CB230\nIRS1 secondary protostar, the 7.3 mm emission is resolved into structures\nconsistent with $\\sim20$ AU radius disks. For the other protostars, including\nL1157-mm, the emission is unresolved, suggesting disks with radii $< 20$ AU.\n",
        "  In this paper we present SABRINA (Sentiment Analysis: a Broad Resource for\nItalian Natural language Applications) a manually annotated prior polarity\nlexical resource for Italian natural language applications in the field of\nopinion mining and sentiment induction. The resource consists in two different\nsets, an Italian dictionary of more than 277.000 words tagged with their prior\npolarity value, and a set of polarity modifiers, containing more than 200\nwords, which can be used in combination with non neutral terms of the\ndictionary in order to induce the sentiment of Italian compound terms. To the\nbest of our knowledge this is the first prior polarity manually annotated\nresource which has been developed for the Italian natural language.\n",
        "  We study the coupled dynamics of two populations of random replicators by\nmeans of statistical mechanics methods, and focus on the effects of relative\npopulation size, strategy correlations and heterogeneities in the respective\nco-operation pressures. To this end we generalise existing path-integral\napproaches to replicator systems with random asymmetric couplings. This\ntechnique allows one to formulate an effective dynamical theory, which is exact\nin the thermodynamic limit and which can be solve for persistent order\nparameters in a fixed-point regime regardless of the symmetry of the\ninteractions. The onset of instability can be determined self-consistently. We\ncalculate quantities such as the diversity of the respective populations and\ntheir fitnesses in the stationary state, and compare results with data from a\nnumerical integration of the replicator equations\n",
        "  The growing volume of data usually creates an interesting challenge for the\nneed of data analysis tools that discover regularities in these data. Data\nmining has emerged as disciplines that contribute tools for data analysis,\ndiscovery of hidden knowledge, and autonomous decision making in many\napplication domains. The purpose of this study is to compare the performance of\ntwo data mining techniques viz., factor analysis and multiple linear regression\nfor different sample sizes on three unique sets of data. The performance of the\ntwo data mining techniques is compared on following parameters like mean square\nerror (MSE), R-square, R-Square adjusted, condition number, root mean square\nerror(RMSE), number of variables included in the prediction model, modified\ncoefficient of efficiency, F-value, and test of normality. These parameters\nhave been computed using various data mining tools like SPSS, XLstat, Stata,\nand MS-Excel. It is seen that for all the given dataset, factor analysis\noutperform multiple linear regression. But the absolute value of prediction\naccuracy varied between the three datasets indicating that the data\ndistribution and data characteristics play a major role in choosing the correct\nprediction technique.\n",
        "  We report on transport properties of grain boundaries fabricated in\nYBa2Cu3O7-x thin films grown by the liquid phase epitaxy (LPE) technique on MgO\nasymmetrical bicrystal substrate with 45o misorientation angle. In total around\n10 samples have been studied. Substantial scatter of zero field values of the\ncritical current density at 5K has been observed. The upper limit of Jc of the\norder of 104 A/cm2 found in our study is close to previously reported data for\n45o bicrystals grown by various physical vapour deposition methods while the\nminimal value of Jc for the LPE grown bicrystals in striking difference to the\nresults published before is exactly equal to zero. For samples with non-zero Jc\nwe have found a few different types of critical current dependence on magnetic\nfield ranging from pattern reminiscent Fraunhover-like Ic(H) to Ic(H) profile\nwith Ic minimum at zero field.\n",
        "  For any given natural number $k$, this paper gives upper bounds on the radius\nof a packing of a complete hyperbolic surface of finite area by $k$\nequal-radius disks in terms of the surface's topology. We show that the bounds\ngiven here are sharp in some cases and not sharp in others.\n",
        "  LangPro is an automated theorem prover for natural language\n(https://github.com/kovvalsky/LangPro). Given a set of premises and a\nhypothesis, it is able to prove semantic relations between them. The prover is\nbased on a version of analytic tableau method specially designed for natural\nlogic. The proof procedure operates on logical forms that preserve linguistic\nexpressions to a large extent. %This property makes the logical forms easily\nobtainable from syntactic trees. %, in particular, Combinatory Categorial\nGrammar derivation trees. The nature of proofs is deductive and transparent. On\nthe FraCaS and SICK textual entailment datasets, the prover achieves high\nresults comparable to state-of-the-art.\n",
        "  Performance tuning of Database Management Systems(DBMS) is both complex and\nchallenging as it involves identifying and altering several key performance\ntuning parameters. The quality of tuning and the extent of performance\nenhancement achieved greatly depends on the skill and experience of the\nDatabase Administrator (DBA). As neural networks have the ability to adapt to\ndynamically changing inputs and also their ability to learn makes them ideal\ncandidates for employing them for tuning purpose. In this paper, a novel tuning\nalgorithm based on neural network estimated tuning parameters is presented. The\nkey performance indicators are proactively monitored and fed as input to the\nNeural Network and the trained network estimates the suitable size of the\nbuffer cache, shared pool and redo log buffer size. The tuner alters these\ntuning parameters using the estimated values using a rate change computing\nalgorithm. The preliminary results show that the proposed method is effective\nin improving the query response time for a variety of workload types. .\n",
        "  We consider the id-density wave (DDW) order, representing the pseudo-gap (PG)\nstate, and the d-wave superconductivity (DSC)within the BCS framework for the\ntwo-dimensional (2D) fermion system on a square lattice starting with a\nmean-field Hamiltonian involving the singlet DDW and the DSC pairings. The\nabsence of nesting in the normal state dispersion leads to the particle-hole\nasymmetry in the single-particle excitation spectrum of the pure DDW state.\nThis is reflected in the coexisting DDW and DSC states though the latter is\ncharacterized by the Bogoluibov quasi-particle bands- a characteristic feature\nof SC state. We solve the coupled gap equations self-consistently together with\nthe equation to determine the chemical potential. We also calculate the\nthermodynamic and transport properties in the PG phase.The electronic specific\nheat displays non-Fermi liquid feature. We show that the PG and DSC are\nrepresenting two competing orders as the former brings about a depletion of the\nspectral weight available for pairing in the anti-nodal region of momentum\nspace. We also show the depletion of the spectral weight below Tc at energies\nlarger than the gap amplitude. This is an important hallmark of the strong\ncoupling superconductivity. Furthermore, the passage from normal state to the\nPG state at a fixed hole under-doping is shown to correspond to a non-sharp\nthermal phase transition.\n",
        "  Efforts to reconstruct phylogenetic trees and understand evolutionary\nprocesses depend fundamentally on stochastic models of speciation and mutation.\nThe simplest continuous-time model for speciation in phylogenetic trees is the\nYule process, in which new species are \"born\" from existing lineages at a\nconstant rate. Recent work has illuminated some of the structural properties of\nYule trees, but it remains mostly unknown how these properties affect sequence\nand trait patterns observed at the tips of the phylogenetic tree. Understanding\nthe interplay between speciation and mutation under simple models of evolution\nis essential for deriving valid phylogenetic inference methods and gives\ninsight into the optimal design of phylogenetic studies. In this work, we\nderive the probability distribution of interspecies covariance under Brownian\nmotion and Ornstein-Uhlenbeck models of phenotypic change on a Yule tree. We\ncompute the probability distribution of the number of mutations shared between\ntwo randomly chosen taxa in a Yule tree under discrete Markov mutation models.\nOur results suggest summary measures of phylogenetic information content,\nilluminate the correlation between site patterns in sequences or traits of\nrelated organisms, and provide heuristics for experimental design and\nreconstruction of phylogenetic trees.\n",
        "  For any pseudo-Anosov diffeomorphism on a closed orientable surface $S$ of\ngenus greater than one, it is known by the work of Bers and Thurston that the\ntopological entropy agrees with the translation distance on the Teichm\\\"uller\nspace with respect to the Teichm\\\"uller metric. In this paper, we consider\nrandom walks on the mapping class group of $S$. The drift of a random walk is\ndefined as the translation distance of the random walk. We define the\ntopological entropy of a random walk and prove that it almost surely agrees\nwith the drift on the Teichm\\\"uller space with respect to the Teichm\\\"uller\nmetric.\n",
        "  Many large-scale machine learning (ML) systems allow specifying custom ML\nalgorithms by means of linear algebra programs, and then automatically generate\nefficient execution plans. In this context, optimization opportunities for\nfused operators---in terms of fused chains of basic operators---are ubiquitous.\nThese opportunities include (1) fewer materialized intermediates, (2) fewer\nscans of input data, and (3) the exploitation of sparsity across chains of\noperators. Automatic operator fusion eliminates the need for hand-written fused\noperators and significantly improves performance for complex or previously\nunseen chains of operations. However, existing fusion heuristics struggle to\nfind good fusion plans for complex DAGs or hybrid plans of local and\ndistributed operations. In this paper, we introduce an optimization framework\nfor systematically reason about fusion plans that considers materialization\npoints in DAGs, sparsity exploitation, different fusion template types, as well\nas local and distributed operations. In detail, we contribute algorithms for\n(1) candidate exploration of valid fusion plans, (2) cost-based candidate\nselection, and (3) code generation of local and distributed operations over\ndense, sparse, and compressed data. Our experiments in SystemML show end-to-end\nperformance improvements with optimized fusion plans of up to 21x compared to\nhand-written fused operators, with negligible optimization and code generation\noverhead.\n",
        "  We have made CO(J=2-1) observations towards the HII region RCW 49 and its\nionizing source, the rich stellar cluster Westerlund 2 (hereafter Wd2), with\nthe NANTEN2 sub-mm telescope. These observations have revealed that two\nmolecular clouds in velocity ranges of -11 to +9 km/s and 11 to 21 km/s\nrespectively, show remarkably good spatial correlations with the Spitzer IRAC\nmid-infrared image of RCW 49, as well a velocity structures indicative of\nlocalized expansion around the bright central regions and stellar cluster. This\nstrongly argues that the two clouds are physically associated with RCW 49. We\nobtain a new kinematic distance estimate to RCW 49 and Wd2 of 5.4^{+ 1.1}_{-\n1.4} kpc, based on the mean velocity and velocity spread of the associated gas.\nWe argue that acceleration of the gas by stellar winds from Wd2 is insufficient\nto explain the entire observed velocity dispersion of the molecular gas, and\nsuggest a scenario in which a collision between the two clouds ~4 Myrs ago may\nhave triggered the formation of the stellar cluster.\n",
        "  Magnetic stimulation is a standard tool in brain research and many fields of\nneurology, as well as psychiatry. From a physical perspective, one key aspect\nof this method is the inefficiency of available setups. Whereas the spatial\nfield properties have been studied rather intensively with coil designs, the\ndynamics have been neglected almost completely for a long time. Instead, the\ndevices and their technology defined the waveform. Here, an analysis of the\nwaveform space is performed. Based on these data, an appropriate optimisation\napproach is outlined which makes use of a modern nonlinear axon description of\na mammalian motor nerve. The approach is based on a hybrid global-local method;\ndifferent coordinate systems for describing the continuous waveforms in a\nlimited parameter space are defined for sufficient stability. The results of\nthe numeric setup suggest that there is plenty of room for waveforms with\nhigher efficiency than the traditional shapes. One class of such pulses is\nanalysed further. Although the voltage profile of these waveforms is almost\nrectangular, the current shape presents distinct characteristics, such as a\nfirst phase which precedes the main pulse and decreases the losses. The single\nrepresentatives, which differ in their maximum voltage shape, are linked by a\nnonlinear transformation. The main phase, however, seems to scale in time only.\n",
        "  The aim of this study was to describe the behaviour of a shock machine\ndesigned for testing hip prostheses. A microseparation between head and cup\noccurs inducing a shock of several times the body weight, leading to fracture\nof ceramic femoral components. Femoral heads and cups of diameter 32 mm\nmanufactured from alumina were tested in dry and wet conditions. Implants were\nsubjected to shocks with a load profile of 9 kN load at 2 Hz and various\nmicroseparations. Position is monitored and force is measured with two\nacquisition systems. The working range and the device capabilities were\ninvestigated. Only cups tested in dry conditions failed. Observations by\nscanning electron microscopy revealed intergranular and transgranular\nfractures. Two wear stripes were observed on the heads. Three-dimensional\nroughness of wear stripes was measured. Since experimental results are in good\nagreement with retrieved femoral heads, the shocks machine reproduces the in\nvivo degradations.\n",
        "  We prove that the degree of the Brandt-Lickorish-Millet polynomial of any\nquasi-alternating link is less than its determinant. Therefore, we obtain a new\nand a simple obstruction criterion for quasi-alternateness. As an application,\nwe identify some knots of 12 crossings or less and some links of 9 crossings or\nless that are not quasi-alternating. Also, we show that there are only finitely\nmany Kanenobu knots which are quasi-alternating. This last result supports\nConjecture 3.1 of Greene in [10] which states that there are only finitely many\nquasi-alternating links with a given determinant. Moreover, we identify an\ninfinite family of non quasi-alternating Montesinos links and this supports\nConjecture 3.10 in [20] that characterizes quasi-alternating Montesinos links.\n",
        "  A 'dual-field' strategy is often used for tumors with highly complex shapes\nand/or with large volumes exceeding available field-size in both passive and\nscanning irradiations with ion beams. Range and setup uncertainties can cause\nhot and cold doses at the field junction within the target. Such uncertainties\nwill also cause cold doses in the peripheral region of the target. We have\ndeveloped an algorithm to reduce the sensitivity of the dual-field plan to\nthese uncertainties in scanning irradiations. This algorithm is composed of the\nfollowing two steps: 1) generating the expanded target volume, and 2) solving\nthe inverse problem where the terms suppressing the dose gradient of individual\nfields are added into the objective function. The validity of this algorithm is\ndemonstrated through the simulation studies for three extreme cases of two\nfields with unidirectional, opposing and orthogonal geometries. With the\nproposed algorithm, we can obtain a more robust plan to minimize the effects of\nrange and setup uncertainties than the conventional plan. Compared to that for\nthe conventional plan, the optimization time for the robust plan increased by a\nfactor of approximately three.\n",
        "  In this paper we report the comparative study of superconductivity by 3d\n(Co), 4d (Rh), 5d (Ir) element doping in SmFeAsO. X-ray diffraction patterns\nindicate that the material has formed the ZrCuSiAs-type structure with a space\ngroup P4/nmm. It is found that the antiferromagnetic spin-density-wave (SDW)\norder in the parent compounds is rapidly suppressed by Co, Rh, and Ir doping,\nand superconductivity emerges. Both electrical resistance and magnetization\nmeasurements show superconductivity up to around 10 K in SmFe1-xMxAsO (M = Co,\nRh, Ir). Co, Rh and Ir locate in the same column in the periodic table of\nelements but have different electronic band structure, so comparative study\nwould add more ingredients to the underlying physics of the iron-based\nsuperconductors.\n",
        "  The World Wide Web infrastructure together with its more than 2 billion users\nenables to store information at a rate that has never been achieved before.\nThis is mainly due to the will of storing almost all end-user interactions\nperformed on some web applications. In order to reply to scalability and\navailability constraints, many web companies involved in this process recently\nstarted to design their own data management systems. Many of them are referred\nto as NOSQL databases, standing for 'Not only SQL'. With their wide adoption\nemerges new needs and data integration is one of them. In this paper, we\nconsider that an ontology-based representation of the information stored in a\nset of NOSQL sources is highly needed. The main motivation of this approach is\nthe ability to reason on elements of the ontology and to retrieve information\nin an efficient and distributed manner. Our contributions are the following:\n(1) we analyze a set of schemaless NOSQL databases to generate local\nontologies, (2) we generate a global ontology based on the discovery of\ncorrespondences between the local ontologies and finally (3) we propose a query\ntranslation solution from SPARQL to query languages of the sources. We are\ncurrently implementing our data integration solution on two popular NOSQL\ndatabases: MongoDB as a document database and Cassandra as a column family\nstore.\n",
        "  Recurrent Neural Network (RNN) is one of the most popular architectures used\nin Natural Language Processsing (NLP) tasks because its recurrent structure is\nvery suitable to process variable-length text. RNN can utilize distributed\nrepresentations of words by first converting the tokens comprising each text\ninto vectors, which form a matrix. And this matrix includes two dimensions: the\ntime-step dimension and the feature vector dimension. Then most existing models\nusually utilize one-dimensional (1D) max pooling operation or attention-based\noperation only on the time-step dimension to obtain a fixed-length vector.\nHowever, the features on the feature vector dimension are not mutually\nindependent, and simply applying 1D pooling operation over the time-step\ndimension independently may destroy the structure of the feature\nrepresentation. On the other hand, applying two-dimensional (2D) pooling\noperation over the two dimensions may sample more meaningful features for\nsequence modeling tasks. To integrate the features on both dimensions of the\nmatrix, this paper explores applying 2D max pooling operation to obtain a\nfixed-length representation of the text. This paper also utilizes 2D\nconvolution to sample more meaningful information of the matrix. Experiments\nare conducted on six text classification tasks, including sentiment analysis,\nquestion classification, subjectivity classification and newsgroup\nclassification. Compared with the state-of-the-art models, the proposed models\nachieve excellent performance on 4 out of 6 tasks. Specifically, one of the\nproposed models achieves highest accuracy on Stanford Sentiment Treebank binary\nclassification and fine-grained classification tasks.\n",
        "  We show that an arbitrary spatial distribution of complex refractive index\ninside an object can be exactly represented as a sum of two \"monomorphous\"\ncomplex distributions, i.e. the distributions with the ratios of the real part\nto the imaginary part being constant throughout the object. A priori knowledge\nof constituent materials can be used to estimate the global lower and upper\nboundaries for this ratio. This approach can be viewed as an extension of the\nsuccessful phase-retrieval method, based on the Transport of Intensity\nequation, that was previously developed for monomorphous (homogeneous) objects,\nsuch as e.g. objects consisting of a single material. We demonstrate that the\nmonomorphous decomposition can lead to more stable methods for phase retrieval\nusing the Transport of Intensity Equation. Such methods may find application in\nquantitative in-line phase-contrast imaging and phase-contrast tomography.\n",
        "  Sullivan showed that there exists $K_0$ such that if $\\Omega\\subset\n\\hat{\\mathbb{C}}$ is a simply connected hyperbolic domain, then there exists a\nconformally natural $K_0$-quasiconformal map from $\\Omega$ to the boundary\n${\\rm Dome}(\\Omega)$ of the convex hull of its complement which extends to the\nidentity on $\\partial\\Omega$. Explicit upper and lower bounds on $K_0$ were\nobtained by Epstein, Marden, Markovic and Bishop. We improve on these bounds,\nby showing that one may choose $K_0\\le 7.1695$.\n",
        "  Recent works in automated radiotherapy treatment planning have used machine\nlearning based on historical treatment plans to infer the spatial dose\ndistribution for a novel patient directly from the planning image. We present\nan atlas-based approach which learns a dose prediction model for each patient\n(atlas) in a training database, and then learns to match novel patients to the\nmost relevant atlases. The method creates a spatial dose objective, which\nspecifies the desired dose-per-voxel, and therefore replaces any requirement\nfor specifying dose-volume objectives for conveying the goals of treatment\nplanning. A probabilistic dose distribution is inferred from the most relevant\natlases, and is scalarized using a conditional random field to determine the\nmost likely spatial distribution of dose to yield a specific dose prior\n(histogram) for relevant regions of interest. Voxel-based dose mimicking then\nconverts the predicted dose distribution to a deliverable treatment plan dose\ndistribution. In this study, we investigated automated planning for right-sided\noropharaynx head and neck patients treated with IMRT and VMAT. We compare four\nversions of our dose prediction pipeline using a database of 54 training and 12\nindependent testing patients. Our preliminary results are promising; automated\nplanning achieved a higher number of dose evaluation criteria in 7 patients and\nan equal number in 4 patients compared with clinical. Overall, the relative\nnumber of criteria achieved was higher for automated planning versus clinical\n(17 vs 8) and automated planning demonstrated increased sparing for organs at\nrisk (52 vs 44) and better target coverage/uniformity (41 vs 31).\n",
        "  Semantic information is often represented as the entities and the\nrelationships among them with conventional semantic models. This approach is\nstraightforward but is not suitable for many posteriori requests in semantic\ndata modeling. In this paper, we propose a meaning-oriented approach to\nmodeling semantic data and establish a graph-based semantic data model. In this\napproach we use the meanings, i.e., the subjective views of the entities and\nrelationships, to describe the semantic information, and use the semantic\ngraphs containing the meaning nodes and the meta-meaning relations to specify\nthe taxonomy and the compound construction of the semantic concepts. We\ndemonstrate how this meaning-oriented approach can address many important\nsemantic representation issues, including dynamic specialization and natural\njoin.\n",
        "  We report on magnetoresistance oscillations in superconducting NbNx nanowires\nsynthesized through ammonia gas annealing of NbSe3 precursor nanostructures.\nEven though the transverse dimensions of the nanowires are much larger than the\nsuperconducting coherence length, the voltage-current characteristics of these\nnanowires at low temperatures are reminiscent of one-dimensional\nsuperconductors where quantum phase slips are associated with the origin of\ndissipation. We show that both the magnetoresistance oscillations and\nvoltage-current characteristics observed in this work result from the granular\nstructure of our nanowires.\n",
        "  Usually, Monte Carlo models are validated against experimental data. However,\nmodels of multiple Coulomb scattering (MCS) in the Gaussian approximation are\nexceptional in that we have theories which are probably more accurate than the\nexperiments which have, so far, been done to test them. In problems directly\nsensitive to the distribution of angles leaving the target, the relevant theory\nis the Moliere/Fano/Hanson variant of Moliere theory. For transverse spreading\nof the beam in the target itself, the theory of Preston and Koehler holds.\n  Therefore, in this paper we compare Geant4 simulations, using the Urban and\nWentzel models of MCS, with theory rather than experiment, revealing trends\nwhich would otherwise be obscured by experimental scatter. For medium-energy\n(radiotherapy) protons, and low-Z (water-like) target materials, Wentzel\nappears to be better than Urban in simulating the distribution of outgoing\nangles. For beam spreading in the target itself, the two models are essentially\nequal.\n",
        "  We study the topologically protected Majorana zero modes induced by lattice\ndislocations in chiral topological superconductors. Dislocations provide a new\nway to realize Majorana zero modes at zero magnetic field. In particular, we\nstudy several different types of dislocations in the candidate material ${\\rm\nSr_2RuO_4}.$ We also discuss the properties of linked dislocation lines and\nlinked dislocation and flux lines. Various experimental consequences are\npredicted which provide a new approach to determine whether nature of the\nsuperconducting phase of ${\\rm Sr_2RuO_4}.$\n",
        "  To explore the connection between the global physical properties of galaxies\nand their far-infrared (FIR) spectral energy distributions (SEDs), we study the\nvariation in the FIR SEDs of a set of hydrodynamically simulated galaxies that\nare generated by performing dust radiative transfer in post-processing. Our\nsample includes both isolated and merging systems at various stages of the\nmerging process and covers infrared (IR) luminosities and dust masses that are\nrepresentative of both low- and high-redshift galaxies. We study the FIR SEDs\nusing principle component analysis (PCA) and find that 97\\% of the variance in\nthe sample can be explained by two principle components (PCs). The first PC\ncharacterizes the wavelength of the peak of the FIR SED, and the second encodes\nthe breadth of the SED. We find that the coefficients of both PCs can be\npredicted well using a double power law in terms of the IR luminosity and dust\nmass, which suggests that these two physical properties are the primary\ndeterminants of galaxies' FIR SED shapes. Incorporating galaxy sizes does not\nsignificantly improve our ability to predict the FIR SEDs. Our results suggest\nthat the observed redshift evolution in the effective dust temperature at fixed\nIR luminosity is not driven by geometry: the SEDs of $z \\sim 2-3$ ultraluminous\nIR galaxies (ULIRGs) are cooler than those of local ULIRGs not because the\nhigh-redshift galaxies are more extended but rather because they have higher\ndust masses at fixed IR luminosity. Finally, based on our simulations, we\nintroduce a two-parameter set of SED templates that depend on both IR\nluminosity and dust mass.\n",
        "  Multi-way Theta-join queries are powerful in describing complex relations and\ntherefore widely employed in real practices. However, existing solutions from\ntraditional distributed and parallel databases for multi-way Theta-join queries\ncannot be easily extended to fit a shared-nothing distributed computing\nparadigm, which is proven to be able to support OLAP applications over immense\ndata volumes. In this work, we study the problem of efficient processing of\nmulti-way Theta-join queries using MapReduce from a cost-effective perspective.\nAlthough there have been some works using the (key,value) pair-based\nprogramming model to support join operations, efficient processing of multi-way\nTheta-join queries has never been fully explored. The substantial challenge\nlies in, given a number of processing units (that can run Map or Reduce tasks),\nmapping a multi-way Theta-join query to a number of MapReduce jobs and having\nthem executed in a well scheduled sequence, such that the total processing time\nspan is minimized. Our solution mainly includes two parts: 1) cost metrics for\nboth single MapReduce job and a number of MapReduce jobs executed in a certain\norder; 2) the efficient execution of a chain-typed Theta-join with only one\nMapReduce job. Comparing with the query evaluation strategy proposed in [23]\nand the widely adopted Pig Latin and Hive SQL solutions, our method achieves\nsignificant improvement of the join processing efficiency.\n",
        "  The plethora of graphs and relational data give rise to many interesting\ngraph-relational queries in various domains, e.g., finding related proteins\nsatisfying relational predicates in a biological network. The maturity of\nRDBMSs motivated academia and industry to invest efforts in leveraging RDBMSs\nfor graph processing, where efficiency is proven for vital graph queries.\nHowever, none of these efforts process graphs natively inside the RDBMS, which\nis particularly challenging due to the impedance mismatch between the\nrelational and the graph models. In this paper, we propose to treat graphs as\nfirst-class citizens inside the relational engine so that operations on graphs\nare executed natively inside the RDBMS. We realize our approach inside VoltDB,\nan open-source in-memory relational database, and name this realization\nGRFusion. The SQL and the query engine of GRFusion are empowered to\ndeclaratively define graphs and execute cross-data-model query plans formed by\ngraph and relational operators, resulting in up to four orders-of-magnitude in\nquery-time speedup w.r.t. state-of-the-art approaches.\n",
        "  We study the symmetry of Cooper pair in a two-dimensional Hubbard model with\nthe Rashba-type spin-orbit interaction as a minimal model of electron gas\ngenerated at a heterointerface of SrTiO$_3$/LaAlO$_3$. Solving the Eliashberg\nequation based on the third-order perturbation theory, we find that the gap\nfunction consists of the mixing of the spin-singlet $d_{xy}$-wave component and\nthe spin-triplet $(p_x\\pm ip_y)$-wave one due to the broken inversion symmetry\noriginating from the Rashba-type spin-orbit interaction. The ratio of the\nd-wave and the p-wave component continuously changes with the carrier\nconcentration. We propose that the pairing symmetry is controlled by tuning the\ngate voltage.\n",
        "  The Meissner effect has been studied in Ba(Fe0.926Co0.074)2As2 and\nBa0.6K0.4Fe2As2 single crystals and compared to well known, type-II\nsuperconductors LuNi2B2C and V3Si. Whereas flux penetration is mostly\ndetermined by the bulk pinning (and, perhaps, surface barrier) resulting in a\nlarge negative magnetization, the flux expulsion upon cooling in a magnetic\nfield is very small, which could also be due to pinning and/or surface barrier\neffects. However, in stark contrast with the expected behavior, the amount of\nthe expelled flux increases almost linearly with the applied magnetic field, at\nleast up to our maximum field of 5.5 T, which far exceeds the upper limit for\nthe surface barrier. One interpretation of the observed behavior is that there\nis a field-driven suppression of magnetic pair-breaking.\n",
        "  We present very high signal-to-noise ratio absorption-line observations of CN\nand CH+ along 13 lines of sight through diffuse molecular clouds. The data are\nexamined to extract precise isotopologic ratios of 12CN/13CN and 12CH+/13CH+ in\norder to assess predictions of diffuse cloud chemistry. Our results on\n12CH+/13CH+ confirm that this ratio does not deviate from the ambient 12C/13C\nratio in local interstellar clouds, as expected if the formation of CH+\ninvolves nonthermal processes. We find that 12CN/13CN, however, can be\nsignificantly fractionated away from the ambient value. The dispersion in our\nsample of 12CN/13CN ratios is similar to that found in recent surveys of\n12CO/13CO. For sight lines where both ratios have been determined, the\n12CN/13CN ratios are generally fractionated in the opposite sense compared to\n12CO/13CO. Chemical fractionation in CO results from competition between\nselective photodissociation and isotopic charge exchange. An inverse\nrelationship between 12CN/13CN and 12CO/13CO follows from the coexistence of CN\nand CO in diffuse cloud cores. However, an isotopic charge exchange reaction\nwith CN may mitigate the enhancements in 12CN/13CN for lines of sight with low\n12CO/13CO ratios. For two sight lines with high values of 12CO/13CO, our\nresults indicate that about 50 percent of the carbon is locked up in CO, which\nis consistent with the notion that these sight lines probe molecular cloud\nenvelopes where the transition from C+ to CO is expected to occur. An analysis\nof CN rotational excitation yields a weighted mean value for T_01(12CN) of\n2.754 +/- 0.002 K, which implies an excess over the temperature of the cosmic\nmicrowave background of only 29 +/- 3 mK. This modest excess eliminates the\nneed for a local excitation mechanism beyond electron and neutral collisions.\nThe rotational excitation temperatures in 13CN show no excess over the\ntemperature of the CMB.\n",
        "  Dwarf Irregular galaxies (dIrrs) are the smallest stellar systems with\nextended HI discs. The study of the kinematics of such discs is a powerful tool\nto estimate the total matter distribution at these very small scales. In this\nwork, we study the HI kinematics of 17 galaxies extracted from the `Local\nIrregulars That Trace Luminosity Extremes, The HI Nearby Galaxy Survey' (LITTLE\nTHINGS). Our approach differs significantly from previous studies in that we\ndirectly fit 3D models (two spatial dimensions plus one spectral dimension)\nusing the software $^\\text{3D}$BAROLO, fully exploiting the information in the\nHI datacubes. For each galaxy we derive the geometric parameters of the HI disc\n(inclination and position angle), the radial distribution of the surface\ndensity, the velocity-dispersion ($\\sigma_v$) profile and the rotation curve.\nThe circular velocity (V$_{\\text{c}}$), which traces directly the galactic\npotential, is then obtained by correcting the rotation curve for the asymmetric\ndrift. As an initial application, we show that these dIrrs lie on a baryonic\nTully-Fisher relation in excellent agreement with that seen on larger scales.\nThe final products of this work are high-quality, ready-to-use kinematic data\n($\\textrm{V}_\\textrm{c}$ and $\\sigma_v$) that we make publicly available. These\ncan be used to perform dynamical studies and improve our understanding of these\nlow-mass galaxies.\n",
        "  XML data warehouses form an interesting basis for decision-support\napplications that exploit complex data. However, native XML database management\nsystems currently bear limited performances and it is necessary to design\nstrategies to optimize them. In this paper, we propose an automatic strategy\nfor the selection of XML materialized views that exploits a data mining\ntechnique, more precisely the clustering of the query workload. To validate our\nstrategy, we implemented an XML warehouse modeled along the XCube\nspecifications. We executed a workload of XQuery decision-support queries on\nthis warehouse, with and without using our strategy. Our experimental results\ndemonstrate its efficiency, even when queries are complex.\n",
        "  Atrial fibrillation (AF) is the most common arrhythmia affecting millions of\npeople in the Western countries and, due to the widespread impact on the\npopulation and its medical relevance, is largely investigated in both clinical\nand bioengineering sciences. However, some important feedback mechanisms are\nstill not clearly established. The present study aims at understanding the\nglobal response of the cardiovascular system during paroxysmal AF through a\nlumped-parameter approach, which is here performed paying particular attention\nto the stochastic modeling of the irregular heartbeats and the reduced\ncontractility of the heart. AF can be here analyzed by means of a wide number\nof hemodynamic parameters and avoiding the presence of other pathologies, which\nusually accompany AF. Reduced cardiac output with correlated drop of ejection\nfraction and decreased amount of energy converted to work by the heart during\nblood pumping, as well as higher left atrial volumes and pressures are some of\nthe most representative results aligned with the existing clinical literature\nand here emerging during acute AF. The present modeling, providing new insights\non cardiovascular variables which are difficult to measure and rarely reported\nin literature, turns out to be an efficient and powerful tool for a deeper\ncomprehension and prediction of the arrythmia impact on the whole\ncardiovascular system.\n",
        "  Here, we address the essential question of whether, in the context of\nevolving populations, ecosystems attain properties that enable persistence of\nthe ecosystem itself. We use a simple ecosystem model describing resource,\nproducer, and consumer dynamics to analyze how evolution affects dynamical\nstability properties of the ecosystem. In particular, we compare resilience of\nthe entire system after allowing the producer and consumer populations to\nevolve to their evolutionarily stable strategy (ESS), to the maximum attainable\nresilience. We find a substantial reduction in ecosystem resilience when\nproducers and consumers are allowed to evolve compared to the maximal\nattainable resilience. This study illustrates the inherent difference and\npossible conflict between maximizing individual-level fitness and maximizing\nresilience of entire ecosystems.\n",
        "  We present new L-band ($1\\mbox{--}2$ GHz) observations of the nearby spiral\ngalaxy M33 with 80 pc resolution obtained with the Karl G. Jansky Very Large\nArray. The HI observations, combined with HI measurements from the Green Bank\nTelescope, improve the spectral resolution and sensitivity ($2.8$ K rms noise\nin a $0.2$ km s$^{-1}$ channel) compared to previous observations. We find\nindividual profiles are usually non-Gaussian, harbouring line wings, multiple\ncomponents, and asymmetries. Given this spectral complexity, we quantify the\nmotions in the atomic ISM through moment analysis of the spectra and fits to\naligned, stacked profiles. The measured value of the HI line width depends\nstrongly on the method used, with the velocity stacked profiles aligned to the\npeak velocity giving the minimum value of $\\sigma = 7$ km s$^{-1}$ and all\nother methods giving higher values ($\\sigma\\sim10$ km s$^{-1}$). All\nmeasurements of the line width show a shallow radial trend, with $\\sigma$\ndecreasing by $\\sim2$ km s$^{-1}$ from $R_{\\rm gal}=0$ to $R_{\\rm gal}=8$ kpc.\nWe consider a number of energy sources that might maintain the line width\nagainst turbulent dissipation, but no single source is adequate. We find excess\nemission relative to a Gaussian in the stacked profile line wings, ranging from\n9% to 26% depending on how the spectra are aligned. By splitting the line wings\ninto symmetric and asymmetric components, we find that the lagging rotational\ndisk accounts for one-third of the line wing flux. We also find emission far\nfrom the rotation axis of the galaxy in multiple discrete HI clouds, including\na filament with a projected length of $\\sim8$ kpc.\n",
        "  Quantitative MRI is highly desirable in terms of intrinsic tissue parameters\nsuch as T1, T2 and proton density. This approach promises to minimize\ndiagnostic variability and differentiate normal and pathological tissues by\ncomparing tissue parameters to the normal ranges. Also, absolute quantification\ncan help segment MRI tissue images with better accuracy compared to traditional\nqualitative segmentation methods. Currently, there are several methods proposed\nto quantify tissue parameters; however, all of them require excessive scan time\nand thus are difficult to be applied in clinical applications. In this paper,\nwe propose a novel machine learning approach for MRI quantification, which can\ndramatically decrease the scan time and improve image quality.\n",
        "  Two giants of evolutionary theory, Sewall Wright and R. A. Fisher, fought\nbitterly for over thirty years. The Wright-Fisher controversy forms a\ncornerstone of the history and philosophy of biology. I argue that the standard\ninterpretations of the Wright-Fisher controversy do not accurately represent\nthe ideas and arguments of these two key historical figures. The usual account\ncontrasts the major slogans attached to each name: Wright's adaptive landscape\nand shifting balance theory of evolution versus Fisher's fundamental theorem of\nnatural selection. These alternative theories are in fact incommensurable.\nWright's theory is a detailed dynamical model of evolutionary change in actual\npopulations. Fisher's theory is an abstract invariance and conservation law\nthat, like all physical laws, captures essential features of a system but does\nnot account for all aspects of dynamics in real examples. This key contrast\nbetween embodied theories of real cases and abstract laws is missing from prior\nanalyses of Wright versus Fisher. They never argued about this contrast.\nInstead, the issue at stake in their arguments concerned the actual dynamics of\nreal populations. Both agreed that fluctuations of nonadditive (epistatic) gene\ncombinations play a central role in evolution. Wright emphasized stochastic\nfluctuations of gene combinations in small, isolated populations. By contrast,\nFisher believed that fluctuating selection in large populations was the main\ncause of fluctuation in nonadditive gene combinations. Close reading shows that\nwidely cited views attributed to Fisher mostly come from what Wright said about\nFisher, whereas Fisher's own writings clearly do not support such views.\n",
        "  We investigate relationships between bounds on the crossing number and the\nmosaic number of mosaic knots.\n",
        "  Using Andreev reflection (AR) as an experimental gauge of the superconducting\nproximity effect (PE), we assess the topological purity of the\nsuperconductivity that is induced by the c-axis PE between an s-wave\nsuperconductor and the topological insulators Bi$_{2}$X$_{3}$ (X=Se,Te).\nPoint-contact AR spectroscopy is performed with Nb tips on Bi$_{2}$X$_{3}$\nsingle crystals at 4.2 K. Scanning tunneling spectroscopy is also used, to\nlocate the Fermi level $E_F$ relative to the Dirac point in the crystals. The\nAR data is analyzed with Blonder-Tinkham-Klapwijk theory, taking into account\ntip-induced spin-orbit coupling, Fermi-surface mismatch, and the co-presence of\nbulk band and topological surface states at $E_F$. Our results indicate that\nthe superconductivity that can be proximity-induced into Bi$_{2}$X$_{3}$ is\npredominantly non-topological.\n",
        "  In order to reduce the potential radiation risk, low-dose CT has attracted\nmore and more attention. However, simply lowering the radiation dose will\nsignificantly degrade the imaging quality. In this paper, we propose a noise\nreduction method for low-dose CT via deep learning without accessing the\noriginal projection data. An architecture of deep convolutional neural network\nwas considered to map the low-dose CT images into its corresponding normal-dose\nCT images patch by patch. Qualitative and quantitative evaluations demonstrate\na state-the-art performance of the proposed method.\n",
        "  In-memory computing has changed the landscape of database technology. Within\nthe database and technology field, advancements occur over the course of time\nthat has had the capacity to transform some fundamental tenants of the\ntechnology and how it is applied. The concept of Database Management Systems\n(DBMS) was realized in industry during the 1960s, allowing users and developers\nto use a navigational model to access the data stored by the computers of that\nday as they grew in speed and capability. This manuscript is specifically\nexamines the SAPHigh Performance Analytics Appliance(HANA) approach, which is\none of the commonly used technologies today. Additionally, this manuscript\nprovides the analysis of the first two of the four common main usecases to\nutilize SAP HANA's in-memory computing database technology. The performance\nbenefits are important factors for DB calculations.Some of the benefits are\nquantified and the demonstrated by the defined sets of data.\n",
        "  We present a catalog of low-mass dense cores observed with the SHARC-II\ninstrument at 350 microns. Our observations have an effective angular\nresolution of 10\", approximately 2.5 times higher than observations at the same\nwavelength obtained with the Herschel Space Observatory, albeit with lower\nsensitivity, especially to extended emission. The catalog includes 81 maps\ncovering a total of 164 detected sources. For each detected source, we tabulate\nbasic source properties including position, peak intensity, flux density in\nfixed apertures, and radius. We examine the uncertainties in the pointing model\napplied to all SHARC-II data and conservatively find that the model corrections\nare good to within ~3\", approximately 1/3 of the SHARC-II beam. We examine the\ndifferences between two array scan modes and find that the instrument\ncalibration, beam size, and beam shape are similar between the two modes. We\nalso show that the same flux densities are measured when sources are observed\nin the two different modes, indicating that there are no systematic effects\nintroduced into our catalog by utilizing two different scan patterns during the\ncourse of taking observations. We find a detection rate of 95% for protostellar\ncores but only 45% for starless cores, and demonstrate the existence of a\nSHARC-II detection bias against all but the most massive and compact starless\ncores. Finally, we discuss the improvements in protostellar classification\nenabled by these 350 micron observations.\n",
        "  We show that any smooth, closed, oriented, connected 4--manifold can be\ntrisected into three copies of $\\natural^k (S^1 \\times B^3)$, intersecting\npairwise in 3--dimensional handlebodies, with triple intersection a closed\n2--dimensional surface. Such a trisection is unique up to a natural\nstabilization operation. This is analogous to the existence, and uniqueness up\nto stabilization, of Heegaard splittings of 3--manifolds. A trisection of a\n4--manifold $X$ arises from a Morse 2--function $G:X \\to B^2$ and the obvious\ntrisection of $B^2$, in much the same way that a Heegaard splitting of a\n3--manifold $Y$ arises from a Morse function $g : Y \\to B^1$ and the obvious\nbisection of $B^1$.\n",
        "  We present the first evidence of clear signatures of tidal distortions in the\ndensity distribution of the fascinating open cluster NGC~6791. We find that the\n2D density map shows a clear elongation and an irregular distribution starting\nfrom $\\sim 300^{\\prime\\prime}$ from the cluster center and two tails extending\nin opposite directions beyond the tidal radius. These features are aligned to\nboth the absolute proper motion and to the Galactic centre directions.\nAccordingly we find that both the surface brightness and star count density\nprofiles reveal a departure from a King model starting from\n$\\sim600^{\\prime\\prime}$. These observational evidences suggest that NGC~6791\nis currently undergoing mass-loss likely due to gravitational shocking and\ninteractions with the tidal field of the Milky Way. We derive the expected\nmass-loss due to stellar evolution and tidal interactions and we estimate the\ninitial cluster mass to be $M_{ini} = (1.5-4.0 ) \\times 10^5 M_{\\odot}$.\n",
        "  We investigate the use of different syntactic dependency representations in a\nneural relation classification task and compare the CoNLL, Stanford Basic and\nUniversal Dependencies schemes. We further compare with a syntax-agnostic\napproach and perform an error analysis in order to gain a better understanding\nof the results.\n",
        "  Neural network-based dialog systems are attracting increasing attention in\nboth academia and industry. Recently, researchers have begun to realize the\nimportance of speaker modeling in neural dialog systems, but there lacks\nestablished tasks and datasets. In this paper, we propose speaker\nclassification as a surrogate task for general speaker modeling, and collect\nmassive data to facilitate research in this direction. We further investigate\ntemporal-based and content-based models of speakers, and propose several\nhybrids of them. Experiments show that speaker classification is feasible, and\nthat hybrid models outperform each single component.\n",
        "  We investigate the characteristics of factual and emotional argumentation\nstyles observed in online debates. Using an annotated set of \"factual\" and\n\"feeling\" debate forum posts, we extract patterns that are highly correlated\nwith factual and emotional arguments, and then apply a bootstrapping\nmethodology to find new patterns in a larger pool of unannotated forum posts.\nThis process automatically produces a large set of patterns representing\nlinguistic expressions that are highly correlated with factual and emotional\nlanguage. Finally, we analyze the most discriminating patterns to better\nunderstand the defining characteristics of factual and emotional arguments.\n",
        "  Traditionally, studies of coevolving systems have considered cases where a\nparasite may inhabit only a single host. The case where a parasite may infect\nmany hosts, widespread parasitism, has until recently gained little traction.\nThis is due in part to the computational complexity involved in reconstructing\nthe coevolutionary histories where parasites may infect only a single host,\nwhich is NP-Hard. Allowing parasites to inhabit more than one host has been\nseen to only further compound this computationally intractable problem.\nRecently however, well-established algorithms for estimating the problem\ninstance where a parasite may infect only a single host have been extended to\nhandle widespread parasites. Although this has offered significant progress, it\nhas been noted that these algorithms poorly handle parasites that inhabit\nphylogenetically distant hosts.\n  In this work we extend these previous algorithms to handle cases where\nparasites inhabit phylogenetically distant hosts using an additional\nevolutionary event which we call spread. Our new framework is shown to infer\nsignificantly more congruent coevolutionary histories compared to existing\nmethods over both synthetic and biological data sets. We then apply the newly\nproposed algorithm, which we call WiSPA (WideSpread Parasitism Analyser), to\nthe well studied coevolutionary system of Primates and Enterobius (pinworms),\nwhere existing methods have been unable to reconcile the widespread parasitism\npresent without permitting additional divergence events. Using WiSPA and the\nnew biological event, spread, we provide the first statistically significant\ncoevolutionary hypothesis for this system.\n",
        "  In tropical regions, fires propagate readily in grasslands but typically\nconsume only edges of forest patches. Thus forest patches grow due to tree\npropagation and shrink by fires in surrounding grasslands. The interplay\nbetween these competing edge effects is unknown, but critical in determining\nthe shape and stability of individual forest patches, as well the\nlandscape-level spatial distribution and stability of forests. We analyze\nhigh-resolution remote-sensing data from protected areas of the Brazilian\nCerrado and find that forest shapes obey a robust perimeter-area scaling\nrelation across climatic zones. We explain this scaling by introducing a\nheterogeneous fire propagation model of tropical forest-grassland ecotones.\nDeviations from this perimeter-area relation determine the stability of\nindividual forest patches. At a larger scale, our model predicts that the\nrelative rates of tree growth due to propagative expansion and long-distance\nseed dispersal determine whether collapse of regional-scale tree cover is\ncontinuous or discontinuous as fire frequency changes.\n",
        "  X-ray fluorescence computed tomography (XFCT) and K-edge computed tomography\n(CT) are two important modalities to quantify a distribution of gold\nnanoparticles (GNPs) in a small animal for preclinical studies. It is valuable\nto determine which modality is more efficient for a given application. In this\npaper, we report a theoretical analysis in terms of signal-to-noise ratio (SNR)\nfor the two modalities, showing that there is a threshold for the GNPs\nconcentration and XFCT has a better SNR than K-edge CT if GNPs concentration is\nless than this threshold. Numerical simulations are performed and two kinds of\nphantoms are used to represent multiple concentration levels and feature sizes.\nExperimental results illustrate that XFCT is superior to K-edge CT when\ncontrast concentration is lower than 0.4% which coincides with the theoretical\nanalysis.\n",
        "  Coevolution is expected to follow two alternative dynamics, often called\ntrench warfare and arms races in plant-pathogen systems. Trench warfare\nsituations are stable cycles of allele frequencies at the coevolving loci of\nboth host and parasite, and it is predicted that the loci will show molecular\nevolutionary signatures of balancing selection, while arms races involve\nsuccessive selective sweeps at the interacting loci. We study a haploid\ngene-for-gene model that includes mutation and genetic drift due to finite\npopulation size. We study the outcomes under different coevolutionary\nparameters to quantify the frequency of fixation of alleles, i.e. occurrence of\nan arms race dynamics. We find that contrary to the conventional wisdom, trench\nwarfare situations do not imply larger numbers of coevolutionary cycles per\nunit time than arms races. Therefore, one cannot infer the nature of the\ndynamics in such systems based on the speed of coevolution estimated from cycle\ntimes. We subsequently perform coalescent simulations to generate sequences at\nthe host and parasite loci. We ask whether the signatures expected under\nbalancing selection or selective sweeps (unexpectedly high or low diversity,\nand high or low Tajima D values, respectively) are likely to be observable in\ngenomic data. Genomic footprints of recurrent selective sweeps are often found,\nwhereas trench warfare yields signatures of balancing selection only in\nparasite sequences, and only in a limited parameter space with high effective\npopulation sizes and long-term selection. Therefore, the existence of a\ndeterministic polymorphic equilibrium does not imply long-term trench warfare\nnecessary for the signature of balancing selection to be observed in the\ncoevolving genes sequence. Our results suggest that to search for signatures of\ncoevolution via population genomics, it is best to study pathogen rather than\nhost genomes.\n",
        "  Many applications rely on Web data and extraction systems to accomplish\nknowledge-driven tasks. Web information is not curated, so many sources provide\ninaccurate, or conflicting information. Moreover, extraction systems introduce\nadditional noise to the data. We wish to automatically distinguish correct data\nand erroneous data for creating a cleaner set of integrated data. Previous work\nhas shown that a na\\\"ive voting strategy that trusts data provided by the\nmajority or at least a certain number of sources may not work well in the\npresence of copying between the sources. However, correlation between sources\ncan be much broader than copying: sources may provide data from complementary\ndomains (\\emph{negative correlation}), extractors may focus on different types\nof information (\\emph{negative correlation}), and extractors may apply common\nrules in extraction (\\emph{positive correlation, without copying}). In this\npaper we present novel techniques modeling correlations between sources and\napplying it in truth finding.\n",
        "  We construct cobordism maps on link Floer homology associated to decorated\nlink cobordisms. The maps are defined on a curved chain homotopy type\ninvariant. We describe the construction, and prove invariance. We also make a\ncomparison with the graph TQFT for Heegaard Floer homology.\n",
        "  Flattening filter free (FFF) beams due to their non-uniformity, are\nsub-optimal for larger field sizes. The purpose of this study was to\ninvestigate the incident electron beam distributions that would produce flat\nFFF beams without the use of flattening filter. Monte Carlo (MC) simulations\nwith BEAMnrc and DOSXYZnrc codes have been performed to evaluate the\nfeasibility of this approach. The dose distributions in water for open 6MV\nbeams were simulated using Varian 21EX linac head model, which will be called\nflattening filter (FF) model. Flattening filter has then been removed from FF\nmodel, and MC simulations were performed using (1) 6 MeV electrons incident on\nthe target, (2) 6 MeV electron beam with electron angular distributions\noptimized to provide as flat dose profiles as possible. Configuration (1)\nrepresents FFF beam while configuration (2) allowed producing a flat FFF (F4)\nbeam. Optimizations have also been performed to produce flattest profiles for a\nset of dose rates (DRs) in the range from 1.25 to 2.4 of the DR of FF beam.\nProfiles and percentage depth doses PDDs from 6MV F4 beams have been calculated\nand compared to those from FF beam. Calculated profiles demonstrated improved\nflatness of the FFF beams. In fact, up to field sizes within the circle of 35\ncm diameter the flatness of F4 beam at dmax was better or comparable to that of\nFF beam. At 20 cm off-axis the dose increased from 52% for FFF to 92% for F4\nbeam. Also, profiles of F4 beams did not change considerably with depth, and\nfor large fields out-of-field dose was reduced by about a factor of two\ncompared to FF beam. PDDs from F4 beams were similar to those of FFF beam. The\nDR for the largest modeled (44 cm diameter) F4 beam was higher than the DR from\nFF beam by a factor of 1.25. It was shown that the DR can be increased while\nmaintaining beam flatness, but at the cost of reduced field size.\n",
        "  When mutation rates are low, natural selection remains effective, and\nincreasing the mutation rate can give rise to an increase in adaptation rate.\nWhen mutation rates are high to begin with, however, increasing the mutation\nrate may have a detrimental effect because of the overwhelming presence of\ndeleterious mutations. Indeed, if mutation rates are high enough: 1) adaptation\nrate can become negative despite the continued availability of adaptive and/or\ncompensatory mutations, or 2) natural selection may be disabled because\nadaptive and/or compensatory mutations -- whether established or newly-arising\n-- are eroded by excessive mutation and decline in frequency. We apply these\ntwo criteria to a standard model of asexual adaptive evolution and derive\nmathematical expressions -- some new, some old in new guise -- delineating the\nmutation rates under which either adaptive evolution or natural selection is\nneutralized. The expressions are simple and require no \\emph{a priori}\nknowledge of organism- and/or environment-specific parameters. Our discussion\nconnects these results to each other and to previous theory, showing\nconvergence or equivalence of the different results in most cases.\n",
        "  We study the problem of group linkage: linking records that refer to entities\nin the same group. Applications for group linkage include finding businesses in\nthe same chain, finding conference attendees from the same affiliation, finding\nplayers from the same team, etc. Group linkage faces challenges not present for\ntraditional record linkage. First, although different members in the same group\ncan share some similar global values of an attribute, they represent different\nentities so can also have distinct local values for the same or different\nattributes, requiring a high tolerance for value diversity. Second, groups can\nbe huge (with tens of thousands of records), requiring high scalability even\nafter using good blocking strategies.\n  We present a two-stage algorithm: the first stage identifies cores containing\nrecords that are very likely to belong to the same group, while being robust to\npossible erroneous values; the second stage collects strong evidence from the\ncores and leverages it for merging more records into the same group, while\nbeing tolerant to differences in local values of an attribute. Experimental\nresults show the high effectiveness and efficiency of our algorithm on various\nreal-world data sets.\n",
        "  We propose tunable superconducting split-ring resonators (SRRs) employing\nnonlinear Josephson inductance. A fraction of SRR is replaced by Nb-AlOx-Nb\nJosephson tunnel junctions connected in parallel and forming a superconducting\nquantum interference device (SQUID), whose inductance is sensitive to the\nexternal dc magnetic field. Due to the lumped nature of the Josephson\ninductance, the SRR can be made very compact and its resonance frequency can be\ntuned by applying magnetic field. We present the model, results of extensive\nEM-simulation and experimental data for the SRR weakly coupled to a\ntransmission line within frequency range 11-13 GHz.\n",
        "  We show that a random link defined by random bridge splitting is hyperbolic\nwith asymptotic probability 1.\n",
        "  The Andreev reflection between a normal metal (N) and a heavy-fermion\nsuperconductor (HFS) is studied and the boundary conditions for the electron's\nwave function in the two systems are established in the framework of a two band\nmodel for the HFS. Hence we show in a simple and explicit way that the mass\nenhancement factors in the heavy-fermion (HF) metal do not cause impedance at\nthe N/HFS interface, in accordance with arguments previously presented. We also\npresent an extension of the theory to a two-fluid model for the heavy-fermion,\nas possibly applicable to, e.g., CeCoIn_5.\n",
        "  Several dedicated surveys focusing on early-type galaxies (ETGs) reveal that\nsignificant fractions of them are detectable in all interstellar medium phases\nstudied to date. We select ETGs from the Herschel Reference Survey that have\nboth far-infrared Herschel and either HI or CO detection (or both). We derive\ntheir star formation rates (SFR), stellar masses and dust masses via modelling\ntheir spectral energy distributions. We combine these with literature\ninformation on their atomic and molecular gas properties, in order to relate\ntheir star formation, total gas mass and dust mass on global scales. The ETGs\ndeviate from the dust mass-SFR relation and the Schmidt-Kennicutt relation that\nSDSS star forming galaxies define: compared to SDSS galaxies, ETGs have more\ndust at the same SFR, or less SFR at the same dust mass. When placing them in\nthe M*-SFR plane, ETGs show a much lower specific SFR as compared to normal\nstar-forming galaxies. ETGs show a large scatter compared to the\nSchmidt-Kennicutt relation found locally within our Galaxy, extending to lower\nSFRs and gas mass surface densities. Using an ETG's SFR and the\nSchmidt-Kennicutt law to predict its gas mass leads to an underestimate. ETGs\nhave similar observed-gas-to-modelled-dust mass ratios to star forming-galaxies\nof the same stellar mass, as well as they exhibit a similar scatter.\n",
        "  Given a knot K and an irreducible metabelian SL(n,C) representation we\nestablish an equality for the dimension of the first twisted cohomology. In the\ncase of equality, we prove that the representation must have finite image and\nthat it is conjugate to an SU(n) representation. In this case we show it\ndetermines a smooth point x in the SL(n,C) character variety, and we use a\ndeformation argument to establish the existence of a smooth (n-1)-dimensional\nfamily of characters of irreducible SL(n,C) representations near x.\n  Combining this with our previous existence results, we deduce the existence\nof large families of irreducible SU(n) and SL(n,C) non-metabelian\nrepresentation for knots K in homology 3-spheres S with nontrivial Alexander\npolynomial.\n  We then relate the condition on twisted cohomology to a more accessible\ncondition on untwisted cohomology of a certain metabelian branched cover of S\nbranched along K.\n",
        "  Atomic commit protocols are used where data integrity is more important than\ndata availability. Two-Phase commit (2PC) is a standard commit protocol for\ncommercial database management systems. To reduce certain drawbacks in 2PC\nprotocol people have suggested different variance of this protocol.\nShort-Commit protocol is developed with an objective to achieve low cost\ntransaction commitment cost with non-blocking capability. In this paper we have\nbriefly explained short-commit protocol executing pattern. Experimental\nanalysis and results are presented to support the claim that short-commit can\nwork efficiently in extreme database environment.\n",
        "  We compile position and inclination angles for tilted ring fits to the warped\ndusty and gaseous disk spanning radius 1.8 to 6500 pc from recent observations.\nFor radii exterior to a kpc, tilted ring orientations lie on an arc on a polar\ninclination versus position angle plot, suggesting that precession following a\nmerger can account for the ring morphology. Three kinks in the ring\norientations are seen on the polar plot, the one at radius of about 1.3 kpc we\nsuspect corresponds to the location where self-gravity in the disk affects the\nring precession rate. Another at a radius of about 600 pc may be associated\nwith a gap in the gas distribution. A third kink is seen at a radius of 100 pc.\nA constant inclination tilted disk precessing about the jet axis may describe\nthe disk between 100 and 20 pc but not interior to this. A model with disk\norientation matching the molecular circumnuclear disk at 100 pc that decays at\nsmaller radii to an inner flat disk perpendicular to the jet may account for\ndisk orientations within 100 pc. Neither model would account for the cusps or\nchanges in disk orientation at 100 or 600 pc.\n",
        "  Ejection fraction (EF) is commonly measured by echocardiography, by dividing\nthe volume ejected by the heart (stroke volume) by the volume of the filled\nheart (end-diastolic volume). Utilizing volume changes of left myocardial\nsegments per a cardiac cycle, physical laws and mathematical equations specific\nechocardiographic data, this paper serves to generalize EF by a novel parameter\nover the time that it can make available, more detailed, valuable and practical\ninformation to fully describe the left ventricular (LV) contractility\nfunction.Patients who underwent clinically-directed standard transthoracic\nechocardiography using 2D conventional echocardiography machines armed to\nmeasuring strain components, were asked to estimate displacements and\nlongitudinal, radial and circumferential strains for each LV echocardiographic\nsegments per a cardiac cycle. Volume fractional changes of the LV\nechocardiographic segments are expanded based on their strain components over\nthe time. Ejected blood volume fraction induced by a left myocardial sample, is\ncomputed within a cardiac cycle. Total fraction of the ejected blood volume in\nthe left ventricular cavity was obtained by integrating over the times and LV\nmyocardial segments. EF is an especial value of this total fraction at the end\nsystolic time. The common measurement of EF is only based on LV cavity volumes\nat the end diastolic and systolic phases. These findings lead to determine\ndetailed aspects of the left ventricular contraction. This generalized\nparameter has important implications to give the real value of EF in the sever\nMitral valve regurgitations.\n",
        "  The ability of large grain, REBa$_{2}$Cu$_{3}$O$_{7-\\delta}$ [(RE)BCO; RE =\nrare earth] bulk superconductors to trap magnetic field is determined by their\ncritical current. With high trapped fields, however, bulk samples are subject\nto a relatively large Lorentz force, and their performance is limited primarily\nby their tensile strength. Consequently, sample reinforcement is the key to\nperformance improvement in these technologically important materials. In this\nwork, we report a trapped field of 17.6 T, the largest reported to date, in a\nstack of two, silver-doped GdBCO superconducting bulk samples, each of diameter\n25 mm, fabricated by top-seeded melt growth (TSMG) and reinforced with\nshrink-fit stainless steel. This sample preparation technique has the advantage\nof being relatively straightforward and inexpensive to implement and offers the\nprospect of easy access to portable, high magnetic fields without any\nrequirement for a sustaining current source.\n",
        "  The offset method for solving word analogies has become a standard evaluation\ntool for vector-space semantic models: it is considered desirable for a space\nto represent semantic relations as consistent vector offsets. We show that the\nmethod's reliance on cosine similarity conflates offset consistency with\nlargely irrelevant neighborhood structure, and propose simple baselines that\nshould be used to improve the utility of the method in vector space evaluation.\n",
        "  Current approaches to cross-lingual sentiment analysis try to leverage the\nwealth of labeled English data using bilingual lexicons, bilingual vector space\nembeddings, or machine translation systems. Here we show that it is possible to\nuse a single linear transformation, with as few as 2000 word pairs, to capture\nfine-grained sentiment relationships between words in a cross-lingual setting.\nWe apply these cross-lingual sentiment models to a diverse set of tasks to\ndemonstrate their functionality in a non-English context. By effectively\nleveraging English sentiment knowledge without the need for accurate\ntranslation, we can analyze and extract features from other languages with\nscarce data at a very low cost, thus making sentiment and related analyses for\nmany languages inexpensive.\n",
        "  Purpose: Photon counting (PC) computed tomography (CT) can provide material\nselective CT imaging at lowest patient dose but it suffers from suboptimal\ncount rate. A dynamic beam attenuator (DBA) can help with count rate by\nmodulating x-ray beam intensity such that the low attenuating areas of the\npatient receive lower exposure, and detector behind these areas is not\noverexposed. However, DBA may harden the beam and cause artifacts and errors.\nThis work investigates positive and negative effects of using DBA in PCCT.\nMethods: A simple PCCT with single energy bin, spectroscopic PCCT with 2 and 5\nenergy bins, and conventional energy integrating CT with and without DBA were\nsimulated and investigated using 120kVp tube voltage and 14mGy air dose. The\nDBAs were modeled as made from soft tissue (ST) equivalent material, iron (Fe),\nand holmium (Ho) K-edge material. A cylindrical CT phantom and chest phantom\nwith iodine and CaCO3 contrast elements were used. Image artifacts and\nquantification errors in general and material decomposed CT were determined.\nResults: Simple PCCT exhibited major image artifacts and quantification errors\nwhen DBAs were used. The artifacts and errors were decreased with 2bin\nspectroscopic PCCT and nearly eliminated with 5bin spectroscopic CT. The photon\nstarvation noise did present with Fe-DBA due to strong absorption of lower\nenergy photons. The 5bin PCCT with ST-DBA and Ho-DBA were nearly free of\nartifacts and photon starvation noise. The Ho-DBA better preserved low energy\nphotons and decreased beam hardening artifacts, and improved material\ndecomposition. Also, the Ho-DBA was also miniature having 1.4mm thickness and\n2cm length, much smaller than Fe-DBA. Conclusion: DBA fabricated from K-edge\nmaterial such as Ho can address count rate problem of PCCT and provide\nminiature size, minimal image artifacts, and improved material decomposition at\nlowest patient dose.\n",
        "  The Moran process on graphs is a popular model to study the dynamics of\nevolution in a spatially structured population. Exact analytical solutions for\nthe fixation probability and time of a new mutant have been found for only a\nfew classes of graphs so far. Simulations are time-expensive and many\nrealizations are necessary, as the variance of the fixation times is high. We\npresent an algorithm that numerically computes these quantities for arbitrary\nsmall graphs by an approach based on the transition matrix. The advantage over\nsimulations is that the calculation has to be executed only once. Building the\ntransition matrix is automated by our algorithm. This enables a fast and\ninteractive study of different graph structures and their effect on fixation\nprobability and time. We provide a fast implementation in C with this note\nhttps://github.com/hindersin/efficientFixation. Our code is very flexible, as\nit can handle two different update mechanisms (Birth-death or death-Birth), as\nwell as arbitrary directed or undirected graphs.\n",
        "  The ratio of males to females in a population is a meaningful characteristic\nof sexual species. The reason for this biological property to be available to\nthe observers of nature seems to be a question never asked. Introducing the\nnotion of historically adapted populations as global minimizers of maintenance\ncost functions, we propose a theoretical explanation for the reported stability\nof this feature. This mathematical formulation suggests that sex ratio could be\nconsidered as an indirect result shaped by the antagonism between the size of\nthe population and the finiteness of resources.\n",
        "  Purpose: To improve the delivery efficiency of VMAT by extending the recently\npublished VMAT treatment planning algorithm vmerge to automatically generate\noptimal partial-arc plans.\n  Methods and materials: A high-quality initial plan is created by solving a\nconvex multicriteria optimization problem using 180 equi-spaced beams. This\ninitial plan is used to form a set of dose constraints, and a set of\npartial-arc plans is created by searching the space of all possible partial-arc\nplans that satisfy these constraints. For each partial-arc, an iterative\nfluence map merging and sequencing algorithm (vmerge) is used to improve the\ndelivery efficiency. Merging continues as long as the dose quality is\nmaintained above a user-defined threshold. The final plan is selected as the\npartial arc with the lowest treatment time. The complete algorithm is called\npmerge.\n  Results: Partial-arc plans are created using pmerge for a lung, liver and\nprostate case, with final treatment times of 127, 245 and 147 seconds.\nTreatment times using full arcs with vmerge are 211, 357 and 178 seconds. Dose\nquality is maintained across the initial, vmerge, and pmerge plans to within 5%\nof the mean doses to the critical organs-at-risk and with target coverage above\n98%. Additionally, we find that the angular distribution of fluence in the\ninitial plans is predictive of the start and end angles of the optimal\npartial-arc.\n  Conclusions: The pmerge algorithm is an extension to vmerge that\nautomatically finds the partial-arc plan that minimizes the treatment time.\nVMAT delivery efficiency can be improved by employing partial-arcs without\ncompromising dose quality. Partial arcs are most applicable to cases with\nnon-centralized targets, where the time savings is greatest.\n",
        "  We use computational simulations to compare the impact response of different\nfootball and U.S. Army helmet pad materials. We conduct experiments to\ncharacterize the material response of different helmet pads. We simulate\nexperimental helmet impact tests performed by the U.S. Army to validate our\nmethods. We then simulate a cylindrical impactor striking different pads. The\nacceleration history of the impactor is used to calculate the Head Injury\nCriterion for each pad. We conduct sensitivity studies exploring the effects of\npad composition, geometry, and material stiffness. We find that: (1) The\nfootball pad materials do not outperform the currently used military pad\nmaterial in militarily-relevant impact scenarios; (2) Optimal material\nproperties for a pad depend on impact energy; and (3) Thicker pads perform\nbetter at all velocities. Our analysis suggests that by using larger helmet\nshells with correspondingly thicker pads, impact-induced traumatic brain injury\nmay be significantly reduced.\n  Keywords: helmet, pad, head injury, traumatic brain injury, head injury\ncriterion, impact\n",
        "  High-volume, high-speed data streams may overwhelm the capabilities of stream\nprocessing systems; techniques such as data prioritization, avoidance of\nunnecessary processing and on-demand result production may be necessary to\nreduce processing requirements. However, the dynamic nature of data streams, in\nterms of both rate and content, makes the application of such techniques\nchallenging. Such techniques have been addressed in the context of static and\ncentralized query optimization; however, they have not been fully addressed for\ndata stream management systems. In this work, we present a comprehensive\nframework that supports prioritization, avoidance of unnecessary work, and\non-demand result production over distributed, unreliable, bursty, disordered\ndata sources, typical of many data streams. We propose a form of inter-operator\nfeedback, which flows against the stream direction, to communicate the\ninformation needed to enable execution of these techniques. This feedback\nleverages punctuations to describe the subsets of interest. We identify\npotential sources of feedback information, characterize new types of\npunctuation to support feedback, and describe the roles of producers,\nexploiters, and relayers of feedback that query operators may implement. We\npresent initial experimental observations using the NiagaraST data-stream\nsystem.\n",
        "  Metallic implants introduce severe artifacts in CT images, which degrades the\nimage quality. It is an effective method to reduce metal artifacts by replacing\nthe metal affected projection with the forward projection of a prior image. How\nto find a good prior image is the key of this class methods, and numerous\nalgorithms have been proposed to address this issue recently. In this work, by\nusing image mutual correlation, pixels in the original reconstructed image or\nlinear interpolation corrected image, which are less affected by artifacts, are\nselected to build a combined image. Thereafter, a better prior image is\ngenerated from the combined image by using tissue classification. The results\nof three patients' CT images show that the proposed method can reduce metal\nartifacts remarkably.\n",
        "  We intend to identify relationships between cancer cases and pollutant\nemissions and attempt to understand whether cancer in children is typically\nlocated together with some specific chemical combinations or is independent.\nCo-location pattern analysis seems to be the appropriate investigation to\nperform. Co-location mining is one of the tasks of spatial data mining which\nfocuses on the detection of co-location patterns, the sets of spatial features\nfrequently located in close proximity of each other. Most previous works are\nbased on transaction-free apriori-like algorithms which are dependent on\nuser-defined thresholds and are designed for boolean data points. Due to the\nabsence of a clear notion of transactions, it is nontrivial to use association\nrule mining techniques to tackle the co-location mining problem. The approach\nwe propose is based on a grid \"transactionization\" of the geographic space and\nis designed to mine datasets with extended spatial objects. Uncertainty of the\nfeature presence in transactions is taken into account in our model. The\nstatistical test is used instead of global thresholds to detect significant\nco-location patterns and rules. We evaluate our approach on synthetic and real\ndatasets. This approach can be used by researchers looking for spatial\nassociations between environmental and health factors. In addition, we explain\nthe data modelling framework which is used on real datasets of pollutants\n(PRTR/NPRI) and childhood cancer cases.\n",
        "  We compare two different types of mapping class invariants: the Hochschild\nhomology of an $A_\\infty$ bimodule coming from bordered Heegaard Floer\nhomology, and fixed point Floer cohomology. We first compute the bimodule\ninvariants and their Hochschild homology in the genus two case. We then compare\nthe resulting computations to fixed point Floer cohomology, and make a\nconjecture that the two invariants are isomorphic. We also discuss a\nconstruction of a map potentially giving the isomorphism. It comes as an\nopen-closed map in the context of a surface being viewed as a $0$-dimensional\nLefschetz fibration over the complex plane.\n",
        "  Hierarchical structures and size distribution of star formation regions in\nthe nearby spiral galaxy NGC 628 are studied over a range of scale from 50 to\n1000 pc using optical images obtained with 1.5 m telescope of the Maidanak\nObservatory. We found hierarchically structured concentrations of star\nformation regions in the galaxy, smaller regions with a higher surface\nbrightness are located inside larger complexes having a lower surface\nbrightness. We illustrate this hierarchy by dendrogram, or structure tree of\nthe detected star formation regions, which demonstrates that most of these\nregions are combined into larger structures over several levels. We found three\ncharacteristic sizes of young star groups: 65 pc (OB associations), 240 pc\n(stellar aggregates) and 600 pc (star complexes). The cumulative size\ndistribution function of star formation regions is found to be a power law with\na slope of approximately -1.5 on scales appropriate to diameters of\nassociations, aggregates and complexes. This slope is close to the slope which\nwas found earlier by B. Elmegreen et al. for star formation regions in the\ngalaxy on scales from 2 to 100 pc.\n",
        "  The evolutionary significance of hybridization and subsequent introgression\nhas long been appreciated, but evaluation of the genome-wide effects of these\nphenomena has only recently become possible. Crop-wild study systems represent\nideal opportunities to examine evolution through hybridization. For example,\nmaize and the conspecific wild teosinte Zea mays ssp. mexicana, (hereafter,\nmexicana) are known to hybridize in the fields of highland Mexico. Despite\nwidespread evidence of gene flow, maize and mexicana maintain distinct\nmorphologies and have done so in sympatry for thousands of years. Neither the\ngenomic extent nor the evolutionary importance of introgression between these\ntaxa is understood. In this study we assessed patterns of genome-wide\nintrogression based on 39,029 single nucleotide polymorphisms genotyped in 189\nindividuals from nine sympatric maize-mexicana populations and reference\nallopatric populations. While portions of the maize and mexicana genomes were\nparticularly resistant to introgression (notably near known\ncross-incompatibility and domestication loci), we detected widespread evidence\nfor introgression in both directions of gene flow. Through further\ncharacterization of these regions and preliminary growth chamber experiments,\nwe found evidence suggestive of the incorporation of adaptive mexicana alleles\ninto maize during its expansion to the highlands of central Mexico. In\ncontrast, very little evidence was found for adaptive introgression from maize\nto mexicana. The methods we have applied here can be replicated widely, and\nsuch analyses have the potential to greatly informing our understanding of\nevolution through introgressive hybridization. Crop species, due to their\nexceptional genomic resources and frequent histories of spread into sympatry\nwith relatives, should be particularly influential in these studies.\n",
        "  Deep learning methods employ multiple processing layers to learn hierarchical\nrepresentations of data and have produced state-of-the-art results in many\ndomains. Recently, a variety of model designs and methods have blossomed in the\ncontext of natural language processing (NLP). In this paper, we review\nsignificant deep learning related models and methods that have been employed\nfor numerous NLP tasks and provide a walk-through of their evolution. We also\nsummarize, compare and contrast the various models and put forward a detailed\nunderstanding of the past, present and future of deep learning in NLP.\n",
        "  Clustering is an important data mining technique where we will be interested\nin maximizing intracluster distance and also minimizing intercluster distance.\nWe have utilized clustering techniques for detecting deviation in product sales\nand also to identify and compare sales over a particular period of time.\nClustering is suited to group items that seem to fall naturally together, when\nthere is no specified class for any new item. We have utilizedannual sales data\nof a steel major to analyze Sales Volume & Value with respect to dependent\nattributes like products, customers and quantities sold. The demand for steel\nproducts is cyclical and depends on many factors like customer profile,\nprice,Discounts and tax issues. In this paper, we have analyzed sales data with\nclustering algorithms like K-Means&EMwhichrevealed many interesting\npatternsuseful for improving sales revenue and achieving higher sales volume.\nOur study confirms that partition methods like K-Means & EM algorithms are\nbetter suited to analyze our sales data in comparison to Density based methods\nlike DBSCAN & OPTICS or Hierarchical methods like COBWEB.\n",
        "  The longitudinal resistivity (rho_{xx}) and transverse resistivity (rho_{xy})\nof MgB2 thin films in the mixed state were studied in detail. We found that the\ntemperature dependencies of rho_{xx} and \\rho_{xy} at a fixed magnetic field\n(H) satisfy the scaling law of $\\rho_{xy}=A\\rho_{xx}^\\beta$, where the exponent\nbeta varies around 2.0 for different fields. In the low field region (below\n1T), beta maintains a constant value of 2.0 due to the weak pinning strength of\nthe vortices, mainly from the superfluid of the pi band. When H>1T, beta drops\nabruptly to its lowest value at about 2T because of the proliferation of\nquasiparticles from the pi-band and, hence, the motion of the vortices from the\nsuperfluid of the sigma-band dominates the dissipation. As the field is\nincreased further, the vortex pinning strength is weakened and beta increases\nmonotonically towards 2.0 at a high field. All the results presented here are\nin good agreement with the expectation of the vortex physics of a multi-band\nsuperconductor.\n",
        "  One method for obtaining every closed orientable 3-manifold is as branched\ncovering of the 3-sphere over a link. There is a classical topological result\nshowing that the minimun possible number of sheets in the covering is three. In\nthis paper we obtain a geometric version of this result. The interest is given\nby the growing importance of geometry in 3-manifolds theory.\n",
        "  We generalize the Morton-Franks-Williams inequality to the colored\n$\\mathfrak{sl}(N)$ link homology defined in arXiv:0907.0695, which gives\ninfinitely many new bounds for the braid index and the self linking number. A\nkey ingredient of our proof is a composition product for the general MOY graph\npolynomial, which generalizes that of Wagner arXiv:math/0702230v1.\n",
        "  Established image recovery methods in fast ultrasound imaging, e.g.\ndelay-and-sum, trade the image quality for the high frame rate. Cutting-edge\ninverse scattering methods based on compressed sensing (CS) disrupt this\ntradeoff via a priori information. They iteratively recover a high-quality\nimage from only a few sequential pulse-echo measurements or less echo signals,\nif (i) a known dictionary of structural building blocks represents the image\nalmost sparsely, and (ii) their individual pulse echoes, which are predicted by\na linear model, are sufficiently uncorrelated. The exclusive modeling of the\nincident waves as steered plane waves or cylindrical waves, however, has so far\nlimited the convergence speed, the image quality, and the potential to meet\ncondition (ii). Motivated by the benefits of randomness in CS, a novel method\nfor the fast compressed acquisition and the subsequent recovery of images is\nproposed to overcome these limitations. It recovers the spatial compressibility\nfluctuations in weakly-scattering soft tissue structures, where an orthonormal\nbasis meets condition (i), by a sparsity-promoting $\\ell_{q}$-minimization\nmethod, $q \\in [0; 1]$. A realistic $d$-dimensional model, $d \\in \\{2, 3\\}$,\naccounting for diffraction, single monopole scattering, the combination of\npower-law absorption and dispersion, and the specifications of a planar\ntransducer array, predicts the pulse echoes of the individual basis functions.\nThree innovative types of incident waves, whose syntheses leverage random\napodization weights, time delays, or combinations thereof, aid in meeting\ncondition (ii). In two-dimensional numerical simulations, single realizations\nof these waves outperform the prevalent quasi-plane wave for both the canonical\nand the Fourier bases. They significantly reduce the full extents at half\nmaximum of the point spread functions by up to 73.7 %.\n",
        "  An oriented link is positive if it has a link diagram whose crossings are all\npositive. An oriented link is almost positive if it is not positive and has a\nlink diagram with exactly one negative crossing. It is known that the Rasmussen\ninvariant, $4$-genus and $3$-genus of a positive knot are equal. In this paper,\nwe prove that the Rasmussen invariant, $4$-genus and $3$-genus of an almost\npositive knot are equal. Moreover, we determine the Rasmussen invariant of an\nalmost positive knot in terms of its almost positive knot diagram. As\ncorollaries, we prove that any almost positive knot is not homogeneous, and\nthere is no almost positive knot of $4$-genus one.\n",
        "  Our purpose is to measure the internal radiation dose (ID) using human blood\nsample. In the literature, there is no process that allows the direct\nmeasurement of ID received by a person. This study has shown that it is\npossible to determine ID in human blood exposed to internal or external\nionizing radiation treatment both directly and retrospectively. OSL technique\nwas used to measure the total dose from the blood sample. OSL counts from the\nwaste blood of the patient injected with a radiopharmaceutical for diagnostic\nor treatment purposes and from a blood sample having a laboratory-injected\nradiation dose were both used for measurements. The decay and dose-response\ncurves (DRC) were plotted for different doses. The doses received by different\nblood aliquots have been determined by interpolating the natural luminescence\ncounts to DRC. In addition, OSL counts from a healthy blood sample exposed to\nan external radiation source were measured. The blood aliquots were given\ndifferent 0-200Gy beta doses and their decay and dose-response curves were\nplotted. The internal dose received by the blood aliquots injected with\nradioisotope was determined by interpolating the natural luminescence counts to\nDRC. The internal dose values were found as 0.46Gy and 0.51Gy for different\ndose range. The blood aliquots were exposed to different external laboratory\ndoses. The internal dose values corresponding to 10Gy laboratory dose from the\naliquots exposed to external radiation were found as 10.94Gy for Disc3 and\n~10.79Gy for Disc1.This study shows that the dose received by a person can be\nmeasured directly, simply and retrospectively by using only a very small amount\nof blood sample. The results will have important ramifications for the medicine\nand healthcare fields in particular.\n",
        "  The stream of words produced by Automatic Speech Recognition (ASR) systems is\ntypically devoid of punctuations and formatting. Most natural language\nprocessing applications expect segmented and well-formatted texts as input,\nwhich is not available in ASR output. This paper proposes a novel technique of\njointly modeling multiple correlated tasks such as punctuation and\ncapitalization using bidirectional recurrent neural networks, which leads to\nimproved performance for each of these tasks. This method could be extended for\njoint modeling of any other correlated sequence labeling tasks.\n",
        "  Cooperative behavior is widespread in nature, even though cooperating\nindividuals always run the risk to be exploited by free-riders. Population\nstructure effectively promotes cooperation given that a threshold in the level\nof cooperation was already reached. However, the question how cooperation can\nemerge from a single mutant, which cannot rely on a benefit provided by other\ncooperators, is still puzzling. Here, we investigate this question for a\nwell-defined but generic situation based on typical life-cycles of microbial\npopulations where individuals regularly form new colonies followed by growth\nphases. We analyze two evolutionary mechanisms favoring cooperative behavior\nand study their strength depending on the inoculation size and the length of a\nlife-cycle. In particular, we find that population bottlenecks followed by\nexponential growth phases strongly increase the survival and fixation\nprobabilities of a single cooperator in a free-riding population.\n",
        "  Direct current (DC) characterization of high temperature superconducting\n(HTS) coils is important for HTS applications, such as electric machines,\nsuperconducting magnetic energy storage (SMES) and transformers. In this paper,\nDC characterization of a circular, epoxy-impregnated HTS coil made from YBCO\ncoated conductor for use as a prototype axial flux HTS electric machine is\npresented. Multiple voltage taps were utilized within the coil during\nmeasurement to help provide further detailed information on its DC behavior as\na function of length. Based on the experimental results, there exist regions of\nnon-uniformity along the length of superconductor in the coil, resulting in\nnon-ideal superconducting properties of the coil. By studying the\ncurrent-voltage (I-V) curves across different regions, it is found that a\ndecreasing n-value and critical current exists in the non-uniform parts of the\nHTS coil.\n",
        "  For data integration in information ecosystems, semantic heterogeneity is a\nknown difficulty. In this paper, we propose Shadow Theory as the philosophical\nfoundation to address this issue. It is based on the notion of shadows in\nPlato's Allegory of the Cave. What we can observe are just shadows, and\nmeanings of shadows are mental entities that only exist in viewers' cognitive\nstructures. With enterprise customer data integration example, we proposed six\ndesign principles and algebra to support required operations.\n",
        "  A graph is 2-apex if it is planar after the deletion of at most two vertices.\nSuch graphs are not intrinsically knotted, IK. We investigate the converse,\ndoes not IK imply 2-apex? We determine the simplest possible counterexample, a\ngraph on nine vertices and 21 edges that is neither IK nor 2-apex. In the\nprocess, we show that every graph of 20 or fewer edges is 2-apex. This provides\na new proof that an IK graph must have at least 21 edges. We also classify IK\ngraphs on nine vertices and 21 edges and find no new examples of minor minimal\nIK graphs in this set.\n",
        "  My system utilizes the outcomes feature found in Moodle and other learning\ncontent management systems (LCMSs) to keep track of where students are in terms\nof what language competencies they have mastered and the competencies they need\nto get where they want to go. These competencies are based on the Common\nEuropean Framework for (English) Language Learning. This data can be available\nfor everyone involved with a given student's progress (e.g. educators, parents,\nsupervisors and the students themselves). A given student's record of past\naccomplishments can also be meshed with those of his classmates. Not only are a\nstudent's competencies easily seen and tracked, educators can view competencies\nof a group of students that were achieved prior to enrollment in the class.\nThis should make curriculum decision making easier and more efficient for\neducators.\n",
        "  Word vector specialisation (also known as retrofitting) is a portable,\nlight-weight approach to fine-tuning arbitrary distributional word vector\nspaces by injecting external knowledge from rich lexical resources such as\nWordNet. By design, these post-processing methods only update the vectors of\nwords occurring in external lexicons, leaving the representations of all unseen\nwords intact. In this paper, we show that constraint-driven vector space\nspecialisation can be extended to unseen words. We propose a novel\npost-specialisation method that: a) preserves the useful linguistic knowledge\nfor seen words; while b) propagating this external signal to unseen words in\norder to improve their vector representations as well. Our post-specialisation\napproach explicits a non-linear specialisation function in the form of a deep\nneural network by learning to predict specialised vectors from their original\ndistributional counterparts. The learned function is then used to specialise\nvectors of unseen words. This approach, applicable to any post-processing\nmodel, yields considerable gains over the initial specialisation models both in\nintrinsic word similarity tasks, and in two downstream tasks: dialogue state\ntracking and lexical text simplification. The positive effects persist across\nthree languages, demonstrating the importance of specialising the full\nvocabulary of distributional word vector spaces.\n",
        "  We investigate the dynamics of cancer initiation in a mathematical model with\none driver mutation and several passenger mutations. Our analysis is based on a\nmulti type branching process: We model individual cells which can either divide\nor undergo apoptosis. In case of a cell division, the two daughter cells can\nmutate, which potentially confers a change in fitness to the cell. In contrast\nto previous models, the change in fitness induced by the driver mutation\ndepends on the genetic context of the cell, in our case on the number of\npassenger mutations. The passenger mutations themselves have no or only a very\nsmall impact on the cell's fitness. While our model is not designed as a\nspecific model for a particular cancer, the underlying idea is motivated by\nclinical and experimental observations in Burkitt Lymphoma. In this tumor, the\nhallmark mutation leads to deregulation of the MYC oncogene which increases the\nrate of apoptosis, but also the proliferation rate of cells. This increase in\nthe rate of apoptosis hence needs to be overcome by mutations affecting\napoptotic pathways, naturally leading to an epistatic fitness landscape. This\nmodel shows a very interesting dynamical behavior which is distinct from the\ndynamics of cancer initiation in the absence of epistasis. Since the driver\nmutation is deleterious to a cell with only a few passenger mutations, there is\na period of stasis in the number of cells until a clone of cells with enough\npassenger mutations emerges. Only when the driver mutation occurs in one of\nthose cells, the cell population starts to grow rapidly.\n",
        "  Gauge invariability guarantees the same form of the Maxwell equations in\ndifferent coordinate systems, and is instrumental for electromagnetic cloaking\nto hide a region of interest (ROI) perfectly. On the other hand, interior\ntomography is to reconstruct an ROI exactly. In this article, the recent\nresults in these two disconnected areas are brought together to justify the\ngeneral interior tomography principle. Several opportunities are suggested for\ntomographic research.\n",
        "  In multiorbital materials, superconductivity can exhibit new exotic forms\nthat include several coupled condensates. In this context, quantum confinement\nin two-dimensional superconducting oxide interfaces offers new degrees of\nfreedom to engineer the band structure and selectively control 3d-orbitals\noccupancy by electrostatic doping. However, the presence of multiple\nsuperconducting condensates in these systems has not yet been demonstrated.\nHere, we use resonant microwave transport to extract the superfluid stiffness\nof the (110)-oriented LaAlO3/SrTiO3 interface in the entire phase diagram. We\nevidence a transition from single-band to two-band superconductivity driven by\nelectrostatic doping, which we relate to the filling of the different\n3d-orbitals based on numerical simulations of the quantum well. Interestingly,\nthe superconducting transition temperature decreases while the second band is\npopulated, which challenges the Bardeen-Cooper-Schrieffer theory. To explain\nthis behaviour, we propose that the superconducting order parameters associated\nwith the two bands have opposite signs with respect to each other.\n",
        "  Bitmap indexes are commonly used in databases and search engines. By\nexploiting bit-level parallelism, they can significantly accelerate queries.\nHowever, they can use much memory, and thus we might prefer compressed bitmap\nindexes. Following Oracle's lead, bitmaps are often compressed using run-length\nencoding (RLE). Building on prior work, we introduce the Roaring compressed\nbitmap format: it uses packed arrays for compression instead of RLE. We compare\nit to two high-performance RLE-based bitmap encoding techniques: WAH (Word\nAligned Hybrid compression scheme) and Concise (Compressed `n' Composable\nInteger Set). On synthetic and real data, we find that Roaring bitmaps (1)\noften compress significantly better (e.g., 2 times) and (2) are faster than the\ncompressed alternatives (up to 900 times faster for intersections). Our results\nchallenge the view that RLE-based bitmap compression is best.\n",
        "  How the Milky Way has accumulated its mass over the Hubble time, whether\nsignificant amounts of gas and stars were accreted from satellite galaxies, or\nwhether the Milky Way has experienced an initial gas assembly and then evolved\nmore-or-less in isolation is one of the burning questions in modern astronomy,\nbecause it has consequences for our understanding of galaxy formation in the\ncosmological context. Here we present the evolutionary model of a Milky\nWay-type satellite system zoomed into a cosmological large-scale simulation.\nEmbedded into Dark Matter halos and allowing for baryonic processes these\nchemo-dynamical simulations aim at studying the gas and stellar loss from the\nsatellites to feed the Milky Way halo and the stellar chemical abundances in\nthe halo and the satellite galaxies.\n",
        "  By exploiting an analogy between population genetics and statistical\nmechanics, we study the evolution of a polygenic trait under stabilizing\nselection, mutation, and genetic drift. This requires us to track only four\nmacroscopic variables, instead of the distribution of all the allele\nfrequencies that influence the trait. These macroscopic variables are the\nexpectations of: the trait mean and its square, the genetic variance, and of a\nmeasure of heterozygosity, and are derived from a generating function that is\nin turn derived by maximizing an entropy measure. These four macroscopics are\nenough to accurately describe the dynamics of the trait mean and of its genetic\nvariance (and in principle of any other quantity). Unlike previous approaches\nthat were based on an infinite series of moments or cumulants, which had to be\ntruncated arbitrarily, our calculations provide a well-defined approximation\nprocedure. We apply the framework to abrupt and gradual changes in the optimum,\nas well as to changes in the strength of stabilizing selection. Our\napproximations are surprisingly accurate, even for systems with as few as 5\nloci. We find that when the effects of drift are included, the expected genetic\nvariance is hardly altered by directional selection, even though it fluctuates\nin any particular instance. We also find hysteresis, showing that even after\naveraging over the microscopic variables, the macroscopic trajectories retain a\nmemory of the underlying genetic states.\n",
        "  Most commercially available treatment planning systems for brachytherapy\noperate based on physical dose and do not incorporate fractionation or\ntissue-specific response. The purpose of this study is to investigate the\npotential for hypofractionation in HDR brachytherapy, thereby reducing the\nnumber of implants required. A new treatment planning algorithm was built in\norder to optimize based on tissue and fractionation specific parameters.\nDifferent fractionation schemes were considered for 6 patients, and plans were\ncreated using the new algorithm. A baseline fractionation scheme consisting of\n5 fractions was compared to hypofractionated plans of 1 to 4 fractions. The\neffectiveness of each plan was evaluated using radiobiological criteria taken\nfrom GEC-ESTRO guidelines. The results of this study indicate that an\noptimization algorithm based on biological parameters has similar functionality\nto traditional planning methods with the additional ability to account for\nfractionation effects. Using this algorithm, it was shown that plans consisting\nof 3 and 4 fractions have comparable target coverage with equivalent normal\ntissue exposure. In some specific cases, further fractionation may present\nacceptable target coverage as well.\n",
        "  In France, 50% of the population per year is suffering from low back pain.\nLumbar belt are frequently proposed as a part of the treatment of this\npathology. However mechanical ways of working of this medical device is not\nclearly understood, but abdominal pressure is often related. So an optical\nmethod was developed in this study to measure strain in lumbar belt and trunk\ninterface and to derive a pressure estimation. Optical method consisted of\ncoupling fringe projection and digital image correlation (DIC). Measurement has\nbeen carried out on the right side of a manikin wearing a lumbar belt. Average\nstrain is 0.2 and average pressure is 1 kPa. Continuation of this study will be\ncomparison of strain and pressure in different areas of lumbar belt (left side,\nfront and back) and comparison of different lumbar belts. Results will be used\nin a finite elements model to determine lumbar belt impact in intern body. In\nlong term, this kind of study will be done on human.\n",
        "  The theoretical description of the Fulde-Ferrell-Larkin-Ovchinnikov like\nstate establishing in nanostructered bilayers of ferromagnetic (F) and\nsuperconducting (S) material leads to critical temperature oscillations and\nreentrant superconductivity as the F-layer thickness gradually increases. The\nexperimental realization of these phenomena is an important prerequisite for\nthe fabrication of the Ferromagnet/Superconductor/Ferromagnet core structure of\nthe superconducting spin-valve. A switching of the spin-valve is only expected\nif such non-monotonic critical temperature behavior is observed in F/S bilayers\nas well as in the S/F bilayers, a combination of which the spin-valve core\nstructure can be regarded to consist of. In our former investigations we could\ndemonstrate the required non-monotonic behavior of the critical temperature in\nS/F bilayers. In this study we succeeded in the preparation of F/S bilayers,\nwhere the superconducting material is now grown on top of the ferromagnetic\nmetal, which show deep critical temperature oscillations as a function of the\nferromagnetic layer thickness as well as an extinction and recovery, i.e. a\nreentrant behavior, of superconductivity. Especially, the latter is necessary\nto obtain a spin-valve with a large critical temperature shift between the\nparallel and antiparallel configurations of magnetizations in the F layers.\n",
        "  Relation classification is an important semantic processing task in the field\nof natural language processing (NLP). In this paper, we present a novel model,\nStructure Regularized Bidirectional Recurrent Convolutional Neural\nNetwork(SR-BRCNN), to classify the relation of two entities in a sentence, and\nthe new dataset of Chinese Sanwen for named entity recognition and relation\nclassification. Some state-of-the-art systems concentrate on modeling the\nshortest dependency path (SDP) between two entities leveraging convolutional or\nrecurrent neural networks. We further explore how to make full use of the\ndependency relations information in the SDP and how to improve the model by the\nmethod of structure regularization. We propose a structure regularized model to\nlearn relation representations along the SDP extracted from the forest formed\nby the structure regularized dependency tree, which benefits reducing the\ncomplexity of the whole model and helps improve the $F_{1}$ score by 10.3.\nExperimental results show that our method outperforms the state-of-the-art\napproaches on the Chinese Sanwen task and performs as well on the SemEval-2010\nTask 8 dataset\\footnote{The Chinese Sanwen corpus this paper developed and used\nwill be released in the further.\n",
        "  A homogenization method to model a stack of second generation (2G) High\nTemperature Superconducting (HTS) tapes under AC applied transport current or\nmagnetic field has been obtained. The idea is to find an anisotropic bulk\nequivalent for the stack, such that the geometrical layout of the internal\nalternating structures of insulating, metallic, superconducting and substrate\nlayers is \"washed\" out while keeping the overall electromagnetic behavior of\nthe original stack. We disregard assumptions upon the shape of the critical\nregion and use a power law E-J relationship allowing for overcritical current\ndensities to be considered. The method presented here allows for a\ncomputational speedup factor of up to 2 orders of magnitude when compared to\nfull 2-D simulations taking into account the actual dimensions of the stacks\nwithout compromising accuracy.\n",
        "  We investigate what distinguishes reported dreams from other personal\nnarratives. The continuity hypothesis, stemming from psychological dream\nanalysis work, states that most dreams refer to a person's daily life and\npersonal concerns, similar to other personal narratives such as diary entries.\nDifferences between the two texts may reveal the linguistic markers of dream\ntext, which could be the basis for new dream analysis work and for the\nautomatic detection of dream descriptions. We used three text analytics\nmethods: text classification, topic modeling, and text coherence analysis, and\napplied these methods to a balanced set of texts representing dreams, diary\nentries, and other personal stories. We observed that dream texts could be\ndistinguished from other personal narratives nearly perfectly, mostly based on\nthe presence of uncertainty markers and descriptions of scenes. Important\nmarkers for non-dream narratives are specific time expressions and\nconversational expressions. Dream texts also exhibit a lower discourse\ncoherence than other personal narratives.\n",
        "  The discovery of superfluid $^{3}$He in high porosity silica aerogels, and\nsubsequent experimental and theoretical work, have led to a better general\nunderstanding of quasiparticle scattering from impurities in unconventional\npairing systems. It is immensely helpful for understanding impurity effects in\nthe case of superfluid $^{3}$He that the structure of its order parameter is\nwell-established. An overview of impurity effects is presented with emphasis on\nthose experiments which have a quantitative interpretation in terms of\ntheoretical models for homogeneous and inhomogeneous scattering. The latter can\naccount successfully for most experimental results.\n",
        "  Stars form in dense, dusty structures, which are embedded in larger clumps of\nmolecular clouds often showing a clear filamentary structure on large scales (>\n1pc). One of the best-studied regions in the Hi-GAL survey can be observed\ntoward the l=224deg field. Here, a filamentary region has been studied and it\nhas been found that protostellar clumps are mostly located along the main\nfilament, whereas starless clumps are detected off this filament and are\ninstead found on secondary, less prominent filaments. We want to investigate\nthis segregation effect and how it may affect the clumps properties. We mapped\nthe 12CO(1-0) line and its main three isotopologues toward the two most\nprominent filaments observed toward the l=224deg field using the Mopra radio\ntelescope, in order to set observational constraints on the dynamics of these\nstructures and the associated starless and protostellar clumps. Compared to the\nstarless clumps, the protostellar clumps are more luminous, more turbulent and\nlie in regions where the filamentary ambient gas shows larger linewidths. We\nsee evidence of gas flowing along the main filament, but we do not find any\nsigns of accretion flow from the filament onto the Hi-GAL clumps. We analyze\nthe radial column density profile of the filaments and their gravitational\nstability. The more massive and highly fragmented main filament appears to be\nthermally supercritical and gravitationally bound, assuming that all of the\nnon-thermal motion is contributing thermal-like support, suggesting a later\nstage of evolution compared to the secondary filament. The status and\nevolutionary phase of the Hi-GAL clumps would then appear to correlate with\nthat of the host filament.\n",
        "  We report on the Andreev spectroscopy and specific heat of high-quality\nsingle crystals BaFe$_{1.9}$Ni$_{0.1}$As$_{2}$. The intrinsic multiple Andreev\nreflection spectroscopy reveals two anisotropic superconducting gaps $\\Delta_L\n\\approx 3.2 \\textendash 4.5$\\,meV, $\\Delta_S \\approx 1.2 \\textendash 1.6$\\,meV\n(the ranges correspond to the minimum and maximum value of the coupling energy\nin the $k_xk_y$-plane). The $25 \\textendash 30 \\%$ anisotropy shows the absence\nof nodes in the superconducting gaps. Using a two-band model with s-wave-like\ngaps $\\Delta_L \\approx 3.2$\\,meV and $\\Delta_S \\approx 1.6$\\,meV, the\ntemperature dependence of the electronic specific heat can be well described. A\nlinear magnetic field dependence of the low-temperature specific heat offers a\nfurther support of s-wave type of the order parameter. We find that a d-wave or\nsingle-gap BCS theory under the weak-coupling approach cannot describe our\nexperiments.\n",
        "  When dealing with large collections of documents, it is imperative to quickly\nget an overview of the texts' contents. In this paper we show how this can be\nachieved by using a clustering algorithm to identify topics in the dataset and\nthen selecting and visualizing relevant words, which distinguish a group of\ndocuments from the rest of the texts, to summarize the contents of the\ndocuments belonging to each topic. We demonstrate our approach by discovering\ntrending topics in a collection of New York Times article snippets.\n",
        "  Representing the semantics of linguistic items in a machine-interpretable\nform has been a major goal of Natural Language Processing since its earliest\ndays. Among the range of different linguistic items, words have attracted the\nmost research attention. However, word representations have an important\nlimitation: they conflate different meanings of a word into a single vector.\nRepresentations of word senses have the potential to overcome this inherent\nlimitation. Indeed, the representation of individual word senses and concepts\nhas recently gained in popularity with several experimental results showing\nthat a considerable performance improvement can be achieved across different\nNLP applications upon moving from word level to the deeper sense and concept\nlevels. Another interesting point regarding the representation of concepts and\nword senses is that these models can be seamlessly applied to other linguistic\nitems, such as words, phrases and sentences.\n",
        "  Alzheimer's disease (AD) is the most common type of dementia accompanied with\nbrain atrophy. Structural measurements of brain atrophy in specific brain\nstructures such as hippocampus using magnetic resonance imaging (MRI) have been\nreported to detect the development of dementia early in the course of the\ndisease. The purpose of this study was to develop a computer-aided diagnostic\nsystem for AD using MRI, which is based on the automatic volumetry of segmented\nbrain images and generation of three-dimensional cortical thickness images\nusing the Eulerian partial differential equation (PDE) approach. We\ninvestigated the effect of the inhomogeneity of magnetic field strength and\nstatistical noise on the accuracy of our automatic volumetry and the PDE\napproach using the simulated MR images generated from BrainWeb. Our automatic\nvolumetry and PDE approach were robust against inhomogeneous magnetic field\nstrength. Although the accuracy of our automatic volumetry decreased with\nincreasing statistical noise, it was maintained when the statistical noise was\nless than 7-8%. The cortical thickness obtained by the PDE method tended to\ndecrease with increasing statistical noise. When we applied our method to\nclinical data, the cortical thinning due to brain atrophy was clearly\ndemonstrated in patients with brain atrophy. These results suggest that our\nsystem appears to be useful for the diagnosis of AD, because it allows us to\nautomatically evaluate the extent of brain atrophy and cortical thinning in a\nthree-dimensional manner.\n",
        "  We report on the critical current density Jc and the vortex dynamics of\npristine and 3 MeV proton irradiated (cumulative dose equal to 2x10^16 cm^-2)\n$\\beta$-FeSe single crystals. We also analyze a remarkable dependence of the\nsuperconducting critical temperature Tc, Jc and the flux creep rate S on the\nsample mounting method. Free-standing crystals present Tc =8.4(1)K, which\nincreases to 10.5(1)K when they are fixed to the sample holder by embedding\nthem with GE-7031 varnish. On the other hand, the irradiation has a marginal\neffect on Tc. The pinning scenario can be ascribed to twin boundaries and\nrandom point defects. We find that the main effect of irradiation is to\nincrease the density of random point defects, while the embedding mainly\nreduces the density of twin boundaries. Pristine and irradiated crystals\npresent two outstanding features in the temperature dependence of the flux\ncreep rate: S(T) presents large values at low temperatures, which can be\nattributed to small pinning energies, and a plateau at intermediate\ntemperatures, which can be associated with glassy relaxation. From Maley\nanalysis, we observe that the characteristic glassy exponent {\\mu} changes from\n~ 1.7 to 1.35-1.4 after proton irradiation.\n",
        "  We examine the question of thermal melting of the triangular Abrikosov vortex\nsolid in two-dimensional superconductors or neutral superfluids. We introduce a\nmodel, which combines lowest Landau level (LLL) projection with the magnetic\nWannier basis to represent degenerate eigenstates in the LLL. Solving the model\nnumerically via large-scale Monte Carlo simulations, we find clear evidence for\na continuous melting transition, in perfect agreement with the\nKosterlitz-Thouless-Halperin-Nelson-Young theory and with recent experiments.\n",
        "  Background Elimination of malaria can only be achieved through removal of all\nvectors or complete depletion of the infectious reservoir in humans.\nMechanistic models can be built to synthesize diverse observations from the\nfield collected under a variety of conditions and subsequently used to query\nthe infectious reservoir in great detail. Methods The EMOD model of malaria\ntransmission was calibrated to prevalence, incidence, asexual parasite density,\ngametocyte density, infection duration, and infectiousness data from 9 study\nsites. The infectious reservoir was characterized by diagnostic detection limit\nand age group over a range of transmission intensities with and without case\nmanagement and vector control. Mass screen-and-treat drug campaigns were tested\nfor likelihood of achieving elimination. Results The composition of the\ninfectious reservoir by diagnostic threshold is similar over a range of\ntransmission intensities, and higher intensity settings are biased toward\ninfections in children. Recent ramp-ups in case management and use of\ninsecticide-treated bednets reduce the infectious reservoir and shift the\ncomposition toward submicroscopic infections. Mass campaigns with antimalarial\ndrugs are highly effective at interrupting transmission if deployed shortly\nafter ITN campaigns. Conclusions Low density infections comprise a substantial\nportion of the infectious reservoir. Proper timing of vector control, seasonal\nvariation in transmission intensity, and mass drug campaigns allows lingering\npopulation immunity to help drive a region toward elimination.\n",
        "  A large number of web databases are only accessible through proprietary\nform-like interfaces which require users to query the system by entering\ndesired values for a few attributes. A key restriction enforced by such an\ninterface is the top-k output constraint - i.e., when there are a large number\nof matching tuples, only a few (top-k) of them are preferentially selected and\nreturned by the website, often according to a proprietary ranking function.\nSince most web database owners set k to be a small value, the top-k output\nconstraint prevents many interesting third-party (e.g., mashup) services from\nbeing developed over real-world web databases. In this paper we consider the\nnovel problem of \"digging deeper\" into such web databases. Our main\ncontribution is the meta-algorithm GetNext that can retrieve the next ranked\ntuple from the hidden web database using only the restrictive interface of a\nweb database without any prior knowledge of its ranking function. This\nalgorithm can then be called iteratively to retrieve as many top ranked tuples\nas necessary. We develop principled and efficient algorithms that are based on\ngenerating and executing multiple reformulated queries and inferring the next\nranked tuple from their returned results. We provide theoretical analysis of\nour algorithms, as well as extensive experimental results over synthetic and\nreal-world databases that illustrate the effectiveness of our techniques.\n",
        "  A word's sentiment depends on the domain in which it is used. Computational\nsocial science research thus requires sentiment lexicons that are specific to\nthe domains being studied. We combine domain-specific word embeddings with a\nlabel propagation framework to induce accurate domain-specific sentiment\nlexicons using small sets of seed words, achieving state-of-the-art performance\ncompetitive with approaches that rely on hand-curated resources. Using our\nframework we perform two large-scale empirical studies to quantify the extent\nto which sentiment varies across time and between communities. We induce and\nrelease historical sentiment lexicons for 150 years of English and\ncommunity-specific sentiment lexicons for 250 online communities from the\nsocial media forum Reddit. The historical lexicons show that more than 5% of\nsentiment-bearing (non-neutral) English words completely switched polarity\nduring the last 150 years, and the community-specific lexicons highlight how\nsentiment varies drastically between different communities.\n",
        "  With the growing use of location-based services, location privacy attracts\nincreasing attention from users, industry, and the research community. While\nconsiderable effort has been devoted to inventing techniques that prevent\nservice providers from knowing a user's exact location, relatively little\nattention has been paid to enabling so-called peer-wise privacy--the protection\nof a user's location from unauthorized peer users. This paper identifies an\nimportant efficiency problem in existing peer-privacy approaches that simply\napply a filtering step to identify users that are located in a query range, but\nthat do not want to disclose their location to the querying peer. To solve this\nproblem, we propose a novel, privacy-policy enabled index called the PEB-tree\nthat seamlessly integrates location proximity and policy compatibility. We\npropose efficient algorithms that use the PEB-tree for processing privacy-aware\nrange and kNN queries. Extensive experiments suggest that the PEB-tree enables\nefficient query processing.\n",
        "  Automated metrics such as BLEU are widely used in the machine translation\nliterature. They have also been used recently in the dialogue community for\nevaluating dialogue response generation. However, previous work in dialogue\nresponse generation has shown that these metrics do not correlate strongly with\nhuman judgment in the non task-oriented dialogue setting. Task-oriented\ndialogue responses are expressed on narrower domains and exhibit lower\ndiversity. It is thus reasonable to think that these automated metrics would\ncorrelate well with human judgment in the task-oriented setting where the\ngeneration task consists of translating dialogue acts into a sentence. We\nconduct an empirical study to confirm whether this is the case. Our findings\nindicate that these automated metrics have stronger correlation with human\njudgments in the task-oriented setting compared to what has been observed in\nthe non task-oriented setting. We also observe that these metrics correlate\neven better for datasets which provide multiple ground truth reference\nsentences. In addition, we show that some of the currently available corpora\nfor task-oriented language generation can be solved with simple models and\nadvocate for more challenging datasets.\n",
        "  In this paper, we develop a novel phase retrieval approach to reconstruct\nx-ray differential phase shift induced by an object. A primary advantage of our\napproach is a higher-order accuracy over that with the conventional linear\napproximation models, relaxing the current restriction of weak absorption and\nslow phase variation scenario. The optimal utilization of the diffraction\nimages at different distance in Fresnel diffraction region eliminates the\nnonlinear terms in phase and attenuation, and simplifies the reconstruction to\na linear inverse problem. Numerical studies are also described to demonstrate\nthe accuracy and stability of our approach.\n",
        "  Standard evolutionary theories of aging and mortality, implicitly based on\nassumptions of spatial averaging, hold that natural selection cannot favor\nshorter lifespan without direct compensating benefit to individual reproductive\nsuccess. Here we show that both theory and phenomenology are consistent with\nprogrammed death. Spatial evolutionary models show that self-limited lifespan\nrobustly results in long-term benefit to a lineage; longer-lived variants may\nhave a reproductive advantage for many generations, but shorter lifespan\nultimately confers long-term reproductive advantage through environmental\nfeedback acting on much longer time scales. Numerous model variations produce\nthe same qualitative result, demonstrating insensitivity to detailed\nassumptions; the key conditions under which self-limited lifespan is favored\nare spatial extent and locally exhaustible resources. Numerous empirical\nobservations can parsimoniously be explained in terms of long-term selective\nadvantage for intrinsic mortality. Classically anomalous empirical data on\nnatural lifespans and intrinsic mortality, including observations of longer\nlifespan associated with increased predation, and evidence of programmed death\nin both unicellular and multicellular organisms, are consistent with specific\nmodel predictions. The generic nature of the spatial model conditions under\nwhich intrinsic mortality is favored suggests a firm theoretical basis for the\nidea that evolution can quite generally select for shorter lifespan directly.\n",
        "  Observations show that spiral galaxies in galaxy clusters tend to have on\naverage less neutral hydrogen (HI) than galaxies of the same type and size in\nthe field. There is accumulating evidence that such HI-deficient galaxies are\nalso relatively frequent in galaxy groups. An important question is, which\nmechanisms are responsible for the gas deficiency in galaxy groups. To gain a\nbetter understanding of how environment affects the gas content of galaxies, we\nidentified a sample of six HI-deficient galaxies from the HI Parkes All Sky\nSurvey (HIPASS) using HI-optical scaling relations. One of the galaxies is\nlocated in the outskirts of the Fornax cluster, four are in loose galaxy groups\nand one is in a galaxy triplet. We present new high resolution HI observations\nwith the Australia Telescope Compact Array (ATCA) of these galaxies. We discuss\nthe possible cause of HI-deficiency in these galaxies based on HI observations\nand various multi-wavelength data. We find that the galaxies have truncated HI\ndisks, lopsided gas distribution and some show asymmetries in their stellar\ndisks. We conclude that both ram pressure stripping and tidal interactions are\nimportant gas removal mechanisms in low density environments.\n",
        "  One of the goals in understanding any new class of superconductors is to\nsearch for commonalities with other known superconductors. The present work\ninvestigates the superconducting condensation energy, U, in the iron based\nsuperconductors (IBS), and compares their U with a broad range of other\ndistinct classes of superconductor, including conventional BCS elements and\ncompounds and the unconventional heavy Fermion, Sr2RuO4, Li0.1ZrNCl,\nkappa-(BEDT-TTF)2Cu(NCS)2 and optimally doped cuprate superconductors.\nSurprisingly, both the magnitude and Tc dependence (U=0.1Tc**3.4 +- 0.2) of U\nare, contrary to the previously observed behavior of the specific heat\ndiscontinuity at Tc, deltaC, quite similar in the IBS and BCS materials for\nTc>1.4 K. In contrast, the heavy Fermion superconductors U vs Tc are strongly\n(up to a factor of 100) enhanced above the IBS/BCS while the cuprate\nsuperconductors U are strongly (factor of 8) reduced. However, scaling of U\nwith the specific heat gamma (or deltaC) brings all the superconductors\ninvestigated onto one universal dependence upon Tc. This apparent universal\nscaling U/gamma = 0.2Tc**2 for all superconductor classes investigated, both\nweak and strong coupled and both conventional and unconventional, links\ntogether extremely disparate behaviors over almost seven orders of magnitude\nfor U and almost three orders of magnitude for Tc. Since U has not yet been\nexplicitly calculated beyond the weak coupling limit, the present results can\nhelp direct theoretical efforts into the medium and strong coupling regimes.\n",
        "  Mutualistic networks are formed when the interactions between two classes of\nspecies are mutually beneficial. They are important examples of cooperation\nshaped by evolution. Mutualism between animals and plants plays a key role in\nthe organization of ecological communities. Such networks in ecology have\ngenerically evolved a nested architecture independent of species composition\nand latitude - specialists interact with proper subsets of the nodes with whom\ngeneralists interact. Despite sustained efforts to explain observed network\nstructure on the basis of community-level stability or persistence, such\ncorrelative studies have reached minimal consensus. Here we demonstrate that\nnested interaction networks could emerge as a consequence of an optimization\nprinciple aimed at maximizing the species abundance in mutualistic communities.\nUsing analytical and numerical approaches, we show that because of the\nmutualistic interactions, an increase in abundance of a given species results\nin a corresponding increase in the total number of individuals in the\ncommunity, as also the nestedness of the interaction matrix. Indeed, the\nspecies abundances and the nestedness of the interaction matrix are correlated\nby an amount that depends on the strength of the mutualistic interactions.\nNestedness and the observed spontaneous emergence of generalist and specialist\nspecies occur for several dynamical implementations of the variational\nprinciple under stationary conditions. Optimized networks, while remaining\nstable, tend to be less resilient than their counterparts with randomly\nassigned interactions. In particular, we analytically show that the abundance\nof the rarest species is directly linked to the resilience of the community.\nOur work provides a unifying framework for studying the emergent structural and\ndynamical properties of ecological mutualistic networks.\n",
        "  The integration of Geographic Information Systems (GIS) and On-Line\nAnalytical Processing (OLAP), denoted SOLAP, is aimed at exploring and\nanalyzing spatial data. In real-world SOLAP applications, spatial and\nnon-spatial data are subject to changes. In this paper we present a temporal\nquery language for SOLAP, called TPiet-QL, supporting so-called discrete\nchanges (for example, in land use or cadastral applications there are\nsituations where parcels are merged or split). TPiet-QL allows expressing\nintegrated GIS-OLAP queries in an scenario where spatial objects change across\ntime.\n",
        "  We propose a hypothesis only baseline for diagnosing Natural Language\nInference (NLI). Especially when an NLI dataset assumes inference is occurring\nbased purely on the relationship between a context and a hypothesis, it follows\nthat assessing entailment relations while ignoring the provided context is a\ndegenerate solution. Yet, through experiments on ten distinct NLI datasets, we\nfind that this approach, which we refer to as a hypothesis-only model, is able\nto significantly outperform a majority class baseline across a number of NLI\ndatasets. Our analysis suggests that statistical irregularities may allow a\nmodel to perform NLI in some datasets beyond what should be achievable without\naccess to the context.\n",
        "  Graphs are fundamental data structures and have been employed for centuries\nto model real-world systems and phenomena. Random walk with restart (RWR)\nprovides a good proximity score between two nodes in a graph, and it has been\nsuccessfully used in many applications such as automatic image captioning,\nrecommender systems, and link prediction. The goal of this work is to find\nnodes that have top-k highest proximities for a given node. Previous approaches\nto this problem find nodes efficiently at the expense of exactness. The main\nmotivation of this paper is to answer, in the affirmative, the question, `Is it\npossible to improve the search time without sacrificing the exactness?'. Our\nsolution, {it K-dash}, is based on two ideas: (1) It computes the proximity of\na selected node efficiently by sparse matrices, and (2) It skips unnecessary\nproximity computations when searching for the top-k nodes. Theoretical analyses\nshow that K-dash guarantees result exactness. We perform comprehensive\nexperiments to verify the efficiency of K-dash. The results show that K-dash\ncan find top-k nodes significantly faster than the previous approaches while it\nguarantees exactness.\n",
        "  We report here that magnetic fields of almost 34 T, far above the upper 24 T\nlimit of Nb3Sn, can be generated using a multifilament round wire conductor\nmade of the high temperature cuprate superconductor Bi2Sr2CaCu2O8-x (Bi-2212).\nA remarkable attribute of this Bi-2212 conductor is that it does not exhibit\nmacroscopic texture and contains many high angle grain boundaries but\nnevertheless attains very high superconducting critical current densities Jc of\n2500 A/mm2 at 20 T and 4.2 K. This Bi-2212 conductor does not possess the\nextreme texture that high Jc coated conductors of REBa2Cu3O7-x (REBCO) require,\navoiding also its high aspect ratio, large superconducting anisotropy and the\ninherent sensitivity to defects of a single filament conductor. Bi-2212 wires\ncan be wound or cabled into almost any type of superconducting magnet and will\nbe especially valuable for very high field NMR magnets beyond the present 1 GHz\nproton resonance limit of Nb3Sn technology. This demonstration that grain\nboundary limits to high Jc can be practically overcome suggests the huge value\nof a renewed focus on grain boundary properties in non-ideal geometries,\nespecially with the goal of translating the lessons of this Bi-2212 conductor\ninto fabrication of multifilament round wire REBCO or Fe-based superconductors.\n",
        "  The Milky Way is acquiring gas from infalling high-velocity clouds. The\nmaterial enters a disk-halo interface that in many places is populated with HI\nclouds that have been ejected from the disk through processes linked to star\nformation. The Smith Cloud is an extraordinary example of a high-velocity cloud\nthat is bringing $>10^6$ M$_{\\odot}$ of relatively low metallicity gas into the\nMilky Way. It may be part of a larger stream, components of which are now\npassing through the disk.\n",
        "  We discuss the structure of the superconducting gap in iron pnictides. In the\nitinerant electron picture, gaps with or without nodes have the extended s-wave\n(s+) symmetry and emerge within the same pairing mechanism, determined by the\ninterplay between intra-pocket repulsion and inter-band pair hopping. If the\npair hopping is stronger, the system develops an s+ gap without nodes. In the\nopposite case the superconductivity is governed by of the momentum-dependent\npart of the pair-hopping, and an s+ gap shows nodes on electron Fermi surfaces.\nWe argue that the gap without/with nodes emerges in systems with a\nstronger/weaker tendency towards a spin-density-wave order.\n",
        "  We propose a mathematical framework for natural selection in finite\npopulations. Traditionally, many of the selection-based processes used to\ndescribe cultural and genetic evolution (such as imitation and birth-death\nmodels) have been studied on a case-by-case basis. Over time, these models have\ngrown in sophistication to include population structure, differing phenotypes,\nand various forms of interaction asymmetry, among other features. Furthermore,\nmany processes inspired by natural selection, such as evolutionary algorithms\nin computer science, possess characteristics that should fall within the realm\nof a \"selection process,\" but so far there is no overarching theory\nencompassing these evolutionary processes. The framework of $\\textit{stochastic\nselection processes}$ we present here provides such a theory and consists of\nthree main components: a $\\textit{population state space}$, an\n$\\textit{aggregate payoff function}$, and an $\\textit{update rule}$. A\npopulation state space is a generalization of the notion of population\nstructure, and it can include non-spatial information such as strategy-mutation\nrates and phenotypes. An aggregate payoff function allows one to generically\ntalk about the fitness of traits without explicitly specifying a method of\npayoff accounting or even the nature of the interactions that determine\npayoff/fitness. An update rule is a fitness-based function that updates a\npopulation based on its current state, and it includes as special cases the\nclassical update mechanisms (Moran, Wright-Fisher, etc.) as well as more\ncomplicated mechanisms involving chromosomal crossover, mutation, and even\ncomplex cultural syntheses of strategies of neighboring individuals. Our\nframework covers models with variable population size as well as with\narbitrary, measurable trait spaces.\n",
        "  We investigate the mutation-selection dynamics for an evolutionary\ncomputation model based on Turing Machines that we introduced in a previous\narticle.\n  The use of Turing Machines allows for very simple mechanisms of code growth\nand code activation/inactivation through point mutations. To any value of the\npoint mutation probability corresponds a maximum amount of active code that can\nbe maintained by selection and the Turing machines that reach it are said to be\nat the error threshold. Simulations with our model show that the Turing\nmachines population evolve towards the error threshold.\n  Mathematical descriptions of the model point out that this behaviour is due\nmore to the mutation-selection dynamics than to the intrinsic nature of the\nTuring machines. This indicates that this result is much more general than the\nmodel considered here and could play a role also in biological evolution.\n",
        "  Having a precise knowledge of the dispersal ability of a population in a\nheterogeneous environment is of critical importance in agroecology and\nconservation biology as it can provide management tools to limit the effects of\npests or to increase the survival of endangered species. In this paper, we\npropose a mechanistic-statistical method to estimate space-dependent diffusion\nparameters of spatially-explicit models based on stochastic differential\nequations, using genetic data. Dividing the total population into\nsubpopulations corresponding to different habitat patches with known allele\nfrequencies, the expected proportions of individuals from each subpopulation at\neach position is computed by solving a system of reaction-diffusion equations.\nModelling the capture and genotyping of the individuals with a statistical\napproach, we derive a numerically tractable formula for the likelihood function\nassociated with the diffusion parameters.\n  In a simulated environment made of three types of regions, each associated\nwith a different diffusion coefficient, we successfully estimate the diffusion\nparameters with a maximum-likelihood approach. Although higher genetic\ndifferentiation among subpopulations leads to more accurate estimations, once a\ncertain level of differentiation has been reached, the finite size of the\ngenotyped population becomes the limiting factor for accurate estimation.\n",
        "  The main purpose of this work was to design, develop and construct a simple,\nlow-cost AC susceptometer to measure large, bulk superconducting samples (up to\n32 mm in diameter) in the temperature range 78-120 K. The design incorporates a\ndouble heating system that enables a high heating rate (25 K/hour) while\nmaintaining a small temperature gradient (< 0.2 K) across the sample. The\napparatus can be calibrated precisely using a copper coil connected in series\nwith the primary coil. The system has been used successfully to measure the\ntemperature dependence of the AC magnetic properties of entire RE-Ba-Cu-O\n[(RE)BCO] bulk superconducting domains. A typical AC susceptibility measurement\nrun from 78 K to 95 K takes about 2 hours, with excellent temperature\nresolution (temperature step ~ 4 mK) around the critical temperature, in\nparticular.\n",
        "  We present follow-up observations to those of Geballe & Oka (2010), who found\nhigh column densities of H3+ ~100 pc off of the Galactic center (GC) on the\nlines of sight to 2MASS J17432173-2951430 (J1743) and 2MASS J17470898-2829561\n(J1747). The wavelength coverages on these sightlines have been extended in\norder to observe two key transitions of H3+, R(3,3)l and R(2,2)l, that\nconstrain the temperatures and densities of the environments. The profiles of\nthe H3+ R(3,3)l line, which is due only to gas in the GC, closely matches the\ndifferences between the H3+ R(1,1)l and CO line profiles, just as it does for\npreviously studied sightlines in the GC. Absorption in the R(2,2)l line of H3+\nis present in J1747 at velocities between -60 and +100 km/s. This is the second\nclear detection of this line in the interstellar medium after GCIRS 3 in the\nCentral Cluster. The temperature of the absorbing gas in this velocity range is\n350 K, significantly warmer than in the diffuse clouds in other parts of the\nCentral Molecular Zone. This indicates that the absorbing gas is local to Sgr B\nmolecular cloud complex. The warm and diffuse gas revealed by Oka et al. (2005)\napparently extends to ~100 pc, but there is a hint that its temperature is\nsomewhat lower in the line of sight to J1743 than elsewhere in the GC. The\nobservation of H3+ toward J1747 is compared with the recent Herschel\nobservation of H2O+ toward Sgr B2 and their chemical relationship and\nremarkably similar velocity profiles are discussed.\n",
        "  We present deep Hubble Space Telescope (HST) NICMOS 2 F160W band observations\nof the central 56*57\" (14pc*14.25pc) region around R136 in the starburst\ncluster 30 Dor (NGC 2070) located in the Large Magellanic Cloud. Our aim is to\nderive the stellar Initial Mass Function (IMF) down to ~1 Msun in order to test\nwhether the IMF in a massive metal-poor cluster is similar to that observed in\nnearby young clusters and the field in our Galaxy. We estimate the mean age of\nthe cluster to be 3 Myr by combining our F160W photometry with previously\nobtained HST WFPC2 optical F555W and F814W band photometry and comparing the\nstellar locus in the color-magnitude diagram with main sequence and pre-main\nsequence isochrones. The color-magnitude diagrams show the presence of\ndifferential extinction and possibly an age spread of a few megayears. We\nconvert the magnitudes into masses adopting both a single mean age of 3 Myr\nisochrone and a constant star formation history from 2 to 4 Myr. We derive the\nIMF after correcting for incompleteness due to crowding. The faintest stars\ndetected have a mass of 0.5 Msun and the data are more than 50% complete\noutside a radius of 5 pc down to a mass limit of 1.1 Msun for 3 Myr old\nobjects. We find an IMF of dN/dlog(M) M^(-1.20+-0.2) over the mass range\n1.1--20 Msun only slightly shallower than a Salpeter IMF. In particular, we\nfind no strong evidence for a flattening of the IMF down to 1.1 Msun at a\ndistance of 5 pc from the center, in contrast to a flattening at 2 Msun at a\nradius of 2 pc, reported in a previous optical HST study. We examine several\npossible reasons for the different results. If the IMF determined here applies\nto the whole cluster, the cluster would be massive enough to remain bound and\nevolve into a relatively low-mass globular cluster.\n",
        "  Patient respiratory signal associated with the cone beam CT (CBCT)\nprojections is important for lung cancer radiotherapy. In contrast to\nmonitoring an external surrogate of respiration, such signal can be extracted\ndirectly from the CBCT projections. In this paper, we propose a novel local\nprinciple component analysis (LPCA) method to extract the respiratory signal by\ndistinguishing the respiration motion-induced content change from the gantry\nrotation-induced content change in the CBCT projections. The LPCA method is\nevaluated by comparing with three state-of-the-art projection-based methods,\nnamely, the Amsterdam Shroud (AS) method, the intensity analysis (IA) method,\nand the Fourier-transform based phase analysis (FT-p) method. The clinical CBCT\nprojection data of eight patients, acquired under various clinical scenarios,\nwere used to investigate the performance of each method. We found that the\nproposed LPCA method has demonstrated the best overall performance for cases\ntested and thus is a promising technique for extracting respiratory signal. We\nalso identified the applicability of each existing method.\n",
        "  Online adaptive radiation therapy (ART) has great promise to significantly\nreduce normal tissue toxicity and/or improve tumor control through real-time\ntreatment adaptations based on the current patient anatomy. However, the major\ntechnical obstacle for clinical realization of online ART, namely the inability\nto achieve real-time efficiency in treatment re-planning, has yet to be solved.\nTo overcome this challenge, this paper presents our work on the implementation\nof an intensity modulated radiation therapy (IMRT) direct aperture optimization\n(DAO) algorithm on graphics processing unit (GPU) based on our previous work on\nCPU. We formulate the DAO problem as a large-scale convex programming problem,\nand use an exact method called column generation approach to deal with its\nextremely large dimensionality on GPU. Five 9-field prostate and five 5-field\nhead-and-neck IMRT clinical cases with 5\\times5 mm2 beamlet size and\n2.5\\times2.5\\times2.5 mm3 voxel size were used to evaluate our algorithm on\nGPU. It takes only 0.7~2.5 seconds for our implementation to generate optimal\ntreatment plans using 50 MLC apertures on an NVIDIA Tesla C1060 GPU card. Our\nwork has therefore solved a major problem in developing ultra-fast\n(re-)planning technologies for online ART.\n",
        "  Purpose: We develop an iterative image-reconstruction algorithm for\napplication to low-intensity computed tomography (CT) projection data, which is\nbased on constrained, total-variation (TV) minimization. The algorithm design\nfocuses on recovering structure on length scales comparable to a detector-bin\nwidth.\n  Method: Recovering the resolution on the scale of a detector bin, requires\nthat pixel size be much smaller than the bin width. The resulting image array\ncontains many more pixels than data, and this undersampling is overcome with a\ncombination of Fourier upsampling of each projection and the use of\nconstrained, TV-minimization, as suggested by compressive sensing. The\npresented pseudo-code for solving constrained, TV-minimization is designed to\nyield an accurate solution to this optimization problem within 100 iterations.\n  Results: The proposed image-reconstruction algorithm is applied to a\nlow-intensity scan of a rabbit with a thin wire, to test resolution. The\nproposed algorithm is compared with filtered back-projection (FBP).\n  Conclusion: The algorithm may have some advantage over FBP in that the\nresulting noise-level is lowered at equivalent contrast levels of the wire.\n",
        "  We present a novel reconstruction algorithm based on a general cone-beam CT\nforward model which is capable of incorporating the blur and noise correlations\nthat are exhibited in flat-panel CBCT measurement data. Specifically, the\nproposed model may include scintillator blur, focal-spot blur, and noise\ncorrelations due to light spread in the scintillator. The proposed algorithm\n(GPL-BC) uses a Gaussian Penalized-Likelihood objective function which\nincorporates models of Blur and Correlated noise. In a simulation study, GPL-BC\nwas able to achieve lower bias as compared to deblurring followed by FDK as\nwell as a model-based reconstruction method without integration of measurement\nblur. In the same study, GPL-BC was able to achieve better line-pair\nreconstructions (in terms of segmented-image accuracy) as compared to\ndeblurring followed by FDK, a model based method without blur, and a model\nbased method with blur but not noise correlations. A prototype extremities\nquantitative cone-beam CT test bench was used to image a physical sample of\nhuman trabecular bone. These data were used to compare reconstructions using\nthe proposed method and model based methods without blur and/or correlation to\na registered {\\mu}CT image of the same bone sample. The GPL-BC reconstructions\nresulted in more accurate trabecular bone segmentation. Multiple trabecular\nbone metrics, including Trabecular Thickness (Tb.Th.) were computed for each\nreconstruction approach as well as the {\\mu}CT volume. The GPL-BC\nreconstruction provided the most accurate Tb.Th. measurement, 0.255 mm, as\ncompared to the {\\mu}CT derived value of 0.193 mm, followed by the GPL-B\nreconstruction, the GPL-I reconstruction, and then the FDK reconstruction\n(0.271 mm, 0.309 mm, and 0.335 mm, respectively).\n",
        "  Near-field observations may provide tight constraints - i.e. \"boundary\nconditions\" - on any model of structure formation in the Universe. Detailed\nobservational data have long been available for the Milky Way (e.g. Freeman\n$\\&$ Bland-Hawthorn 2002) and have provided tight constraints on several Galaxy\nformation models (e.g. Abadi et al. 2003, Bekki $\\&$ Chiba 2001). An implicit\nassumption still remains unanswered though: is the Milky Way a \"normal\" spiral?\nSearching for directions, it feels natural to look at our neighbour: Andromeda.\nAn intriguing piece of the puzzle is provided by contrasting its stellar halo\nwith that of our Galaxy, even more so since Mouhcine et al. (2005) have\nsuggested that a correlation between stellar halo metallicity and galactic\nluminosity is in place and would leave the Milky Way halo as an outlier with\nrespect to other spirals of comparable luminosities. Further questions hence\narise: is there any stellar halo-galaxy formation symbiosis? Our first step has\nbeen to contrast the chemical evolution of the Milky Way with that of Andromeda\nby means of a semi-analytic model. We have then pursued a complementary\napproach through the analysis of several semi-cosmological late-type galaxy\nsimulations which sample a wide variety of merging histories. We have focused\non the stellar halo properties in the simulations at redshift zero and shown\nthat - at any given galaxy luminosity - the metallicities of the stellar halos\nin the simulations span a range in excess of $\\sim$ 1 dex, a result which is\nstrengthened by the robustness tests we have performed. We suggest that the\nunderlying driver of the halo metallicity dispersion can be traced to the\ndiversity of galactic mass assembly histories inherent within the hierarchical\nclustering paradigm.\n",
        "  Understanding overfishing phenomenon and regulating fishing quotas is a major\nglobal challenge for the 21st Century both in terms of providing food for\nhumankind and to preserve the oceans ecosystems. However, fishing is a complex\neconomic activity, affected not just by overfishing but also by such factors as\npollution, technology, financial factors and more. For this reason, it is often\ndifficult to state with complete certainty that overfishing is the cause of the\ndecline of a fishery. In this study, we developed a simple dynamic model based\non the earlier, well-known Lotka-Volterra model or Prey-Predator model. To\ndescribe exploitation patterns, we assume that the fish stock and the fishing\nindustry are coupled stock variables in the model and they dynamically affect\neach other, with the fishing yield proportional to both the fishing capital and\nthe fish stock. The model is based on the concept that the fishing industry\nacts as the predator of the resource and that its growth and subsequent decline\nis directly related to the abundance of the fish stock. If the model can be fit\nhistorical data relative to specific fisheries, then it is a strong indication\nthat the fishing industry is strongly affected by the magnitude of the fish\nstock and that, in particular, the decline of the yield and the decline of the\nstock are linked to each other. The model does not pretend to be a general\ndescription of the fishing industry in all its varied forms; however, the data\nreported here show that the model can indeed qualitatively describe several\nhistorical case of the collapse of fisheries. The model can also be used as a\nqualitative guide to understand the behavior of several other fisheries. These\nresult indicate that one of the main factors causing the present crisis of the\nworld's fisheries is the overexploitation of the fish stocks.\n",
        "  The semimetal antimony (Sb) element doped into hydrogen has been performed\ntheoretically to explored high-pressure crystal structure and superconductivity\nof antimony hydrides. The unexpected stoichiometry $\\textrm{SbH}_\\textrm{4}$\nwith $P6_3/mmc$ symmetry is found to have most negative enthalpy and embody the\ncoexistence of covalent and ionic bonds. It is a metallic phase and stable in\nthe pressure ranges of 127-300 GPa. Furthermore, a superconducting critical\ntemperature ($T_c$) of 106 K is obtained at 150 GPa by employing the\nAllen-Dynes modified McMillan equation. In addition, an extrusive\ndistinguishing feature is the presence of soft phonon modes, which is primary\ncontribution to the strength of electron-phonon coupling.\n",
        "  We investigate the electronic and magnetic structures of the 122\n(AM$_2$B$_2$) hexagonal transition-metal pnictides with A=(Sr, Ca), M=(Cr, Mn,\nFe, Co, Ni) and B=(As, P, Sb). It is found that the family of materials share\ncritical similarities with those of tetragonal structures that include the\nfamous iron-based high temperature superconductors. In both families, the next\nnearest neighbor(NNN) effective antiferromagnetic(AFM) exchange couplings reach\nthe maximum value in the iron-based materials. While the NNN couplings in the\nlatter are known to be responsible for the C-type AFM state and to result in\nthe extended s-wave superconducting state upon doping, they cause the former to\nbe extremely frustrated magnetic systems and can lead to an time reversal\nsymmetry broken $d+id$ superconducting state upon doping. The iron-based\ncompounds with the hexagonal structure, thus if synthesized, can help us to\ndetermine the origin of high temperature superconductivity.\n",
        "  We have demonstrated a pressure-induced phase transition from a low-Tc phase\nto a high-Tc phase in a single crystal of the superconductor LaO0.5F0.5BiSe2.\nThe high-Tc phase appears at 2.16 GPa and the maximum superconducting\ntransition temperature (Tc) is observed at 6.7 K under 2.44 GPa. Although the\nanisotropy ({\\gamma}) for the low-Tc phase is estimated to be 20, it is reduced\nby around half (9.3) in the high-Tc phase. This tendency is the same for the\nBiS2 system. The Tc of LaO0.5F0.5BiSe2 has continued to increase up to the\nmaximum pressure of this study (2.44 GPa). Therefore applied further pressure\nhas the potential to induce a much higher Tc in this system.\n",
        "  Morphologically rich languages often lack the annotated linguistic resources\nrequired to develop accurate natural language processing tools. We propose\nmodels suitable for training morphological taggers with rich tagsets for\nlow-resource languages without using direct supervision. Our approach extends\nexisting approaches of projecting part-of-speech tags across languages, using\nbitext to infer constraints on the possible tags for a given word type or\ntoken. We propose a tagging model using Wsabie, a discriminative\nembedding-based model with rank-based learning. In our evaluation on 11\nlanguages, on average this model performs on par with a baseline\nweakly-supervised HMM, while being more scalable. Multilingual experiments show\nthat the method performs best when projecting between related language pairs.\nDespite the inherently lossy projection, we show that the morphological tags\npredicted by our models improve the downstream performance of a parser by +0.6\nLAS on average.\n",
        "  The paper [1] shows that simple linear classifier can compete with complex\ndeep learning algorithms in text classification applications. Combining bag of\nwords (BoW) and linear classification techniques, fastText [1] attains same or\nonly slightly lower accuracy than deep learning algorithms [2-9] that are\norders of magnitude slower. We proved formally that fastText can be transformed\ninto a simpler equivalent classifier, which unlike fastText does not have any\nhidden layer. We also proved that the necessary and sufficient dimensionality\nof the word vector embedding space is exactly the number of document classes.\nThese results help constructing more optimal linear text classifiers with\nguaranteed maximum classification capabilities. The results are proven exactly\nby pure formal algebraic methods without attracting any empirical data.\n",
        "  Flattened bulges with disk-like properties are considered to be the end\nproduct of secular evolution processes at work in the inner regions of\ngalaxies. On the contrary, classical bulges are characterized by rounder shapes\nand thought to be similar to low-luminosity elliptical galaxies. We aim at\ntesting the variety of observational diagnostics which are commonly adopted to\nseparate classical from disk-like bulges in nearby galaxies. We select a sample\nof eight unbarred lenticular galaxies to be morphologically and kinematically\nundisturbed with no evidence of other components than bulge and disk. We\nanalyze archival data of broad-band imaging from SDSS and integral-field\nspectroscopy from the ATLAS$^{\\rm 3D}$ survey to derive the photometric and\nkinematic properties, line-strength indices, and intrinsic shape of the sample\nbulges. We argue that the bulge S\\'ersic index is a poor diagnostics to\ndiscriminate different bulge types. We find that the combination of\nline-strength with either kinematic or photometric diagnostics does not provide\na clear separation for half of the sample bulges. We include for the first time\nthe intrinsic three-dimensional shape of bulges as a possible discriminant of\ntheir nature. All bulges turn out to be thick oblate spheroids, but only one\nhas a flattening consistent with that expected for outer disks. We conclude\nthat bulge classification may be difficult even adopting all observational\ndiagnostics proposed so far and that classical and disk-like bulges could be\nmore confidently identified by considering their intrinsic shape.\n",
        "  Youtopia is a platform for collaborative management and integration of\nrelational data. At the heart of Youtopia is an update exchange abstraction:\nchanges to the data propagate through the system to satisfy user-specified\nmappings. We present a novel change propagation model that combines a\ndeterministic chase with human intervention. The process is fundamentally\ncooperative and gives users significant control over how mappings are repaired.\nAn additional advantage of our model is that mapping cycles can be permitted\nwithout compromising correctness.\n  We investigate potential harmful interference between updates in our model;\nwe introduce two appropriate notions of serializability that avoid such\ninterference if enforced. The first is very general and related to classical\nfinal-state serializability; the second is more restrictive but highly\npractical and related to conflict-serializability. We present an algorithm to\nenforce the latter notion. Our algorithm is an optimistic one, and as such may\nsometimes require updates to be aborted. We develop techniques for reducing the\nnumber of aborts and we test these experimentally.\n",
        "  We classify link diagrams with Turaev genus one and two in terms of an\nalternating tangle structure of the link diagram. The proof involves surgery\nalong simple closed curves on the Turaev surface, called cutting loops, which\nhave corresponding cutting arcs that are visible on the planar link diagram.\nThese also provide new obstructions for a link diagram on a surface to come\nfrom the Turaev surface algorithm.\n",
        "  We developed mathematical models that simulate community dynamics derived\nfrom a series of perturbation experiments. These experiments were performed in\nan Atlantic Forest stream. The three trophic level community was submitted to\ntwo combinations of press perturbation experiment. In the first, the top\ntrophic level was removed by electrical exclusion. In the second configuration,\nthe top trophic level was still excluded plus a group of species from the\nsecond trophic level was inhibited, also by electrical pulses. Experiments were\nrepeated at different times to increase confidence in the observed mechanisms.\nCommunity responses indicated the existence of cascading interactions similar\nto a classic trophic cascade. The community was composed of Macrobrachium\nshrimps at the top trophic level, Ephemeroptera and Chironomidae larval insects\nat the second trophic level and periphyton (= epilithon = biofilm) at the first\ntrophic level. The shrimp exclusion caused an increase in ephemeropterans and\nchironomids and led the periphyton to decrease in the exclusion zone. When\nephemeropterans were inhibited together with the exclusion of shrimps, the\nchironomids and the periphyton increased. We modelled the community\ninteractions by means of differential equation systems, simulating the three\nconfigurations: 1. natural condition, 2. shrimp exclusion condition and 3.\nshrimp-exclusion-plus-ephemeropteran-inhibition condition. All parameters were\ncalculated via experimental results and the stability of the models was\ndetermined by their matrix eigenvalues. The models successfully reproduced the\nqualitative responses of the community. They proved to have attraction points,\nwhich enables us to predict that the system has some ability to return to its\nprevious condition after local perturbation.\n",
        "  This is a PhD thesis in low-dimensional topology.\n  Its main purpose is to examine so-called t\\^ete-\\`a-t\\^ete twists. Those were\ndefined by A'Campo and give an easy combinatorial description of certain\nmapping classes on surfaces with boundary. Whereas the well-known Dehn twists\nare twists around a simple closed curve, t\\^ete-\\`a-t\\^ete twists are twists\naround a graph. It is shown that t\\^ete-\\`a-t\\^ete twists describe all the\n(freely) periodic mapping classes. This leads, among other things, to a\nstronger version of Wiman's 4g+2 theorem from 1895 for surfaces with boundary.\n  On closed surfaces, some t\\^ete-\\`a-t\\^ete twists can be used to generate the\nmapping class group.\n  Another main result is a simple criterion to decide whether a Seifert surface\nof a link is a fibre surface. This gives a short topological proof of the fact\nthat a Murasugi is fibred if and only if its two summands are.\n",
        "  Identifying implicit discourse relations between text spans is a challenging\ntask because it requires understanding the meaning of the text. To tackle this\ntask, recent studies have tried several deep learning methods but few of them\nexploited the syntactic information. In this work, we explore the idea of\nincorporating syntactic parse tree into neural networks. Specifically, we\nemploy the Tree-LSTM model and Tree-GRU model, which are based on the tree\nstructure, to encode the arguments in a relation. Moreover, we further leverage\nthe constituent tags to control the semantic composition process in these\ntree-structured neural networks. Experimental results show that our method\nachieves state-of-the-art performance on PDTB corpus.\n",
        "  A surprisingly strong variation of resistance with perpendicular magnetic\nfield, and a peak in the resistance vs. field, R(B) has been found in\ninsulating films of a sequence of homogeneous, quench-condensed films of\namorphous Bi undergoing a thickness-tuned superconductor-insulator transition.\nIsotherms of magnetoresistance, rather than resistance, vs. field were found to\ncross at a well-defined magnetic field higher than the field corresponding to\nthe peak in R(B). For all values of B, R(T) was found to obey an Arrhenius\nform. At the crossover magnetic field the prefactor became equal to the quantum\nresistance of electron pairs, h/4e^2, and the activation energy returned to its\nzero field value. These observations suggest that the crossover is the\nsignature of a quantum phase transition between two distinct insulating ground\nstates, tuned by magnetic field.\n",
        "  An amplification of a weak low-frequency harmonic signal is experimentally\nobserved in a single-junction-superconducting quantum interferometer (RF SQUID\nloop) due to stochastic transitions between two or more metastable states of\nthe loop under the applied noise flux of varying intensity (the effect of\nstochastic resonance, SR). In addition to the usual scenario of the SR in a\nbistable system with Gaussian noise, the transitions between multiple\nmetastable states of the multi-well SQUID loop potential under the influence of\na binary noise is observed. This can be interpreted as a kind of noise\n\"spectroscopy\" of the metastable states of the SQUID loop with different values\nof the trapped magnetic flux.\n",
        "  The real-time database service selection depends typically to the system\nstability in order to handle the time-constrained transactions within their\ndeadline. However, applying the real-time database system in the mobile ad hoc\nnetworks requires considering the mobile nodes limited capacities. In this\npaper, we propose cross-layer service selection which combines performance\nmetrics measured in the real-time database system to those used by the routing\nprotocol in order to make the best selection decision. It ensures both\ntimeliness and energy efficiency by avoiding low-power and busy service\nprovider node. A multicast packet is used in order to reduce the transmission\ncost and network load when sending the same packet to multiple service\nproviders. In this paper, we evaluate the performance of our proposed protocol.\nSimulation results, using the Network Simulator NS2, improve that the protocol\ndecreases the deadline miss ratio of packets, increases the service\navailability and reduces the service response time.\n",
        "  A smooth curve $\\gamma: [0,1] \\to S^2$ is locally convex if its geodesic\ncurvature is positive at every point. J. A. Little showed that the space of all\nlocally positive curves $\\gamma$ with $\\gamma(0) = \\gamma(1) = e_1$ and\n$\\gamma'(0) = \\gamma'(1) = e_2$ has three connected components $L_{-1,c}$,\n$L_{+1}$, $L_{-1,n}$. The space $L_{-1,c}$ is known to be contractible but the\ntopology of the other two connected components is not well understood. We study\nthe homotopy and cohomology of these spaces. In particular, for $L_{-1} =\nL_{-1,c} \\sqcup L_{-1,n}$, we show that $\\dim H^{2k}(L_{(-1)^{k}}, \\RR) \\ge 1$,\nthat $\\dim H^{2k}(L_{(-1)^{(k+1)}}, \\RR) \\ge 2$, that $\\pi_2(L_{+1})$ contains\na copy of $Z^2$ and that $\\pi_{2k}(L_{(-1)^{(k+1)}})$ contains a copy of $Z$.\n",
        "  Monte Carlo simulation is the most accurate method for absorbed dose\ncalculations in radiotherapy. Its efficiency still requires improvement for\nroutine clinical applications, especially for online adaptive radiotherapy. In\nthis paper, we report our recent development on a GPU-based Monte Carlo dose\ncalculation code for coupled electron-photon transport. We have implemented the\nDose Planning Method (DPM) Monte Carlo dose calculation package (Sempau et al,\nPhys. Med. Biol., 45(2000)2263-2291) on GPU architecture under CUDA platform.\nThe implementation has been tested with respect to the original sequential DPM\ncode on CPU in phantoms with water-lung-water or water-bone-water slab\ngeometry. A 20 MeV mono-energetic electron point source or a 6 MV photon point\nsource is used in our validation. The results demonstrate adequate accuracy of\nour GPU implementation for both electron and photon beams in radiotherapy\nenergy range. Speed up factors of about 5.0 ~ 6.6 times have been observed,\nusing an NVIDIA Tesla C1060 GPU card against a 2.27GHz Intel Xeon CPU\nprocessor.\n",
        "  We study Khovanov homology classes which have state cycle representatives,\nand examine how they interact with Jacobsson homomorphisms and Lee's map\n$\\Phi$. As an application, we describe a general procedure, quasipositive\nmodification, for constructing H-thick knots in rational Khovanov homology.\nMoreover, we show that specific families of such knots cannot be detected by\nKhovanov's thickness criteria. We also exhibit a sequence of prime links\nrelated by quasipositive modification whose width is increasing.\n",
        "  Let $\\Gamma$ be a nonuniform lattice acting on real hyperbolic n-space. We\nshow that in dimension greater than or equal to 4, the volume of a\nrepresentation is constant on each connected component of the representation\nvariety of $\\Gamma$ in SO(n,1). Furthermore, in dimensions 2 and 3, there is a\nsemialgebraic subset of the representation variety such that the volume of a\nrepresentation is constant on connected components of the semialgebraic subset.\nOur approach gives a new proof of the local rigidity theorem for nonuniform\nhyperbolic lattices and the analogue of Soma's theorem, which shows that the\nnumber of orientable hyperbolic manifolds dominated by a closed, connected,\norientable 3-manifold is finite, for noncompact 3-manifolds.\n",
        "  Universal Networking Language (UNL) is a declarative formal language that is\nused to represent semantic data extracted from natural language texts. This\npaper presents a novel approach to converting Bangla natural language text into\nUNL using a method known as Predicate Preserving Parser (PPP) technique. PPP\nperforms morphological, syntactic and semantic, and lexical analysis of text\nsynchronously. This analysis produces a semantic-net like structure represented\nusing UNL. We demonstrate how Bangla texts are analyzed following the PPP\ntechnique to produce UNL documents which can then be translated into any other\nsuitable natural language facilitating the opportunity to develop a universal\nlanguage translation method via UNL.\n",
        "  Recent computational studies confirmed by experiment have established the\noccurrence of superconducting temperatures, $T_c$, near 200 K when the pressure\nis close to 200 GPa in the compound H$_3S$. Motivated by these findings we\ninvestigate in this work the possibility of discovering high-temperature\nsuperconductivity in the material H$_3$F. We performed linearized augmented\nplane wave(LAPW) calculations followed by the determination of the angular\nmomentum components of the density of states, the scattering phase shifts at\nthe Fermi level and the electron-ion matrix element known as the Hopfield\nparameter. Our calculated Hopfield parameters are much larger than those found\nin H$_3$S suggesting that they may lead to large electron-phonon coupling\nconstant and hence a large Tc similar or even larger than that of H$_3$S.\nHowever, calculations of elastic constants are inconclusive regarding the\nstability of this material.\n",
        "  We have analyzed the angular momentum of the molecular cloud cores in the\nOrion A giant molecular cloud observed in the N2H+ J = 1-0 line with the\nNobeyama 45 m radio telescope. We have measured the velocity gradient using\nposition velocity diagrams passing through core centers, and made sinusoidal\nfitting against the position angle. 27 out of 34 N2H+ cores allowed us to\nmeasure the velocity gradient without serious confusion. The derived velocity\ngradient ranges from 0.5 to 7.8 km/s/pc. We marginally found that the specific\nangular momentum J/M (against the core radius R) of the Orion N2H+ cores tends\nto be systematically larger than that of molecular cloud cores in cold dark\nclouds obtained by Goodman et al., in the J/M-R relation. The ratio beta of\nrotational to gravitational energy is derived to be beta = 10^{-2.3+/-0.7}, and\nis similar to that obtained for cold dark cloud cores in a consistent\ndefinition. The large-scale rotation of the integral-shaped filament of the\nOrion A giant molecular cloud does not likely govern the core rotation at\nsmaller scales.\n",
        "  The superconducting state in vanadium characterizes with the critical\ntemperature ($T_{c}$) equal to $5.3$~K. The Coulomb pseudopotential, calculated\nwith the help of the Eliashberg equations, possesses anomalously high value\n$\\mu^{\\star}\\left(3\\Omega_{\\rm max}\\right)=0.259$ or\n$\\mu^{\\star}\\left(10\\Omega_{\\rm max}\\right)=0.368$ ($\\Omega_{\\rm max}$ denotes\nthe maximum phonon frequency). Despite the relatively large electron-phonon\ncoupling constant ($\\lambda=0.91$), the quantities such as: the order parameter\n($\\Delta$), the specific heat ($C$), and the thermodynamic critical field\n($H_{c}$) determine the values of the dimensionless ratios not deviating much\nfrom the predictions of the BCS theory: $R_{\\Delta}=2\\Delta\\left(0\\right)/\nk_{B}T_{c}=3.68$, $R_{C}=\\Delta C\\left(T_{c}\\right)/\nC^{N}\\left(T_{c}\\right)=1.69$, and $R_{H}=T_{c}C^{N}\\left(T_{c}\\right)/\nH^{2}_{c}\\left(0\\right)=0.171$. This result is associated with the reduction of\nthe strong-coupling and the retardation effects by the high value of the\nCoulomb pseudopotential. It has been shown that the results of the Eliashberg\nformalism can be relatively precisely reproduced with the help of the\nsemi-analytical formulas, if the value of $\\mu^{\\star}$ is determined on the\nbasis of the $T_{c}$-Allen-Dynes expression ($\\mu^{\\star}_{AD}=0.198$). The\nattention should be paid to the fact that in the numerical and in the\nsemi-analytical approach the comparable values of the thermodynamic parameters\nfor the same $\\mu^{\\star}$ have been obtained only in the vicinity of the point\n$\\mu^{\\star}=0.1$.\n",
        "  As with general graph processing systems, partitioning data over a cluster of\nmachines improves the scalability of graph database management systems.\nHowever, these systems will incur additional network cost during the execution\nof a query workload, due to inter-partition traversals. Workload-agnostic\npartitioning algorithms typically minimise the likelihood of any edge crossing\npartition boundaries. However, these partitioners are sub-optimal with respect\nto many workloads, especially queries, which may require more frequent\ntraversal of specific subsets of inter-partition edges. Furthermore, they\nlargely unsuited to operating incrementally on dynamic, growing graphs.\n  We present a new graph partitioning algorithm, Loom, that operates on a\nstream of graph updates and continuously allocates the new vertices and edges\nto partitions, taking into account a query workload of graph pattern\nexpressions along with their relative frequencies.\n  First we capture the most common patterns of edge traversals which occur when\nexecuting queries. We then compare sub-graphs, which present themselves\nincrementally in the graph update stream, against these common patterns.\nFinally we attempt to allocate each match to single partitions, reducing the\nnumber of inter-partition edges within frequently traversed sub-graphs and\nimproving average query performance.\n  Loom is extensively evaluated over several large test graphs with realistic\nquery workloads and various orderings of the graph updates. We demonstrate\nthat, given a workload, our prototype produces partitionings of significantly\nbetter quality than existing streaming graph partitioning algorithms Fennel and\nLDG.\n",
        "  230 MeV proton beam out of a cyclotron was delivered into a Zebra multi\nlayered IC detector (IBA) calibrated in terms of penetration range in water.\nThe analysis of the measured Bragg peak determines penetration range in water\nwhich can be subsequently converted into proton beam energy using Range-Energy\ntables. We extended this analysis to obtain an estimate of the beam energy\nspread out of the cyclotron. Using Monte Carlo simulations we established the\ncorrelation between Bragg peak shape parameters (width at 50% and 80% dose\nlevels, distal falloff) and penetration range for a monoenergetic proton beam.\nThen we studied how this correlation changes when the shape of Bragg peak is\ndistorted by the beam focusing conditions. We found that small field size or\ndiverging beam cause Bragg peak deformation predominantly in the proximal\nregion. The distal shape of the renormalized Bragg peaks stays nearly constant.\nThis excludes usage of Bragg peak width parameters for energy spread estimates.\nThe measured Bragg peaks had an average distal falloff of 4.86mm, which\ncorresponds to an effective range of 35.5cm for a monoenergetic beam. The\n32.7cm measured penetration range is 2.8cm less. Passage of a 230 MeV proton\nbeam through a 2.8cm thick slab of water results in a 0.56 MeV energy spread.\nAs a final check, we confirmed agreement between shapes of the measured Bragg\npeak and one generated by Monte-Carlo code for proton beam with 0.56 MeV energy\nspread.\n",
        "  Data exchange is the problem of transforming data that is structured under a\nsource schema into data structured under another schema, called the target\nschema, so that both the source and target data satisfy the relationship\nbetween the schemas. Even though the formal framework of data exchange for\nrelational database systems is well-established, it does not immediately carry\nover to the settings of temporal data, which necessitates reasoning over\nunbounded periods of time. In this work, we study data exchange for temporal\ndata. We first motivate the need for two views of temporal data: the concrete\nview, which depicts how temporal data is compactly represented and on which the\nimplementations are based, and the abstract view, which defines the semantics\nof temporal data as a sequence of snapshots. We first extend the chase\nprocedure for the abstract view to have a conceptual basis for the data\nexchange for temporal databases. Considering non-temporal source-to-target\ntuple generating dependencies and equality generating dependencies, the chase\nalgorithm can be applied on each snapshot independently. Then we define a chase\nprocedure (called c-chase) on concrete instances and show the result of c-chase\non a concrete instance is semantically aligned with the result of chase on the\ncorresponding abstract instance. In order to interpret intervals as constants\nwhile checking if a dependency or a query is satisfied by a concrete database,\nwe will normalize the instance with respect to the dependency or the query. To\nobtain the semantic alignment, the nulls in the concrete view are annotated\nwith temporal information. Furthermore, we show that the result of the concrete\nchase provides a foundation for query answering. We define naive evaluation on\nthe result of the c-chase and show it produces certain answers.\n",
        "  The influence of two important features of magnesium diboride on the\nmacroscopic transport properties of polycrystalline MgB2 is discussed in the\nframework of a percolation model. While two band superconductivity does not\nhave significant consequences in the field and temperature range of possible\npower applications, the opposite is true for the anisotropy of the upper\ncritical field. The field dependence of the critical current densities strongly\nincreases and the macroscopic supercurrents disappear well below the apparent\nupper critical field. The common scaling laws for the field dependence of the\nvolume pinning force are altered and Kramer's plot is no longer linear,\nalthough grain boundary pinning dominates in nearly all polycrystalline MgB2\nconductors. In contrast to the conventional superconductors NbTi and Nb3Sn, a\nsignificant critical current anisotropy can be induced by the preparation\ntechnique of MgB2 tapes.\n",
        "  Millions of users share their experiences on social media sites, such as\nTwitter, which in turn generate valuable data for public health monitoring,\ndigital epidemiology, and other analyses of population health at global scale.\nThe first, critical, task for these applications is classifying whether a\npersonal health event was mentioned, which we call the (PHM) problem. This task\nis challenging for many reasons, including typically short length of social\nmedia posts, inventive spelling and lexicons, and figurative language,\nincluding hyperbole using diseases like \"heart attack\" or \"cancer\" for\nemphasis, and not as a health self-report. This problem is even more\nchallenging for rarely reported, or frequent but ambiguously expressed\nconditions, such as \"stroke\". To address this problem, we propose a general,\nrobust method for detecting PHMs in social media, which we call WESPAD, that\ncombines lexical, syntactic, word embedding-based, and context-based features.\nWESPAD is able to generalize from few examples by automatically distorting the\nword embedding space to most effectively detect the true health mentions.\nUnlike previously proposed state-of-the-art supervised and deep-learning\ntechniques, WESPAD requires relatively little training data, which makes it\npossible to adapt, with minimal effort, to each new disease and condition. We\nevaluate WESPAD on both an established publicly available Flu detection\nbenchmark, and on a new dataset that we have constructed with mentions of\nmultiple health conditions. Our experiments show that WESPAD outperforms the\nbaselines and state-of-the-art methods, especially in cases when the number and\nproportion of true health mentions in the training data is small.\n",
        "  This paper presents a novel approach to machine translation by combining the\nstate of art name entity translation scheme. Improper translation of name\nentities lapse the quality of machine translated output. In this work, name\nentities are transliterated by using statistical rule based approach. This\npaper describes the translation and transliteration of name entities from\nEnglish to Punjabi. We have experimented on four types of name entities which\nare: Proper names, Location names, Organization names and miscellaneous.\nVarious rules for the purpose of syllabification have been constructed.\nTransliteration of name entities is accomplished with the help of Probability\ncalculation. N-Gram probabilities for the extracted syllables have been\ncalculated using statistical machine translation toolkit MOSES.\n",
        "  A new set of fluence-to-dose conversion coefficients based on the Chinese\nreference adult voxel phantoms CRAM and CRAF are presented for six idealized\nexternal neutron exposures from 10-8 MeV to 20 MeV. The voxel phantoms CRAM and\nCRAF were adjusted from the previous phantoms CNMAN and CNWM respectively, and\nthe masses of individual organs have been adjusted to the Chinese reference\ndata. The calculation of organ-absorbed doses and effective doses were\nperformed with the Monte Carlo transport code MCNPX. The resulting dose\nconversion coefficients were compared with those published in ICRP Publication\n116, which represents the reference Caucasian. The organ-absorbed dose\nconversion coefficients of most organs are in good agreement with the results\nin ICRP Publication 116, however, obvious discrepancies are observed for some\norgans and certain geometries. For neutrons with energies above 2 MeV, the\neffective dose conversion coefficients of Chinese reference adult are almost\nidentical to those of ICRP Publication 116 in AP, PA, ROT and ISO geometries.\nWhen energies range from 10-8 MeV to 1 MeV, differences are within 10% in AP\n(5%), PA (8%) and ROT (-4%) geometries. However, relatively large discrepancies\nare shown in lateral and ISO geometries when energies are below 1 MeV, with\ndifferences of -15% for LLAT, -20% for RLAT and -12% for ISO, respectively.\n",
        "  Data curation - the process of discovering, integrating, and cleaning data -\nis one of the oldest, hardest, yet inevitable data management problems. Despite\ndecades of efforts from both researchers and practitioners, it is still one of\nthe most time consuming and least enjoyable work of data scientists. In most\norganizations, data curation plays an important role so as to fully unlock the\nvalue of big data. Unfortunately, the current solutions are not keeping up with\nthe ever-changing data ecosystem, because they often require substantially high\nhuman cost. Meanwhile, deep learning is making strides in achieving remarkable\nsuccesses in multiple areas, such as image recognition, natural language\nprocessing, and speech recognition. In this vision paper, we explore how some\nof the fundamental innovations in deep learning could be leveraged to improve\nexisting data curation solutions and to help build new ones. In particular, we\nprovide a thorough overview of the current deep learning landscape, and\nidentify interesting research opportunities and dispel common myths. We hope\nthat the synthesis of these important domains will unleash a series of research\nactivities that will lead to significantly improved solutions for many data\ncuration tasks.\n",
        "  We report the detection of unusual superconductivity up to 49 K in single\ncrystalline CaFe2As2 via electron-doping by partial replacement of Ca by\nrare-earth. The superconducting transition observed suggests the possible\nexistence of two phases: one starting at ~ 49 K, which has a low critical field\n~ 4 Oe, and the other at ~ 21 K, with a much higher critical field > 5 T. Our\nobservations are in strong contrast to previous reports of doping or\npressurizing layered compounds AeFe2As2 (or Ae122), where Ae = Ca, Sr or Ba. In\nAe122, hole-doping has been previously observed to generate superconductivity\nwith a transition temperature (Tc) only up to 38 K and pressurization has been\nreported to produce superconductivity with a Tc up to 30 K. The unusual 49 K\nphase detected will be discussed.\n",
        "  The influence of fatiguing stimuli applied to the quadriceps femoris on the\ntremor of the index finger on sixteen healthy subjects has been investigated,\nby measuring the acceleration and the electromyography activity of the extensor\ndigitorum at two different speeds. No significant changes in the basal\ncondition could be measured at 0.52 rad/s. However, at 1.05 rad/s the tremor\namplitude (12.23%, p<0.05) and the electromyography activity (20.19%, p<0.05)\nincreased in the time domain. In the frequential domain, the peak associated to\nthe acceleration increased in 26.30% (p<0.05), while the electromiography\nactivity experienced an increase of 81.34% (p<0.05). The largest change in\nfrequency took place in the range 7-17 Hz. The discussion of the results leads\nto the conclusion that the supraespinal mechanism is responsible for the\nmeasured effect.\n",
        "  The Galactic All-Sky Survey is a survey of Galactic atomic hydrogen emission\nin the southern sky observed with the Parkes 64-m Radio Telescope. The first\ndata release (GASS I) concerned survey goals and observing techniques, the\nsecond release (GASS II) focused on stray radiation and instrumental\ncorrections. We seek to remove the remaining instrumental effects and present a\nthird data release. We use the HEALPix tessellation concept to grid the data on\nthe sphere. Individual telescope records are compared with averages on the\nnearest grid position for significant deviations. All averages are also\ndecomposed into Gaussian components with the aim of segregating unacceptable\nsolutions. Improved priors are used for an iterative baseline fitting and\ncleaning. In the last step we generate 3-D FITS data cubes and examine them for\nremaining problems. We have removed weak, but systematic baseline offsets with\nan improved baseline fitting algorithm. We have unraveled correlator failures\nthat cause time dependent oscillations; errors cause stripes in the scanning\ndirection. The remaining problems from radio frequency interference (RFI) are\nspotted. Classifying the severeness of instrumental errors for each individual\ntelescope record (dump) allows us to exclude bad data from averages. We derive\nparameters that allow us to discard dumps without compromising the noise of the\nresulting data products too much. All steps are reiterated several times: in\neach case, we check the Gaussian parameters for remaining problems and inspect\n3-D FITS data cubes visually. We find that in total ~1.5% of the telescope\ndumps need to be discarded in addition to ~0.5% of the spectral channels that\nwere excluded in GASS II.The new data release facilitates data products with\nimproved quality. A new web interface, compatible with the previous version, is\navailable for download of GASS III FITS cubes and spectra.\n",
        "  The management requires a hospital organization to provision their\ncosts/expenses with tools that approximate reality. The task of measuring\nproductivity can be complex and uncertain, several methods are tested and the\nuse of the DRG has been efficient, being used to assess the productivity\nthrough clinical outcomes. Cross-sectional study evaluated 145.710\nhospitalizations in the period 2012-2014, using the DRG methodology for\nmeasuring productivity from the median length of hospitalization. When we group\nall hospitalizations in clinical (37.6%) and surgical (62.4%), multiple\nanalyzes could be made according to this criterion. The DRG as a tool for\nprediction of hospital days is an effective alternative, thereby contributing\nto the control of productivity that directly influences the costs of hospital\nexpenses and product and service quality.\n",
        "  Let $X$ be a closed, simply-connected, smooth, spin 4-manifold whose\nintersection form is isomorphic to $n(-E_8)\\bigoplus mH$, where $H$ is the\nhyperbolic form.\n  In this paper, we prove that for $n$ such that $n\\equiv 2 ~{\\rm mod} ~4$,\nthere exists a locally linear pseudofree $\\mathbb{Z}_2$-action on $X$ which is\nnonsmoothable with respect to any possible smooth structure on $X$.\n",
        "  The large field and wavelength range of MUSE is well suited to mapping\nGalactic planetary nebulae (PN). The bright PN NGC 7009 was observed with MUSE\non the VLT during the Science Verification of the instrument in seeing of 0.6\".\nEmission line maps in hydrogen Balmer and Paschen lines were formed from\nanalysis of the MUSE cubes. The measured electron temperature and density from\nthe MUSE cube were employed to predict the theoretical hydrogen line ratios and\nmap the extinction distribution across the nebula. After correction for the\ninterstellar extinction to NGC 7009, the internal dust-to-gas ratio (A_V/N_H)\nhas been mapped for the first time in a PN. The extinction map of NGC 7009 has\nconsiderable structure, broadly corresponding to the morphological features of\nthe nebula. A large-scale feature in the extinction map, consisting of a crest\nand trough, occurs at the rim of the inner shell. The nature of this feature\nwas investigated and instrumental and physical causes considered; no convincing\nmechanisms were identified to produce this feature, other than mass loss\nvariations in the earlier asymptotic giant branch phase. The dust-to-gas ratio\nA_V/N_H increases from 0.7 times the interstellar value to >5 times from the\ncentre towards the periphery of the ionized nebula. The integrated A_V/N_H is\nabout 2 times the mean ISM value. It is demonstrated that extinction mapping\nwith MUSE provides a powerful tool for studying the distribution of PN internal\ndust and the dust-to-gas ratio. (Abridged.)\n",
        "  We numerically investigate the Spin Density Functional theory for\nsuperconductors (SpinSCDFT) and the approximated exchange-correlation\nfunctional, derived and presented in the preceding paper I. As a test system we\nemploy a free electron gas featuring an exchange-splitting, a phononic pairing\nfield and a Coulomb repulsion. SpinSCDFT results are compared with Sarma, the\nBardeen Cooper and Schrieffer theory and with an Eliashberg type of approach.\nWe find that the spectrum of the superconducting Kohn-Sham SpinSCDFT system is\nnot in agreement with the true quasi particle structure. Therefore, starting\nfrom the Dyson equation, we derive a scheme that allows to compute the many\nbody excitations of the superconductor and represents the extension to\nsuperconductivity of the G0W0 method in band structure theory. This\nsuperconducting G0 W0 method vastly improves the predicted spectra.\n",
        "  We announce the discovery of the Aquarius~2 dwarf galaxy, a new distant\nsatellite of the Milky Way, detected on the fringes of the VST ATLAS and the\nSDSS surveys. The object was originally identified as an overdensity of Red\nGiant Branch stars, but chosen for subsequent follow-up based on the presence\nof a strong Blue Horizontal Branch, which was also used to measure its distance\nof $\\sim 110$ kpc. Using deeper imaging from the IMACS camera on the 6.5m Baade\nand spectroscopy with DEIMOS on Keck, we measured the satellite's half-light\nradius $5.1\\pm 0.8$ arcmin, or $\\sim 160$ pc at this distance, and its stellar\nvelocity dispersion of $5.4^{+3.4}_{-0.9}$ km s$^{-1}$. With $\\mu=30.2$ mag\narcsec$^{-2}$ and $M_V=-4.36$, the new satellite lies close to two important\ndetection limits: one in surface brightness; and one in luminosity at a given\ndistance, thereby making Aquarius~2 one of the hardest dwarfs to find.\n",
        "  Multiple ultrasound elastography techniques rely on acoustic radiation force\n(ARF) in monitoring high-intensity focused ultrasound (HIFU) therapy. However,\nARF is dependent on tissue attenuation and sound speed, both of which are also\nknown to change with temperature making the therapy monitoring more\nchallenging. Furthermore, the viscoelastic properties of tissue are also\ntemperature dependent, which affects the displacements induced by ARF. The aim\nof this study is to quantify the temperature dependent changes in the acoustic\nand viscoelastic properties of liver and investigate their effect on ARF\ninduced displacements by using both experimental methods and simulations.\nFurthermore, the temperature dependent viscoelastic properties of liver are\nexperimentally measured over a frequency range of 0.1-200 Hz at temperatures\nreaching 80 C, and both conventional and fractional Zener models are used to\nfit the data.\n  The fractional Zener model was found to fit better with the experimental\nviscoelasticity data with respect to the conventional model with up to two\nmagnitudes lower sum of squared errors (SSE). The characteristics of\nexperimental displacement data were also seen in the simulations due to the\nchanges in attenuation coefficient and lesion development. At low temperatures\nbefore thermal ablation, attenuation was found to affect the displacement\namplitude. At higher temperature, the decrease in displacement amplitude occurs\napproximately at 60-70 C due to the combined effect of viscoelasticity changes\nand lesion growth overpowering the effect of attenuation. The results suggest\nthat it is necessary to monitor displacement continuously during HIFU therapy\nin order to ascertain when ablation occurs.\n",
        "  Edwards et al question aspects of the methods used in two of our published\npapers that report results showing Levy walk like and Levy flight movement\npatterns of marine predators.The criticisms are focused on the applicability of\nsome statistical methodologies used to detect power law distributions.We reply\nto the principal criticisms levelled at each of these papers in turn including\nour own reanalysis of specific datasets and find that neither of our papers\nconclusions are overturned in any part by the issues raised.Indeed, in addition\nto the findings of our research reported in these papers there is strong\nevidence accumulating from studies worldwide that organisms show movements and\nbehaviour consistent with scale invariant patterns such as Levy flights.\n",
        "  BABYSCAN, a whole body counter for small children with a detection limit for\n$^{137}$Cs of better than 50 Bq/body, was developed, and the first unit has\nbeen installed at a hospital in Fukushima, to help families with small children\nwho are very much concerned about internal exposures. The design principles,\nimplementation details and the initial operating experience are described.\n",
        "  The origin of the nematic state is an important puzzle to be solved in iron\npnictides. Iron superconductors are multiorbital systems and these orbitals\nplay an important role at low energy. The singular $C_4$ symmetry of $d_{zx}$\nand $d_{yz}$ orbitals has a profound influence at the Fermi surface since the\n$\\Gamma$ pocket has vortex structure in the orbital space and the X/Y electron\npockets have $yz$/$zx$ components respectively. We propose a low energy theory\nfor the spin--nematic model derived from a multiorbital Hamiltonian. In the\nstandard spin--nematic scenario the ellipticity of the electron pockets is a\nnecessary condition for nematicity. In the present model nematicity is\nessentially due to the singular $C_4$ symmetry of $yz$ and $zx$ orbitals. By\nanalyzing the ($\\pi, 0$) spin susceptibility in the nematic phase we find\nspontaneous generation of orbital splitting extending previous calculations in\nthe magnetic phase. We also find that the ($\\pi, 0$) spin susceptibility has an\nintrinsic anisotropic momentum dependence due to the non trivial topology of\nthe $\\Gamma$ pocket.\n",
        "  The second Betti number of a smooth, closed, connected and simply connected,\nfour-dimensional spin manifold is greater or equal 11/8 times the abolute value\nof its signature.\n",
        "  We report pressure tuned Raman and x-ray diffraction data of\nBi1.98Sr2.06Y0.68Cu2O8 revealing a critical pressure at 21 GPa with anomalies\nin six physical quantities: electronic Raman background, electron-phonon\ncoupling, spectral weight transfer from high to low frequency, density\ndependent behaviour of phonon and magnon frequencies, and a compressibility\nchange in the c-axis. For the first time in a cuprate, mobile charge carriers,\nlattice, and magnetism all show anomalies at a distinct critical pressure in\nthe same experimental setting. Furthermore, the Raman spectral changes are\nsimilar to that seen traversing the superconducting dome with doping,\nsuggesting that the critical pressure at 21 GPa is related to the much\ndiscussed critical point at optimal doping.\n",
        "  In this work we establish and investigate connections between causality for\nquery answers in databases, database repairs wrt. denial constraints, and\nconsistency-based diagnosis. The first two are relatively new problems in\ndatabases, and the third one is an established subject in knowledge\nrepresentation. We show how to obtain database repairs from causes and the\nother way around. Causality problems are formulated as diagnosis problems, and\nthe diagnoses provide causes and their responsibilities. The vast body of\nresearch on database repairs can be applied to the newer problem of determining\nactual causes for query answers and their responsibilities. These connections,\nwhich are interesting per se, allow us, after a transition -inspired by\nconsistency-based diagnosis- to computational problems on hitting sets and\nvertex covers in hypergraphs, to obtain several new algorithmic and complexity\nresults for causality in databases.\n",
        "  Diabetes is one of the chronic diseases, which is increasing from year to\nyear. The problems begin when diabetes is not detected at an early phase and\ndiagnosed properly at the appropriate time. Different machine learning\ntechniques, as well as ontology-based ML techniques, have recently played an\nimportant role in medical science by developing an automated system that can\ndetect diabetes patients. This paper provides a comparative study and review of\nthe most popular machine learning techniques and ontology-based Machine\nLearning classification. Various types of classification algorithms were\nconsidered namely: SVM, KNN, ANN, Naive Bayes, Logistic regression, and\nDecision Tree. The results are evaluated based on performance metrics like\nRecall, Accuracy, Precision, and F-Measure that are derived from the confusion\nmatrix. The experimental results showed that the best accuracy goes for\nontology classifiers and SVM.\n",
        "  We use polarization-resolved electronic Raman spectroscopy to study charge\ndynamics in non-magnetic FeSe$_{1-x}$S$_x$ superconductor. We observe two\nfeatures of the $XY$ quadrupole symmetry: a low-energy quasi-elastic peak (QEP)\nand an electronic continuum. The QEP exhibits critical enhancement upon cooling\ntowards the structural transition at $T_S(x)$. Below $T_S(x)$, the QEP\ndiminishes gradually, and a gap with temperature evolution reminiscent of a\nmean-field order parameter opens in the continuum. The intensity of the QEP\ndevelops with increasing sulfur doping $x$ and maximizes at $x\\approx$ 0.15,\nwhile the gap magnitude decreases with the suppression of $T_S(x)$. We\ninterpret the development of the gap in the quadrupole scattering channel as\nthe formation of a stripe quadrupole order: a wave of quadrupole moment without\ncharge or spin modulation.\n",
        "  In most photoacoustic (PA) measurements, variations in speed-of-sound (SOS)\nof the subject are neglected under the assumption of acoustic homogeneity.\nBiological tissue with spatially heterogeneous SOS cannot be accurately\nreconstructed under this assumption. We present experimental and image\nreconstruction methods with which 2-D SOS distributions can be accurately\nacquired and reconstructed, and with which the SOS map can be used subsequently\nto reconstruct highly accurate PA tomograms. We begin with a 2-D iterative\nreconstruction approach in an ultrasound transmission tomography (UTT) setting,\nwhich uses ray refracted paths instead of straight ray paths to recover\naccurate SOS images of the subject. Subsequently, we use the SOS distribution\nin a new 2-D iterative approach, where refraction of rays originating from PA\nsources are accounted for in accurately retrieving the distribution of these\nsources. Both the SOS reconstruction and SOS-compensated PA reconstruction\nmethods utilize the Eikonal equation to model acoustic wavefront propagation,\nwhich is solved using a high accuracy fast marching method (HAFMM). We validate\nthe new reconstruction algorithms using numerical phantoms. For experiments we\nuse the PER-PACT method which can be used to simultaneously acquire SOS and PA\ndata from subjects. We test the reconstruction algorithms using experimental\ndata acquired with the PER-PACT setup from challenging physical phantoms. The\nresults show that it is important to take SOS inhomogeneities into account. The\niterative reconstruction algorithms, that model acoustic refractive effects,\nyield artifact-free highly accurate images. Our approach of using the hybrid\nmeasurement method and the new reconstruction algorithms, is successful in\nsubstantially improving the quality of PA images with a minimization of\nblurring and artefacts.\n",
        "  Database-backed applications are nearly ubiquitous in our daily lives.\nApplications that make many small accesses to the database create two\nchallenges for developers: increased latency and wasted resources from numerous\nnetwork round trips. A well-known technique to improve transactional database\napplication performance is to convert part of the application into stored\nprocedures that are executed on the database server. Unfortunately, this\nconversion is often difficult. In this paper we describe Pyxis, a system that\ntakes database-backed applications and automatically partitions their code into\ntwo pieces, one of which is executed on the application server and the other on\nthe database server. Pyxis profiles the application and server loads,\nstatically analyzes the code's dependencies, and produces a partitioning that\nminimizes the number of control transfers as well as the amount of data sent\nduring each transfer. Our experiments using TPC-C and TPC-W show that Pyxis is\nable to generate partitions with up to 3x reduction in latency and 1.7x\nimprovement in throughput when compared to a traditional non-partitioned\nimplementation and has comparable performance to that of a custom stored\nprocedure implementation.\n",
        "  A fundamental problem in the fields of population genetics, evolution, and\ncommunity ecology, is the fate of a single mutant, or invader, introduced in a\nfinite population of wild types. For a fixed-size community of $N$ individuals,\nwith Markovian, zero-sum dynamics driven by stochastic birth-death events, the\nmutant population eventually reaches either fixation or extinction. The\nclassical analysis, provided by Kimura and his coworkers, is focused on the\nneutral case, [where the dynamics is only due to demographic stochasticity\n(drift)], and on \\emph{time-independent} selective forces\n(deleterious/beneficial mutation). However, both theoretical arguments and\nempirical analyses suggest that in many cases the selective forces fluctuate in\ntime (temporal environmental stochasticity). Here we consider a generic model\nfor a system with demographic noise and fluctuating selection. Our system is\ncharacterized by the time-averaged (log)-fitness $s_0$ and zero-mean fitness\nfluctuations. These fluctuations, in turn, are parameterized by their amplitude\n$\\gamma$ and their correlation time $\\delta$. We provide asymptotic (large $N$)\nformulas for the chance of fixation, the mean time to fixation and the mean\ntime to absorption. Our expressions interpolate correctly between the constant\nselection limit $\\gamma \\to 0$ and the time-averaged neutral case $s_0=0$.\n",
        "  This paper assesses the roles of the presence of warm H2, and the increased\nformation rate due to the ion-neutral drift. We performed ideal MHD simulations\nthat include the heating and cooling of the multiphase ISM, and where we treat\ndynamically the formation of H2. In a post-processing step we compute the\nabundances of species at chemical equilibrium. We show that CH+ is efficiently\nformed at the edge of clumps, in regions where the H2 fraction is low, but\nnevertheless higher than its equilibrium value, and where the gas temperature\nis high. We show that warm and out of equilibrium H2 increases the integrated\ncolumn densities of CH+ by one order of magnitude, up to values still 3-10\ntimes lower than those observed in the diffuse ISM. We balance the Lorentz\nforce with the ion-neutral drag to estimate the ion-drift velocities (vd). We\nfind that the vd distribution peaks around 0.04 km s-1, and that high vd are\ntoo rare to have a significant statistical impact on the abundances of CH+.\nCompared to previous works, our multiphase simulations reduce the spread in vd,\nand our self-consistent treatment of the ionisation leads to much reduced vd.\nNevertheless, our resolution study shows that this velocity distribution is not\nconverged: the ion-neutral drift has a higher impact on CH+ at higher\nresolution. On the other hand, our ideal MHD simulations do not include\nambipolar diffusion, which would yield lower drift velocities. Within these\nlimitations, we conclude that warm H2 is a key ingredient in the efficient\nformation of CH+ and that the ambipolar diffusion has very little influence on\nthe abundance of CH+, mainly due to the small drift velocities obtained.\nHowever, we point out that small-scale processes and other non-thermal\nprocesses not included in our MHD simulation may be of crucial importance, and\nhigher resolution studies with better controlled dissipation processes are\nneeded.\n",
        "  Topological nodal superconductors are generally realized based on\nunconventional pairings. In this work, we propose a minimal model to realize\nthese topological nodal phases with only $s$-wave interaction. In our model the\nlinear and quadratic spin-orbit couplings along the two directions break the\nisotropy in momentum space and introduce effective unconventional pairings on\nthe Fermi surface. This model may support different nodal superconducting\nphases characterized by either winding number in BDI class or Pfaffian in D\nclass at the particle-hole invariant axes. In the vicinity of the nodal points\nthe effective Hamiltonian can be described by either type-I or type-II Dirac\nequation; and the crossover between these two nodal points can be driven by\nexternal Zeeman fields. We show that these nodal phases are robust against weak\ndisorders, thus are possible to be realized in experiments with real materials.\nThe smoking-gun evidences to verify these phases based on scanning tunneling\nspectroscopy are also briefly discussed.\n",
        "  Metric validation in Grammatical Error Correction (GEC) is currently done by\nobserving the correlation between human and metric-induced rankings. However,\nsuch correlation studies are costly, methodologically troublesome, and suffer\nfrom low inter-rater agreement. We propose MAEGE, an automatic methodology for\nGEC metric validation, that overcomes many of the difficulties with existing\npractices. Experiments with \\maege\\ shed a new light on metric quality, showing\nfor example that the standard $M^2$ metric fares poorly on corpus-level\nranking. Moreover, we use MAEGE to perform a detailed analysis of metric\nbehavior, showing that correcting some types of errors is consistently\npenalized by existing metrics.\n",
        "  It is possible to make image reconstruction based on the dose dependence of\nthe therapeutic XA (X-ray induced acoustic signal) amplitude which is then used\nto make dose mapping. We give further explicit parametrization for the acoustic\nsignal in terms of the absorption parameters based on a physical model of the\nabsorption process. The first step is to obtain pressure waveform due to a\npoint dose absorption by solving the thermo-acoustic equation governing the\nheat absorption-pressure induction process based on the analytic integration\ntechnique. Then, clinically relevant XA signal profile at the detection point\nis obtained by generalizing point-dose-gradient induced acoustic signal to\nsurface-dose-gradient of a uniform spherical 3D dose distribution based on the\nreciprocity principle for pressure waves in fluid media. Therapeutic XA signal\ninduced from the surface of the uniform spherical dose distribution due to\nX-ray irradiation onto $5x5$ $cm^2$ field of the water surface by $1~\\mu s$\npulses delivering $1.7$ mGy/pulse is simulated in time and frequency domain. XA\nwaves obtained in previous empirical studies are simulated and compared by\nmeans of shape and relative amplitude. Considering the previous studies on this\nsubject, we believe that the significance of this study is the foundation of a\nnovel and self-contained analytic approach to simulate the therapeutic X-ray\nacoustic waves based on the physical parametrization of the energy transfer\nprocess. This not only provides a better understanding of the physical\nphenomena underlying the medical technique in terms of the medically relevant\nparameters such as field size, pulse duration, absorbed dose per pulse etc.\ntogether with the physical assumptions used to obtain a solution to the\nphoto-acoustic equation, but also brings consistent simulation results with\nprevious experimental and k-Wave results.\n",
        "  We construct dynamical models of the Milky Way's Box/Peanut (B/P) bulge,\nusing the recently measured 3D density of Red Clump Giants (RCGs) as well as\nkinematic data from the BRAVA survey. We match these data using the NMAGIC\nMade-to-Measure method, starting with N-body models for barred discs in\ndifferent dark matter haloes. We determine the total mass in the bulge volume\nof the RCGs measurement (+-2.2 x +- 1.4 x +- 1.2 kpc) with unprecedented\naccuracy and robustness to be 1.84 +- 0.07 x10^10 Msun. The stellar mass in\nthis volume varies between 1.25-1.6 x10^10 Msun, depending on the amount of\ndark matter in the bulge. We evaluate the mass-to-light and mass-to-clump\nratios in the bulge and compare them to theoretical predictions from population\nsynthesis models. We find a mass-to-light ratio in the K-band in the range\n0.8-1.1. The models are consistent with a Kroupa or Chabrier IMF, but a\nSalpeter IMF is ruled out for stellar ages of 10 Gyr. To match predictions from\nthe Zoccali IMF derived from the bulge stellar luminosity function requires\nabout 40% or 0.7 x10^10 Msun dark matter in the bulge region. The BRAVA data\ntogether with the RCGs 3D density imply a low pattern speed for the Galactic\nB/P bulge of 25-30 km.s-1.kpc-1. This would place the Galaxy among the slow\nrotators (R >= 1.5). Finally, we show that the Milky Way's B/P bulge has an\noff-centred X structure, and that the stellar mass involved in the peanut shape\naccounts for at least 20% of the stellar mass of the bulge, significantly\nlarger than previously thought.\n",
        "  We compute an explicit formula for the expected value of the Colless index of\na phylogenetic tree generated under the Yule model, and an explicit formula for\nthe expected value of the Sackin index of a phylogenetic tree generated under\nthe uniform model.\n",
        "  A Blink Tree latch method and protocol supports synchronous node deletion in\na high concurrency environment. Full source code is available.\n",
        "  Topological texture features were compared in their ability to classify\nmorphological patterns known as 'honeycombing' that are considered indicative\nfor the presence of fibrotic interstitial lung diseases in high-resolution\ncomputed tomography (HRCT) images. For 14 patients with known occurrence of\nhoney-combing, a stack of 70 axial, lung kernel reconstructed images were\nacquired from HRCT chest exams. A set of 241 regions of interest of both\nhealthy and pathological (89) lung tissue were identified by an experienced\nradiologist. Texture features were extracted using six properties calculated\nfrom gray-level co-occurrence matrices (GLCM), Minkowski Dimensions (MDs), and\nthree Minkowski Functionals (MFs, e.g. MF.euler). A k-nearest-neighbor (k-NN)\nclassifier and a Multilayer Radial Basis Functions Network (RBFN) were\noptimized in a 10-fold cross-validation for each texture vector, and the\nclassification accuracy was calculated on independent test sets as a\nquantitative measure of automated tissue characterization. A Wilcoxon\nsigned-rank test was used to compare two accuracy distributions and the\nsignificance thresholds were adjusted for multiple comparisons by the\nBonferroni correction. The best classification results were obtained by the MF\nfeatures, which performed significantly better than all the standard GLCM and\nMD features (p < 0.005) for both classifiers. The highest accuracy was found\nfor MF.euler (97.5%, 96.6%; for the k-NN and RBFN classifier, respectively).\nThe best standard texture features were the GLCM features 'homogeneity' (91.8%,\n87.2%) and 'absolute value' (90.2%, 88.5%). The results indicate that advanced\ntopological texture features can provide superior classification performance in\ncomputer-assisted diagnosis of interstitial lung diseases when compared to\nstandard texture analysis methods.\n",
        "  Superconducting properties change in confined geometries. Here we study the\neffects of strong confinement in nanosized Pb-islands on Si(111) 7x7. Small\nhexagonal islands with diameters less than 50 nm and a uniform height of 7\natomic layers are formed by depositing Pb at low temperature and annealing at\n300 K. We measure the tunneling spectra of individual Pb-nanoislands using a\nlow-temperature scanning tunneling microscope operated at 0.6 K, and follow the\nnarrowing of the superconducting gap as a function of magnetic field. We find\nthe critical magnetic field, at which the superconducting gap vanishes, reaches\nseveral Tesla, which represents a greater than 50-fold enhancement compared to\nthe bulk value. By independently measuring the size of the superconducting gap,\nand the critical magnetic field that quenches superconductivity for a range of\nnanoislands we can correlate these two fundamental parameters and estimate the\nmaximal achievable critical field for 7 ML Pb-nanoislands to 7 T.\n",
        "  Although the existence of a cause and effect relationship between emotions\nand Psychosomatic Illnesses (manifestation of organic diseases produced by\nemotional problems) is an unquestionable fact, yet it remains to be explained\nthe mechanism by which an emotion affects the cure or worsening of an illness.\nIn this paper we will examine some articles written by doctors and\npsychologists which confirm the Prevention Effect (or even cure) of such\nillnesses, and hypothesize that this cause and effect relationship is mediated\nby a substance, D (+) - adrenaline, which is one of the enantiomers [1] of the\nadrenaline molecule.\n",
        "  The division of labor (DOL) and task allocation among groups of ants living\nin a colony is thought to be highly efficient, and key to the robust survival\nof a colony. A great deal of experimental and theoretical work has been done\ntoward gaining a clear understanding of the evolution of, and underlying\nmechanisms of these phenomena. Much of this research has utilized mathematical\nmodeling. Here we continue this tradition by developing a mathematical model\nfor a particular aspect of task allocation, known as age-related repertoire\nexpansion, that has been observed in the minor workers of the ant species\n\\emph{Pheidole dentata}. In fact, we present a relatively broad mathematical\nmodeling framework based on the dynamics of the frequency with which members of\nspecific age groups carry out distinct tasks. We apply our modeling approach to\na specific task allocation scenario, and compare our theoretical results with\nexperimental data. It is observed that the model predicts perceived behavior,\nand provides a possible explanation for the aforementioned experimental\nresults.\n",
        "  The Linguistic Data Consortium (LDC) has developed hundreds of data corpora\nfor natural language processing (NLP) research. Among these are a number of\nannotated treebank corpora for Arabic. Typically, these corpora consist of a\nsingle collection of annotated documents. NLP research, however, usually\nrequires multiple data sets for the purposes of training models, developing\ntechniques, and final evaluation. Therefore it becomes necessary to divide the\ncorpora used into the required data sets (divisions). This document details a\nset of rules that have been defined to enable consistent divisions for old and\nnew Arabic treebanks (ATB) and related corpora.\n",
        "  Effective demagnetizing factors that connect the sample magnetic moment with\nthe applied magnetic field are calculated numerically for perfectly diamagnetic\nsamples of various non-ellipsoidal shapes. The procedure is based on\ncalculating total magnetic moment by integrating the magnetic induction\nobtained from a full three dimensional solution of the Maxwell equations using\nadaptive mesh. The results are relevant for superconductors (and conductors in\nAC fields) when the London penetration depth (or the skin depth) is much\nsmaller than the sample size. Simple but reasonably accurate approximate\nformulas are given for practical shapes including rectangular cuboids, finite\ncylinders in axial and transverse field as well as infinite rectangular and\nelliptical cross-section strips.\n",
        "  The accumulation of beneficial mutations on many competing genetic\nbackgrounds in rapidly adapting populations has a striking impact on\nevolutionary dynamics. This effect, known as clonal interference, causes\nerratic fluctuations in the frequencies of observed mutations, randomizes the\nfixation times of successful mutations, and leaves distinct signatures on\npatterns of genetic variation. Here, we show how this form of `genetic draft'\naffects the forward-time dynamics of site frequencies in rapidly adapting\nasexual populations. We calculate the probability that mutations at individual\nsites shift in frequency over a characteristic timescale, extending Gillespie's\noriginal model of draft to the case where many strongly selected beneficial\nmutations segregate simultaneously. We then derive the sojourn time of mutant\nalleles, the expected fixation time of successful mutants, and the site\nfrequency spectrum of beneficial and neutral mutations. We show how this form\nof draft affects inferences in the McDonald-Kreitman test, and how it relates\nto recent observations that some aspects of genetic diversity are described by\nthe Bolthausen-Sznitman coalescent in the limit of very rapid adaptation.\nFinally, we describe how our method can be extended to model evolution on\nfitness landscapes that include some forms of epistasis, such as landscapes\nthat are partitioned into two or more incompatible evolutionary trajectories.\n",
        "  Weighted association rule mining reflects semantic significance of item by\nconsidering its weight. Classification constructs the classifier and predicts\nthe new data instance. This paper proposes compact weighted class association\nrule mining method, which applies weighted association rule mining in the\nclassification and constructs an efficient weighted associative classifier.\nThis proposed associative classification algorithm chooses one non class\ninformative attribute from dataset and all the weighted class association rules\nare generated based on that attribute. The weight of the item is considered as\none of the parameter in generating the weighted class association rules. This\nproposed algorithm calculates the weight using the HITS model. Experimental\nresults show that the proposed system generates less number of high quality\nrules which improves the classification accuracy.\n",
        "  There are many clustering methods, such as hierarchical clustering method.\nMost of the approaches to the clustering of variables encountered in the\nliterature are of hierarchical type. The great majority of hierarchical\napproaches to the clustering of variables are of agglomerative nature. The\nagglomerative hierarchical approach to clustering starts with each observation\nas its own cluster and then continually groups the observations into\nincreasingly larger groups. Higher Learning Institution (HLI) provides training\nto introduce final-year students to the real working environment. In this\nresearch will use Euclidean single linkage and complete linkage. MATLAB and HCE\n3.5 software will used to train data and cluster course implemented during\nindustrial training. This study indicates that different method will create a\ndifferent number of clusters.\n",
        "  The first part of this paper exposits a simple geometric description of the\nKirby-Siebenmann invariant of a 4--manifold in terms of a quadratic refinement\nof its intersection form. This is the first in a sequence of higher-order\nintersection invariants of Whitney towers studied by the authors, particularly\nfor the 4--ball.\n  In the second part of this paper, a general theory of quadratic forms is\ndeveloped and then specialized from the non-commutative to the commutative to\nfinally, the symmetric settings. The intersection invariant for twisted Whitney\ntowers is shown to be the universal symmetric refinement of the framed\nintersection invariant. As a corollary we obtain a short exact sequence that\nhas been essential in the understanding of Whitney towers in the 4--ball.\n",
        "  The evolution of star clusters is largely affected by the tidal field\ngenerated by the host galaxy. It is thus in principle expected that under the\nassumption of an \"universal\" initial cluster mass function the properties of\nthe evolved present-day mass function of star cluster systems should show a\ndependency on the properties of the galactic environment in which they evolve.\nTo explore this expectation a sophisticated model of the tidal field is\nrequired in order to study the evolution of star cluster systems in realistic\ngalaxies. Along these lines, in the present work we first describe a method\ndeveloped for coupling $N$-body simulations of galaxies and star clusters. We\nthen generate a database of galaxy models along the Hubble sequence and\ncalibrate evolutionary equations to the results of direct $N$-body simulations\nof star clusters in order to predict the clusters' mass evolution as function\nof the galactic environment. We finally apply our methods to explore the\nproperties of evolved \"universal\" initial cluster mass functions and any\ndependence on the host galaxy morphology and mass distribution. The preliminary\nresults show that an initial power-law distribution of the masses \"universally\"\nevolves into a log-normal distribution, with the properties correlated with the\nstellar mass and stellar mass density density of the host galaxy.\n",
        "  Economic incentives to harvest a species usually diminish as its abundance\ndeclines, because harvest costs increase. This prevents harvesting to\nextinction. A known exception can occur if consumer demand causes a declining\nspecies' harvest price to rise faster than costs. This threat may affect rare\nand valuable species, such as large land mammals, sturgeons, and bluefin tunas.\nWe analyze a similar but underappreciated threat, which arises when the\ngeographic area (range) occupied by a species contracts as its abundance\ndeclines. Range contractions maintain the local densities of declining\npopulations, which facilitates harvesting to extinction by preventing abundance\ndeclines from causing harvest costs to rise. Factors causing such range\ncontractions include schooling, herding, or flocking behaviors--which,\nironically, can be predator-avoidance adaptations; patchy environments; habitat\nloss; and climate change. We use a simple model to identify combinations of\nrange contractions and price increases capable of causing extinction from\nprofitable overharvesting, and we compare these to an empirical review. We find\nthat some aquatic species that school or forage in patchy environments\nexperience sufficiently severe range contractions as they decline to allow\nprofitable harvesting to extinction even with little or no price increase; and\nsome high-value declining aquatic species experience severe price increases.\nFor terrestrial species, the data needed to evaluate our theory are scarce, but\navailable evidence suggests that extinction-enabling range contractions may be\ncommon among declining mammals and birds. Thus, factors causing range\ncontraction as abundance declines may pose unexpectedly large extinction risks\nto harvested species.\n",
        "  For a genus-1 1-bridge knot in the 3-sphere, that is, a (1,1)-knot, a middle\ntunnel is a tunnel that is not an upper or lower tunnel for some\n(1,1)-position. Most torus knots have a middle tunnel, and non-torus-knot\nexamples were obtained by Goda, Hayashi, and Ishihara. We generalize their\nconstruction and calculate the slope invariants for the resulting middle\ntunnels. In particular, we obtain the slope sequence of the original example of\nGoda, Hayashi, and Ishihara.\n",
        "  An accurate calculation of proton ranges in phantoms or detector geometries\nis crucial for decision making in proton therapy and proton imaging. To this\nend, several parameterizations of the range-energy relationship exist, with\ndifferent levels of complexity and accuracy. In this study we compare the\naccuracy four different parameterizations models: Two analytical models derived\nfrom the Bethe equation, and two different interpolation schemes applied to\nrange-energy tables. In conclusion, a spline interpolation scheme yields the\nhighest reproduction accuracy, while the shape of the energy loss-curve is best\nreproduced with the differentiated Bragg-Kleeman equation.\n",
        "  Superconducting metal dichalcogenides (MDCs) present several similarities to\nthe other layered superconductors like cuprates. The superconductivity in\natomically thin MDCs has been demonstrated by recent experiments, however, the\ninvestigation of the superconductivity intertwined with other orders are\nscarce. Investigating the pseudogap in atomic layers of MDCs may help to\nunderstand the superconducting mechanism for these true two-dimensional (2D)\nsuperconducting systems. Herein we report a pseudogap opening in the tunneling\nspectra of thin layers of SnSe2 epitaxially grown on highly oriented pyrolytic\ngraphite (HOPG) with scanning tunneling microscopy/spectroscopy (STM/STS). A\nsignificant V-shaped pseudogap was observed to open near the Fermi level (EF)\nin the STS. And at elevated temperatures, the gap gradually evolves to a\nshallow dip. Our experimental observations provide direct evidence of a\npseudogap state in the electron-doped SnSe2 atomic layers on the HOPG surface,\nwhich may stimulate further exploration of the mechanism of superconductivity\nat 2D limit in MDCs.\n",
        "  Understanding why strains with different metabolic pathways that compete for\na single limiting resource coexist is a challenging issue within a theoretical\nperspective. Previous investigations rely on mechanisms such as group or\nspatial structuring to achieve a stable coexistence between competing metabolic\nstrategies. Nevertheless, coexistence has been experimentally reported even in\nsituations where it cannot be attributed to spatial effects [Heredity {\\bf\n100}, 471 (2008)]. According to that study a toxin expelled by one of the\nstrains can be responsible for the stable maintenance of the two strain types.\nWe propose a resource-based model in which an efficient strain with a slow\nmetabolic rate competes with a second strain type which presents a fast but\ninefficient metabolism. Moreover, the model assumes that the inefficient strain\nproduces a toxin as a byproduct. This toxin affects the growth rate of both\nstrains with different strength. Through an extensive exploration of the\nparameter space we determine the situations at which the coexistence of the two\nstrains is possible. Interestingly, we observe that the resource influx rate\nplays a key role in the maintenance of the two strain types. In a scenario of\nresource scarcity the inefficient is favored, though as the resource influx\nrate is augmented the coexistence becomes possible and its domain is enlarged.\n",
        "  We present sub-arcsecond resolution HCN (4-3) and CO (3-2) observations made\nwith the Submillimeter Array (SMA), toward an extremely young intermediate-mass\nprotostellar core, MMS 6-main, located in the Orion Molecular Cloud 3 region\n(OMC-3). We have successfully imaged a compact molecular outflow lobe (~1500\nAU) associated with MMS6-main, which is also the smallest molecular outflow\never found in the intermediate-mass protostellar cores. The dynamical time\nscale of this outflow is estimated to be <100 yr. The line width dramatically\nincreases downstream at the end of the molecular outflow ({\\Delta}v~25 km\ns^{-1}), and clearly shows the bow-shock type velocity structure. The estimated\noutflow mass (~10^{-4} M_{sun}) and outflow size are approximately 2-4 orders\nand 1-3 orders of magnitude smaller, while the outflow force (~10^{-4} M_{sun}\nkm s^{-1} yr^{-1}) is similar, as compared to the other molecular outflows\nstudied in OMC-2/3. These results show that MMS 6-main is a protostellar core\nat the earliest evolutionary stage, most likely shortly after the 2nd core\nformation.\n",
        "  Since the amount of information on the internet is growing rapidly, it is not\neasy for a user to find relevant information for his/her query. To tackle this\nissue, much attention has been paid to Automatic Document Summarization. The\nkey point in any successful document summarizer is a good document\nrepresentation. The traditional approaches based on word overlapping mostly\nfail to produce that kind of representation. Word embedding, distributed\nrepresentation of words, has shown an excellent performance that allows words\nto match on semantic level. Naively concatenating word embeddings makes the\ncommon word dominant which in turn diminish the representation quality. In this\npaper, we employ word embeddings to improve the weighting schemes for\ncalculating the input matrix of Latent Semantic Analysis method. Two\nembedding-based weighting schemes are proposed and then combined to calculate\nthe values of this matrix. The new weighting schemes are modified versions of\nthe augment weight and the entropy frequency. The new schemes combine the\nstrength of the traditional weighting schemes and word embedding. The proposed\napproach is experimentally evaluated on three well-known English datasets, DUC\n2002, DUC 2004 and Multilingual 2015 Single-document Summarization for English.\nThe proposed model performs comprehensively better compared to the\nstate-of-the-art methods, by at least 1% ROUGE points, leading to a conclusion\nthat it provides a better document representation and a better document summary\nas a result.\n",
        "  We have investigated the low temperature physical properties of BaTi2Sb2O and\nBa1-xNaxTi2Sb2O (x = 0.05, 0.1, 0.15, 0.2, 0.25, 0.3) by means of muon spin\nrotation (muSR) and SQUID magnetometry. Our measurements reveal the absence of\nmagnetic ordering below TDW = 58 K in the parent compound. Therefore the phase\ntransition at this temperature observed by magnetometry is most likely due to\nthe formation of a charge density wave (CDW). Upon substitution of barium by\nsodium in Ba1-xNaxTi2Sb2O we find for x = 0.25 superconductivity with a maximum\nT_{c} = 5.1 K in the magnetization and a bulk T_{c,bulk} = 4.5 K in the muSR\nmeasurements. The temperature dependency of the London penetration depth\nlambda^-2(T) of the optimally doped compound can be well explained within a\nconventional weak-coupling scenario in the clean limit.\n",
        "  F-substituted LaOBiSe2 single crystals were grown using CsCl flux. The\nobtained single crystals showed a plate-like shape with a size of about 1.0 mm\nsquare. The c-axis lattice constant of the grown crystals was determined to be\n14.114(3) {\\AA}. The superconducting critical temperature of the single crystal\nwas approximately 3.5 K. The superconducting anisotropies were determined to be\n49 and 24 using the upper critical field and the effective mass model,\nrespectively.\n",
        "  Noncollinear magnetic interfaces introduced in superconductor\n(SC)/ferromagnet/SC heterostructures allow for spin-flipping processes and are\nable to generate equal-spin spin-triplet pairing correlations within the\nferromagnetic region. This leads to the occurrence of the so-called long-range\nproximity effect. Particular examples of noncollinear magnetic interfaces\ninclude Bloch and N\\'{e}el domain walls. Here, we present results for\nheterostructures containing Bloch and N\\'{e}el domain walls based on\nself-consistent solutions of the spin-dependent Bogoliubov$-$de Gennes\nequations in the clean limit. In particular, we investigate the thickness\ndependence of Bloch and N\\'{e}el domain walls on induced spin-triplet pairing\ncorrelations and compare with other experimental and theoretical results,\nincluding conical magnetic layers as noncollinear magnetic interfaces. It is\nshown that both, Bloch and N\\'{e}el domain walls lead to the generation of\nunequal-spin spin-triplet pairing correlations of similar strength as for\nconical magnetic layers. However, for the particular heterostructure geometries\ninvestigated, only Bloch domain walls lead to the generation of equal-spin\nspin-triplet pairing correlations. They are stronger than those generated by an\nequivalent thickness of conical magnetic layers. In order for N\\'{e}el domain\nwalls to induce equal-spin spin-triplet pairing correlations, they have to be\noriented such that the noncollinearity appears within the plane parallel to the\ninterface region.\n",
        "  C$^+$ is a critical constituent of many regions of the interstellar medium,\nas it can be a major reservoir of carbon and, under a wide range of conditions,\nthe dominant gas coolant. Emission from its 158$\\mu$m fine structure line is\nused to trace the structure of photon dominated regions in the Milky Way and is\noften employed as a measure of the star formation rate in external galaxies.\nUnder most conditions, the emission from the single [CII] line is proportional\nto the collisional excitation rate coefficient. We here used improved\ncalculations of the deexcitation rate of [CII] by collisions with H$_2$ to\ncalculate more accurate expressions for interstellar C$^+$ fine structure\nemission, its critical density, and its cooling rate. The collision rates in\nthe new quantum calculation are $\\sim$ 25% larger than those previously\navailable, and narrow the difference between rates for excitation by atomic and\nmolecular hydrogen. This results in [CII] excitation being quasi-independent of\nthe molecular fraction and thus dependent only on the total hydrogen particle\ndensity. A convenient expression for the cooling rate at temperatures between\n20 K and 400 K, assuming an LTE H$_2$ ortho to para ration is $\\Lambda ({\\rm\nLTE~OPR}) = \\left(11.5 + 4.0\\,e^{-100\\,\\mathrm K/T^{\\rm\nkin}}\\right)\\;e^{-91.25\\,\\mathrm K/T^{\\rm kin}}\\,n ({\\rm C}^{+})\\,n({\\rm\nH}_2)\\times 10^{-24}\\;{\\rm ergs}~{\\rm cm}^{-3}~{\\rm s}^{-1}$. The present work\nshould allow more accurate and convenient analysis of the [\\CII] line emission\nand its cooling.\n",
        "  For the past few years, we have observed the central half parsec of our\nGalaxy in the mid-infrared from 2.8 to 5.1 micron. Our aim is to improve our\nunderstanding of the direct environment of SgrA*, the supermassive blackhole at\nthe centre of the Milky Way. This work is described in the present paper and by\nMoultaka et al. 2015 (submitted). Here, we focus on the study of the spatial\ndistribution of the 12CO ice and gas-phase absorptions. We observed the central\nhalf parsec with ISAAC spectrograph located at the UT3/VLT ESO telescope in\nChile. The slit was placed along 22 positions arranged parallel to each other\nto map the region. We built the first data cube in this wavelength range\ncovering the central half parsec. The wavelength interval of the used M-band\nfilter ranges from 4.6 to 5.1 micron. It hosts the P- and R- branches of the\nro-vibrational transitions of the gaseous 12CO and 13CO, as well as the\nabsorption band attributed to the 12CO ice at 4.675 micron. Using two\ncalibrators, we could disentangle the local from the line-of-sight absorptions\nand provide a first-order estimate of the foreground extinction. We find\nresidual ices and gase-phase CO that can be attributed to local absorptions due\nto material from the interstellar and/or the circumstellar medium of the\ncentral parsec. Our finding implies temperatures of the order of 10 to 60K\nwhich is in agreement with the presence of water ices in the region highlighted\nby Moultaka et al. (2004, 2005).\n",
        "  In this contribution we present numerical and experimental results of a\nparametric quantitative study of radiative dipole antennas in a phased array\nconfiguration for efficient body magnetic resonance imaging at 7T via parallel\ntransmission. For magnetic resonance imaging (MRI) at ultrahigh fields (7T and\nhigher) dipole antennas are commonly used in phased arrays, particularly for\nbody imaging targets. This study reveals the effects of dipole positioning in\nthe array (elevation of dipoles above the subject and inter-dipole spacing) on\ntheir mutual coupling, $B_1^{+}$ per $P_{acc}$ and $B_1^{+}$ per maximum local\nSAR efficiencies as well as the RF-shimming capability. The numerical and\nexperimental results are obtained and compared for a homogeneous phantom as\nwell as for a real human models confirmed by in-vivo experiments.\n",
        "  In this paper, we study a simple one-dimensional model of reaction-diffusion\nwith bistable non-linearity and in heterogeneous media. The bistable term\naccounts for the so-called Allee effect, and the heterogeneity in the media is\nlocalized. We recall the definition of a transition wave used in similar\nsituations by well-known authors, and propose an alternative definition for\ndiscussion. We call it generalized traveling wave. As a consequence, we give\nnew results of pinning in such media, due to Allee effect.\n",
        "  We introduce a framework for lightweight dependency syntax annotation. Our\nformalism builds upon the typical representation for unlabeled dependencies,\npermitting a simple notation and annotation workflow. Moreover, the formalism\nencourages annotators to underspecify parts of the syntax if doing so would\nstreamline the annotation process. We demonstrate the efficacy of this\nannotation on three languages and develop algorithms to evaluate and compare\nunderspecified annotations.\n",
        "  We show that the generation time -- a notion usually described in a\nbiological context -- can be defined in a general way as a return time in a\nconveniently constructed finite Markov chain. The simple formula we obtain\nagrees with previous results derived for structured populations projected in\ndiscrete time, and allows to define the generation time of any process\ndescribed by a weighted directed graph whose matrix is primitive.\n",
        "  Plant-pollinator associations are often seen as purely mutualistic, while in\nreality they can be more complex. Indeed they may also display a diverse array\nof antagonistic interactions, such as competition and victim--exploiter\ninteractions. In some cases mutualistic and antagonistic interactions are\ncarried-out by the same species but at different life-stages. As a consequence,\npopulation structure affects the balance of inter-specific associations, a\ntopic that is receiving increased attention. In this paper, we developed a\nmodel that captures the basic features of the interaction between a flowering\nplant and an insect with a larval stage that feeds on the plant's vegetative\ntissues (e.g. leaves) and an adult pollinator stage. Our model is able to\ndisplay a rich set of dynamics, the most remarkable of which involves\nvictim--exploiter oscillations that allow plants to attain abundances above\ntheir carrying capacities, and the periodic alternation between states\ndominated by mutualism or antagonism. Our study indicates that changes in the\ninsect's life cycle can modify the balance between mutualism and antagonism,\ncausing important qualitative changes in the interaction dynamics. These\nchanges in the life cycle could be caused by a variety of external drivers,\nsuch as temperature, plant nutrients, pesticides and changes in the diet of\nadult pollinators.\n  Abstract Keywords: mutualism, pollination, herbivory, insects,\nstage-structure, oscillations\n",
        "  The present study aimed at investigating the effects of an artificial head\nposition-based tongue-placed electrotactile biofeedback on postural control\nduring quiet standing under different somatosensory conditions from the support\nsurface. Eight young healthy adults were asked to stand as immobile as possible\nwith their eyes closed on two Firm and Foam support surface conditions executed\nin two conditions of No-biofeedback and Biofeedback. In the Foam condition, a\n6-cm thick foam support surface was placed under the subjects' feet to alter\nthe quality and/or quantity of somatosensory information at the plantar sole\nand the ankle. The underlying principle of the biofeedback consisted of\nproviding supplementary information about the head orientation with respect to\ngravitational vertical through electrical stimulation of the tongue. Centre of\nfoot pressure (CoP) displacements were recorded using a force platform. Larger\nCoP displacements were observed in the Foam than Firm conditions in the two\nconditions of No-biofeedback and Biofeedback. Interestingly, this destabilizing\neffect was less accentuated in the Biofeedback than No-biofeedback condition.\nIn accordance with the sensory re-weighting hypothesis for balance control, the\npresent findings evidence that the availability of the central nervous system\nto integrate an artificial head orientation information delivered through\nelectrical stimulation of the tongue to limit the postural perturbation induced\nby alteration of somatosensory input from the support surface.\n",
        "  Traditional learning-based coreference resolvers operate by training the\nmention-pair model for determining whether two mentions are coreferent or not.\nThough conceptually simple and easy to understand, the mention-pair model is\nlinguistically rather unappealing and lags far behind the heuristic-based\ncoreference models proposed in the pre-statistical NLP era in terms of\nsophistication. Two independent lines of recent research have attempted to\nimprove the mention-pair model, one by acquiring the mention-ranking model to\nrank preceding mentions for a given anaphor, and the other by training the\nentity-mention model to determine whether a preceding cluster is coreferent\nwith a given mention. We propose a cluster-ranking approach to coreference\nresolution, which combines the strengths of the mention-ranking model and the\nentity-mention model, and is therefore theoretically more appealing than both\nof these models. In addition, we seek to improve cluster rankers via two\nextensions: (1) lexicalization and (2) incorporating knowledge of anaphoricity\nby jointly modeling anaphoricity determination and coreference resolution.\nExperimental results on the ACE data sets demonstrate the superior performance\nof cluster rankers to competing approaches as well as the effectiveness of our\ntwo extensions.\n",
        "  Various mathematical models represent the effects of local mechanical\nenvironment on the regulation of skeletal regeneration. Their relevance relies\non an accurate description of the evolving mechanical properties of the\nregenerating tissue. The object of this study was to develop an experimental\nmodel which made it possible to characterize the temporal evolution of the\nstructural and mechanical properties during unloaded enchondral osteogenesis in\nthe New Zealand rabbit, a standard animal model for studies of osteogenesis and\nchondrogenesis. A 25mm segment of tibial diaphysis was removed sub-periosteally\nfrom rabbits. The defect was repaired by the preserved periosteum. An external\nfixator was applied to prevent mechanical loading during osteogenesis. The\nregenerated skeletal tissues were studied by CT scan, histology and mechanical\ntests. The traction tests between 7 to 21 days post-surgery were done on\nformaldehyde-fixated tissue allowing to obtain force/displacement curves. The\nviscoelastic properties of the regenerating skeletal tissues were visualized\nthroughout the repair process.\n",
        "  By Gromov's mapping theorem for bounded cohomology, the projection of a group\nto the quotient by an amenable normal subgroup is isometric on group homology\nwith respect to the $\\ell^1$-semi-norm. Gromov's description of the diffusion\nof cycles also implicitly produces efficient cycles in this situation. We\npresent an elementary version of this explicit construction.\n",
        "  We present a new methodology for high-quality labeling in the fashion domain\nwith crowd workers instead of experts. We focus on the Aspect-Based Sentiment\nAnalysis task. Our methods filter out inaccurate input from crowd workers but\nwe preserve different worker labeling to capture the inherent high variability\nof the opinions. We demonstrate the quality of labeled data based on Facebook's\nFastText framework as a baseline.\n",
        "  We use integral field spectroscopy to study in detail the Wolf-Rayet (WR)\npopulation in NGC 3310, spatially resolving 18 star-forming knots with typical\nsizes of 200-300 pc in the disc of the galaxy hosting a substantial population\nof WRs. The detected emission in the so-called blue bump is attributed mainly\nto late-type nitrogen WRs (WNL), ranging from a few dozens to several hundreds\nof stars per region. Our estimated WNL/(WNL+O) ratio is comparable to reported\nempirical relations once the extinction-corrected emission is further corrected\nby the presence of dust grains inside the nebula that absorb a non-negligible\nfraction of UV photons. Comparisons of observables with stellar population\nmodels show disagreement by factors larger than 2-3. However, if the effects of\ninteracting binaries and/or photon leakage are taken into account, observations\nand predictions tend to converge. We estimate the binary fraction of the \\hii\nregions hosting WRs to be significant in order to recover the observed X-ray\nflux, hence proving that the binary channel can be critical when predicting\nobservables. We also explore the connection of the environment with the current\nhypothesis that WRs can be progenitors to long-duration gamma-ray bursts\n(GRBs). Galaxy interactions, which can trigger strong episodes of star\nformation in the central regions, may be a plausible environment where WRs may\nact as progenitors of GRBs. Finally, even though the chemical abundance is\ngenerally homogeneous, we also find weak evidence for rapid N pollution by WR\nstellar winds at scales of ~ 200 pc.\n",
        "  We consider birth-and-death stochastic evolution of genotypes with different\nlengths. The genotypes might mutate that provides a stochastic changing of\nlengthes by a free diffusion law. The birth and death rates are length\ndependent which corresponds to a selection effect. We study an asymptotic\nbehavior of a density for an infinite collection of genotypes. The cases of\nspace homogeneous and space heterogeneous densities are considered.\n",
        "  Predator-prey relationships are one of the most studied interactions in\npopulation ecology. However, little attention has been paid to the possibility\nof role exchange between species once determined as predators and preys,\ndespite firm field evidence of such phenomena in the nature. In this paper, we\nbuild a model capable of reproducing the main phenomenological features of one\nreported predator-prey role-reversal system, and present results for both the\nhomogeneous and the space explicit cases. We find that, depending on the choice\nof parameters, our role-reversal dynamical system exhibits excitable-like\nbehaviour, generating waves of species' concentrations that propagate through\nspace.\n",
        "  By means of magnetization, specific heat and muon-spin relaxation\nmeasurements, we investigate high-pressure oxidized \\mohpo, in which overdoping\nis achieved up to $p \\sim 0.45$ hole/Cu, well beyond the $T_c - p$\nsuperconducting dome of cuprates, where Fermi liquid behavior is expected.\nSurprisingly, we find bulk superconductivity with $T_c$=84 K and superfluid\ndensity similar to those of optimally doped Y123. On the other hand, specific\nheat data display a large electronic contribution at low temperature,\ncomparable to that of nonsuperconducting overdoped La214. These results point\nat an unusual high-$T_c$ phase with a large fraction of unpaired holes. Further\nexperiments may assess the Fermi liquid properties of the present phase, which\nwould put into question the paradigm that the high $T_c$ of cuprates originates\nfrom a non-Fermi liquid ground state.\n",
        "  We investigate the problem of vortex trapping in cyclically coupled\nBose-Josephson junctions. Starting with $N$ independent BECs we couple the\ncondensates through Josephson links and allow the system to reach a stable\ncirculation by adding a dissipative term in our semiclassical equations of\nmotion. The central question we address is what is the probability to trap a\nvortex with winding number $m$. Our numerical simulations reveal that the final\ndistribution of winding numbers is narrower than the initial distribution of\ntotal phases, indicating an increased probability for no-vortex configurations.\nFurther, the nonlinearity of the problem manifests itself in the somewhat\ncounter-intuitive result that it is possible to obtain a non-zero circulation\nstarting with zero total phase around the loop. The final width of the\ndistribution of winding numbers for $N$ sites scales as $\\lambda N^{\\alpha}$,\nwhere $\\alpha=0.47\\pm 0.01$ and $\\lambda <0.67$ (value predicted for the\ninitial distribution) indicating a shrinking of the final distribution. The\nactual value of $\\lambda$ is found to depend on the strength of dissipation.\n",
        "  Life can be viewed as a localized chemical system that sits on, or in the\nbasin of attraction of, a metastable dynamical attractor state that remains out\nof equilibrium with the environment. Such a view of life allows that new living\nstates can arise through chance changes in local chemical concentration\n(=mutations) that move points in space into the basin of attraction of a life\nstate - the attractor being an autocatalytic sets whose essential (=keystone)\nspecies are produced at a higher rate than they are lost to the environment by\ndiffusion, such that growth in expected. This conception of life yields several\nnew insights and conjectures. (1) This framework suggests that the first new\nlife states to arise are likely at interfaces where the rate of diffusion of\nkeystone species is tied to a low-diffusion regime, while precursors and waste\nproducts diffuse at a higher rate. (2) There are reasons to expect that once\nthe first life state arises, most likely on a mineral surface, additional\nmutations will generate derived life states with which the original state will\ncompete. (3) I propose that in the resulting adaptive process there is a\ngeneral tendency for higher complexity life states (i.e., ones that are further\nfrom being at equilibrium with the environment) to dominate a given mineral\nsurface. (4) The framework suggests a simple and predictable path by which\ncells evolve and provides pointers on why such cells are likely to acquire\nparticulate inheritance. Overall, the dynamical systems theoretical framework\ndeveloped provides an integrated view of the origin and early evolution of life\nand supports novel empirical approaches.\n",
        "  Forty years ago, Robert May questioned a central belief in ecology by proving\nthat sufficiently large or complex ecological networks have probability of\npersisting close to zero. To prove this point, he analyzed large networks in\nwhich species interact at random. However, in natural systems pairs of species\nhave well-defined interactions (e.g., predator-prey, mutualistic or\ncompetitive). Here we extend May's results to these relationships and find\nremarkable differences between predator-prey interactions, which increase\nstability, and mutualistic and competitive, which are destabilizing. We provide\nanalytic stability criteria for all cases. These results have broad\napplicability in ecology. For example, we show that, surprisingly, the\nprobability of stability for predator-prey networks is decreased when we impose\nrealistic food web structure or we introduce a large preponderance of weak\ninteractions. Similarly, stability is negatively impacted by nestedness in\nbipartite mutualistic networks.\n",
        "  Given a dense triplet set $\\mathcal{T}$, there arise two interesting\nquestions: Does there exists any phylogenetic network consistent with\n$\\mathcal{T}$? And if so, can we find an effective algorithm to construct one?\nFor cases of networks of levels $k=0$ or 1 or 2, these questions were answered\nwith effective polynomial algorithms. For higher levels $k$, partial answers\nwere recently obtained with an $O(|\\mathcal{T}|^{k+1})$ time algorithm for\nsimple networks. In this paper we give a complete answer to the general case.\nThe main idea is to use a special property of SN-sets in a level-k network. As\na consequence, we can also find the level-k network with the minimum number of\nreticulations in polynomial time.\n",
        "  XML access control policies involving updates may contain security flaws,\nhere called inconsistencies, in which a forbidden operation may be simulated by\nperforming a sequence of allowed operations. This paper investigates the\nproblem of deciding whether a policy is consistent, and if not, how its\ninconsistencies can be repaired. We consider policies expressed in terms of\nannotated DTDs defining which operations are allowed or denied for the XML\ntrees that are instances of the DTD. We show that consistency is decidable in\nPTIME for such policies and that consistent partial policies can be extended to\nunique \"least-privilege\" consistent total policies. We also consider repair\nproblems based on deleting privileges to restore consistency, show that finding\nminimal repairs is NP-complete, and give heuristics for finding repairs.\n",
        "  We report low temperature muon spin relaxation (muSR) measurements of the\nhigh-transition-temperature (Tc) cuprate superconductors\nBi{2+x}Sr{2-x}CaCu2O{8+\\delta} and YBa2Cu3O6.57, aimed at detecting the\nmysterious intra-unit cell (IUC) magnetic order that has been observed by spin\npolarized neutron scattering in the pseudogap phase of four different cuprate\nfamilies. A lack of confirmation by local magnetic probe methods has raised the\npossibility that the magnetic order fluctuates slowly enough to appear static\non the time scale of neutron scattering, but too fast to affect $\\mu$SR or\nnuclear magnetic resonance (NMR) signals. The IUC magnetic order has been\nlinked to a theoretical model for the cuprates, which predicts a long-range\nordered phase of electron-current loop order that terminates at a quantum\ncrictical point (QCP). Our study suggests that lowering the temperature to T ~\n25 mK and moving far below the purported QCP does not cause enough of a slowing\ndown of fluctuations for the IUC magnetic order to become detectable on the\ntime scale of muSR. Our measurements place narrow limits on the fluctuation\nrate of this unidentified magnetic order.\n",
        "  One of the most important characterizations of social health is existence the\navailability of safe drinking water. Since one of the sources of water\ncontamination is nuclear contamination from radon gas, so in this research\nradon 222 concentration levels in water supplies in the Toyserkan (a region\nlocated in the west of Iran) is investigated. For measuring radon gas in water\nwells and springs Lucas chamber method is used. Review the results of these\nmeasurements that taken from 15th place show that, only five sites have radon\nconcentrations above the limit dose. To reduce radon concentration, it is\nbetter to keep water in open pools in contact with air before the water is\ndelivered to users.\n",
        "  For any rational homology 3-sphere and one of its spin^{c}-structures,\nOzsvath and Szabo defined a topological invariant, called d-invariant. Given a\nknot in the 3-sphere, the d-invariants associated with the prime-power-fold\nbranched covers of the knot, obstruct the smooth sliceness of the knot. These\ninvariants bear some structural resemblances to Casson-Gordon invariants, which\nobstruct the topological sliceness of a knot. Se-Goo Kim found a polynomial\nsplitting property for Casson-Gordon invariants. In this paper, we show a\nsimilar result for Ozsvath and Szabo's d-invariants. We give an application of\nthe result.\n",
        "  We study the field-angle resolved electronic Raman scattering in\n2-dimensional d-wave superconducting vortex states theoretically by\nquasi-classical approximation, the so-called Doppler-shift method. An analytic\nexpression is obtained for the field angle dependence of the Raman scattering\namplitude at zero temperature. After numerical integration, we obtain the\nscattering intensity for various field angles by changing the Raman shift\nenergy. Field-angle resolved electronic Raman scattering turns out to be an\neffective method for probing unconventional superconducting gap structures. It\nshows a novel phenomenon: reversal of extrema as a function of frequency\nwithout changing temperature or field magnitude.\n",
        "  By constructing scaling relations for galaxies in the massive cluster\nMACSJ0717.5 at $z=0.545$ and comparing with those of Coma, we model the\nluminosity evolution of the stellar populations and the structural evolution of\nthe galaxies. We calculate magnitudes, surface brightnesses and effective radii\nusing HST/ACS images and velocity dispersions using Gemini/GMOS spectra, and\npresent a catalogue of our measurements for 17 galaxies. We also generate\nphotometric catalogues for $\\sim 3000$ galaxies from the HST imaging. With\nthese, we construct the colour-magnitude relation, the fundamental plane, the\nmass-to-light versus mass relation, the mass-size relation and the\nmass-velocity dispersion relation for both clusters. We present a new, coherent\nway of modelling these scaling relations simultaneously using a simple physical\nmodel in order to infer the evolution in luminosity, size and velocity\ndispersion as a function of redshift, and show that the data can be fully\naccounted for with this model. We find that (a) the evolution in size and\nvelocity dispersion undergone by these galaxies between $z \\sim 0.5$ and $z\n\\sim 0$ is mild, with $R_e(z) \\sim (1+z)^{-0.40\\pm0.32}$ and $\\sigma(z) \\sim\n(1+z)^{0.09 \\pm 0.27}$, and (b) the stellar populations are old, $\\sim 10$ Gyr,\nwith a $\\sim 3$ Gyr dispersion in age, and are consistent with evolving purely\npassively since $z \\sim 0.5$ with $\\Delta \\log M/L_B = -0.55_{-0.07}^{+0.15}\nz$. The implication is that these galaxies formed their stars early and\nsubsequently grew dissipationlessly so as to have their mass already in place\nby $z \\sim 0.5$, and suggests a dominant role for dry mergers, which may have\naccelerated the growth in these high-density cluster environments.\n",
        "  We give examples of closed, oriented 3-manifolds whose fundamental groups are\nnot isomorphic, but yet have the same sets of finite quotient groups; hence the\nsame profinite completions. We also give examples of compact, oriented\n3-manifolds with non-empty boundaries whose fundamental groups though\nisomorphic have distinct peripheral structures, but yet have the same sets of\nfinite peripheral pair quotients (defined below). The examples are Seifert\nFibered Spaces with zero rational Euler number; moreover most of these\nmanifolds give rise to such examples.\n",
        "  Privacy Preserving Data Mining(PPDM) is an ongoing research area aimed at\nbridging the gap between the collaborative data mining and data confidentiality\nThere are many different approaches which have been adopted for PPDM, of them\nthe rule hiding approach is used in this article. This approach ensures output\nprivacy that prevent the mined patterns(itemsets) from malicious inference\nproblems. An efficient algorithm named as Pattern-based Maxcover Algorithm is\nproposed with experimental results. This algorithm minimizes the dissimilarity\nbetween the source and the released database; Moreover the patterns protected\ncannot be retrieved from the released database by an adversary or counterpart\neven with an arbitrarily low support threshold.\n",
        "  Homotopy classes of nanowords and nanophrases are combinatorial\ngeneralizations of virtual knots and links. Goussarov, Polyak and Viro defined\nfinite type invariants for virtual knots and links via semi-virtual crossings.\nWe extend their definition to nanowords and nanophrases. We study finite type\ninvariants of low degrees. In particular, we show that the linking matrix and T\ninvariant defined by Fukunaga are finite type of degree one and degree two\nrespectively. We also give a finite type invariant of degree 4 for open\nhomotopy of Gauss words.\n",
        "  We show, within the context of the standard class of deterministic ODE\npredator-prey mathematical models, that predator culling does not produce a\nlong term decrease in the predator population.\n",
        "  We show that if M is a complete, finite-volume, hyperbolic 3-manifold having\nexactly one cusp, and if H_1(M;Z_2) has dimension at least 6, then M has volume\ngreater than 5.06. We also show that if M is a closed, orientable hyperbolic\n3-manifold such that H_1(M;Z_2) has dimension at least 4, and if the image of\nthe cup product map in H^2(M;Z_2) has dimension at most 1, then M has volume\ngreater than 3.08. The proofs of these geometric results involve new\ntopological results relating the Heegaard genus of a closed Haken manifold M to\nthe Euler characteristic of the kishkes (i.e guts) of the complement of an\nincompressible surface in M.\n",
        "  We present a sequence of diagrams of the unknot for which the minimum number\nof Reidemeister moves required to pass to the trivial diagram is quadratic with\nrespect to the number of crossings. These bounds apply both in $S^2$ and in\n$\\R^2$.\n",
        "  The Linked Data principles provide a decentral approach for publishing\nstructured data in the RDF format on the Web. In contrast to structured data\npublished in relational databases where a key is often provided explicitly,\nfinding a set of properties that allows identifying a resource uniquely is a\nnon-trivial task. Still, finding keys is of central importance for manifold\napplications such as resource deduplication, link discovery, logical data\ncompression and data integration. In this paper, we address this research gap\nby specifying a refinement operator, dubbed ROCKER, which we prove to be\nfinite, proper and non-redundant. We combine the theoretical characteristics of\nthis operator with two monotonicities of keys to obtain a time-efficient\napproach for detecting keys, i.e., sets of properties that describe resources\nuniquely. We then utilize a hash index to compute the discriminability score\nefficiently. Therewith, we ensure that our approach can scale to very large\nknowledge bases. Results show that ROCKER yields more accurate results, has a\ncomparable runtime, and consumes less memory w.r.t. existing state-of-the-art\ntechniques.\n",
        "  The mechanical properties of human soft tissue are crucial for impact\nbiomechanics, rehabilitation engineering and surgical simulation. Validation of\nthese constitutive models using human data remains challenging and often\nrequires the use of non-invasive imaging and inverse finite element (FE)\nanalysis. Post processing data from imaging methods such as tagged magnetic\nresonance imaging (MRI) can be challenging. Digital Image Correlation (DIC)\nhowever is a relatively straightforward imaging method and thus the goal of\nthis study was to assess the use of DIC in combination with FE modelling to\ndetermine the bulk material properties of human soft tissue. Indentation\nexperiments were performed on a silicone gel soft tissue phantom. A two camera\nDIC setup was then used to record the 3D surface deformation. The experiment\nwas then simulated using a FE model.\n",
        "  Large sense-annotated datasets are increasingly necessary for training deep\nsupervised systems in Word Sense Disambiguation. However, gathering\nhigh-quality sense-annotated data for as many instances as possible is a\nlaborious and expensive task. This has led to the proliferation of automatic\nand semi-automatic methods for overcoming the so-called knowledge-acquisition\nbottleneck. In this short survey we present an overview of sense-annotated\ncorpora, annotated either manually- or (semi)automatically, that are currently\navailable for different languages and featuring distinct lexical resources as\ninventory of senses, i.e. WordNet, Wikipedia, BabelNet. Furthermore, we provide\nthe reader with general statistics of each dataset and an analysis of their\nspecific features.\n",
        "  Recent Herschel observations have confirmed that filaments are ubiquitous in\nmolecular clouds and suggest that irrespectively of the column density, there\nis a characteristic width of about 0.1 pc whose physical origin remains\nunclear. We develop an analytical model that can be applied to self-gravitating\naccreting filaments. It is based on one hand on the virial equilibrium of the\ncentral part of the filament and on the other hand on energy balance between\nthe turbulence driven by accretion onto the filament and dissipation. We\nconsider two dissipation mechanisms the turbulent cascade and the ion-neutral\nfriction. Our model predicts that the width of the filament inner part is\nalmost independent of the column density and leads to values comparable to what\nis inferred observationally if dissipation is due to ion-neutral friction. On\nthe contrary turbulent dissipation leads to a structure that is bigger and\ndepends significantly on the column density. Our model provides a reasonable\nphysical explanation which could explain the observed filament width when they\nare self-gravitating. It predicts the correct order or magnitude though\nhampered by some uncertainties.\n",
        "  Dynamics of the response of type-II superconductors to a time-varying\nmagnetic field can exhibit a rate-independent or rate-dependent hysteresis. An\nenergy dissipation rate in a superconductor placed in a time-varying magnetic\nfield depends on its wave form and type of hysteresis, which depends on\ntemperature. The same wave form may reduce the energy dissipation rate in the\ncase of true hysteresis, while it may increase the energy dissipation rate in\nthe case of dynamic hysteresis compared with an energy dissipation rate in a\npure sinusoidal field. We present experimental data which confirm the energy\ndissipation rate calculated using the critical state theory for the case of\nrate-independent hysteresis and limiting behavior in a normal state for the\ncase of rate-dependent hysteresis.\n",
        "  Ecosystems are commonly conceptualized as networks of interacting species.\nHowever, partitioning natural diversity of organisms into discrete units is\nnotoriously problematic, and mounting experimental evidence raises the\nintriguing question whether this perspective is appropriate for the microbial\nworld. Here, an alternative formalism is proposed that does not require\npostulating the existence of species as fundamental ecological variables, and\nprovides a naturally hierarchical description of community dynamics. This\nformalism allows approaching the \"species problem\" from the opposite direction.\nWhile the classical models treat a world of imperfectly clustered organism\ntypes as a perturbation around well-clustered \"species\", the presented approach\nallows gradually adding structure to a fully disordered background. The\nrelevance of this theoretical construct for describing highly diverse natural\necosystems is discussed.\n",
        "  In recent years, volumetric modulated arc therapy (VMAT) has been becoming a\nmore and more important radiation technique widely used in clinical application\nfor cancer treatment. One of the key problems in VMAT is treatment plan\noptimization, which is complicated due to the constraints imposed by the\ninvolved equipments. In this paper, we consider a model with four major\nconstraints: the bound on the beam intensity, an upper bound on the rate of the\nchange of the beam intensity, the moving speed of leaves of the multi-leaf\ncollimator (MLC) and its directional-convexity. We solve the model by a\ntwo-stage algorithm: performing minimization with respect to the shapes of the\naperture and the beam intensities alternatively. Specifically, the shapes of\nthe aperture are obtained by a greedy algorithm whose performance is enhanced\nby random sampling in the leaf pairs with a decremental rate. The beam\nintensity is optimized using a gradient projection method with non-monotonic\nline search. We further improve the proposed algorithm by an incremental random\nimportance sampling of the voxels to reduce the computational cost of the\nenergy functional. Numerical simulations on two clinical cancer date sets\ndemonstrate that our method is highly competitive to the state-of-the-art\nalgorithms in terms of both computational time and quality of treatment\nplanning.\n",
        "  The spatial arrangement of trees in a tropical forest reflects the interplay\nbetween aggregating processes, like dispersal limitation, and negative feedback\nthat induces effective repulsion among individuals. Monitoring the\nvariance-mean ratio for conspecific individuals along length-scales, we show\nthat the effect of negative feedback is dominant at short scales, while\naggregation characterizes the large-scale patterns. A comparison of different\nspecies indicates, surprisingly, that both aggregation and negative feedback\nscales are related to the overall abundance of the species. This suggests a\nbottom-up control mechanism, in which the negative feedback dictates the\ndispersal kernel and the overall abundance.\n",
        "  The emergence of superconductivity in the iron pnictide and cuprate high\ntemperature superconductors usually accompanies the suppression of an\nantiferromagnetically (AFM) ordered state in a corresponding parent compound\nthrough the use of chemical doping or external pressure1-5. A great deal of\neffort has been made to find superconductivity in Mn-based compounds6-14, which\nare thought to bridge the gap between the two families of high temperature\nsuperconductors7,15,16, but long-ranged AFM order was not successfully\nsuppressed via chemical doping in these investigations. Here we report the\nfirst observation of the pressure-induced elimination of long-ranged AFM order\nin LaMnPO single crystals that are iso-structural to the LaFeAsO\nsuperconductor15,17. By combining in-situ high pressure resistance and ac\nsusceptibility measurements, we found that LaMnPO undergoes a crossover from an\nAFM insulating to an AFM metallic state at a pressure ~20 GPa and that the\nlong-ranged AFM order collapses at a higher pressure ~32 GPa. Our findings are\nof importance to explore potential superconductivity in Mn-based compounds and\nto shed new light on the underlying mechanism of high temperature\nsuperconductivity.\n",
        "  Inclusive Fitness Theory (IFT) was proposed half a century ago by W.D.\nHamilton to explain the emergence and maintenance of cooperation between\nindividuals that allows the existence of society. Contemporary evolutionary\necology identified several factors that increase inclusive fitness, in addition\nto kin-selection, such as assortation or homophily, and social synergies\ntriggered by cooperation. Here we propose an Extend Inclusive Fitness Theory\n(EIFT) that includes in the fitness calculation all direct and indirect\nbenefits an agent obtains by its own actions, and through interactions with kin\nand with genetically unrelated individuals. This formulation focuses on the\nsustainable cost/benefit threshold ratio of cooperation and on the probability\nof agents sharing mutually compatible memes or genes. This broader description\nof the nature of social dynamics allows to compare the evolution of cooperation\namong kin and non-kin, intra- and inter-specific cooperation, co-evolution, the\nemergence of symbioses, of social synergies, and the emergence of division of\nlabor. EIFT promotes interdisciplinary cross fertilization of ideas by allowing\nto describe the role for division of labor in the emergence of social\nsynergies, providing an integrated framework for the study of both, biological\nevolution of social behavior and economic market dynamics.\n",
        "  Identifying nominals with no head match is a long-standing challenge in\ncoreference resolution with current systems performing significantly worse than\nhumans. In this paper we present a new neural network architecture which\noutperforms the current state-of-the-art system on the English portion of the\nCoNLL 2012 Shared Task. This is done by using a logistic regression on features\nproduced by two submodels, one of which is has the architecture proposed in\n[CM16a] while the other combines domain specific embeddings of the antecedent\nand the mention. We also propose some simple additional features which seem to\nimprove performance for all models substantially, increasing F1 by almost 4% on\nbasic logistic regression and other complex models.\n",
        "  Grammar induction is the task of learning a grammar from a set of examples.\nRecently, neural networks have been shown to be powerful learning machines that\ncan identify patterns in streams of data. In this work we investigate their\neffectiveness in inducing a regular grammar from data, without any assumptions\nabout the grammar. We train a recurrent neural network to distinguish between\nstrings that are in or outside a regular language, and utilize an algorithm for\nextracting the learned finite-state automaton. We apply this method to several\nregular languages and find unexpected results regarding the connections between\nthe network's states that may be regarded as evidence for generalization.\n",
        "  This paper describes our submission to the shared task on word/phrase level\nQuality Estimation (QE) in the First Conference on Statistical Machine\nTranslation (WMT16). The objective of the shared task was to predict if the\ngiven word/phrase is a correct/incorrect (OK/BAD) translation in the given\nsentence. In this paper, we propose a novel approach for word level Quality\nEstimation using Recurrent Neural Network Language Model (RNN-LM) architecture.\nRNN-LMs have been found very effective in different Natural Language Processing\n(NLP) applications. RNN-LM is mainly used for vector space language modeling\nfor different NLP problems. For this task, we modify the architecture of\nRNN-LM. The modified system predicts a label (OK/BAD) in the slot rather than\npredicting the word. The input to the system is a word sequence, similar to the\nstandard RNN-LM. The approach is language independent and requires only the\ntranslated text for QE. To estimate the phrase level quality, we use the output\nof the word level QE system.\n",
        "  We analyze evolutionary dynamics on graphs, where the nodes represent\nindividuals of a population. The links of a node describe which other\nindividuals can be displaced by the offspring of the individual on that node.\nAmplifiers of selection are graphs for which the fixation probability is\nincreased for advantageous mutants and decreased for disadvantageous mutants. A\nfew examples of such amplifiers have been developed, but so far it is unclear\nhow many such structures exist and how to construct them. Here, we show that\nalmost any undirected random graph is an amplifier of selection for Birth-death\nupdating, where an individual is selected to reproduce with probability\nproportional to its fitness and one of its neighbors is replaced by that\noffspring at random. If we instead focus on death-Birth updating, in which a\nrandom individual is removed and its neighbors compete for the empty spot, then\nthe same ensemble of graphs consists of almost only suppressors of selection\nfor which the fixation probability is decreased for advantageous mutants and\nincreased for disadvantageous mutants. Thus, the impact of population structure\non evolutionary dynamics is a subtle issue that will depend on seemingly minor\ndetails of the underlying evolutionary process.\n",
        "  Data Mining is the process of examining the information from different point\nof view and compressing it for the relevant data. This data can also be\nutilized to build the incomes. Data Mining is also known as Data or Knowledge\nDiscovery. The basic purpose of data mining is to search patterns which have\nminimal user inputs and efforts. Data Mining plays a very crucial role in the\nvarious fields. There are various data mining procedures which can be connected\nin different fields of innovation. By using data mining techniques, it is\nobserved that less time is taken for the prediction of any disease with more\naccuracy. In this paper we would review various data mining techniques which\nare categorized under classification, regression and clustering and apply these\nalgorithms over an ECG dataset. The purpose of this work is to determine the\nmost suitable data mining technique and use it to improve the accuracy of\nanalyzing ECG data for better decision making.\n",
        "  Although a number of studies have shown that natural and laboratory\npopulations initially well-adapted to their environment can evolve rapidly when\nconditions suddenly change, the dynamics of rapid adaptation are not well\nunderstood. Here a population genetic model of polygenic selection is analyzed\nto describe the short-term response of a quantitative trait after a sudden\nshift of the phenotypic optimum. We provide explicit analytical expressions for\nthe time scales over which the trait mean approaches the new optimum. We find\nthat when the effect sizes are small relative to a scaled mutation rate, the\ngenomic signatures of polygenic selection are small to moderate allele\nfrequency changes that occur in the short-term phase in a synergistic fashion.\nIn contrast, selective sweeps, i.e., dramatic changes in the allele frequency\nmay occur provided the size of the effect is sufficiently large. Applications\nof our theoretical results to the relationship between QTL and selective sweep\nmapping and to tests of fast polygenic adaptation are discussed.\n",
        "  A plethora of Entity Linking (EL) approaches has recently been developed.\nWhile many claim to be multilingual, the MAG (Multilingual AGDISTIS) approach\nhas been shown recently to outperform the state of the art in multilingual EL\non 7 languages. With this demo, we extend MAG to support EL in 40 different\nlanguages, including especially low-resources languages such as Ukrainian,\nGreek, Hungarian, Croatian, Portuguese, Japanese and Korean. Our demo relies on\nonline web services which allow for an easy access to our entity linking\napproaches and can disambiguate against DBpedia and Wikidata. During the demo,\nwe will show how to use MAG by means of POST requests as well as using its\nuser-friendly web interface. All data used in the demo is available at\nhttps://hobbitdata.informatik.uni-leipzig.de/agdistis/\n",
        "  Evolutionary forces shape patterns of genetic diversity within populations\nand contribute to phenotypic variation. In particular, recurrent positive\nselection has attracted significant interest in both theoretical and empirical\nstudies. However, most existing theoretical models of recurrent positive\nselection cannot easily incorporate realistic confounding effects such as\ninterference between selected sites, arbitrary selection schemes, and\ncomplicated demographic processes. It is possible to quantify the effects of\narbitrarily complex evolutionary models by performing forward population\ngenetic simulations, but forward simulations can be computationally prohibitive\nfor large population sizes ($> 10^5$). A common approach for overcoming these\ncomputational limitations is rescaling of the most computationally expensive\nparameters, especially population size. Here, we show that ad hoc approaches to\nparameter rescaling under the recurrent hitchhiking model do not always provide\nsufficiently accurate dynamics, potentially skewing patterns of diversity in\nsimulated DNA sequences. We derive an extension of the recurrent hitchhiking\nmodel that is appropriate for strong selection in small population sizes, and\nuse it to develop a method for parameter rescaling that provides the best\npossible computational performance for a given error tolerance. We perform a\ndetailed theoretical analysis of the robustness of rescaling across the\nparameter space. Finally, we apply our rescaling algorithms to parameters that\nwere previously inferred for Drosophila, and discuss practical considerations\nsuch as interference between selected sites.\n",
        "  The fundamentals of near infrared spectroscopy (NIRS) are reviewed. Among the\nmajor factors controlling the cerebral blood flow (CBF), the effect of PaCO2 is\npeculiar in that it violates autoregulatory CBF mechanisms and allows to\nexplore the full range of the CBF. A simple physical model, with a four\nparameter formula, relating the CBF to PaCO2 is presented. It can be used to\ntransform the fits of one animal to the fits of another one. It enable the use\nof rats data as monkeys data simply by rescaling the PaCO2 values and the CBF\ndata. Controlled breathing can change the PaCO2. Experiments on human subjects\nrelating the PaCO2 to rSO2, measured with brain oximeters, are presented. A\nsimple model relating the mean blood pressure to CBF is worked out.\n",
        "  Over half a century old and showing no signs of aging, k-means remains one of\nthe most popular data processing algorithms. As is well-known, a proper\ninitialization of k-means is crucial for obtaining a good final solution. The\nrecently proposed k-means++ initialization algorithm achieves this, obtaining\nan initial set of centers that is provably close to the optimum solution. A\nmajor downside of the k-means++ is its inherent sequential nature, which limits\nits applicability to massive data: one must make k passes over the data to find\na good initial set of centers. In this work we show how to drastically reduce\nthe number of passes needed to obtain, in parallel, a good initialization. This\nis unlike prevailing efforts on parallelizing k-means that have mostly focused\non the post-initialization phases of k-means. We prove that our proposed\ninitialization algorithm k-means|| obtains a nearly optimal solution after a\nlogarithmic number of passes, and then show that in practice a constant number\nof passes suffices. Experimental evaluation on real-world large-scale data\ndemonstrates that k-means|| outperforms k-means++ in both sequential and\nparallel settings.\n",
        "  Very low-mass binaries (VLMBs), with system masses <0.2 Msun appear to have\nvery different properties to stellar binaries. This has led to the suggestion\nthat VLMBs form a distinct and different population. As most stars are born in\nclusters, dynamical evolution can significantly alter any initial binary\npopulation, preferentially destroying wide binaries. In this paper we examine\nthe dynamical evolution of initially different VLMB distributions in clusters\nto investigate how different the initial and final distributions can be.\n  We find that the majority of the observed VLMB systems, which have\nseparations <20 au, cannot be destroyed in even the densest clusters.\nTherefore, the distribution of VLMBs with separations <20 au now must have been\nthe birth population (although we note that the observations of this population\nmay be very incomplete). Most VLMBs with separations >100 au can be destroyed\nin high-density clusters, but are mainly unaffected in low-density clusters.\nTherefore, the initial VLMB population must contain many more binaries with\nthese separations than now, or such systems must be made by capture during\ncluster dissolution. M-dwarf binaries are processed in the same way as VLMBs\nand so the difference in the current field populations either points to\nfundamentally different birth populations, or significant observational\nincompleteness in one or both samples.\n",
        "  The database community has developed numerous tools and techniques for data\ncuration and exploration, from declarative languages, to specialized techniques\nfor data repair, and more. Yet, there is currently no consensus on how to best\nexpose these powerful tools to an analyst in a simple, intuitive, and above\nall, flexible way. Thus, analysts continue to rely on tools such as\nspreadsheets, imperative languages, and notebook style programming environments\nlike Jupyter for data curation. In this work, we explore the integration of\nspreadsheets, notebooks, and relational databases. We focus on a key advantage\nthat both spreadsheets and imperative notebook environments have over classical\nrelational databases: ease of exception. By relying on set-at-a-time\noperations, relational databases sacrifice the ability to easily define\nsingleton operations, exceptions to a normal data processing workflow that\naffect query processing for a fixed set of explicitly targeted records. In\ncomparison, a spreadsheet user can easily change the formula for just one cell,\nwhile a notebook user can add an imperative operation to her notebook that\nalters an output 'view'. We believe that enabling such idiosyncratic manual\ntransformations in a classical relational database is critical for curation, as\ncuration operations that are easy to declare for individual values can often be\nextremely challenging to generalize. We explore the challenges of enabling\nsingletons in relational databases, propose a hybrid spreadsheet/relational\nnotebook environment for data curation, and present our vision of Vizier, a\nsystem that exposes data curation through such an interface.\n",
        "  The effect of the magnetic field on the critical behavior of Sr0:9La0:1CuO2\nis explored in terms of reversible magnetization data. As the correlation\nlength transverse to the magnetic field Hi,applied along the i-axis, cannot\ngrow beyond the limiting magnetic length LHi, related to the average distance\nbetween vortex lines, one expects a magnetic field induced finite size effect.\nInvoking the scaling theory of critical phenomena we provide clear evidence for\nthis effect. It implies that in type II superconductors there is a 3D to 1D\ncrossover line Hpi(T). Consequently, below Tc and above Hpi(T) uperconductivity\nis confined to cylinders with diameter LHi(1D). Accordingly, there is no\ncontinuous phase transition in the (H,T)-plane along the Hc2-lines as predicted\nby the mean-field treatment.\n",
        "  We present a new algorithm for 3D cone-beam tomography. The algorithm is\nbased on decomposition of the cone-beam backprojection operation and angular\ndecimation. It has computational complexity of $O(N^{3.5})$ and allows\nconsiderable reduction of peak memory usage in comparison with conventional\nalgorithms. Tests with real data demonstrate the acceleration, achieved by\nusing our algorithm instead of FDK, with 20-fold speedup for a $2000 \\times\n2000 \\times 720$ image. The algorithm is compared with other fast FDK\nalgorithms.\n",
        "  Among all kinds of apes, only gibbons have the slim body as human. Gibbons\ncan move in the forest by cross arm swing, what was the locomotion mode of our\narboreal ancestor. Since our ancestor had much heavier body but weaker arms\nthan gibbons, we suppose they had to move with two arm brachiation. Such mode\nof locomotion can account reasonably for the transition to bipedalism. Firstly,\nit needed our ancestor to straighten knee and hip joints and flex their lumbar\nspine. secondly, it evolved the feet of our ancestor with longitudinal arche.\nAnd most importantly, it made the ratio of the length of the upper limbs to\nthat of the lower limbs unsuitable for quadruped walking.\n",
        "  We present direct measurements of the superconducting order parameter in\nnearly optimal FeSe$_{0.5}$Te$_{0.5}$ single crystals with critical temperature\n$T_C \\approx 14$ K. Using intrinsic multiple Andreev reflection effect (IMARE)\nspectroscopy and measurements of lower critical field, we directly determined\ntwo superconducting gaps, $\\Delta_L \\approx 3.3 - 3.4$ meV and $\\Delta_S\n\\approx 1$ meV, and their temperature dependences. We show that a two-band\nmodel fits well the experimental data. The estimated electron-boson coupling\nconstants indicate a strong intraband and a moderate interband interaction.\n",
        "  Motivated by cloud security concerns, there is an increasing interest in\ndatabase systems that can store and support queries over encrypted data. A\ncommon architecture for such systems is to use a trusted component such as a\ncryptographic co-processor for query processing that is used to securely\ndecrypt data and perform computations in plaintext. The trusted component has\nlimited memory, so most of the (input and intermediate) data is kept encrypted\nin an untrusted storage and moved to the trusted component on ``demand.''\n  In this setting, even with strong encryption, the data access pattern from\nuntrusted storage has the potential to reveal sensitive information; indeed,\nall existing systems that use a trusted component for query processing over\nencrypted data have this vulnerability. In this paper, we undertake the first\nformal study of secure query processing, where an adversary having full\nknowledge of the query (text) and observing the query execution learns nothing\nabout the underlying database other than the result size of the query on the\ndatabase. We introduce a simpler notion, oblivious query processing, and show\nformally that a query admits secure query processing iff it admits oblivious\nquery processing. We present oblivious query processing algorithms for a rich\nclass of database queries involving selections, joins, grouping and\naggregation. For queries not handled by our algorithms, we provide some initial\nevidence that designing oblivious (and therefore secure) algorithms would be\nhard via reductions from two simple, well-studied problems that are generally\nbelieved to be hard. Our study of oblivious query processing also reveals\ninteresting connections to database join theory.\n",
        "  This study presents the first observation of shear wave induced remotely\nwithin soft tissues. It was performed through the combination of a transcranial\nmagnetic stimulation device and a permanent magnet. A physical model based on\nMaxwell and Navier equations was developed. Experiments were performed on a\ncryogel phantom and a chicken breast sample. Using an ultrafast ultrasound\nscanner, shear waves of respective amplitude of 5 and 0.5 micrometers were\nobserved. Experimental and numerical results were in good agreement. This study\nconstitutes the framework of an alternative shear wave elastography method.\n",
        "  We propose an efficient and scalable architecture for processing generalized\ngraph-pattern queries as they are specified by the current W3C recommendation\nof the SPARQL 1.1 \"Query Language\" component. Specifically, the class of\nqueries we consider consists of sets of SPARQL triple patterns with labeled\nproperty paths. From a relational perspective, this class resolves to\nconjunctive queries of relational joins with additional graph-reachability\npredicates. For the scalable, i.e., distributed, processing of this kind of\nqueries over very large RDF collections, we develop a suitable partitioning and\nindexing scheme, which allows us to shard the RDF triples over an entire\ncluster of compute nodes and to process an incoming SPARQL query over all of\nthe relevant graph partitions (and thus compute nodes) in parallel. Unlike most\nprior works in this field, we specifically aim at the unified optimization and\ndistributed processing of queries consisting of both relational joins and\ngraph-reachability predicates. All communication among the compute nodes is\nestablished via a proprietary, asynchronous communication protocol based on the\nMessage Passing Interface.\n",
        "  We describe several methods to construct minimal foliations by hyperbolic\nsurfaces on closed 3-manifolds, and discuss the properties of the examples thus\nobtained.\n",
        "  In this paper we focus our attention on small-to-intermediate N-body systems\nthat are, initially, distributed uniformly in space and dynamically cool\n(virial ratios $Q=2T/|\\Omega|$ below ~0.3). In this work, we study the mass\nsegregation that emerges after the initial violent dynamical evolution. At this\nscope, we ran a set of high precision N-body simulations of isolated clusters\nby means of HiGPUs, our direct summation N-body code. After the collapse, the\nsystem shows a clear mass segregation. This (quick) mass segregation occurs in\ntwo phases: the first shows up in clumps originated by sub-fragmentation before\nthe deep overall collapse; this segregation is partly erased during the deep\ncollapse to re-emerge, abruptly, during the second phase, that follows the\nfirst bounce of the system. In this second stage, the proper clock to measure\nthe rate of segregation is the dynamical time after virialization, which (for\ncold and cool systems) may be significantly different from the crossing time\nevaluated from initial conditions. This result is obtained for isolated\nclusters composed of stars of two different masses (in the ratio $m_h/m_l=2$),\nat varying their number ratio, and is confirmed also in presence of a massive\ncentral object (simulating a black hole of stellar size). Actually, in stellar\nsystems starting their dynamical evolution from cool conditions, the fast mass\nsegregation adds to the following, slow, secular segregation which is\ncollisionally induced. The violent mass segregation is an effect persistent\nover the whole range of N ($128 \\leq N \\leq 1024$) investigated, and is an\ninteresting feature on the astronomical-observational side, too. The\nsemi-steady state reached after virialization corresponds to a mass segregated\ndistribution function rather than that of equipartition of kinetic energy per\nunit mass as it should result from violent relaxation.\n",
        "  Chloroplast microsatellites have been widely used in population genetic\nstudies of conifers in recent years. However, their haplotype configurations\nsuggest that they could have high levels of homoplasy, thus limiting the power\nof these molecular markers. A coalescent-based computer simulation was used to\nexplore the influence of homoplasy on measures of genetic diversity based on\nchloroplast microsatellites. The conditions of the simulation were defined to\nfit isolated populations originating from the colonization of one single\nhaplotype into an area left available after a glacial retreat. Simulated data\nwere compared with empirical data available from the literature for a species\nof Pinus that has expanded north after the Last Glacial Maximum. In the\nevaluation of genetic diversity, homoplasy was found to have little influence\non Nei's unbiased haplotype diversity (H(E)) while Goldstein's genetic distance\nestimates (D2sh) were much more affected. The effect of the number of\nchloroplast microsatellite loci for evaluation of genetic diversity is also\ndiscussed.\n",
        "  Gyrosonics refers to novel audio binaural stimulus that produces rotational\nperceptions of sound movement in head at a particular predetermined frequency.\nTherapeutic effect observed with this is considered to be associated with\nmodification of arousal of autonomic nervous system. The heart rate variability\n(HRV), non-invasive measure of autonomic nervous system, has been measured for\ngroup of 30 subjects for pre- and post- gyrosonic installation. The time- and\nfrequency- domain analysis of HRV results show overall decrease in sympathetic\nresponse and increase in para- sympathetic response due to listening of gyro\nsonics.\n",
        "  We calculate the star formation quenching timescales in green valley galaxies\nat intermediate redshifts ($z\\sim0.5-1$) using stacked zCOSMOS spectra of\ndifferent galaxy morphological types: spheroidal, disk-like, irregular and\nmerger, dividing disk-like galaxies further into unbarred, weakly-barred and\nstrongly-barred, assuming a simple exponentially-decaying star formation\nhistory model and based on the H$_{\\delta}$ absorption feature and the $4000$\n\\AA ~break. We find that different morphological types present different star\nformation quenching timescales, reinforcing the idea that the galaxy morphology\nis strongly correlated with the physical processes responsible for quenching\nstar formation. Our quantification of the star formation quenching timescale\nindicates that disks have typical timescales $60\\%$ to 5 times longer than that\nof galaxies presenting spheroidal, irregular or merger morphologies. Barred\ngalaxies in particular present the slowest transition timescales through the\ngreen valley. This suggests that although secular evolution may ultimately lead\nto gas exhaustion in the host galaxy via bar-induced gas inflows that trigger\nstar formation activity, secular agents are not major contributors in the rapid\nquenching of galaxies at these redshifts. Galaxy interaction, associated with\nthe elliptical, irregular and merger morphologies contribute, to a more\nsignificant degree, to the fast transition through the green valley at these\nredshifts. In the light of previous works suggesting that both secular and\nmerger processes are responsible for the star formation quenching at low\nredshifts, our results provide an explanation to the recent findings that star\nformation quenching happened at a faster pace at $z\\sim0.8$.\n",
        "  Synchrotron emission pervades the Galactic plane at low radio frequencies,\noriginating from cosmic ray electrons interacting with the Galactic magnetic\nfield. Using a low-frequency radio telescope, the Murchison Widefield Array\n(MWA), we measure the free-free absorption of this Galactic synchrotron\nemission by intervening HII regions along the line of sight. These absorption\nmeasurements allow us to calculate the Galactic cosmic-ray electron emissivity\nbehind and in front of 47 detected HII regions in the region $250^\\circ < l <\n355^\\circ$, $|b| < 2^\\circ$. We find that all average emissivities between the\nHII regions and the Galactic edge along the line of sight ($\\epsilon_b$) are in\nthe range of 0.24$\\,\\,\\sim\\,\\,$0.70$\\,\\,$K$\\,\\,$pc$^{-1}$ with a mean of\n0.40$\\,\\,$K$\\,\\,$pc$^{-1}$ and a variance of 0.10$\\,\\,$K$\\,\\,$pc$^{-1}$ at\n88$\\,\\,$MHz. Our best model, the Two-circle model, divides the Galactic disk\ninto three regions using two circles centring on the Galactic centre. It shows\na high emissivity region near the Galactic centre, a low emissivity region near\nthe Galactic edge, and a medium emissivity region between these two regions,\ncontrary to the trend found by previous studies.\n",
        "  We have two main contributions in this work: 1. We explore the usage of a\nstacked denoising autoencoder, and a paragraph vector model to learn\ntask-independent dense patient representations directly from clinical notes. We\nevaluate these representations by using them as features in multiple supervised\nsetups, and compare their performance with those of sparse representations. 2.\nTo understand and interpret the representations, we explore the best encoded\nfeatures within the patient representations obtained from the autoencoder\nmodel. Further, we calculate the significance of the input features of the\ntrained classifiers when we use these pretrained representations as input.\n",
        "  The end-to-end nature of neural machine translation (NMT) removes many ways\nof manually guiding the translation process that were available in older\nparadigms. Recent work, however, has introduced a new capability: lexically\nconstrained or guided decoding, a modification to beam search that forces the\ninclusion of pre-specified words and phrases in the output. However, while\ntheoretically sound, existing approaches have computational complexities that\nare either linear (Hokamp and Liu, 2017) or exponential (Anderson et al., 2017)\nin the number of constraints. We present a algorithm for lexically constrained\ndecoding with a complexity of O(1) in the number of constraints. We demonstrate\nthe algorithms remarkable ability to properly place these constraints, and use\nit to explore the shaky relationship between model and BLEU scores. Our\nimplementation is available as part of Sockeye.\n",
        "  A set of views defined by selection queries splits a database relation into\nsub-relations, each containing a subset of the original rows. This\ndecomposition into horizontal fragments is lossless when the initial relation\ncan be reconstructed from the fragments by union. In this paper, we consider\nhorizontal decomposition in a setting where some of the attributes in the\ndatabase schema are interpreted over a specific domain, on which a set of\nspecial predicates and functions is defined.\n  We study losslessness in the presence of integrity constraints on the\ndatabase schema. We consider the class of conditional domain constraints\n(CDCs), which restrict the values that the interpreted attributes may take\nwhenever a certain condition holds on the non-interpreted ones, and investigate\nlossless horizontal decomposition under CDCs in isolation, as well as in\ncombination with functional and unary inclusion dependencies.\n",
        "  We apply the density functional theory for superconductors (SCDFT) based on\nthe local-density approximation (LDA) to alkali-doped fullerides A3C60 with the\nface-centered cubic structure. We evaluate the superconducting transition\ntemperature (Tc) from first principles considering energy dependence of\nelectron-phonon coupling, the mass renormalization, and the retardation effect.\nThe calculated Tc=7.5, 9.0 and 15.7 K for A=K, Rb, Cs are approximately 60 %\nsmaller than the experimentally observed values. Our results strongly suggest\nnecessity to go beyond the framework of the Migdal-Eliashberg theory based on\nthe LDA.\n",
        "  We report effect of Gd inclusion in the NbN superconductor thin films. The\nfilms are deposited on single crystalline Silicon (100) by DC reactive\nsputtering technique i.e., deposition of Nb and Gd in presence of reactive N2\ngas. The fabricated relatively thick films (400 nm) are crystallized in cubic\nstructure. These films are characterized for their morphology, elemental\nanalysis and roughness by Scanning Electron Microscopy (SEM), Energy Dispersive\nX-ray spectroscopy (EDAX) and Atomic Force Microscopy (AFM) respectively. The\noptimized film (maximum Tc) is achieved with gas ratio of Ar:N2 (80:20) for\nboth pristine and Gd doped films. The optimized NbN film possesses Tc (R=0) in\nzero and 140kOe fields are at 14.8K and 8.8K respectively. The Gd doped NbN\nfilm showed Tc (R=0) in zero and 130kOe fields at 11.2K and 6.8 K respectively.\nThe upper critical field Hc2(0) of the studied superconducting films is\ncalculated from the magneto-transport [R(T)H] measurements using GL equations.\nIt is found that Gd doping deteriorated the superconducting performance of NbN.\n",
        "  Variance is a popular and often necessary component of sampled aggregation\nqueries. It is typically used as a secondary measure to ascertain statistical\nproperties of the result such as its error. Yet, it is more expensive to\ncompute than simple, primary measures such as \\texttt{SUM}, \\texttt{MEAN}, and\n\\texttt{COUNT}.\n  There exist numerous techniques to compute variance. While the definition of\nvariance is considered to require multiple passes on the data, other\nmathematical representations can compute the value in a single pass. Some\nsingle-pass representations, however, can suffer from severe precision loss,\nespecially for large number of data points.\n  In this paper, we study variance implementations in various real-world\nsystems and find that major database systems such as PostgreSQL 9.4 and most\nlikely System X, a major commercially used closed-source database, use a\nrepresentation that is efficient, but suffers from floating point precision\nloss resulting from catastrophic cancellation. We note deficiencies in another\npopular representation, used by databases such as MySQL and Impala, that\nsuffers from not being distributive and therefore cannot take advantage of\nmodern parallel computational resources. We review literature over the past\nfive decades on variance calculation in both the statistics and database\ncommunities, and summarize recommendations on implementing variance functions\nin various settings, such as approximate query processing and large-scale\ndistributed aggregation.\n",
        "  S. Satoh has defined a construction to obtain a ribbon torus knot given a\nwelded knot. This construction is known to be surjective. We show that it is\nnot injective. Using the invariant of the peripheral structure, it is possible\nto provide a restriction on this failure of injectivity. In particular we also\nprovide an algebraic classification of the construction when restricted to\nclassical knots, where it is equivalent to the torus spinning construction.\n",
        "  We propose a simple method to produce quandle cocycles from group cocycles,\nas a modification of Inoue-Kabaya chain map. We further show that, in respect\nto \"universal central extended quandles\", the chain map induces an isomorphism\nbetween their third homologies. For example, all Mochizuki's quandle 3-cocycles\nare shown to be derived from group cocycles of some non-abelian group. As an\napplication, we calculate some $\\Z$-equivariant parts of the Dijkgraaf-Witten\ninvariants of some cyclic branched covering spaces, via some cocycle invariant\nof links.\n",
        "  In the coming decade the Gaia satellite will precisely measure the positions\nand velocities of millions of stars in the Galactic halo, including stars in\nmany tidal streams. These streams, the products of hierarchical accretion of\nsatellite galaxies by the Milky Way (MW), can be used to infer the Galactic\ngravitational potential thanks to their initial compactness in phase space.\nPlans for observations to extend Gaia's radial velocity (RV) measurements to\nfaint stars, and to determine precise distances to RR Lyrae (RRLe) in streams,\nwould further extend the power of Gaia's kinematic catalog to characterize the\nMW's potential at large Galactocentric distances. In this work I explore the\nimpact of these extra data on the ability to fit the potential using the method\nof action clustering, which statistically maximizes the information content\n(clumpiness) of the action space of tidal streams, eliminating the need to\ndetermine stream membership for individual stars. Using a mock halo in a toy\nspherical potential, updated post-launch error models for Gaia, and estimates\nfor RV and distance errors for the tracers to be followed up, I show that\ncombining either form of additional information with the Gaia catalog greatly\nreduces the bias in determining the scale radius and total mass of the Galaxy,\ncompared to the use of Gaia data alone.\n",
        "  In this paper, the problem of disambiguating a target word for Polish is\napproached by searching for related words with known meaning. These relatives\nare used to build a training corpus from unannotated text. This technique is\nimproved by proposing new rich sources of replacements that substitute the\ntraditional requirement of monosemy with heuristics based on wordnet relations.\nThe na\\\"ive Bayesian classifier has been modified to account for an unknown\ndistribution of senses. A corpus of 600 million web documents (594 billion\ntokens), gathered by the NEKST search engine allows us to assess the\nrelationship between training set size and disambiguation accuracy. The\nclassifier is evaluated using both a wordnet baseline and a corpus with 17,314\nmanually annotated occurrences of 54 ambiguous words.\n",
        "  Given a branched covering of degree d between closed surfaces, it determines\na collection of partitions of d, the branch data. In this work we show that any\nbranch data are realized by an indecomposable primitive branched covering on a\nconnected close surface N with Euler's characteristic less than or equal to 0.\nThis shows that decomposable and indecomposable realizations may coexist.\nMoreover, we characterize the branch data of a decomposable primitive branched\ncovering.\n",
        "  We study the energy deposition by light and heavy nuclei in tissue-like media\nas used for cancer therapy. The depth-dose distributions for protons, $^{3}$He,\n$^{12}$C, $^{20}$Ne, and $^{58}$Ni nuclei are calculated within a Monte Carlo\nmodel based on the GEANT4 toolkit. These distributions are compared with each\nother and with available experimental data. It is demonstrated that nuclear\nfragmentation reactions essentially reduce the peak-to-plateau ratio of the\ndose profiles for deeply penetrating energetic ions heavier than $^{3}$He. On\nthe other hand, all projectiles up to $^{20}$Ne were found equally suitable for\ntherapeutic use at low penetration depths.\n",
        "  During the past decade, there has been an extensive investigation of the\ncomputational complexity of the consistent answers of Boolean conjunctive\nqueries under primary key constraints. Much of this investigation has focused\non self-join-free Boolean conjunctive queries. In this paper, we study the\nconsistent answers of Boolean conjunctive queries involving a single binary\nrelation, i.e., we consider arbitrary Boolean conjunctive queries on directed\ngraphs. In the presence of a single key constraint, we show that for each such\nBoolean conjunctive query, either the problem of computing its consistent\nanswers is expressible in first-order logic, or it is polynomial-time solvable,\nbut not expressible in first-order logic.\n",
        "  When a population inhabits an inhomogeneous environment, the fitness value of\ntraits can vary with the position in the environment. Gene flow caused by\nrandom mating can nevertheless prevent that a sexually reproducing population\nsplits into different species under such circumstances. This is the problem of\nsympatric speciation. However, mating need not be entirely random. Here, we\npresent a model where the individually advantageous preference for partners of\nhigh fitness can lead to genetic clustering as a precondition for speciation.\nIn simulations, in appropriate parameter regimes, our model leads to the rapid\nfixation of the corresponding alleles.\n",
        "  Let N be a compact, orientable hyperbolic 3-manifold with connected, totally\ngeodesic boundary of genus 2. If N has Heegaard genus at least 5, then its\nvolume is greater than 6.89. The proof of this result uses the following\ndichotomy: either N has a long return path (defined by Kojima-Miyamoto), or N\nhas an embedded, codimension-0 submanifold X with incompressible boundary $T\n\\sqcup \\partial N$, where T is the frontier of X in N, which is not a book of\nI-bundles. As an application of this result, we show that if M is a closed,\norientable hyperbolic 3-manifold such that H_1(M;Z_2) has dimension at least 5,\nand if the image in H^2(M;Z_2) of the cup product map has image of dimension at\nmost 1, then M has volume greater than 3.44.\n",
        "  To learn about the past from a sample of genomic sequences, one needs to\nunderstand how evolutionary processes shape genetic diversity. Most population\ngenetic inference is based on frameworks assuming adaptive evolution is rare.\nBut if positive selection operates on many loci simultaneously, as has recently\nbeen suggested for many species including animals such as flies, a different\napproach is necessary. In this review, I discuss recent progress in\ncharacterizing and understanding evolution in rapidly adapting populations\nwhere random associations of mutations with genetic backgrounds of different\nfitness, i.e., genetic draft, dominate over genetic drift. As a result, neutral\ngenetic diversity depends weakly on population size, but strongly on the rate\nof adaptation or more generally the variance in fitness. Coalescent processes\nwith multiple mergers, rather than Kingman's coalescent, are appropriate\ngenealogical models for rapidly adapting populations with important\nimplications for population genetic inference.\n",
        "  Phenotypic plasticity and its evolution may help evolutionary rescue in a\nnovel and stressful environment, especially if environmental novelty reveals\ncryptic genetic variation that enables the evolution of increased plasticity.\nHowever, the environmental stochasticity ubiquitous in natural systems may\nalter these predictions because high plasticity may amplify\nphenotype-environment mismatches. Although previous studies have highlighted\nthis potential detrimental effect of plasticity in stochastic environments,\nthey have not investigated how it affects extinction risk in the context of\nevolutionary rescue and with evolving plasticity. We investigate this question\nhere by integrating stochastic demography with quantitative genetic theory in a\nmodel with simultaneous change in the mean and predictability (temporal\nautocorrelation) of the environment. We develop an approximate prediction of\nlong-term persistence under the new pattern of environmental fluctuations, and\ncompare it with numerical simulations for short- and long-term extinction risk.\nWe find that reduced predictability increases extinction risk and reduces\npersistence because it increases stochastic load during rescue. This\nunderstanding of how stochastic demography, phenotypic plasticity, and\nevolution interact when evolution acts on cryptic genetic variation revealed in\na novel environment can inform expectations for invasions, extinctions, or the\nemergence of chemical resistance in pests.\n",
        "  Antimalarial drugs are a powerful tool for malaria control and elimination.\nArtemisinin-based combination therapies (ACTs) can reduce transmission when\nwidely distributed in a campaign setting. Modelling mass antimalarial campaigns\ncan elucidate how to most effectively deploy drug-based interventions and\nquantitatively compare the effects of cure, prophylaxis, and\ntransmission-blocking in suppressing parasite prevalence. A previously\nestablished agent-based model that includes innate and adaptive immunity was\nused to simulate malaria infections and transmission. Pharmacokinetics of\nartemether, lumefantrine, dihydroartemisinin, piperaquine, and primaquine were\nmodelled with a double-exponential distribution-elimination model including\nweight-dependent parameters and age-dependent dosing. Drug killing of asexual\nparasites and gametocytes was calibrated to clinical data. Mass distribution of\nACTs and primaquine was simulated with seasonal mosquito dynamics at a range of\ntransmission intensities. A single mass campaign with antimalarial drugs is\ninsufficient to permanently reduce malaria prevalence when transmission is\nhigh. Current diagnostics are insufficiently sensitive to accurately identify\nasymptomatic infections, and mass-screen-and-treat campaigns are much less\nefficacious than mass drug administrations. Improving campaign coverage leads\nto decreased prevalence one month after the end of the campaign, while\nincreasing compliance lengthens the duration of protection against reinfection.\nUse of a long-lasting prophylactic as part of a mass drug administration\nregimen confers the most benefit under conditions of high transmission and\nmoderately high coverage. Addition of primaquine can reduce prevalence but\nexerts its largest effect when coupled with a long-lasting prophylactic.\n",
        "  Magnetic fields in nearby, star-forming galaxies reveal both large-scale\npatterns and small-scale structures. A large-scale field reversal may exist in\nthe Milky Way but no such reversals have been observed so far in external\ngalaxies. The effects of star-forming regions of galaxies need to be included\nwhen modelling the evolution of their magnetic fields, which can then be\ncompared to future radio polarization observations. The causes of large-scale\nfield reversals also need clarification. Our model of field evolution in\nisolated disc galaxies includes a standard mean-field dynamo and continuous\ninjection of turbulent fields (the effect of supernova explosions) in discrete\nstar forming regions by implicit small-scale dynamo action. Synthetic maps of\nradio synchrotronemission and Faraday rotation measures are computed. A\nlarge-scale dynamo is essential to obtain regular large-scale spiral magnetic\nfields, observed in many galaxies. These appear, on kpc scales in near energy\nequilibrium with the turbulence, after 1-2 Gyr (redshift 4-3). Turbulent field\ninjection generates small-scale field structures. Depending on model\nparameters, large-scale field reversals may persist over many Gyrs and can\nsurvive until the present epoch. Significant polarized radio synchrotron\nemission from young galaxies is expected at redshifts less than 4. Faraday\nrotation measures (RM) are crucial to detect field reversals. Large-scale\npatterns ofrotation measures can be observed at redshifts less than 3. Our\nmodel can explain the general form of axisymmetric spiral fields with many\nlocal distortions, as observed in nearby galaxies. For a slightly different\nchoice of parameters, large-scale field reversals can persist over the lifetime\nof a galaxy. Comparison of our synthetic maps with future observations of\ndistant galaxies with the planned Square Kilometre Array (SKA) will allow\nrefinement of models.\n",
        "  The hyperelliptic mapping class group has been studied in various contexts\nwithin topology and algebraic geometry. What makes this study tractable is that\nthere is a surjective map from the hyperelliptic mapping class group to a\nmapping class group of a punctured sphere. The more general family of\nsuperelliptic mapping class groups does not, in general, surject on to a\nmapping class group of a punctured sphere, but on to a finite index subgroup.\nWe call this finite index subgroup the liftable mapping class group. In order\nto initiate the generalization of results on the hyperelliptic mapping class\ngroup to the broader family of superelliptic mapping class groups, we study an\nintermediate family called the balanced superelliptic mapping class group. We\ncompute the index of the liftable mapping class group in the full mapping class\ngroup of the sphere and show that the liftable mapping class group is\nindependent of the degree of the cover. We also build a presentation for the\nliftable mapping class group, compute its abelianization, and show that the\nbalanced superelliptic mapping class group has finite abelianization. Although\nour calculations focus on the subfamily of balanced superelliptic mapping class\ngroups, our techniques can be extended to any superelliptic mapping class\ngroup, even those not within the balanced family.\n",
        "  Data sets for identifying Alzheimer's disease (AD) are often relatively\nsparse, which limits their ability to train generalizable models. Here, we\naugment such a data set, DementiaBank, with each of two normative data sets,\nthe Wisconsin Longitudinal Study and Talk2Me, each of which employs a\nspeech-based picture-description assessment. Through minority class\noversampling with ADASYN, we outperform state-of-the-art results in binary\nclassification of people with and without AD in DementiaBank. This work\nhighlights the effectiveness of combining sparse and difficult-to-acquire\npatient data with relatively large and easily accessible normative datasets.\n",
        "  This study provides an accurate, efficient, and simple multiple scattering\nformulation for heavy charged particles such as protons and heavier ions with a\nnew form of scattering power that is a key quantity for beam transport in\nmatter. The Highland formula for multiple scattering angle was modified to a\nscattering-power formula to be used within the Fermi-Eyges theory in the\npresence of heterogeneity. An analytical formula for RMS end-point displacement\nin homogeneous matter was also derived for arbitrary ions. The formulation was\nexamined in terms of RMS angles and displacements in comparison with other\nformulations and measurements. The results for protons, helium ions, and carbon\nions in water agreed with them at a level of 2% or the differences were\ndiscussed.\n",
        "  In this note, we use the recent work of Honda-Kazez-Matic [HKM] to prove that\na closed contact 3-manifold admitting a compatible open book decomposition with\na nontrivial monodromy which can be presented as a product of left handed Dehn\ntwists is overtwisted.\n",
        "  Ultrathin $\\rm Bi_2Se_3$-NbN bilayers comprise a simple proximity system of a\ntopological insulator and an s-wave superconductor for studying gating effects\non topological superconductors. Here we report on 3 nm thick NbN layers of\nweakly connected superconducting islands, overlayed with 10 nm thick $\\rm\nBi_2Se_3$ film which facilitates enhanced proximity coupling between them.\nResistance versus temperature of the most resistive bilayers shows insulating\nbehavior but with signs of superconductivity. We measured the magnetoresistance\n(MR) of these bilayers versus temperature with and without a magnetic field H\nnormal to the wafer (MR=[R(H)-R(0)]/\\{[R(H)+R(0)]/2\\}), and under three\nelectric gate-fields of 0 and $\\pm2$ MV/cm. The MR results showed a complex set\nof gate sensitive peaks which extended up to about 30 K. The results are\ndiscussed in terms of vortex physics, and the origin of the different MR peaks\nis identified and attributed to flux-flow MR in the isolated NbN islands and\nthe different proximity regions in the $\\rm Bi_2Se_3$ cap-layer. The dominant\nMR peak was found to be consistent with enhanced proximity induced\nsuperconductivity in the topological edge currents regions. The high\ntemperature MR data suggest a possible pseudogap phase or a highly extended\nfluctuation regime.\n",
        "  In [2] it was proven that the Cass algorithm is a polynomial-time algorithm\nfor constructing level<=2 networks from clusters. Here we demonstrate, for each\nk>=0, a polynomial-time algorithm for constructing level-k phylogenetic\nnetworks from clusters. Unlike Cass the algorithm scheme given here is only of\ntheoretical interest. It does, however, strengthen the hope that efficient\npolynomial-time algorithms (and perhaps fixed parameter tractable algorithms)\nexist for this problem.\n",
        "  We demonstrate the fabrication of superconducting SmFeAsO1-xFx (Sm-1111)\nwires by using the ex-situ powder-in-tube technique. Sm-1111 powder and a\nbinder composed of SmF3, samarium arsenide, and iron arsenide were used to\nsynthesize the superconducting core. Although the F content of Sm-1111 is\nreduced in the process of ex-situ fabrication, the binder compensates by\nsufficiently supplementing the F content, thereby preventing a decrease in the\nsuperconducting transition temperature and a shrinkage of the superconducting\nvolume fraction. Thus, in the superconducting Sm-1111 wire with the binder, the\ntransport critical current density reaches the highest value of ~4 kA/cm2 at\n4.2 K.\n",
        "  We survey work on the topology of the space AH(M) of all (marked) hyperbolic\n3-manifolds homotopy equivalent to a fixed compact 3-manifold M with boundary.\nThe interior of AH(M) is quite well-understood, but the topology of the entire\nspace can be quite complicated. However, the topology is well-behaved at many\npoints in the boundary of AH(M).\n",
        "  We synthesise traditional unstructured food webs, allometric body size\nscaling, trait-based modelling, and physiologically structured modelling to\nprovide a novel and ecologically relevant tool for size-structured food webs.\nThe framework allows food web models to include ontogenetic growth and\nlife-history omnivory at the individual level by resolving the population\nstructure of each species as a size-spectrum. Each species is characterised by\nthe trait 'size at maturation', and all model parameters are made species\nindependent through scaling with individual body size and size at maturation.\nParameter values are determined from cross-species analysis of fish communities\nas life-history omnivory is widespread in aquatic systems, but may be\nreparameterised for other systems. An ensemble of food webs is generated and\nthe resulting communities are analysed at four levels of organisation:\ncommunity level, species level, trait level, and individual level. The model\nmay be solved analytically by assuming that the community spectrum follows a\npower law. The analytical solution provides a baseline expectation of the\nresults of complex food web simulations, and agrees well with the predictions\nof the full model on 1) biomass distribution as a function of individual size,\n2) biomass distribution as a function of size at maturation, and 3) relation\nbetween predator-prey mass ratio of preferred and eaten food. The full model\nadditionally predicts the diversity distribution as a function of size at\nmaturation.\n",
        "  Purpose: This goal of this study was to evaluate the effects of a data-driven\nclinical productivity system that leverages Electronic Health Record (EHR) data\nto provide productivity decision support functionality in a real-world clinical\nsetting. The system was implemented for a large behavioral health care provider\nseeing over 75,000 distinct clients a year. Design/methodology/approach: The\nkey metric in this system is a \"VPU\", which simultaneously optimizes multiple\naspects of clinical care. The resulting mathematical value of clinical\nproductivity was hypothesized to tightly link the organization's performance to\nits expectations and, through transparency and decision support tools at the\nclinician level, affect significant changes in productivity, quality, and\nconsistency relative to traditional models of clinical productivity. Findings:\nIn only 3 months, every single variable integrated into the VPU system showed\nsignificant improvement, including a 30% rise in revenue, 10% rise in clinical\npercentage, a 25% rise in treatment plan completion, a 20% rise in case rate\neligibility, along with similar improvements in compliance/audit issues,\noutcomes collection, access, etc. Practical implications: A data-driven\nclinical productivity system employing decision support functionality is\neffective because of the impact on clinician behavior relative to traditional\nclinical productivity systems. Critically, the model is also extensible to\nintegration with outcomes-based productivity. Originality/Value: EHR's are only\na first step - the problem is turning that data into useful information.\nTechnology can leverage the data in order to produce actionable information\nthat can inform clinical practice and decision-making. Without additional\ntechnology, EHR's are essentially just copies of paper-based records stored in\nelectronic form.\n",
        "  Circumstellar disks are the cradles of planetary systems and their physical\nand chemical properties directly influence the planet formation process. As\nmost planets supposedly form in the inner disk regions, i.e., within a few tens\nof AU, it is crucial to study circumstellar disk on these scales to constrain\nthe conditions for planet formation. Our aims are to characterize the inner\nregions of the circumstellar disk around the young Herbig Ae/Be star HD97048 in\npolarized light. We use VLT/NACO to observe HD97048 in polarimetric\ndifferential imaging (PDI) mode in the H and Ks band. We spatially resolve the\ndisk around HD97048 in polarized flux in both filters on scales between\n~0.1\"-1.0\" corresponding to the inner ~16-160 AU. Fitting isophots to the flux\ncalibrated H-band image between 13 - 14 mag/arcsec^2 and 14 - 15 mag/arcsec^2\nwe derive a apparent disk inclination angle of 34+-5 deg and 47+-2 deg,\nrespectively. The disk position angle in both brightness regimes is almost\nidentical and roughly 80 deg. Along the disk major axis the surface brightness\nof the polarized flux drops from ~11 mag/arcsec^2 at ~0.1\" (~16 AU) to ~15.3\nmag/arcsec^2 at ~1.0\" (~160 AU). The brightness profiles along the major axis\nare fitted with power-laws falling off as ~r^(-1.78+-0.02) in H and\n~r^(-2.34+-0.04) in Ks. As the surface brightness drops off more rapidly in Ks\ncompared to H the disks becomes relatively bluer at larger separations possibly\nindicating changing dust grain properties as a function of radius. For the\nfirst time the inner ~0.1\"-1.0\" (~16-160 AU) of the surface layer of the\nHD97048 circumstellar disk have been imaged in scattered light demonstrating\nthe power of ground-based imaging polarimetry. Our data fill an important gap\nin a large collection of existing data including resolved thermal dust and PAH\nemission images as well as resolved gas emission lines.\n",
        "  Data cubes are widely used as a powerful tool to provide multidimensional\nviews in data warehousing and On-Line Analytical Processing (OLAP). However,\nwith increasing data sizes, it is becoming computationally expensive to perform\ndata cube analysis. The problem is exacerbated by the demand of supporting more\ncomplicated aggregate functions (e.g. CORRELATION, Statistical Analysis) as\nwell as supporting frequent view updates in data cubes. This calls for new\nscalable and efficient data cube analysis systems. In this paper, we introduce\nHaCube, an extension of MapReduce, designed for efficient parallel data cube\nanalysis on large-scale data by taking advantages from both MapReduce (in terms\nof scalability) and parallel DBMS (in terms of efficiency). We also provide a\ngeneral data cube materialization algorithm which is able to facilitate the\nfeatures in MapReduce-like systems towards an efficient data cube computation.\nFurthermore, we demonstrate how HaCube supports view maintenance through either\nincremental computation (e.g. used for SUM or COUNT) or recomputation (e.g.\nused for MEDIAN or CORRELATION). We implement HaCube by extending Hadoop and\nevaluate it based on the TPC-D benchmark over billions of tuples on a cluster\nwith over 320 cores. The experimental results demonstrate the efficiency,\nscalability and practicality of HaCube for cube analysis over a large amount of\ndata in a distributed environment.\n",
        "  The natural language generation (NLG) component of a spoken dialogue system\n(SDS) usually needs a substantial amount of handcrafting or a well-labeled\ndataset to be trained on. These limitations add significantly to development\ncosts and make cross-domain, multi-lingual dialogue systems intractable.\nMoreover, human languages are context-aware. The most natural response should\nbe directly learned from data rather than depending on predefined syntaxes or\nrules. This paper presents a statistical language generator based on a joint\nrecurrent and convolutional neural network structure which can be trained on\ndialogue act-utterance pairs without any semantic alignments or predefined\ngrammar trees. Objective metrics suggest that this new model outperforms\nprevious methods under the same experimental conditions. Results of an\nevaluation by human judges indicate that it produces not only high quality but\nlinguistically varied utterances which are preferred compared to n-gram and\nrule-based systems.\n",
        "  It is unknown whether an unknotting tunnel is always isotopic to a geodesic\nin a finite volume hyperbolic 3-manifold. In this paper, we address the\ngeneralization of this problem to hyperbolic 3-manifolds admitting tunnel\nsystems. We show that there exist finite volume hyperbolic 3-manifolds with a\nsingle cusp, with a system of at least two tunnels, such that all but one of\nthe tunnels come arbitrarily close to self-intersecting. This gives evidence\nthat systems of unknotting tunnels may not be isotopic to geodesics in tunnel\nnumber n manifolds. In order to show this result, we prove there is a\ngeometrically finite hyperbolic structure on a (1;n)-compression body with a\nsystem of core tunnels such that all but one of the core tunnels\nself-intersect.\n",
        "  We consider the evaluation of first-order queries over classes of databases\nwith bounded expansion. The notion of bounded expansion is fairly broad and\ngeneralizes bounded degree, bounded treewidth and exclusion of at least one\nminor. It was known that over a class of databases with bounded expansion,\nfirst-order sentences could be evaluated in time linear in the size of the\ndatabase. We give a different proof of this result. Moreover, we show that\nanswers to first-order queries can be enumerated with constant delay after a\nlinear time preprocessing. We also show that counting the number of answers to\na query can be done in time linear in the size of the database.\n",
        "  The primary interface of contact between a robotic or prosthetic hand and the\nexternal world is through the artificial skin. To make sense of that contact,\ntactile sensors are needed. These sensors are normally embedded in soft,\nsynthetic materials for protecting the subsurface sensor from damage or for\nbetter hand-to-object contact. It is important to understand how the mechanical\nsignals transmit from the artificial skin to the embedded tactile sensors. In\nthis paper, we made use of a finite element model of an artificial fingertip\nwith viscoelastic and hyperelastic behaviors to investigate the subsurface\npressure profiles when flat, curved, and Braille surfaces were indented on the\nsurface of the model. Furthermore, we investigated the effects of 1, 3 and 5 mm\nthickness of the skin on the subsurface pressure profiles. The simulation\nresults were experimentally validated using a 25.4 {\\mu}m thin pressure\ndetecting film that was able to follow the contours of a non-planar surface,\nwhich is analogous to an artificial bone. Results show that the thickness of\nthe artificial skin has an effect on the peak pressure, on the span of the\npressure distribution, and on the overall shape of the pressure profile that\nwas encoded on a curved subsurface structure. Furthermore, the flat, curved,\nand Braille surfaces can be discriminated from one another with the 1 and 3 mm\nartificial skin layers, but not with the 5 mm thick skin.\n",
        "  In the SIGMOD 2013 conference, we published a paper extending our earlier\nwork on crowdsourced entity resolution to improve crowdsourced join processing\nby exploiting transitive relationships [Wang et al. 2013]. The VLDB 2014\nconference has a paper that follows up on our previous work [Vesdapunt et al.,\n2014], which points out and corrects a mistake we made in our SIGMOD paper.\nSpecifically, in Section 4.2 of our SIGMOD paper, we defined the \"Expected\nOptimal Labeling Order\" (EOLO) problem, and proposed an algorithm for solving\nit. We incorrectly claimed that our algorithm is optimal. In their paper,\nVesdapunt et al. show that the problem is actually NP-Hard, and based on that\nobservation, propose a new algorithm to solve it. In this note, we would like\nto put the Vesdapunt et al. results in context, something we believe that their\npaper does not adequately do.\n",
        "  We investigate theoretically how the proximity effect in\nsuperconductor/ferromagnet hybrid structures with intrinsic spin-orbit coupling\nmanifests in the density of states and critical temperature. To describe a\ngeneral scenario, we allow for both Rashba and Dresselhaus type spin-orbit\ncoupling. Our results are obtained via the quasiclassical theory of\nsuperconductivity, extended to include spin-orbit coupling in the Usadel\nequation and Kupriyanov--Lukichev boundary conditions. Unlike previous works,\nwe have derived a Riccati parametrization of the Usadel equation with\nspin-orbit coupling which allows us to address the full proximity regime.\nFirst, we consider the density of states in both SF bilayers and SFS trilayers,\nwhere the spectroscopic features in the latter case are sensitive to the phase\ndifference between the two superconductors. We find that the presence of\nspin-orbit coupling leaves clear spectroscopic fingerprints in the density of\nstates due to its role in creating spin-triplet Cooper pairs. Unlike SF and SFS\nstructures without spin-orbit coupling, the density of states in the present\ncase depends strongly on the direction of magnetization. We show that the\nspin-orbit coupling can stabilize singlet superconductivity even in the\npresence of a strong exchange field $h \\gg \\Delta$. This leads to the\npossibility of a magnetically tunable minigap: changing the direction of the\nexchange field opens and closes the minigap. We also determine how the critical\ntemperature $T_c$ of an SF bilayer is affected by spin-orbit coupling and\ndemonstrate that one can achieve a spin-valve effect with a single ferromagnet.\nWe find that $T_c$ displays highly non-monotonic behavior both as a function of\nthe magnetization direction and the type and direction of the spin-orbit\ncoupling, offering a new way to exert control over the superconductivity of\nproximity structures.\n",
        "  The speciation model proposed by Derrida and Higgs demonstrated that a\nsexually reproducing population can split into different species in the absence\nof natural selection or any type of geographic isolation, provided that mating\nis assortative and the number of genes involved in the process is infinite.\nHere we revisit this model and simulate it for finite genomes, focusing on the\nquestion of how many genes it actually takes to trigger neutral sympatric\nspeciation. We find that, for typical parameters used in the original model, it\ntakes of the order of $10^5$ genes. We compare the results with a similar\nspatially explicit model where about 100 genes suffice for speciation. We show\nthat when the number of genes is small the species that emerge are strongly\nsegregated in space. For larger number of genes, on the other hand, the spatial\nstructure of the population is less important and the species distribution\noverlap considerably.\n",
        "  We define a metric filtration of the Gordian graph by an infinite family of\n1-dense subgraphs. The n-th subgraph of this family is generated by all knots\nwhose fundamental groups surject to a symmetric group with parameter at least\nn, where all meridians are mapped to transpositions. Incidentally, we verify\nthe Meridional Rank Conjecture for a family of knots with unknotting number one\nyet arbitrarily high bridge number.\n",
        "  Objectives: To analyze the whole process involved in the production of a new\nbifocal Multizone Contact Lens (MCL) for presbyopia.\n  Methods: The optical quality of a new MCL was evaluated by ray tracing\nsoftware in a model eye with pupil different diameters with the lens centered\nand decentered. A stock of low addition (+1.5 D) MCL for presbyopia was ordered\nfor manufacturing. Power profiles were measured with a contact lens power\nmapper, processed with a custom software and compared with the theoretical\ndesign. Nine lenses from the stock were fitted to presbyopic subjects and the\nvisual performance was evaluated with new APPs for iPad Retina.\n  Results: Numerical simulations showed that the trough the focus curve\nprovided by MCL has an extended depth of focus. The optical quality was not\ndependent on pupil size and only decreased for lens decentered with a pupil\ndiameter of 4.5 mm. The manufactured MCL showed a smoothed power profile with a\nless-defined zones. The bias between experimental and theoretical zone sizes\nwas uniform along the optical zone unless for the most central area. Eyes\nfitted with the manufactured MCL showed an improvement in near Visual Acuity\n(VA) and near stereopsis. Althouh Contrast Sensitivity (CS) at distance\ndecreased, the defocus curve for contrast showed an extended depth of focus\ncorrelated to the ray tracing results.\n  Conclusions: The understanding of vision with MCL requires a process that\ninvolves design and characterization for detecting any defect that may have\nimpact in the final visual performance. Keywords: Multifocal contact lenses,\ndesign, characterization, visual performance\n",
        "  We consider the spread of infectious disease through contact networks of\nConfiguration Model type. We assume that the disease spreads through contacts\nand infected individuals recover into an immune state. We discuss a number of\nexisting mathematical models used to investigate this system, and show\nrelations between the underlying assumptions of the models. In the process we\noffer simplifications of some of the existing models. The distinctions between\nthe underlying assumptions are subtle, and in many if not most cases this\nsubtlety is irrelevant. Indeed, under appropriate conditions the models are\nequivalent. We compare the benefits and disadvantages of the different models,\nand discuss their application to other populations (\\emph{e.g.,} clustered\nnetworks). Finally we discuss ongoing challenges for network-based epidemic\nmodeling.\n",
        "  This paper describes the system architecture of the Vertica Analytic Database\n(Vertica), a commercialization of the design of the C-Store research prototype.\nVertica demonstrates a modern commercial RDBMS system that presents a classical\nrelational interface while at the same time achieving the high performance\nexpected from modern \"web scale\" analytic systems by making appropriate\narchitectural choices. Vertica is also an instructive lesson in how academic\nsystems research can be directly commercialized into a successful product.\n",
        "  The purpose of this work is to provide a fast and accurate scatter artifacts\ncorrection algorithm for cone beam CT (CBCT) imaging. The method starts with an\nestimation of coarse scatter profiles for a set of CBCT data in either image\ndomain or projection domain. A denoising algorithm designed specifically for\nPoisson signals is then applied to derive the final scatter distribution.\nQualitative and quantitative evaluations using thorax and abdomen phantoms with\nMonte Carlo (MC) simulations, experimental Catphan phantom data, and in vivo\nhuman data acquired for a clinical image guided radiation therapy were\nperformed. Results show that the proposed algorithm can significantly reduce\nscatter artifacts and recover the correct HU in either projection domain or\nimage domain. For the MC thorax phantom study, four components segmentation\nyield the best results, while the results of three components segmentation are\nstill acceptable. For the Catphan phantom data, the mean value over all pixels\nin the residual image is reduced from -21.8 HU to -0.2 HU and 0.7 HU for\nprojection domain and image domain, respectively. The contrast of the in vivo\nhuman images are greatly improved after correction. The software-based\ntechnique has a number of advantages, such as high computational efficiency and\naccuracy, and the capability of performing scatter correction without modifying\nthe clinical workflow or modifying the imaging hardware. When implemented\npractically, this should improve the accuracy of CBCT image quantitation and\nsignificantly impact CBCT-based interventional procedures and adaptive\nradiation therapy.\n",
        "  The question, whether the stellar populations in the Milky Way take part in\nflaring of the scale heights as observed for the HI gas is a matter of debate.\nStandard mass models for the Milky Way assume a constant scale height for each\nof the different stellar distributions. However, there is mounting evidence\nthat at least some of the stellar distributions reach at large galactocentric\ndistances high altitudes that are incompatible with a constant scale height. We\ndiscuss recent observational evidence for stellar flaring and compare it with\nHI data from the Leiden/Argentine/Bonn (LAB) survey. Within the systemic and\nstatistical uncertainties we find a good agreement between both.\n",
        "  In an academic environment, student advising is considered a paramount\nactivity for both advisors and student to improve the academic performance of\nstudents. In universities of large numbers of students, advising is a\ntime-consuming activity that may take a considerable effort of advisors and\nuniversity administration in guiding students to complete their registration\nsuccessfully and efficiently. Current systems are traditional and depend\ngreatly on the effort of the advisor to find the best selection of courses to\nimprove students performance. There is a need for a smart system that can\nadvise a large number of students every semester. In this paper, we propose a\nsmart system that uses association rule mining to help both students and\nadvisors in selecting and prioritizing courses. The system helps students to\nimprove their performance by suggesting courses that meet their current needs\nand at the same time improve their academic performance. The system uses\nassociation rule mining to find associations between courses that have been\nregistered by students in many previous semesters. The system successfully\ngenerates a list of association rules that guide a particular student to select\ncourses registered by similar students.\n",
        "  A new approach, to measure normalization completeness for conceptual model,\nis introduced using quantitative fuzzy functionality in this paper. We measure\nthe normalization completeness of the conceptual model in two steps. In the\nfirst step, different normalization techniques are analyzed up to Boyce Codd\nNormal Form (BCNF) to find the current normal form of the relation. In the\nsecond step, fuzzy membership values are used to scale the normal form between\n0 and 1. Case studies to explain schema transformation rules and measurements.\nNormalization completeness is measured by considering completeness attributes,\npreventing attributes of the functional dependencies and total number of\nattributes such as if the functional dependency is non-preventing then the\nattributes of that functional dependency are completeness attributes. The\nattributes of functional dependency which prevent to go to the next normal form\nare called preventing attributes.\n",
        "  A new approach for decoding directly strains from surfaces encoded with\nrandom patterns has been developed and validated. It is based on phase analysis\nof small region of interest. Here we adapt to random patterns new concepts\nproposed by Badulescu (2009) on the grid method. First metrological results are\nencouraging: resolution is proportional to strain level, being 9% of the\nnominal value, for a spatial resolution of 9 pixels (ZOI 64 \\times 64 pixels2).\nRandom noise has to be carefully controlled. A numerical example shows the\nrelevance of the approach. Then, first application on a carbon fiber reinforced\ncomposite is developed. Fabric intertwining is studied using a tensile test.\nOver-strains are clearly visible, and results connect well with the previous\nstudies\n",
        "  The networks of predator-prey interactions in ecological systems are\nremarkably complex, but nevertheless surprisingly stable in terms of long term\npersistence of the system as a whole. In order to understand the mechanism\ndriving the complexity and stability of such food webs, we developed an\neco-evolutionary model in which new species emerge as modifications of existing\nones and dynamic ecological interactions determine which species are viable.\nThe food-web structure thereby emerges from the dynamical interplay between\nspeciation and trophic interactions. The proposed model is less abstract than\nearlier evolutionary food web models in the sense that all three evolving\ntraits have a clear biological meaning, namely the average body mass of the\nindividuals, the preferred prey body mass, and the width of their potential\nprey body mass spectrum. We observed networks with a wide range of sizes and\nstructures and high similarity to natural food webs. The model networks exhibit\na continuous species turnover, but massive extinction waves that affect more\nthan $50 \\%$ of the network are not observed.\n",
        "  We present a detailed study of the extremely isolated Sdm galaxy UGC4722 (M_B\n= -17.4) located in the nearby Lynx-Cancer void. UGC4722 is a member of the\ncatalogue of isolated galaxies, and has also been identified as one of the most\nisolated galaxies in the Local Supercluster. Optical images of the galaxy\nhowever show that it has a peculiar morphology with an elongated ~ 14 kpc long\nplume. New observations with the Russian 6-m telescope (BTA) and the Giant\nMetrewave Radio Telescope (GMRT) of the ionised and neutral gas in UGC4722\nreveal the second component responsible for the disturbed morphology of the\nsystem. This is a small, almost completely destroyed, very gas-rich dwarf (M_B\n= -15.2, M_HI/L_B ~4.3). We estimate the oxygen abundance for both galaxies to\nbe 12+log(O/H) ~ 7.5-7.6, which is 2-3 times lower than what is expected from\nthe luminosity-metallicity relation for similar galaxies in denser\nenvironments. The ugr colours of the plume derived from Sloan Digital Sky\nSurvey (SDSS) images are consistent with a simple stellar population with a\npost starburst age of 0.45-0.5 Gyr. This system hence appears to be the first\nknown case of a minor merger with a prominent tidal feature consisting of a\nyoung stellar population.\n",
        "  Mappings between related ontologies are increasingly used to support data\nintegration and analysis tasks. Changes in the ontologies also require the\nadaptation of ontology mappings. So far the evolution of ontology mappings has\nreceived little attention albeit ontologies change continuously especially in\nthe life sciences. We therefore analyze how mappings between popular life\nscience ontologies evolve for different match algorithms. We also evaluate\nwhich semantic ontology changes primarily affect the mappings. We further\ninvestigate alternatives to predict or estimate the degree of future mapping\nchanges based on previous ontology and mapping transitions.\n",
        "  As XML becomes ubiquitous and XML storage and processing becomes more\nefficient, the range of use cases for these technologies widens daily. One\npromising area is the integration of XML and data warehouses, where an\nXML-native database stores multidimensional data and processes OLAP queries\nwritten in the XQuery interrogation language. This paper explores issues\narising in the implementation of such a data warehouse. We first compare\napproaches for multidimensional data modelling in XML, then describe how\ntypical OLAP queries on these models can be expressed in XQuery. We then show\nhow, regardless of the model, the grouping features of XQuery 1.1 improve\nperformance and readability of these queries. Finally, we evaluate the\nperformance of query evaluation in each modelling choice using the eXist\ndatabase, which we extended with a grouping clause implementation.\n",
        "  Individuals on social media may reveal themselves to be in various states of\ncrisis (e.g. suicide, self-harm, abuse, or eating disorders). Detecting crisis\nfrom social media text automatically and accurately can have profound\nconsequences. However, detecting a general state of crisis without explaining\nwhy has limited applications. An explanation in this context is a coherent,\nconcise subset of the text that rationalizes the crisis detection. We explore\nseveral methods to detect and explain crisis using a combination of neural and\nnon-neural techniques. We evaluate these techniques on a unique data set\nobtained from Koko, an anonymous emotional support network available through\nvarious messaging applications. We annotate a small subset of the samples\nlabeled with crisis with corresponding explanations. Our best technique\nsignificantly outperforms the baseline for detection and explanation.\n",
        "  We establish a relation between the \"large r\" asymptotics of the Turaev-Viro\ninvariants $TV_r$ and the Gromov norm of 3-manifolds. We show that for any\norientable, compact 3-manifold $M$, with (possibly empty) toroidal boundary,\n$\\log |TV_r (M)|$ is bounded above by a function linear in $r$ and whose slope\nis a positive universal constant times the Gromov norm of $M$. The proof\ncombines TQFT techniques, geometric decomposition theory of 3-manifolds and\nanalytical estimates of $6j$-symbols.\n  We obtain topological criteria that can be used to check whether the growth\nis actually exponential; that is one has $\\log| TV_r (M)|\\geqslant B \\ r$, for\nsome $B>0$. We use these criteria to construct infinite families of hyperbolic\n3-manifolds whose $SO(3)$ Turaev-Viro invariants grow exponentially. These\nconstructions are essential for the results of [DK:AMU] where the authors make\nprogress on a conjecture of Andersen, Masbaum and Ueno about the geometric\nproperties of surface mapping class groups detected by the quantum\nrepresentations.\n  We also study the behavior of the Turaev-Viro invariants under cutting and\ngluing of 3-manifolds along tori. In particular, we show that, like the Gromov\nnorm, the values of the invariants do not increase under Dehn filling and we\ngive applications of this result on the question of the extent to which\nrelations between the invariants $TV_r$ and hyperbolic volume are preserved\nunder Dehn filling.\n  Finally we give constructions of 3-manifolds, both with zero and non-zero\nGromov norm, for which the Turaev-Viro invariants determine the Gromov norm.\n",
        "  In food webs, many interacting species coexist despite the restrictions\nimposed by the competitive exclusion principle and apparent competition. For\nthe generalized Lotka-Volterra equations, sustainable coexistence necessitates\nnonzero determinant of the interaction matrix. Here we show that this\nrequirement is equivalent to demanding that each species be part of a\nnon-overlapping pairing, which substantially constrains the food web structure.\nWe demonstrate that a stable food web can always be obtained if a\nnon-overlapping pairing exists. If it does not, the matrix rank can be used to\nquantify the lack of niches, corresponding to unpaired species. For the species\nrichness at each trophic level, we derive the food web assembly rules, which\nspecify sustainable combinations. In neighboring levels, these rules allow the\nhigher level to avert competitive exclusion at the lower, thereby incorporating\napparent competition. In agreement with data, the assembly rules predict high\nspecies numbers at intermediate levels and thinning at the top and bottom.\nUsing comprehensive food web data, we demonstrate how omnivores or parasites\nwith hosts at multiple trophic levels can loosen the constraints and help\nobtain coexistence in food webs. Hence, omnivory may be the glue that keeps\ncommunities intact even under extinction or ecological release of species.\n",
        "  A common problem in phylogenetics is to try to infer a species phylogeny from\ngene trees. We consider different variants of this problem. The first variant,\ncalled Unrestricted Minimal Episodes Inference, aims at inferring a species\ntree based on a model with speciation and duplication where duplications are\nclustered in duplication episodes. The goal is to minimize the number of such\nepisodes. The second variant, Parental Hybridization, aims at inferring a\nspecies \\emph{network} based on a model with speciation and reticulation. The\ngoal is to minimize the number of reticulation events. It is a variant of the\nwell-studied Hybridization Number problem with a more generous view on which\ngene trees are consistent with a given species network. We show that these\nseemingly different problems are in fact closely related and can, surprisingly,\nboth be solved in polynomial time, using a structure we call \"beaded trees\".\nHowever, we also show that methods based on these problems have to be used with\ncare because the optimal species phylogenies always have a restricted form. To\nmitigate this problem, we introduce a new variant of Unrestricted Minimal\nEpisodes Inference that minimizes the duplication episode depth. We prove that\nthis new variant of the problem can also be solved in polynomial time\n",
        "  An experimental setup for consecutive measurement of ion and x-ray absorption\nin tissue or other materials is introduced. With this setup using a 3D-printed\nsample container, the reference stopping-power ratio (SPR) of materials can be\nmeasured with an uncertainty of below 0.1%. A total of 65 porcine and bovine\ntissue samples were prepared for measurement, comprising five samples each of\n13 tissue types representing about 80% of the total body mass (three different\nmuscle and fatty tissues, liver, kidney, brain, heart, blood, lung and bone).\nUsing a standard stoichiometric calibration for single-energy CT (SECT) as well\nas a state-of-the-art dual-energy CT (DECT) approach, SPR was predicted for all\ntissues and then compared to the measured reference. With the SECT approach,\nthe SPRs of all tissues were predicted with a mean error of (-0.84 $\\pm$ 0.12)%\nand a mean absolute error of (1.27 $\\pm$ 0.12)%. In contrast, the DECT-based\nSPR predictions were overall consistent with the measured reference with a mean\nerror of (-0.02 $\\pm$ 0.15)% and a mean absolute error of (0.10 $\\pm$ 0.15)%.\nThus, in this study, the potential of DECT to decrease range uncertainty could\nbe confirmed in biological tissue.\n",
        "  The distinctive features of the electronic structure of vortex states in\nsuperconducting graphene are studied within the Bogolubov-de Gennes theory\napplied to excitations near the Dirac point. We suggest a scenario describing\nthe subgap spectrum transformation which occurs with a change in the doping\nlevel. For an arbitrary vorticity and doping level we investigate the problem\nof existence of zero energy modes. The crossover to a Caroli - de Gennes -\nMatricon type of spectrum is studied.\n",
        "  Discovering a concise schema from given XML documents is an important problem\nin XML applications. In this paper, we focus on the problem of learning an\nunordered schema from a given set of XML examples, which is actually a problem\nof learning a restricted regular expression with interleaving using positive\nexample strings. Schemas with interleaving could present meaningful knowledge\nthat cannot be disclosed by previous inference techniques. Moreover, inference\nof the minimal schema with interleaving is challenging. The problem of finding\na minimal schema with interleaving is shown to be NP-hard. Therefore, we\ndevelop an approximation algorithm and a heuristic solution to tackle the\nproblem using techniques different from known inference algorithms. We do\nexperiments on real-world data sets to demonstrate the effectiveness of our\napproaches. Our heuristic algorithm is shown to produce results that are very\nclose to optimal.\n",
        "  Superparamagnetic iron-oxide nanoparticles can be used in a variety of\nmedical applications like vascular or targeted imaging. Magnetic particle\nimaging (MPI) is a promising tomographic imaging technique that allows\nvisualizing the 3D nanoparticle distribution concentration in a non-invasive\nmanner. The two main strengths of MPI are high temporal resolution and high\nsensitivity. While the first has been proven in the assessment of dynamic\nprocesses like cardiac imaging, it is unknown how far the detection limit of\nMPI can be lowered. Within this work, we will present a highly sensitive\ngradiometric receive-coil unit combined with a noise-matching network tailored\nfor the measurement of mice. The setup is capable of detecting 5 ng of iron in\nvitro at 2.14 sec acquisition time. In terms of iron concentration we are able\nto detect 156 {\\mu}g/L marking the lowest value that has been reported for an\nMPI scanner so far. In vivo MPI mouse images of a 512 ng bolus at 21.5 ms\nacquisition time allow for capturing the flow of an intravenously injected\ntracer through the heart of a mouse. Since it has been rather difficult to\ncompare detection limits across MPI publications we propose guidelines\nimproving the comparability of future MPI studies.\n",
        "  Evidence of AGN interaction with the IGM is observed in some galaxies and\nmany cool core clusters. Radio jets are suspected to dig large cavities into\nthe surrounding gas. In most cases, very large optical filaments are seen\naround the central galaxy. The origin of these filaments is still not\nunderstood. Star-forming regions are sometimes observed inside the filaments\nand are interpreted as evidence of positive feedback. Cen A is a nearby galaxy\nwith huge optical filaments aligned with the AGN radio-jet direction. We\nsearched for line ratio variations along the filaments, kinematic evidence of\nshock-broadend line widths, and large-scale dynamical structures. We observed a\n1'x1' region around the inner filament of Cen A with MUSE on the VLT during\nScience Verification. The brightest lines detected are the Halpha, [NII],\n[OIII] and [SII]. MUSE shows that the filaments are made of clumpy structures\ninside a more diffuse medium aligned with the radio-jet axis. We find evidence\nof shocked shells surrounding the star-forming clumps from the line profiles,\nsuggesting that the star formation is induced by shocks. The clump line ratios\nare best explained by a composite of shocks and star formation illuminated by a\nradiation cone from the AGN. We also report a previously undetected large\narc-like structure: three streams running perpendicular to the main filament;\nthey are kinematically, morphologically, and excitationally distinct. The clear\ndifference in the excitation of the arcs and clumps suggests that the arcs are\nvery likely located outside of the radiation cone and match the position of the\nfilament only in projection. The three arcs are most consistent with neutral\nmaterial swept along by a backflow of the jet plasma from the AGN outburst that\nis ionised through a diffuse radiation field with a low-ionisation parameter\nthat continues to excite gas away from the radiation cone.\n",
        "  We present a system based on sequential decision making for the online\nsummarization of massive document streams, such as those found on the web.\nGiven an event of interest (e.g. \"Boston marathon bombing\"), our system is able\nto filter the stream for relevance and produce a series of short text updates\ndescribing the event as it unfolds over time. Unlike previous work, our\napproach is able to jointly model the relevance, comprehensiveness, novelty,\nand timeliness required by time-sensitive queries. We demonstrate a 28.3%\nimprovement in summary F1 and a 43.8% improvement in time-sensitive F1 metrics.\n",
        "  The superconductivity of the 4-angstrom single-walled carbon nanotubes\n(SWCNTs) was discovered more than a decade ago, and marked the breakthrough of\nfinding superconductivity in pure elemental undoped carbon compounds. The van\nHove singularities in the electronic density of states at the Fermi level in\ncombination with a large Debye temperature of the SWCNTs are expected to cause\nan impressively large superconducting gap. We have developed an innovative\ncomputational algorithm specially tailored for the investigation of\nsuperconductivity in ultrathin SWCNTs. We predict the superconducting\ntransition temperature of various thin carbon nanotubes resulting from\nelectron-phonon coupling by an ab-initio method, taking into account the effect\nof radial pressure, symmetry, chirality (N,M) and bond lengths. By optimizing\nthe geometry of the carbon nanotubes, a maximum Tc of 60K is found. We also use\nour method to calculate the Tc of a linear carbon chain embedded in the center\nof (5,0) SWCNTs. The strong curvature in the (5,0) carbon nanotubes in the\npresence of the inner carbon chain provides an alternative path to increase the\nTc of this carbon composite by a factor of 2.2 with respect to the empty (5,0)\nSWCNTs.\n",
        "  In 1983, Conway-Gordon showed that for every spatial complete graph on 6\nvertices, the sum of the linking numbers over all of the constituent\n2-component links is congruent to 1 modulo 2, and for every spatial complete\ngraph on 7 vertices, the sum of the Arf invariants over all of the Hamiltonian\nknots is also congruent to 1 modulo 2. In this article, we give integral lifts\nof the Conway-Gordon theorems above in terms of the square of the linking\nnumber and the second coefficient of the Conway polynomial. As applications, we\ngive alternative topological proofs of theorems of Brown-Ramirez Alfonsin and\nHuh-Jeon for rectilinear spatial complete graphs which were proved by\ncomputational and combinatorial methods.\n",
        "  Low dose, high contrast x-ray imaging is of general interest in medical\ndiagnostic applications. X-ray Mach-Zehnder interferometers using collimated\nsynchrotron beams demonstrate the highest levels of phase contrast under a\ngiven exposure dose. However, common x-ray sources emit divergent cone beams.\nHere, we developed a spherical-wave inline Mach-Zehnder interferometer for\nphase contrast imaging over an extended area with a broadband and divergent\nsource. The first tabletop system was tested in imaging experiments of a\nmammographic accreditation phantom and various biological specimens. The noise\nlevel of the phase contrast images at a clinical radiation dose corresponded to\na 6 nano radian bending of the x-ray wavefront. Un-resolved structures with\nconventional radiography and near-field interferometer techniques became\nvisible at a fraction of the radiation dose.\n",
        "  Multidimensional data are becoming more prevalent, partly due to the rise of\nthe Internet of Things (IoT), and with that the need to ingest and analyze data\nstreams at rates higher than before. Some industrial IoT applications require\ningesting millions of records per second, while processing queries on recently\ningested and historical data. Unfortunately, existing database systems suited\nto multidimensional data exhibit low per-node ingestion performance, and even\nif they can scale horizontally in distributed settings, they require large\nnumber of nodes to meet such ingest demands. For this reason, in this paper we\nevaluate a single-node multidimensional data store for high-velocity sensor\ndata. Its design centers around a two-level indexing structure, wherein the\nglobal index is an in-memory R*-tree and the local indices are serialized\nkd-trees. This study is confined to records with numerical indexing fields and\nrange queries, and covers ingest throughput, query response time, and storage\nfootprint. We show that the adopted design streamlines data ingestion and\noffers ingress rates two orders of magnitude higher than those of Percona\nServer, SQLite, and Druid. Our prototype also reports query response times\ncomparable to or better than those of Percona Server and Druid, and compares\nfavorably in terms of storage footprint. In addition, we evaluate a kd-tree\npartitioning based scheme for grouping incoming streamed data records. Compared\nto a random scheme, this scheme produces less overlap between groups of\nstreamed records, but contrary to what we expected, such reduced overlap does\nnot translate into better query performance. By contrast, the local indices\nprove much more beneficial to query performance. We believe the experience\nreported in this paper is valuable to practitioners and researchers alike\ninterested in building database systems for high-velocity multidimensional\ndata.\n",
        "  We consider the Moran process in a graph called the \"star\" and obtain the\nasymptotic expression for the fixation probability of a single mutant when the\nsize of the graph is large. The expression obtained corrects the previously\nknown expression announced in reference [E Lieberman, C Hauert, and MA Nowak.\nEvolutionary dynamics on graphs. Nature, 433(7023):312-316, 2005] and further\nstudied in [M. Broom and J. Rychtar. An analysis of the fixation probability of\na mutant on special classes of non-directed graphs. Proc. R. Soc. A-Math. Phys.\nEng. Sci., 464(2098):2609-2627, 2008]. We also show that the star graph is an\naccelerator of evolution, if the graph is large enough.\n",
        "  Detections of molecular lines, mainly from H2$ and CO, reveal molecular\nmaterial in planetary nebulae. Observations of a variety of molecules suggest\nthat the molecular composition in these objects differs from that found in\ninterstellar clouds or in circumstellar envelopes. The success of the models,\nwhich are mostly devoted to explain molecular densities in specific planetary\nnebulae, is still partial, however. The present study aims at identifying the\ninfluence of stellar and nebular properties on the molecular composition of\nplanetary nebulae by means of chemical models. A comparison of theoretical\nresults with those derived from the observations may provide clues to the\nconditions that favor the presence of a particular molecule. A self-consistent\nphotoionization numerical code was adapted to simulate cold molecular regions\nbeyond the ionized zone. The code was used to obtain a grid of models and the\nresulting column densities are compared with those inferred from observations.\nOur models show that the inclusion of an incident flux of X-rays is required to\nexplain the molecular composition derived for planetary nebulae. We also obtain\na more accurate relation for the N(CO)/N(H2) ratio in these objects. Molecular\nmasses obtained by previous works in the literature were then recalculated,\nshowing that these masses can be underestimated by up to three orders of\nmagnitude. We conclude that the problem of the missing mass in planetary\nnebulae can be solved by a more accurate calculation of the molecular mass.\n",
        "  Respiration-correlated CBCT, commonly called 4DCBCT, provide respiratory\nphase-resolved CBCT images. In many clinical applications, it is more\npreferable to reconstruct true 4DCBCT with the 4th dimension being time, i.e.,\neach CBCT image is reconstructed based on the corresponding instantaneous\nprojection. We propose in this work a novel algorithm for the reconstruction of\nthis truly time-resolved CBCT, called cine-CBCT, by effectively utilizing the\nunderlying temporal coherence, such as periodicity or repetition, in those\ncine-CBCT images. Assuming each column of the matrix $\\bm{U}$ represents a CBCT\nimage to be reconstructed and the total number of columns is the same as the\nnumber of projections, the central idea of our algorithm is that the rank of\n$\\bm{U}$ is much smaller than the number of projections and we can use a matrix\nfactorization form $\\bm{U}=\\bm{L}\\bm{R}$ for $\\bm{U}$. The number of columns\nfor the matrix $\\bm{L}$ constraints the rank of $\\bm{U}$ and hence implicitly\nimposing a temporal coherence condition among all the images in cine-CBCT. The\ndesired image properties in $\\bm{L}$ and the periodicity of the breathing\npattern are achieved by penalizing the sparsity of the tight wavelet frame\ntransform of $\\bm{L}$ and that of the Fourier transform of $\\bm{R}$,\nrespectively. A split Bregman method is used to solve the problem. In this\npaper we focus on presenting this new algorithm and showing the proof of\nprinciple using simulation studies on an NCAT phantom.\n",
        "  This paper is an introduction to virtual knot theory and an exposition of new\nideas and constructions, including the parity bracket polynomial, the arrow\npolynomial, the parity arrow polynomial and categorifications of the arrow\npolynomial. The paper is relatively self-contained and it describes virtual\nknot theory both combinatorially and in terms of the knot theory in thickened\nsurfaces. The arrow polynomial (of Dye and Kauffman) is a natural\ngeneralization of the Jones polynomial, obtained by using the oriented\nstructure of diagrams in the state sum. The paper discusses uses of parity\npioneered by Vassily Manturov and uses his parity bracket polynomial to give a\ncounterexample to a conjecture of Fenn, Kauffman and Manturov. The paper gives\nan exposition of the categorification of the arrow polynomial due to Dye,\nKauffman and Manturov and it gives one example (from many found by Aaron\nKaestner) of a pair of virtual knots that are not distinguished by Khovanov\nhomology (mod 2), or by the arrow polynomial, but are distinguished by a\ncategorification of the arrow polynomial. Other examples of parity calculations\nare indicated. For example, this same pair is distinguished by the parity\nbracket polynomial.\n",
        "  We investigate turbulent gas motions in spiral galaxies and their importance\nto star formation in far outer disks, where the column density is typically far\nbelow the critical value for spontaneous gravitational collapse. Following the\nmethods of Burkhart et al. (2010) on the Small Magellanic Cloud, we use the\nthird and fourth statistical moments, as indicators of structures caused by\nturbulence, to examine the neutral hydrogen (HI) column density of a sample of\nspiral galaxies selected from The HI Nearby Galaxy Survey (THINGS, Walter et\nal. 2008). We apply the statistical moments in three different methods- the\ngalaxy as a whole, divided into a function of radii and then into grids. We\ncreate individual grid maps of kurtosis for each galaxy. To investigate the\nrelation between these moments and star formation, we compare these maps with\ntheir far-ultraviolet images taken by the Galaxy Evolution Explorer (GALEX)\nsatellite. We find that the moments are largely uniform across the galaxies, in\nwhich the variation does not appear to trace any star forming regions. This\nmay, however, be due to the spatial resolution of our analysis, which could\npotentially limit the scale of turbulent motions that we are sensitive to\ngreater than ~700 pc. From comparison between the moments themselves, we find\nthat the gas motions in our sampled galaxies are largely supersonic. This\nanalysis also shows that Burkhart et al. (2010)'s methods may be applied not\njust to dwarf galaxies but also to normal spiral galaxies.\n",
        "  We study the influence of dissipation on the switching current statistics of\nmoderately damped Josephson junctions. Different types of both low- and high-\n$T_c$ junctions with controlled damping are studied. The damping parameter of\nthe junctions is tuned in a wide range by changing temperature, magnetic field,\ngate voltage, introducing a ferromagnetic layer or in-situ capacitive shunting.\nA paradoxical collapse of switching current fluctuations occurs with increasing\n$T$ in all studied junctions. The phenomenon critically depends on dissipation\nin the junction and is explained by interplay of two counteracting consequences\nof thermal fluctuations, which on the one hand assist in premature switching\ninto the resistive state and on the other hand help in retrapping back to the\nsuperconducting state. This is one of the rare examples of anticorrelation\nbetween temperature and fluctuation amplitude of a physically measurable\nquantity.\n",
        "  We investigate the kinematics of stars in the mid-plane of the Milky Way on\nscales between 25 pc and 10 kpc with data from the Apache Point Observatory\nGalactic Evolution Experiment (APOGEE), the Radial Velocity Experiment (RAVE),\nand the Geneva-Copenhagen Survey (GCS). Using red-clump stars in APOGEE, we\ndetermine the large-scale line-of-sight velocity field out to 5 kpc from the\nSun in (0.75 kpc)^2 bins. The solar motion V_{sun-c} with respect to the\ncircular velocity V_c is the largest contribution to the power on large scales\nafter subtracting an axisymmetric rotation field; we determine the solar motion\nby minimizing the large-scale power to be V_{sun-c} = 24+/-1 (ran.)+/-2 (syst\n[V_c])+/-5 (syst. [large-scale]) km/s, where the systematic uncertainty is due\nto (a) a conservative 20 km/s uncertainty in V_c and (b) the estimated power on\nunobserved larger scales. Combining the APOGEE peculiar-velocity field with\nred-clump stars in RAVE out to 2 kpc from the Sun and with local GCS stars, we\ndetermine the power spectrum of residual velocity fluctuations in the Milky\nWay's disk on scales between 0.2/kpc < k < 40/kpc. Most of the power is\ncontained in a broad peak between 0.2/kpc < k < 0.9/kpc. We investigate the\nexpected power spectrum for various non-axisymmetric perturbations and\ndemonstrate that the central bar with commonly used parameters but of\nrelatively high mass can explain the bulk of velocity fluctuations in the plane\nof the Galactic disk near the Sun. Streaming motions ~10 km/s on >~3 kpc scales\nin the Milky Way are in good agreement with observations of external galaxies\nand directly explain why local determinations of the solar motion are\ninconsistent with global measurements.\n",
        "  We present 12CO J = 2-1 line observations of G54.1+0.3, a composite supernova\nremnant with a mid-infrared (MIR) loop surrounding the central pulsar wind\nnebula (PWN). We mapped an area of 12' x 9' around the PWN and its associated\nMIR loop. We confirm two velocity components that had been proposed to be\npossibly interacting with the PWN/MIR-loop; the +53 km/s cloud that appears in\ncontact with the eastern boundary of the PWN and the +23 km/s cloud that has CO\nemission coincident with the MIR loop. We have not found a direct evidence for\nthe interaction in either of these clouds. Instead, we detected an 5'-long\narc-like cloud at +15-+23 km/s with a systematic velocity gradient of ~3\nkm/s/arcmin and broad-line emitting CO gas having widths (FWHM) of <7 km/s in\nthe western interior of the supernova remnant. We discuss their association\nwith the supernova remnant.\n",
        "  (Abridged) We combine deep HST grism spectroscopy with a new Bayesian method\nto derive maps of gas-phase metallicity, nebular dust extinction, and\nstar-formation rate for 10 star-forming galaxies at high redshift\n($1.2<z<2.3$). Exploiting lensing magnification by the foreground cluster\nMACS1149.6+2223, we reach sub-kpc spatial resolution and push the stellar mass\nlimit associated with such high-z spatially resolved measurements below\n$10^8M_\\odot$ for the first time. Our maps exhibit diverse morphologies,\nindicative of various effects such as efficient radial mixing from tidal\ntorques, rapid accretion of low-metallicity gas, etc., which can affect the gas\nand metallicity distributions in individual galaxies. Based upon an exhaustive\nsample of all existing sub-kpc metallicity gradients at high-z, we find that\npredictions given by analytical chemical evolution models assuming a relatively\nextended star-formation profile in the early disk formation phase can explain\nthe majority of observed gradients, without involving galactic feedback or\nradial outflows. We observe a tentative correlation between stellar mass and\nmetallicity gradient, consistent with the downsizing galaxy formation picture\nthat more massive galaxies are more evolved into a later phase of disk growth,\nwhere they experience more coherent mass assembly at all radii and thus show\nshallower metallicity gradients. In addition, we compile a sample of\nhomogeneously cross-calibrated integrated metallicity measurements spanning\nthree orders of magnitude in stellar mass at $z\\sim1.8$. We use this sample to\nstudy the mass-metallicity relation (MZR) and test the fundamental metallicity\nrelation (FMR). The slope of the observed MZR can rule out the momentum-driven\nwind model at 3-$\\sigma$ confidence level. We find no significant offset with\nrespect to the FMR, taking into account the intrinsic scatter and measurement\nuncertainties.\n",
        "  Motivated by recent studies on ferroelectric-like order coexisting with\nmetallicity, we investigate ferroelectric (FE) superconductivity in which a\nFE-like structural phase transition occurs in the superconducting state. We\nconsider a two-dimensional s-wave superconductor with Rashba-type antisymmetric\nspin-orbit coupling (ASOC). Assuming linear relationship between polar lattice\ndisplacement and strength of the ASOC, we treat the Rashba-type ASOC as a\nmolecular field of FE-like order. It is shown that the FE-like order is induced\nby the magnetic field when the system is superconducting. Furthermore, we\nclarify the FE superconductivity in a low carrier density regime, which was\nrecently discovered in doped SrTiO$_3$. It is demonstrated that the FE\nsuperconducting state can be stable in this regime in the absence of the\nmagnetic field. Our results open a way to control the electric polarization by\nsuperconductivity, that is, superconducting multiferroics.\n",
        "  It is known by the author that there exist 20 families of Dehn surgeries in\nthe Poincar\\'e homology sphere yielding lens spaces. In this paper, we give the\nconcrete knot diagrams of the families and extend them to families of lens\nspace surgeries in Brieskorn homology spheres. We illustrate families of lens\nspace surgeries in $\\Sigma(2,3,6n\\pm1)$ and $\\Sigma(2,2s+1,2(2s+1)n\\pm1)$ and\nso on. As other examples, we give lens space surgeries in graph homology\nspheres, which are obtained by splicing two Brieskorn homology spheres.\n",
        "  This paper investigates the framework of encoder-decoder with attention for\nsequence labelling based spoken language understanding. We introduce\nBidirectional Long Short Term Memory - Long Short Term Memory networks\n(BLSTM-LSTM) as the encoder-decoder model to fully utilize the power of deep\nlearning. In the sequence labelling task, the input and output sequences are\naligned word by word, while the attention mechanism cannot provide the exact\nalignment. To address this limitation, we propose a novel focus mechanism for\nencoder-decoder framework. Experiments on the standard ATIS dataset showed that\nBLSTM-LSTM with focus mechanism defined the new state-of-the-art by\noutperforming standard BLSTM and attention based encoder-decoder. Further\nexperiments also show that the proposed model is more robust to speech\nrecognition errors.\n",
        "  Data collected nowadays by social-networking applications create fascinating\nopportunities for building novel services, as well as expanding our\nunderstanding about social structures and their dynamics. Unfortunately,\npublishing social-network graphs is considered an ill-advised practice due to\nprivacy concerns. To alleviate this problem, several anonymization methods have\nbeen proposed, aiming at reducing the risk of a privacy breach on the published\ndata, while still allowing to analyze them and draw relevant conclusions. In\nthis paper we introduce a new anonymization approach that is based on injecting\nuncertainty in social graphs and publishing the resulting uncertain graphs.\nWhile existing approaches obfuscate graph data by adding or removing edges\nentirely, we propose using a finer-grained perturbation that adds or removes\nedges partially: this way we can achieve the same desired level of obfuscation\nwith smaller changes in the data, thus maintaining higher utility. Our\nexperiments on real-world networks confirm that at the same level of identity\nobfuscation our method provides higher usefulness than existing randomized\nmethods that publish standard graphs.\n",
        "  We develop a tool called PipeGen for efficient data transfer between database\nmanagement systems (DBMSs). PipeGen targets data analytics workloads on\nshared-nothing engines. It supports scenarios where users seek to perform\ndifferent parts of an analysis in different DBMSs or want to combine and\nanalyze data stored in different systems. The systems may be colocated in the\nsame cluster or may be in different clusters. To achieve high performance,\nPipeGen leverages the ability of all DBMSs to export, possibly in parallel,\ndata into a common data format, such as CSV or JSON. It automatically extends\nthese import and export functions with efficient binary data transfer\ncapabilities that avoid materializing the transmitted data on the file system.\nWe implement a prototype of PipeGen and evaluate it by automatically generating\ndata pipes between five different DBMSs. Our experiments show that PipeGen\ndelivers speedups up to 3.8x compared with manually exporting and importing\ndata across systems using CSV.\n",
        "  Despite a low global prevalence, infections with Sarcoptes scabiei, or\nscabies, are still common in remote communities such as in northern Australia\nand the Solomon Islands. Mass drug administration (MDA) has been utilised in\nthese communities, and although prevalence drops substantially initially, these\nreductions have not been sustained. We develop a compartmental model of scabies\ninfection dynamics and incorporate both ovicidal and non-ovicidal treatment\nregimes. By including the dynamics of mass drug administration, we are able to\nreproduce the phenomena of an initial reduction in prevalence, followed by the\nrecrudescence of infection levels in the population. We show that even under a\n`perfect' two-round MDA, eradication of scabies under a non-ovicidal treatment\nscheme is almost impossible. We then go on to consider how the probability of\nelimination varies with the number of treatment rounds delivered in an MDA. We\nfind that even with infeasibly large numbers of treatment rounds, elimination\nremains challenging.\n",
        "  Nowadays, many web databases \"hidden\" behind their restrictive search\ninterfaces (e.g., Amazon, eBay) contain rich and valuable information that is\nof significant interests to various third parties. Recent studies have\ndemonstrated the possibility of estimating/tracking certain aggregate queries\nover dynamic hidden web databases. Nonetheless, tracking all possible aggregate\nquery answers to report interesting findings (i.e., exceptions), while still\nadhering to the stringent query-count limitations enforced by many hidden web\ndatabases providers, is very challenging. In this paper, we develop a novel\ntechnique for tracking and discovering exceptions (in terms of sudden changes\nof aggregates) over dynamic hidden web databases. Extensive real-world\nexperiments demonstrate the superiority of our proposed algorithms over\nbaseline solutions.\n",
        "  The Yule process generates a class of binary trees which is fundamental to\npopulation genetic models and other applications in evolutionary biology. In\nthis paper, we introduce a family of sub-classes of ranked trees, called\nOmega-trees, which are characterized by imbalance of internal nodes. The degree\nof imbalance is defined by an integer 0 <= w. For caterpillars, the extreme\ncase of unbalanced trees, w = 0. Under models of neutral evolution, for\ninstance the Yule model, trees with small w are unlikely to occur by chance.\nIndeed, imbalance can be a signature of permanent selection pressure, such as\nobservable in the genealogies of certain pathogens. From a mathematical point\nof view it is interesting to observe that the space of Omega-trees maintains\nseveral statistical invariants although it is drastically reduced in size\ncompared to the space of unconstrained Yule trees. Using generating functions,\nwe study here some basic combinatorial properties of Omega-trees. We focus on\nthe distribution of the number of subtrees with two leaves. We show that\nexpectation and variance of this distribution match those for unconstrained\ntrees already for very small values of w.\n",
        "  The work we present here addresses cue-based noun classification in English\nand Spanish. Its main objective is to automatically acquire lexical semantic\ninformation by classifying nouns into previously known noun lexical classes.\nThis is achieved by using particular aspects of linguistic contexts as cues\nthat identify a specific lexical class. Here we concentrate on the task of\nidentifying such cues and the theoretical background that allows for an\nassessment of the complexity of the task. The results show that, despite of the\na-priori complexity of the task, cue-based classification is a useful tool in\nthe automatic acquisition of lexical semantic classes.\n",
        "  Storing XML documents in a relational database is a promising solution\nbecause relational databases are mature and scale very well and they have the\nadvantages that in a relational database XML data and structured data can\ncoexist making it possible to build application that involve both kinds of data\nwith little extra effort . In this paper, we propose an algorithm schema named\nXRecursive that translates XML documents to relational database according to\nthe proposed storing structure. The steps and algorithm are given in details to\ndescribe how to use the storing structure to storage and query XML documents in\nrelational database. Then we report our experimental results on a real database\nto show the performance of our method in some features.\n",
        "  It has been conjectured that the algebraic crossing number of a link is\nuniquely determined in minimal braid representation. This conjecture is true\nfor many classes of knots and links.\n  The Morton-Franks-Williams inequality gives a lower bound for braid index.\nAnd sharpness of the inequality on a knot type implies the truth of the\nconjecture for the knot type.\n  We prove that there are infinitely many examples of knots and links for which\nthe inequality is not sharp but the conjecture is still true. We also show that\nif the conjecture is true for K and L, then it is also true for the (p,q)-cable\nof K and for the connect sum of K and L.\n",
        "  Traditional association rule mining based on the support-confidence framework\nprovides the objective measure of the rules that are of interest to users.\nHowever, it does not reflect the utility of the rules. To extract non-redundant\nassociation rules in support-confidence framework frequent closed itemsets and\ntheir generators play an important role. To extract non-redundant association\nrules among high utility itemsets, high utility closed itemsets (HUCI) and\ntheir generators should be extracted in order to apply traditional\nsupport-confidence framework. However, no efficient method exists at present\nfor mining HUCIs with their generators. This paper addresses this issue. A\npost-processing algorithm, called the HUCI-Miner, is proposed to mine HUCIs\nwith their generators. The proposed algorithm is implemented using both\nsynthetic and real datasets.\n",
        "  Although discovered many decades ago, superconductivity in doped SrTiO$_{3}$\nremains a topic of intense research. Recent experiments revealed that, upon\nincreasing the carrier concentration, multiple bands cross the Fermi level,\nsignaling the onset of Lifshitz transitions. Interestingly, $T_{c}$ was\nobserved to be suppressed across the Lifshitz transition of oxygen-deficient\nSrTiO$_{3}$; a similar behavior was also observed in gated\nLaAlO$_{3}$/SrTiO$_{3}$ interfaces. Such a behavior is difficult to explain in\nthe clean theory of two-band superconductivity, as the additional electronic\nstates provided by the second band should enhance $T_{c}$. Here, we show that\nthis unexpected behavior can be explained by the strong pair-breaking effect\npromoted by disorder, which takes place if the inter-band pairing interaction\nis subleading and repulsive. A consequence of this scenario is that, upon\nmoving away from the Lifshitz transition, the two-band superconducting state\nchanges from opposite-sign gaps to same-sign gaps.\n",
        "  Crowdsourcing is becoming increasingly important in entity resolution tasks\ndue to their inherent complexity such as clustering of images and natural\nlanguage processing. Humans can provide more insightful information for these\ndifficult problems compared to machine-based automatic techniques.\nNevertheless, human workers can make mistakes due to lack of domain expertise\nor seriousness, ambiguity, or even due to malicious intents. The\nstate-of-the-art literature usually deals with human errors via majority voting\nor by assigning a universal error rate over crowd workers. However, such\napproaches are incomplete, and often inconsistent, because the expertise of\ncrowd workers are diverse with possible biases, thereby making it largely\ninappropriate to assume a universal error rate for all workers over all\ncrowdsourcing tasks.\n  To this end, we mitigate the above challenges by considering an uncertain\ngraph model, where the edge probability between two records A and B denotes the\nratio of crowd workers who voted Yes on the question if A and B are same\nentity. In order to reflect independence across different crowdsourcing tasks,\nwe apply the well-established notion of possible worlds, and develop\nparameter-free algorithms both for next crowdsourcing, as well as for entity\nresolution problems. In particular, using our framework, the problem of entity\nresolution becomes equivalent to finding the maximum-likelihood clustering;\nwhereas for the next crowdsourcing, we identify the record pair that maximally\nincreases the reliability of the maximum-likelihood clustering. Based on\ndetailed empirical analysis over real-world datasets, we find that our proposed\nsolution, PERC (probabilistic entity resolution with imperfect crowd) improves\nthe quality by 15% and reduces the overall cost by 50% for the\ncrowdsourcing-based entity resolution problem.\n",
        "  For families of knots and links given in Conway notation we compute lower\nmaximal and upper minimal bound of hyperbolic volume by using source links and\naugmented links.\n",
        "  We present high pressure diamond anvil cell synchrotron X-ray, resistivity,\nand ac-susceptibility measurements on electron-doped cuprate\nPr$_{2-x}$Ce$_{x}$CuO$_{4}$ to much higher pressures than previously reported.\nAt 2.72 GPa between 88 and 98$%$ of the superconducting T$^\\prime$ phase\n\\cite{Tprime} of the optimally doped Pr$_{1.85}$Ce$_{0.15}$CuO$_{4}$ transforms\ninto the insulating phase T. The T$_{c}$ of the remaining 2-12$%$ T$^\\prime$\nphase is suppressed continuously from 22 K to 18.5 K at about 14 GPa.\nRemarkably, the T$_{c}$ of the over doped Pr$_{1.83}$Ce$_{0.17}$CuO$_{4}$\nremains practically unchanged even at 32 GPa. This behavior of the\nelectron-doped cuprate contrasts with that of the hole-doped cuprate for which\nT$_{c}$ is first substantially enhanced with applied pressure.\n",
        "  We present the detection of very extended stellar populations around the\nLarge Magellanic Cloud (LMC) out to R~21 degrees, or ~18.5 kpc at the LMC\ndistance of 50 kpc, as detected in the Survey of the MAgellanic Stellar History\n(SMASH) performed with the Dark Energy Camera on the NOAO Blanco 4m Telescope.\nThe deep (g~24) SMASH color magnitude diagrams (CMDs) clearly reveal old (~9\nGyr), metal-poor ([Fe/H]=-0.8 dex) main-sequence stars at a distance of 50 kpc.\nThe surface brightness of these detections is extremely low with our most\ndistant detection having 34 mag per arcsec squared in g-band. The SMASH radial\ndensity profile breaks from the inner LMC exponential decline at ~13-15 degrees\nand a second component at larger radii has a shallower slope with power-law\nindex of -2.2 that contributes ~0.4% of the LMC's total stellar mass. In\naddition, the SMASH densities exhibit large scatter around our best-fit model\nof ~70% indicating that the envelope of stellar material in the LMC periphery\nis highly disturbed. We also use data from the NOAO Source catalog to map the\nLMC main-sequence populations at intermediate radii and detect a steep dropoff\nin density on the eastern side of the LMC (at R~8 deg) as well as an extended\nstructure to the far northeast. These combined results confirm the existence of\na very extended, low-density envelope of stellar material with disturbed shape\naround the LMC. The exact origin of this structure remains unclear but the\nleading options include a classical accreted halo or tidally stripped outer\ndisk material.\n",
        "  A crucial step in the surgery-theoretic program to classify smooth manifolds\nis that of representing a middle--dimensional homology class by a smoothly\nembedded sphere. This step fails even for the simple 4-manifolds obtained from\nthe 4-ball by adding a 2-handle with framing r along some knot K in S^3. An\nr-shake slice knot is one for which a generator of the second homology of this\n4-manifold can be represented by a smoothly embedded 2-sphere. It is not known\nwhether there exist 0-shake slice knots that are not slice. We define a\nrelative notion of shake sliceness of knots, which we call shake concordance,\nwhich is easily seen to be a generalization of classical concordance, and we\ngive the first examples of knots that are 0-shake concordant but not\nconcordant; these may be chosen to be topologically slice. Additionally, for\neach r we completely characterize r-shake slice and r-shake concordant knots in\nterms of concordance and satellite operators. Our characterization allows us to\nconstruct new families of possible r-shake slice knots that are not slice.\n",
        "  In non-centrosymmetric superconductors, where the crystal structure lacks a\ncentre of inversion, parity is no longer a good quantum number and an\nelectronic antisymmetric spin-orbit coupling (ASOC) is allowed to exist by\nsymmetry. If this ASOC is sufficiently large, it has profound consequences on\nthe superconducting state. For example, it generally leads to a superconducting\npairing state which is a mixture of spin-singlet and spin-triplet components.\nThe possibility of such novel pairing states, as well as the potential for\nobserving a variety of unusual behaviours, led to intensive theoretical and\nexperimental investigations. Here we review the experimental and theoretical\nresults for superconducting systems lacking inversion symmetry. Firstly we give\na conceptual overview of the key theoretical results. We then review the\nexperimental properties of both strongly and weakly correlated bulk materials,\nas well as two dimensional systems. Here the focus is on evaluating the effect\nof ASOC on the superconducting properties and the extent to which there is\nevidence for singlet-triplet mixing. This is followed by a more detailed\noverview of theoretical aspects of non-centrosymmetric superconductivity. This\nincludes the effects of the ASOC on the pairing symmetry and the\nsuperconducting magnetic response, magneto-electric effects, superconducting\nfinite momentum pairing states, and the potential of non-centrosymmetric\nsuperconductors to display topological superconductivity.\n",
        "  Understanding the mechanisms governing population extinctions is of key\nimportance to many problems in ecology and evolution. Stochastic factors are\nknown to play a central role in extinction, but the interactions between a\npopulation's demographic stochasticity and environmental noise remain poorly\nunderstood. Here, we model environmental forcing as a stochastic fluctuation\nbetween two states, one with a higher death rate than the other. We find that\nin general there exists a rate of fluctuations that minimizes the mean time to\nextinction, a phenomenon previously dubbed \"resonant activation.\" We develop a\nheuristic description of the phenomenon, together with a criterion for the\nexistence of resonant activation. Specifically the minimum extinction time\narises as a result of the system approaching a scenario wherein the severity of\nrare events is balanced by the time interval between them. We discuss our\nfindings within the context of more general forms of environmental noise, and\nsuggest potential applications to evolutionary models.\n",
        "  A number of studies carried out on different languages have found that tongue\nmovements in speech are made along two primary degrees of freedom (d.f.s): the\nhigh-front to low-back axis and the high-back to low-front axis. We explore the\nhypothesis that these two main d.f.s could find their origins in the physical\nproperties of the vocal tract. A large set of tongue shapes was generated with\na biomechanical tongue model using a Monte-Carlo method to thoroughly sample\nthe muscle control space. The resulting shapes were analyzed with PCA. The\nfirst two factors explain 84% of the variance, and they are similar to the two\nexperimentally observed d.f.s. This finding suggests that the d.f.s. are not\nspeech-specific, and that speech takes advantage of biomechanically based\ntongue properties to form different sounds.\n",
        "  To support complex data-intensive applications such as personalized\nrecommendations, targeted advertising, and intelligent services, the data\nmanagement community has focused heavily on the design of systems to support\ntraining complex models on large datasets. Unfortunately, the design of these\nsystems largely ignores a critical component of the overall analytics process:\nthe deployment and serving of models at scale. In this work, we present Velox,\na new component of the Berkeley Data Analytics Stack. Velox is a data\nmanagement system for facilitating the next steps in real-world, large-scale\nanalytics pipelines: online model management, maintenance, and serving. Velox\nprovides end-user applications and services with a low-latency, intuitive\ninterface to models, transforming the raw statistical models currently trained\nusing existing offline large-scale compute frameworks into full-blown,\nend-to-end data products capable of recommending products, targeting\nadvertisements, and personalizing web content. To provide up-to-date results\nfor these complex models, Velox also facilitates lightweight online model\nmaintenance and selection (i.e., dynamic weighting). In this paper, we describe\nthe challenges and architectural considerations required to achieve this\nfunctionality, including the abilities to span online and offline systems, to\nadaptively adjust model materialization strategies, and to exploit inherent\nstatistical properties such as model error tolerance, all while operating at\n\"Big Data\" scale.\n",
        "  The stationary distribution of a sample taken from a Wright-Fisher diffusion\nwith general small mutation rates is found using a coalescent approach. The\napproximation is equivalent to having at most one mutation in the coalescent\ntree to the first order in the rates. The sample probabilities characterize an\napproximation for the stationary distribution from the Wright-Fisher diffusion.\nThe approach is different from Burden and Tang (2016,2017) who use a\nprobability flux argument to obtain the same results from a forward diffusion\ngenerator equation. The solution has interest because the solution is not known\nwhen rates are not small. An analogous solution is found for the configuration\nof alleles in a general exchangeable binary coalescent tree. In particular an\nexplicit solution is found for a pure birth process tree when individuals\nreproduce at rate lambda.\n",
        "  Regulations in the Building Industry are becoming increasingly complex and\ninvolve more than one technical area. They cover products, components and\nproject implementation. They also play an important role to ensure the quality\nof a building, and to minimize its environmental impact. In this paper, we are\nparticularly interested in the modeling of the regulatory constraints derived\nfrom the Technical Guides issued by CSTB and used to validate Technical\nAssessments. We first describe our approach for modeling regulatory constraints\nin the SBVR language, and formalizing them in the SPARQL language. Second, we\ndescribe how we model the processes of compliance checking described in the\nCSTB Technical Guides. Third, we show how we implement these processes to\nassist industrials in drafting Technical Documents in order to acquire a\nTechnical Assessment; a compliance report is automatically generated to explain\nthe compliance or noncompliance of this Technical Documents.\n",
        "  Understanding patterns of selectively neutral genetic variation is essential\nin order to model deviations from neutrality, caused for example by different\nforms of selection. Best understood is neutral genetic variation at a single\nlocus, but additional insights can be gained by investigating genetic variation\nat multiple loci. The corresponding patterns of variation reflect linkage\ndisequilibrium and provide information about the underlying multi-locus gene\ngenealogies. The statistical properties of two-locus genealogies have been\nintensively studied for populations of constant census size, as well as for\nsimple demographic histories such as exponential population growth, and single\nbottlenecks. By contrast, the combined effect of recombination and sustained\ndemographic fluctuations is poorly understood. Addressing this issue, we study\na two-locus Wright-Fisher model of a population subject to recurrent\nbottlenecks. We derive coalescent approximations for the covariance of the\ntimes to the most recent common ancestor at two loci. We find, first, that an\neffective population-size approximation describes the numerically observed\nlinkage disequilibrium provided that recombination occurs either much faster or\nmuch more slowly than the population size changes. Second, when recombination\noccurs frequently between bottlenecks but rarely within bottlenecks, we observe\nlong-range linkage disequilibrium. Third, we show that in the latter case, a\ncommonly used measure of linkage disequilibrium, sigma_d^2 (closely related to\nr^2), fails to capture long-range linkage disequilibrium because constituent\nterms, each reflecting long-range linkage disequilibrium, cancel. Fourth, we\nanalyse a limiting case in which long-range linkage disequilibrium can be\ndescribed in terms of a Xi-coalescent process allowing for simultaneous\nmultiple mergers of ancestral lines.\n",
        "  We prove that there are compact submanifolds of the 3-sphere whose interiors\nare not homeomorphic to any geometric limit of hyperbolic knot complements.\n",
        "  XML is gradually employed as a standard of data exchange in web environment\nsince its inception in the 90s until present. It serves as a data exchange\nbetween systems and other applications. Meanwhile the data volume has grown\nsubstantially in the web and thus effective methods of storing and retrieving\nthese data is essential. One recommended way is physically or virtually\nfragments the large chunk of data and distributes the fragments into different\nnodes. Fragmentation design of XML document contains of two parts:\nfragmentation operation and fragmentation method. The three fragmentation\noperations are Horizontal, Vertical and Hybrid. It determines how the XML\nshould be fragmented. This paper aims to give an overview on the fragmentation\ndesign consideration and subsequently, propose a fragmentation technique using\nnumber addressing.\n",
        "  Platforms such as AirBnB, Zillow, Yelp, and related sites have transformed\nthe way we search for accommodation, restaurants, etc. The underlying datasets\nin such applications have numerous attributes that are mostly Boolean or\nCategorical. Discovering the skyline of such datasets over a subset of\nattributes would identify entries that stand out while enabling numerous\napplications. There are only a few algorithms designed to compute the skyline\nover categorical attributes, yet are applicable only when the number of\nattributes is small.\n  In this paper, we place the problem of skyline discovery over categorical\nattributes into perspective and design efficient algorithms for two cases. (i)\nIn the absence of indices, we propose two algorithms, ST-S and ST-P, that\nexploits the categorical characteristics of the datasets, organizing tuples in\na tree data structure, supporting efficient dominance tests over the candidate\nset. (ii) We then consider the existence of widely used precomputed sorted\nlists. After discussing several approaches, and studying their limitations, we\npropose TA-SKY, a novel threshold style algorithm that utilizes sorted lists.\nMoreover, we further optimize TA-SKY and explore its progressive nature, making\nit suitable for applications with strict interactive requirements. In addition\nto the extensive theoretical analysis of the proposed algorithms, we conduct a\ncomprehensive experimental evaluation of the combination of real (including the\nentire AirBnB data collection) and synthetic datasets to study the practicality\nof the proposed algorithms. The results showcase the superior performance of\nour techniques, outperforming applicable approaches by orders of magnitude.\n",
        "  This paper describes XNMT, the eXtensible Neural Machine Translation toolkit.\nXNMT distin- guishes itself from other open-source NMT toolkits by its focus on\nmodular code design, with the purpose of enabling fast iteration in research\nand replicable, reliable results. In this paper we describe the design of XNMT\nand its experiment configuration system, and demonstrate its utility on the\ntasks of machine translation, speech recognition, and multi-tasked machine\ntranslation/parsing. XNMT is available open-source at\nhttps://github.com/neulab/xnmt\n",
        "  Mendez et al. recently report the identification of a Y chromosome lineage\nfrom an African American that is an outgroup to all other known Y haplotypes,\nand report a time to most recent common ancestor, TMRCA, for human Y lineages\nthat is substantially longer than any previous estimate. The identification of\na novel Y haplotype is always exciting, and this haplotype, in particular, is\nunique in its basal position on the Y haplotype tree. However, at 338 (237-581)\nthousand years ago, kya, the extremely ancient TMRCA reported by Mendez et al.\nis inconsistent with the known human fossil record (which estimate the age of\nanatomically modern humans at 195 +- 5 kya), with estimates from mtDNA (176.6\n+- 11.3 kya, and 204.9 (116.8-295.7) kya) and with population genetic theory.\nThe inflated TMRCA can quite easily be attributed to the extremely low Y\nchromosome mutation rate used by the authors.\n",
        "  In this extended abstract we provide a unifying framework that can be used to\ncharacterize and compare the expressive power of query languages for different\ndata base models. The framework is based upon the new idea of valid partition,\nthat is a partition of the elements of a given data base, where each class of\nthe partition is composed by elements that cannot be separated (distinguished)\naccording to some level of information contained in the data base. We describe\ntwo applications of this new framework, first by deriving a new syntactic\ncharacterization of the expressive power of relational algebra which is\nequivalent to the one given by Paredaens, and subsequently by studying the\nexpressive power of a simple graph-based data model.\n",
        "  Web search logs contain extremely sensitive data, as evidenced by the recent\nAOL incident. However, storing and analyzing search logs can be very useful for\nmany purposes (i.e. investigating human behavior). Thus, an important research\nquestion is how to privately sanitize search logs. Several search log\nanonymization techniques have been proposed with concrete privacy models.\nHowever, in all of these solutions, the output utility of the techniques is\nonly evaluated rather than being maximized in any fashion. Indeed, for\neffective search log anonymization, it is desirable to derive the optimal\n(maximum utility) output while meeting the privacy standard. In this paper, we\npropose utility-maximizing sanitization based on the rigorous privacy standard\nof differential privacy, in the context of search logs. Specifically, we\nutilize optimization models to maximize the output utility of the sanitization\nfor different applications, while ensuring that the production process\nsatisfies differential privacy. An added benefit is that our novel\nrandomization strategy ensures that the schema of the output is identical to\nthat of the input. A comprehensive evaluation on real search logs validates the\napproach and demonstrates its robustness and scalability.\n",
        "  The attenuation of ultrasound (the geometric resonance) in an intermediate\nstate of the type I superconductors at kaN >> 1 is researched. The oscillating\ndependence of ultrasound attenuation, which has an amplitude modulation as a\nresult of presence of the Andreev reflec-tions by the electronic excitations on\nthe boundaries between the normal metal layer and the superconducting layer in\nan intermediate state of the type I superconductor, is obtained. The derived\ntheoretical equation explains the nature of experimental results in [1, 2].\n",
        "  We study the inhomogeneity of the electronic pairing gap observed by STM near\nthe surface of Bi$_{2}$Sr$_{2}$CaCu$_{2}$O$_{8+\\delta}$ to be correlated with\ninterstitial O defects. We treat the problem in a slave boson mean field theory\nof a disordered $t-t^{\\prime}-J$ model, and identify three aspects of the O\ndefects related to the inhomogeneity: (1)the superexchange interaction is\nlocally enhanced in their vicinity, which enhances the local pairing gap and\nreduces the coherence peak; (2)they donate holes into CuO$_2$ plane, which\nreduces the spinon density of states of and hence the average gap at large\ndoping; (3)holes are locally attracted to the vicinity of oxygen defects, which\ncauses impurity bound state and further reduces the coherence peak. The\ninterplay of these mechanisms explains simultaneously the locally enhanced\npairing gap around oxygen defects, and the reduction of average gap as\nincreasing oxygen concentration.\n",
        "  We present extensions to a continuous-state dependency parsing method that\nmakes it applicable to morphologically rich languages. Starting with a\nhigh-performance transition-based parser that uses long short-term memory\n(LSTM) recurrent neural networks to learn representations of the parser state,\nwe replace lookup-based word representations with representations constructed\nfrom the orthographic representations of the words, also using LSTMs. This\nallows statistical sharing across word forms that are similar on the surface.\nExperiments for morphologically rich languages show that the parsing model\nbenefits from incorporating the character-based encodings of words.\n",
        "  We present an abundance analysis of eight potential member stars of the old\nGalactic bulge globular cluster NGC6522. The same stars have previously been\nstudied by Chiappini et al. (2011), who found very high abundances of the slow\nneutron capture elements compared with other clusters and field stars of\nsimilar metallicity, which they interpreted as reflecting nucleosynthesis in\nrapidly rotating, massive Population III stars. In contrast to their analysis,\nwe do not find any unusual enhancements of the neutron capture elements Sr, Y,\nBa and Eu and conclude that previous claims result mainly from not properly\naccounting for blending lines. Instead we find NGC6522 to be an unremarkable\nglobular cluster with comparable abundance trends to other Galactic globular\nclusters at the same metallicity ([Fe/H] = -1.15 +/- 0.16). The stars are also\nchemically similar to halo and bulge field stars at the same metallicity,\nspanning a small range in [Y/Ba] and with normal {\\alpha}-element abundances.\nWe thus find no observational evidence for any chemical signatures of rapidly\nrotating Population III stars in NGC 6522.\n",
        "  We investigated Ce-substitution and reduction annealing effects on the\nelectronic states at copper sites by Cu ${K}$-edge x-ray absorption near-edge\nstructure measurements in Pr$_{2-x}$Ce$_x$CuO$_{4+\\alpha-\\delta}$ (PCCO) with\nvarying $x$ and $\\delta$ (the amount of oxygen loss during annealing) values.\nAbsorption near-edge spectra were modified by Ce-substitution and reduction\nannealing in a similar manner with increasing $x$ and $\\delta$. Considering\nelectron doping by Ce-substitution, this similarity indicates an increase of\nelectron number at the copper sites due to annealing $n_{\\rm AN}$. Thus, the\ntotal number of electrons is determined by the amount of Ce and oxygen ions.\nFurthermore, quantitative analyses of the spectra clarified that the number of\nCu$^+$ sites, corresponding to the induced electron number by Ce-substitution\n$n_{\\rm Ce}$ increases linearly with $x$ in the as-sintered PCCO ($\\delta=0$),\nwhereas $n_{\\rm AN}$ is not exactly equal to twice of $\\delta$, which is\nexpected from charge neutrality. For each $x$-fixed sample, $n_{\\rm AN}$ tends\nto exceed 2$\\delta$ with increasing $\\delta$, suggesting the emergence of two\ntypes of carrier due to annealing.\n",
        "  Automated discourse analysis tools based on Natural Language Processing (NLP)\naiming at the diagnosis of language-impairing dementias generally extract\nseveral textual metrics of narrative transcripts. However, the absence of\nsentence boundary segmentation in the transcripts prevents the direct\napplication of NLP methods which rely on these marks to function properly, such\nas taggers and parsers. We present the first steps taken towards automatic\nneuropsychological evaluation based on narrative discourse analysis, presenting\na new automatic sentence segmentation method for impaired speech. Our model\nuses recurrent convolutional neural networks with prosodic, Part of Speech\n(PoS) features, and word embeddings. It was evaluated intrinsically on\nimpaired, spontaneous speech, as well as, normal, prepared speech, and presents\nbetter results for healthy elderly (CTL) (F1 = 0.74) and Mild Cognitive\nImpairment (MCI) patients (F1 = 0.70) than the Conditional Random Fields method\n(F1 = 0.55 and 0.53, respectively) used in the same context of our study. The\nresults suggest that our model is robust for impaired speech and can be used in\nautomated discourse analysis tools to differentiate narratives produced by MCI\nand CTL.\n",
        "  We present a corpus of 5,000 richly annotated abstracts of medical articles\ndescribing clinical randomized controlled trials. Annotations include\ndemarcations of text spans that describe the Patient population enrolled, the\nInterventions studied and to what they were Compared, and the Outcomes measured\n(the `PICO' elements). These spans are further annotated at a more granular\nlevel, e.g., individual interventions within them are marked and mapped onto a\nstructured medical vocabulary. We acquired annotations from a diverse set of\nworkers with varying levels of expertise and cost. We describe our data\ncollection process and the corpus itself in detail. We then outline a set of\nchallenging NLP tasks that would aid searching of the medical literature and\nthe practice of evidence-based medicine.\n",
        "  The contribution of vortex core has been taken into account properly in\nconstructing a torque theory for multiband superconductors. We employ the\nprescription of describing internal magnetic field in the vortex lattice by Hao\n{\\it et al.} and by Yaouanc {\\it et al.} to derive a torque formula as a\nnatural extension of a preceding London theory. In marked contrast with the\npreceding model, our novel formula does not contain a phenomenological\nparameter $\\eta$, which prevents us from obtaining a {\\it true} upper critical\nfield $H_{\\rm c2}$ by analyzing an experimental torque curve. The parameter\n$\\eta$ was originally introduced to take care of the uncertainty in determining\nthe vortex core size $\\xi_v$. Furthermore, we reveal that the $\\eta$ value is\nuniversally scaled by anisotropy $\\gamma$, magnetic field $B$, and $H_{\\rm c2}$\ndue to field dependence of $\\xi_v$. This may revitalize the single-band Kogan\nmodel in combination with a universal function $\\eta(\\gamma, B, H_{\\rm c2})$\ninstead of a constant $\\eta$.\n",
        "  Identifying mathematical relations expressed in text is essential to\nunderstanding a broad range of natural language text from election reports, to\nfinancial news, to sport commentaries to mathematical word problems. This paper\nfocuses on identifying and understanding mathematical relations described\nwithin a single sentence. We introduce the problem of Equation Parsing -- given\na sentence, identify noun phrases which represent variables, and generate the\nmathematical equation expressing the relation described in the sentence. We\nintroduce the notion of projective equation parsing and provide an efficient\nalgorithm to parse text to projective equations. Our system makes use of a high\nprecision lexicon of mathematical expressions and a pipeline of structured\npredictors, and generates correct equations in $70\\%$ of the cases. In $60\\%$\nof the time, it also identifies the correct noun phrase $\\rightarrow$ variables\nmapping, significantly outperforming baselines. We also release a new annotated\ndataset for task evaluation.\n",
        "  The property graph data model of modern graph database systems is\nincreasingly adapted for storing and processing heterogeneous datasets like\nnetworks. Many challenging applications with near real-time requirements --\ne.g. financial fraud detection, recommendation systems, and on-the-fly\nvalidation -- can be captured with graph queries, which are evaluated\nrepeatedly. To ensure quick response time for a changing data set, these\napplications would benefit from applying incremental view maintenance (IVM)\ntechniques, which can perform continuous evaluation of queries and calculate\nthe changes in the result set upon updates. However, currently, no graph\ndatabases provide support for incremental views. While IVM problems have been\nstudied extensively over relational databases, views on property graph queries\nrequire operators outside the scope of standard relational algebra. Hence,\ntackling this problem requires the integration of numerous existing IVM\ntechniques and possibly further extensions. In this paper, we present an\napproach to perform IVM on property graphs, using a nested relational algebraic\nrepresentation for property graphs and graph operations. Then we define a chain\nof transformations to reduce most property graph queries to flat relational\nalgebra and use techniques from discrimination networks (used in rule-based\nexpert systems) to evaluate them. We demonstrate the approach using our\nprototype tool, ingraph, which uses openCypher, an open graph query language\nspecified as part of an industry initiative. However, several aspects of our\napproach can be generalised to other graph query languages such as G-CORE and\nPGQL.\n",
        "  We report LDA calculated band structure, densities of states and Fermi\nsurfaces for recently discovered Pt-pnictide superconductors APt3P\n(A=Ca,Sr,La), confirming their multiple band nature. Electronic structure is\nessentially three dimensional, in contrast to Fe pnictides and chalcogenides.\nLDA calculated Sommerfeld coefficient agrees rather well with experimental\ndata, leaving little space for very strong coupling superconductivity,\nsuggested by experimental data on specific heat of SrPt3P. Elementary estimates\nshow, that the values of critical temperature can be explained by rather weak\nor moderately strong coupling, while the decrease of superconducting transition\ntemperature Tc from Sr to La compound can be explained by corresponding\ndecrease of total density of states at the Fermi level N(E_F). The shape of the\ndensity of states near the Fermi level suggests that in SrPt3P electron doping\n(such as replacement Sr by La) decreases N(E_F) and Tc, while hole doping (e.g.\npartial replacement of Sr with K, Rb or Cs, if possible) would increase N(E_F)\nand possibly Tc.\n",
        "  Left ventricular torsion from helically oriented myofibers is a key parameter\nof cardiac performance. Physicians observing heart motion on echocardiograms,\nduring cardiac catheterization, or in the operating room, are impressed by the\ntwisting or rotary motion of the left ventricle during systole. Conceptually,\nthe heart has been treated as a pressure chamber. The rotary or torsional\ndeformation has been poorly understood by basic scientists and has lacked\nclinical relevance. The aim of this paper attempts to discuss about this\nquestion: Is ventricular twisting related to ventricular fiber arrangement?\nThat is dependent to an assumed model of the left ventricular structure.\n",
        "  We describe the relationship between different forms of linearized\nexpressions for the spatial distribution of intensity of X-ray projection\nimages obtained in the Fresnel region. We prove that under the natural validity\nconditions some of the previously published expressions can be simplified\nwithout a loss of accuracy. We also introduce modified validity conditions\nwhich are likely to be fulfilled in many relevant practical cases, and which\nlead to a further significant simplification of the expression for the\nimage-plane intensity, permitting simple non-iterative linear algorithms for\nthe phase retrieval.\n",
        "  Recently, encoder-decoder models are widely used in social media text\nsummarization. However, these models sometimes select noise words in irrelevant\nsentences as part of a summary by error, thus declining the performance. In\norder to inhibit irrelevant sentences and focus on key information, we propose\nan effective approach by learning sentence weight distribution. In our model,\nwe build a multi-layer perceptron to predict sentence weights. During training,\nwe use the ROUGE score as an alternative to the estimated sentence weight, and\ntry to minimize the gap between estimated weights and predicted weights. In\nthis way, we encourage our model to focus on the key sentences, which have high\nrelevance with the summary. Experimental results show that our approach\noutperforms baselines on a large-scale social media corpus.\n",
        "  Since many real-world concepts are associated with colour, for example danger\nwith red, linguistic information is often complimented with the use of\nappropriate colours in information visualization and product marketing. Yet,\nthere is no comprehensive resource that captures concept-colour associations.\nWe present a method to create a large word-colour association lexicon by\ncrowdsourcing. We focus especially on abstract concepts and emotions to show\nthat even though they cannot be physically visualized, they too tend to have\nstrong colour associations. Finally, we show how word-colour associations\nmanifest themselves in language, and quantify usefulness of co-occurrence and\npolarity cues in automatically detecting colour associations.\n",
        "  In this paper we take a state-of-the-art model for distributed word\nrepresentation that explicitly factorizes the positive pointwise mutual\ninformation (PPMI) matrix using window sampling and negative sampling and\naddress two of its shortcomings. We improve syntactic performance by using\npositional contexts, and solve the need to store the PPMI matrix in memory by\nworking on aggregate data in external memory. The effectiveness of both\nmodifications is shown using word similarity and analogy tasks.\n",
        "  The contact structure between hosts has a critical influence on disease\nspread. However, most networkbased models used in epidemiology tend to ignore\nheterogeneity in the weighting of contacts. This assumption is known to be at\nodds with the data for many contact networks (e.g. sexual contact networks) and\nto have a strong effect on the predictions of epidemiological models. One of\nthe reasons why models usually ignore heterogeneity in transmission is that we\ncurrently lack tools to analyze weighted networks, such that most studies rely\non numerical simulations. Here, we present a novel framework to estimate key\nepidemiological variables, such as the rate of early epidemic expansion and the\nbasic reproductive ratio, from joint probability distributions of number of\npartners (contacts) and number of interaction events through which contacts are\nweighted. This framework also allows for a derivation of the full time course\nof epidemic prevalence and contact behaviour which is validated using numerical\nsimulations. Our framework allows for the incorporation of more realistic\ncontact networks into epidemiological models, thus improving predictions on the\nspread of emerging infectious diseases.\n",
        "  This paper is a presentation, where we compute the HOMFLYPT Skein module of\nsingular links in the 3-sphere. This calculation is based on some results\npreviously proved by Rabenda and the author on Markov traces on singular Hecke\nalgebras, as well as on classical techniques that allow to pass from the\nframework of Markov traces on Hecke algebras to the framework of HOMFLYPT Skein\nmodules. Some open problems on singular Hecke algebras are also presented.\n",
        "  This paper presents a new approach for unsupervised Spoken Term Detection\nwith spoken queries using multiple sets of acoustic patterns automatically\ndiscovered from the target corpus. The different pattern HMM\nconfigurations(number of states per model, number of distinct models, number of\nGaussians per state)form a three-dimensional model granularity space. Different\nsets of acoustic patterns automatically discovered on different points properly\ndistributed over this three-dimensional space are complementary to one another,\nthus can jointly capture the characteristics of the spoken terms. By\nrepresenting the spoken content and spoken query as sequences of acoustic\npatterns, a series of approaches for matching the pattern index sequences while\nconsidering the signal variations are developed. In this way, not only the\non-line computation load can be reduced, but the signal distributions caused by\ndifferent speakers and acoustic conditions can be reasonably taken care of. The\nresults indicate that this approach significantly outperformed the unsupervised\nfeature-based DTW baseline by 16.16\\% in mean average precision on the TIMIT\ncorpus.\n",
        "  We study the space $C(a_0,a_1,\\dots,a_n)$ of hyperbolic 2-spheres with cone\npoints of prescribed apex curvatures $2a_0,2a_1,\\dots,2a_n\\in]0,2\\pi[$ and some\nrelated spaces. For $n=3$, we get a detailed description of such spaces. The\neuclidean 2-spheres were considered by W. P. Thurston: for $n=4$, the\ncorresponding spaces provide the famous 7 examples of nonarithmetic compact\nholomorphic 2-ball quotients previously constructed by Deligne-Mostow.\n",
        "  Sharing musical files via the Internet was the essential motivation of early\nP2P systems. Despite of the great success of the P2P file sharing systems,\nthese systems support only \"simple\" queries. The focus in such systems is how\nto carry out an efficient query routing in order to find the nodes storing a\ndesired file. Recently, several research works have been made to extend P2P\nsystems to be able to share data having a fine granularity (i.e. atomic\nattribute) and to process queries written with a highly expressive language\n(i.e. SQL). These works have led to the emergence of P2P data sharing systems\nthat represent a new generation of P2P systems and, on the other hand, a next\nstage in a long period of the database research area. ? The characteristics of\nP2P systems (e.g. large-scale, node autonomy and instability) make impractical\nto have a global catalog that represents often an essential component in\ntraditional database systems. Usually, such a catalog stores information about\ndata, schemas and data sources. Query routing and processing are two problems\naffected by the absence of a global catalog. Locating relevant data sources and\ngenerating a close to optimal execution plan become more difficult. In this\npaper, we concentrate our study on proposed solutions for the both problems.\nFurthermore, selected case studies of main P2P data sharing systems are\nanalyzed and compared.\n",
        "  The execution logs that are used for process mining in practice are often\nobtained by querying an operational database and storing the result in a flat\nfile. Consequently, the data processing power of the database system cannot be\nused anymore for this information, leading to constrained flexibility in the\ndefinition of mining patterns and limited execution performance in mining large\nlogs. Enabling process mining directly on a database - instead of via\nintermediate storage in a flat file - therefore provides additional flexibility\nand efficiency. To help facilitate this ideal of in-database process mining,\nthis paper formally defines a database operator that extracts the 'directly\nfollows' relation from an operational database. This operator can both be used\nto do in-database process mining and to flexibly evaluate process mining\nrelated queries, such as: \"which employee most frequently changes the 'amount'\nattribute of a case from one task to the next\". We define the operator using\nthe well-known relational algebra that forms the formal underpinning of\nrelational databases. We formally prove equivalence properties of the operator\nthat are useful for query optimization and present time-complexity properties\nof the operator. By doing so this paper formally defines the necessary\nrelational algebraic elements of a 'directly follows' operator, which are\nrequired for implementation of such an operator in a DBMS.\n",
        "  Sandhi means to join two or more words to coin new word. Sandhi literally\nmeans `putting together' or combining (of sounds), It denotes all combinatory\nsound-changes effected (spontaneously) for ease of pronunciation.\nSandhi-vicheda describes [5] the process by which one letter (whether single or\ncojoined) is broken to form two words. Part of the broken letter remains as the\nlast letter of the first word and part of the letter forms the first letter of\nthe next letter. Sandhi- Vicheda is an easy and interesting way that can give\nentirely new dimension that add new way to traditional approach to Hindi\nTeaching. In this paper using the Rule based algorithm we have reported an\naccuracy of 60-80% depending upon the number of rules to be implemented.\n",
        "  Under the effect of strong genetic drift, it is highly probable to observe\ngene fixation or gene loss in a population, shown by infinite peaks on a\ncoherently constructed potential energy landscape. It is then important to ask\nwhat such singular peaks imply, with or without the effects of other biological\nfactors. We studied the stochastic escape time from the infinite potential\npeaks in the Wright-Fisher model, where the typical two-scale diffusion\ndynamics was observed via computer simulations. We numerically found the\naverage escape time for all the bi-stable cases and analytically approximated\nthe results under weak mutations and selections by calculating the mean first\npassage time (MFPT) in singular potential peak. Our results showed that\nKramers' classical escape formula can be extended to the models with\nnon-Gaussian probability distributions, overcoming constraints in previous\nmethods. The constructed landscape provides a global and coherent description\nfor system's evolutionary dynamics, allowing new biological results to be\ngenerated.\n",
        "  Frequency dependent selection and demographic fluctuations play important\nroles in evolutionary and ecological processes. Under frequency dependent\nselection, the average fitness of the population may increase or decrease based\non interactions between individuals within the population. This should be\nreflected in fluctuations of the population size even in constant environments.\nHere, we propose a stochastic model, which naturally combines these two\nevolutionary ingredients by assuming frequency dependent competition between\ndifferent types in an individual-based model. In contrast to previous game\ntheoretic models, the carrying capacity of the population and thus the\npopulation size is determined by pairwise competition of individuals mediated\nby evolutionary games and demographic stochasticity. In the limit of infinite\npopulation size, the averaged stochastic dynamics is captured by the\ndeterministic competitive Lotka-Volterra equations. In small populations,\ndemographic stochasticity may instead lead to the extinction of the entire\npopulation. As the population size is driven by the fitness in evolutionary\ngames, a population of cooperators is less prone to go extinct than a\npopulation of defectors, whereas in the usual systems of fixed size, the\npopulation would thrive regardless of its average payoff.\n",
        "  Understanding the importance that the electronic medical health records\nsystem has, with its various structural types and grades, has led to the\nelaboration of a series of standards and quality control methods, meant to\ncontrol its functioning. In time, the electronic health records system has\nevolved along with the medical data change of structure. Romania has not yet\nmanaged to fully clarify this concept, various definitions still being\nencountered, such as \"Patient's electronic chart\", \"Electronic health file\". A\nslow change from functional interoperability (OSI level 6) to semantic\ninteroperability (level 7) is being aimed at the moment. This current article\nwill try to present the main electronic files models, from a functional\ninteroperability system's possibility to be created perspective.\n",
        "  Rapid crisis response requires real-time analysis of messages. After a\ndisaster happens, volunteers attempt to classify tweets to determine needs,\ne.g., supplies, infrastructure damage, etc. Given labeled data, supervised\nmachine learning can help classify these messages. Scarcity of labeled data\ncauses poor performance in machine training. Can we reuse old tweets to train\nclassifiers? How can we choose labeled tweets for training? Specifically, we\nstudy the usefulness of labeled data of past events. Do labeled tweets in\ndifferent language help? We observe the performance of our classifiers trained\nusing different combinations of training sets obtained from past disasters. We\nperform extensive experimentation on real crisis datasets and show that the\npast labels are useful when both source and target events are of the same type\n(e.g. both earthquakes). For similar languages (e.g., Italian and Spanish),\ncross-language domain adaptation was useful, however, when for different\nlanguages (e.g., Italian and English), the performance decreased.\n",
        "  High resolution longitudinal sound velocity measurements in a magnetic field\nperformed in the temperature tending to zero limit reveal two distinct\nsignatures attributable to multiple superconducting phases in URu2Si2. A step\nchange in the sound velocity, for propagation in the basal plane, is observed\nat the critical field, Bc2. This step broadens considerably as T tends to Tc\nwith a concomitant decrease in magnitude. A second step is observed at a field\n~0.5 Bc2 and it's magnitude remains constant at all temperatures. Inductive\nmeasurements of the transitions in a magnetic field, however, exhibit a single\nsignature which coincides with the lower step with no discernible in-phase\nsignature at the upper transition. Measurements performed with B oriented at\nvarious angles between the a and c-axes reveal a weaker angular dependence of\nthe lower step and confirm the rapid fall off of Bc2 close to B||c-axis. An off\naxis superconducting phase diagram is proposed.\n",
        "  Purpose: Optoacoustic tomography (OAT) is inherently a three-dimensional (3D)\ninverse problem. However, most studies of OAT image reconstruction still employ\ntwo-dimensional (2D) imaging models. One important reason is because 3D image\nreconstruction is computationally burdensome. The aim of this work is to\naccelerate existing image reconstruction algorithms for 3D OAT by use of\nparallel programming techniques.\n  Methods: Parallelization strategies are proposed to accelerate a filtered\nbackprojection (FBP) algorithm and two different pairs of\nprojection/backprojection operations that correspond to two different numerical\nimaging models. The algorithms are designed to fully exploit the parallel\ncomputing power of graphic processing units (GPUs). In order to evaluate the\nparallelization strategies for the projection/backprojection pairs, an\niterative image reconstruction algorithm is implemented. Computer-simulation\nand experimental studies are conducted to investigate the computational\nefficiency and numerical accuracy of the developed algorithms.\n  Results: The GPU implementations improve the computational efficiency by\nfactors of 1, 000, 125, and 250 for the FBP algorithm and the two pairs of\nprojection/backprojection operators, respectively. Accurate images are\nreconstructed by use of the FBP and iterative image reconstruction algorithms\nfrom both computer-simulated and experimental data.\n  Conclusions: Parallelization strategies for 3D OAT image reconstruction are\nproposed for the first time. These GPU-based implementations significantly\nreduce the computational time for 3D image reconstruction, complementing our\nearlier work on 3D OAT iterative image reconstruction.\n",
        "  We consider the lifetimes of metastable states in bistable evolutionary games\n(coordination games), and examine how they are affected by spatial structure. A\nsemiclassical approximation based on a path integral method is applied to\nstochastic evolutionary game dynamics with and without spatial structure, and\nthe lifetimes of the metastable states are evaluated. It is shown that the\npopulation dependence of the lifetimes is qualitatively different in these two\nmodels. Our result indicates that spatial structure can accelerate the\ntransitions between metastable states.\n",
        "  We propose a new data structure, Parallel Adjacency Lists (PAL), for\nefficiently managing graphs with billions of edges on disk. The PAL structure\nis based on the graph storage model of GraphChi (Kyrola et. al., OSDI 2012),\nbut we extend it to enable online database features such as queries and fast\ninsertions. In addition, we extend the model with edge and vertex attributes.\nCompared to previous data structures, PAL can store graphs more compactly while\nallowing fast access to both the incoming and the outgoing edges of a vertex,\nwithout duplicating data. Based on PAL, we design a graph database management\nsystem, GraphChi-DB, which can also execute powerful analytical graph\ncomputation.\n  We evaluate our design experimentally and demonstrate that GraphChi-DB\nachieves state-of-the-art performance on graphs that are much larger than the\navailable memory. GraphChi-DB enables anyone with just a laptop or a PC to work\nwith extremely large graphs.\n",
        "  In its broadest terms, doctoral dissertation entitled \"Track structure\nmodelling for ion radiotherapy\" is part of the supporting research background\nin the development of the ambitious proton radiotherapy project currently under\nway at the Institute of Nuclear Physics PAN in Krak\\'ow. Another broad\nmotivation was the desire to become directly involved in research on a topical\nand challenging subject of possibly developing a therapy planning system for\ncarbon beam radiotherapy, based in its radiobiological part on the Track\nStructure model developed by prof. Robert Katz over 50 years ago. Thus, the\ngeneral aim of this work was, firstly, to recapitulate the Track Structure\nmodel and to propose an updated and complete formulation of this model by\nincorporating advances made by several authors who had contributed to its\ndevelopment in the past. Secondly, the updated and amended (if necessary)\nformulation of the model was presented in a form applicable for use in computer\ncodes which would constitute the \"radiobiological engine\" of the future therapy\nplanning system for carbon radiotherapy, which the Krak\\'ow ion radiotherapy\nresearch group wishes to develop. Lastly, currently available radiobiology data\nwere analysed in terms of Track Structure Theory to supply exemplary parameters\nfor cell lines (preferably, exposed in normal and anoxic conditions) to be used\nas possible input for carbon ion radiotherapy planning studies.\n",
        "  Identifying patients at risk of traumatic brain injury (TBI) is important\nbecause research suggests prophylactic treatments to reduce risk of long-term\nsequelae. Blast pressure waves can cause TBI without penetrating wounds or\nblunt force trauma. Similarly, bullet impacts distant from the brain can\nproduce pressure waves sufficient to cause mild to moderate TBI. The fluid\npercussion model of TBI shows that pressure impulses of 15-30 psi cause mild to\nmoderate TBI in laboratory animals. In pigs and dogs, bullet impacts to the\nthigh produce pressure waves in the brain of 18-45 psi and measurable injury to\nneurons and neuroglia. Analyses of research in goats and epidemiological data\nfrom shooting events involving humans show high correlations (r > 0.9) between\nrapid incapacitation and pressure wave magnitude in the thoracic cavity. A case\nstudy has documented epilepsy resulting from a pressure wave without the bullet\ndirectly hitting the brain. Taken together, these results support the\nhypothesis that bullet impacts distant from the brain produce pressure waves\nthat travel to the brain and can retain sufficient magnitude to induce brain\ninjury. The link to long-term sequelae could be investigated via\nepidemiological studies of patients who were gunshot in the chest to determine\nwhether they experience elevated rates of epilepsy and other neurological\nsequelae.\n",
        "  We present a photoacoustic computed tomography investigation on a healthy\nhuman finger, to image blood vessels with a focus on vascularity across the\ninterphalangeal joints. The cross-sectional images were acquired using an\nimager specifically developed for this purpose. The images show rich detail of\nthe digital blood vessels with diameters between 100 $\\mu$m and 1.5 mm in\nvarious orientations and at various depths. Different vascular layers in the\nskin including the subpapillary plexus could also be visualized. Acoustic\nreflections on the finger bone of photoacoustic signals from skin were visible\nin sequential slice images along the finger except at the location of the joint\ngaps. Not unexpectedly, the healthy synovial membrane at the joint gaps was not\ndetected due to its small size and normal vascularization. Future research will\nconcentrate on studying digits afflicted with rheumatoid arthritis to detect\nthe inflamed synovium with its heightened vascularization, whose\ncharacteristics are potential markers for disease activity.\n",
        "  Many data management applications require integrating information from\nmultiple sources. The sources may not be accurate and provide erroneous values.\nWe thus have to identify the true values from conflicting observations made by\nthe sources. The problem is further complicated when there may exist multiple\ntruths (e.g., a book written by several authors). In this paper we propose a\nmodel called Hybrid that jointly makes two decisions: how many truths there\nare, and what they are. It considers the conflicts between values as important\nevidence for ruling out wrong values, while keeps the flexibility of allowing\nmultiple truths. In this way, Hybrid is able to achieve both high precision and\nhigh recall.\n",
        "  Elucidating the demographic and phylogeographic histories of species provides\ninsight into the processes responsible for generating biological diversity, and\ngenomic datasets are now permitting the estimation of histories and demographic\nparameters with unprecedented accuracy. We used a genomic single nucleotide\npolymorphism (SNP) dataset generated using a RAD-Seq method to investigate the\nhistorical demography and phylogeography of a widespread lowland Neotropical\nbird (Xenops minutus). As expected, we found that prominent landscape features\nthat act as dispersal barriers, such as Amazonian rivers and the Andes\nMountains, are associated with the deepest phylogeographic breaks, and also\nthat isolation by distance is limited in areas between these barriers. In\naddition, we inferred positive population growth for most populations and\ndetected evidence of historical gene flow between populations that are now\nphysically isolated. Even with genomic estimates of historical demographic\nparameters, we found the prominent diversification hypotheses to be untestable.\nWe conclude that investigations into the multifarious processes shaping species\nhistories, aided by genomic datasets, will provide greater resolution of\ndiversification in the Neotropics, but that future efforts should focus on\nunderstanding the processes shaping the histories of lineages rather than\ntrying to reconcile these histories with landscape and climatic events in Earth\nhistory.\n",
        "  The 2014 Ebola virus (EBOV) outbreak in West Africa is the largest outbreak\nof the genus Ebolavirus to date. To better understand the spread of infection\nin the affected countries, it is crucial to know the number of secondary cases\ngenerated by an infected index case in the absence and presence of control\nmeasures, i.e., the basic and effective reproduction number. In this study, I\ndescribe the EBOV epidemic using an SEIR\n(susceptible-exposed-infectious-recovered) model and fit the model to the most\nrecent reported data of infected cases and deaths in Guinea, Sierra Leone and\nLiberia. The maximum likelihood estimates of the basic reproduction number are\n1.51 (95% confidence interval [CI]: 1.50-1.52) for Guinea, 2.53 (95% CI:\n2.41-2.67) for Sierra Leone and 1.59 (95% CI: 1.57-1.60) for Liberia. The model\nindicates that in Guinea and Sierra Leone the effective reproduction number\nmight have dropped to around unity by the end of May and July 2014,\nrespectively. In Liberia, however, the model estimates no decline in the\neffective reproduction number by end-August 2014. This suggests that control\nefforts in Liberia need to be improved substantially in order to stop the\ncurrent outbreak.\n",
        "  Differential privacy is a rigorous privacy condition achieved by randomizing\nquery answers. This paper develops efficient algorithms for answering multiple\nqueries under differential privacy with low error. We pursue this goal by\nadvancing a recent approach called the matrix mechanism, which generalizes\nstandard differentially private mechanisms. This new mechanism works by first\nanswering a different set of queries (a strategy) and then inferring the\nanswers to the desired workload of queries. Although a few strategies are known\nto work well on specific workloads, finding the strategy which minimizes error\non an arbitrary workload is intractable. We prove a new lower bound on the\noptimal error of this mechanism, and we propose an efficient algorithm that\napproaches this bound for a wide range of workloads.\n",
        "  Question answering tasks have shown remarkable progress with distributed\nvector representation. In this paper, we investigate the recently proposed\nFacebook bAbI tasks which consist of twenty different categories of questions\nthat require complex reasoning. Because the previous work on bAbI are all\nend-to-end models, errors could come from either an imperfect understanding of\nsemantics or in certain steps of the reasoning. For clearer analysis, we\npropose two vector space models inspired by Tensor Product Representation (TPR)\nto perform knowledge encoding and logical reasoning based on common-sense\ninference. They together achieve near-perfect accuracy on all categories\nincluding positional reasoning and path finding that have proved difficult for\nmost of the previous approaches. We hypothesize that the difficulties in these\ncategories are due to the multi-relations in contrast to uni-relational\ncharacteristic of other categories. Our exploration sheds light on designing\nmore sophisticated dataset and moving one step toward integrating transparent\nand interpretable formalism of TPR into existing learning paradigms.\n",
        "  We observed in a previous study (PLoS ONE 6:e24522) that the self-regulation\nof amygdala activity via real-time fMRI neurofeedback (rtfMRI-nf) with positive\nemotion induction was associated, in healthy participants, with an enhancement\nin the functional connectivity between the left amygdala (LA) and six regions\nof the prefrontal cortex. These regions included the left rostral anterior\ncingulate cortex (rACC), bilateral dorsomedial prefrontal cortex (DMPFC),\nbilateral superior frontal gyrus (SFG), and right medial frontopolar cortex\n(MFPC). Together with the LA, these six prefrontal regions thus formed the\nfunctional neuroanatomical network engaged during the rtfMRI-nf procedure. Here\nwe perform a structural vector autoregression (SVAR) analysis of the effective\nconnectivity for this network. The SVAR analysis demonstrates that the left\nrACC plays an important role during the rtfMRI-nf training, modulating the LA\nand the other network regions. According to the analysis, the rtfMRI-nf\ntraining leads to a significant enhancement in the time-lagged effect of the\nleft rACC on the LA. The training is also accompanied by significant increases\nin the instantaneous (contemporaneous) effects of the left rACC on four other\nregions - the bilateral DMPFC, the right MFPC, and the left SFG. The\ninstantaneous effects of the LA on the bilateral DMPFC are also significantly\nenhanced. Our results are consistent with a broad literature supporting the\nrole of the rACC in emotion processing and regulation. Our analysis provides,\nfor the first time, insights into the causal relationships within the network\nof regions engaged during the rtfMRI-nf procedure targeting the amygdala. It\nsuggests that the rACC may constitute a promising target for rtfMRI-nf training\nalong with the amygdala in patients with affective disorders, particularly\nposttraumatic stress disorder (PTSD).\n",
        "  We present in this paper our work on comparison between Statistical Machine\nTranslation (SMT) and Rule-based machine translation for translation from\nMarathi to Hindi. Rule Based systems although robust take lots of time to\nbuild. On the other hand statistical machine translation systems are easier to\ncreate, maintain and improve upon. We describe the development of a basic\nMarathi-Hindi SMT system and evaluate its performance. Through a detailed error\nanalysis, we, point out the relative strengths and weaknesses of both systems.\nEffectively, we shall see that even with a small amount of training corpus a\nstatistical machine translation system has many advantages for high quality\ndomain specific machine translation over that of a rule-based counterpart.\n",
        "  Strain engineering has been used to modify materials properties in\nferroelectric, superconducting, and ferromagnetic thin films. The advantage of\nstrain engineering is that it can achieve unexpected enhancement in certain\nproperties, such as an increase in ferroelectric critical temperature, Tc, by\n300 to 500K, with a minimum detrimental effect on the intrinsic properties of\nthe material. The strain engineering has been largely applied to the materials\nin thin film form, where the strain is generated as a result of lattice\nmismatch between the substrate and component film or between layers in\nmultilayer structures. Here, we report the observation of residual thermal\nstress/strain in dense SiC-MgB2 superconductor composites prepared by a\ndiffusion method. We demonstrate that the thermal strain caused by the\ndifferent thermal expansion coefficients between the MgB2 and SiC phases is\nresponsible for the significant improvement in the critical current density,\nJc, the irreversibility field, Hirr, and the upper critical field, Hc2, in the\nSiC-MgB2 composite where the carbon substitution level is low. In contrast to\nthe common practice of improving the Jc and Hc2 of MgB2 through chemical\nsubstitution, by taking advantage of residual thermal strains we are able to\ndesign a composite, which shows only a small drop in Tc and little increase in\nresistivity, but a significant improvement over the Jc and Hc2 of MgB2. The\npresent findings open up a new direction for manipulation of materials\nproperties through strain engineering for materials in various forms.\n",
        "  The origins of life stands among the great open scientific questions of our\ntime. While a number of proposals exist for possible starting points in the\npathway from non-living to living matter, these have so far not achieved states\nof complexity that are anywhere near that of even the simplest living systems.\nA key challenge is identifying the properties of living matter that might\ndistinguish living and non-living physical systems such that we might build new\nlife in the lab. This review is geared towards covering major viewpoints on the\norigin of life for those new to the origin of life field, with a forward look\ntowards considering what it might take for a physical theory that universally\nexplains the phenomenon of life to arise from the seemingly disconnected array\nof ideas proposed thus far. The hope is that a theory akin to our other\ntheories in fundamental physics might one day emerge to explain the phenomenon\nof life, and in turn finally permit solving its origins.\n",
        "  Data warehouse performance is usually achieved through physical data\nstructures such as indexes or materialized views. In this context, cost models\ncan help select a relevant set ofsuch performance optimization structures.\nNevertheless, selection becomes more complex in the cloud. The criterion to\noptimize is indeed at least two-dimensional, with monetary cost balancing\noverall query response time. This paper introduces new cost models that fit\ninto the pay-as-you-go paradigm of cloud computing. Based on these cost models,\nan optimization problem is defined to discover, among candidate views, those to\nbe materialized to minimize both the overall cost of using and maintaining the\ndatabase in a public cloud and the total response time ofa given query\nworkload. We experimentally show that maintaining materialized views is always\nadvantageous, both in terms of performance and cost.\n",
        "  This work studies the impact of systematic uncertainties associated to\ninteraction cross sections on depth dose curves determined by Monte Carlo\nsimulations. The corresponding sensitivity factors are quantified by changing\ncross sections in a given amount and determining the variation in the dose. The\ninfluence of total cross sections for all particles, photons and only for\nCompton scattering is addressed. The PENELOPE code was used in all simulations.\nIt was found that photon cross section sensitivity factors depend on depth. In\naddition, they are positive and negative for depths below and above an\nequilibrium depth, respectively. At this depth, sensitivity factors are null.\nThe equilibrium depths found in this work agree very well with the mean free\npath of the corresponding incident photon energy. Using the sensitivity factors\nreported here, it is possible to estimate the impact of photon cross section\nuncertainties on the uncertainty of Monte Carlo-determined depth dose curves.\n",
        "  Cancer progression is an example of a rapid adaptive process where evolving\nnew traits is essential for survival and requires a high mutation rate.\nPrecancerous cells acquire a few key mutations that drive rapid population\ngrowth and carcinogenesis. Cancer genomics demonstrates that these few 'driver'\nmutations occur alongside thousands of random 'passenger' mutations-a natural\nconsequence of cancer's elevated mutation rate. Some passengers can be\ndeleterious to cancer cells, yet have been largely ignored in cancer research.\nIn population genetics, however, the accumulation of mildly deleterious\nmutations has been shown to cause population meltdown. Here we develop a\nstochastic population model where beneficial drivers engage in a tug-of-war\nwith frequent mildly deleterious passengers. These passengers present a barrier\nto cancer progression that is described by a critical population size, below\nwhich most lesions fail to progress, and a critical mutation rate, above which\ncancers meltdown. We find support for the model in cancer age-incidence and\ncancer genomics data that also allow us to estimate the fitness advantage of\ndrivers and fitness costs of passengers. We identify two regimes of adaptive\nevolutionary dynamics and use these regimes to rationalize successes and\nfailures of different treatment strategies. We find that a tumor's load of\ndeleterious passengers can explain previously paradoxical treatment outcomes\nand suggest that it could potentially serve as a biomarker of response to\nmutagenic therapies. Collective deleterious effect of passengers is currently\nan unexploited therapeutic target. We discuss how their effects might be\nexacerbated by both current and future therapies.\n",
        "  BABYSCAN, a whole body counter (WBC) for small children was developed in\n2013, and units have been installed at three hospitals in Fukushima Prefecture.\nBetween December, 2013 and March, 2015, 2707 children between the ages of 0 and\n11 have been scanned, and none had detectable levels of radioactive cesium. The\nminimum detectable activities (MDAs) for $^{137}$Cs were $\\leq 3.5$ Bq\nkg$^{-1}$ for ages 0-1, decreasing to $\\leq 2$ Bq kg$^{-1}$ for ages 10-11.\nIncluding the $^{134}$Cs contribution, these translate to a maximum committed\neffective dose of $\\sim 16 \\mu$Sv y$^{-1}$ even for newborn babies, and\ntherefore the internal exposure risks can be considered negligibly small.\n  Analysis of the questionnaire filled out by the parents of the scanned\nchildren regarding their families' food and water consumption revealed that the\nmajority of children residing in the town of Miharu regularly consume local or\nhome-grown rice and vegetables, while in Minamisoma, a majority avoid tap water\nand produce from Fukushima. The data show, however, no correlation between\nconsumption of locally produced food and water and the children's body burdens.\n",
        "  This paper describes our experience implementing PostgreSQL's new\nserializable isolation level. It is based on the recently-developed\nSerializable Snapshot Isolation (SSI) technique. This is the first\nimplementation of SSI in a production database release as well as the first in\na database that did not previously have a lock-based serializable isolation\nlevel. We reflect on our experience and describe how we overcame some of the\nresulting challenges, including the implementation of a new lock manager, a\ntechnique for ensuring memory usage is bounded, and integration with other\nPostgreSQL features. We also introduce an extension to SSI that improves\nperformance for read-only transactions. We evaluate PostgreSQL's serializable\nisolation level using several benchmarks and show that it achieves performance\nonly slightly below that of snapshot isolation, and significantly outperforms\nthe traditional two-phase locking approach on read-intensive workloads.\n",
        "  One of the challenging problems in the multidatabase systems is to find the\nmost viable solution to the problem of interoperability of distributed\nheterogeneous autonomous local component databases. This has resulted in the\ncreation of a global schema over set of these local component database schemas\nto provide a uniform representation of local schemas. The aim of this paper is\nto use object oriented approach to integrate schemas of distributed\nheterogeneous autonomous local component database schemas into a global schema.\nThe resulting global schema provides a uniform interface and high level of\nlocation transparency for retrieval of data from the local component databases.\nA set of integration operators are defined to integrate local schemas based on\nthe semantic relevance of their classes and to provide a model independent\nrepresentation of virtual classes of the global schema. The schematic\nrepresentation and heterogeneity is also taken into account in the integration\nprocess. Justifications about Object Oriented Modal are also discussed. Bottom\nup local schema modifications propagation in Global schema is also considered\nto maintain Global schema as local schemas are autonomous and evolve over time.\nAn example illustrates the applicability of the integration operator defined.\n",
        "  The field of astrobiology has made huge strides in understanding the\nhabitable zones around stars (Stellar Habitable Zones) where life can begin,\nsustain its existence and evolve into complex forms. A few studies have\nextended this idea by modelling galactic-scale habitable zones (Galactic\nHabitable Zones) for our Milky Way and specific elliptical galaxies. However,\nestimating the habitability for galaxies spanning a wide range of physical\nproperties has so far remained an outstanding issue. Here, we present a\n\"cosmobiological\" framework that allows us to sift through the entire galaxy\npopulation in the local Universe and answer the question \"Which type of galaxy\nis most likely to host complex life in the cosmos\"? Interestingly, the three\nkey astrophysical criteria governing habitability (total mass in stars, total\nmetal mass and ongoing star formation rate) are found to be intricately linked\nthrough the \"fundamental metallicity relation\" as shown by SDSS (Sloan Digital\nSky Survey) observations of more than a hundred thousand galaxies in the local\nUniverse. Using this relation we show that metal-rich, shapeless giant\nelliptical galaxies at least twice as massive as the Milky Way (with a tenth of\nits star formation rate) can potentially host ten thousand times as many\nhabitable (earth-like) planets, making them the most probable \"cradles of life\"\nin the Universe.\n",
        "  In this paper, we analyze several neural network designs (and their\nvariations) for sentence pair modeling and compare their performance\nextensively across eight datasets, including paraphrase identification,\nsemantic textual similarity, natural language inference, and question answering\ntasks. Although most of these models have claimed state-of-the-art performance,\nthe original papers often reported on only one or two selected datasets. We\nprovide a systematic study and show that (i) encoding contextual information by\nLSTM and inter-sentence interactions are critical, (ii) Tree-LSTM does not help\nas much as previously claimed but surprisingly improves performance on Twitter\ndatasets, (iii) the Enhanced Sequential Inference Model is the best so far for\nlarger datasets, while the Pairwise Word Interaction Model achieves the best\nperformance when less data is available. We release our implementations as an\nopen-source toolkit.\n",
        "  We calculate microscopically the viscous friction coefficient and the\neffective mass of domain walls separating regions of opposite chirality in\np-wave superconductors with k_x\\pm ik_y order parameter. The domain wall\nviscosity and inertia are determined by the transitions between different\nBogoliubov quasiparticle states induced by the domain wall motion. As a\nby-product, we present a detailed analysis of the quasiparticle spectrum, both\nbound and scattering, in the presence of a general domain wall with an\narbitrary phase difference between the domains.\n",
        "  An evolutionary tree (phylogenetic tree) is a binary, rooted, unordered tree\nthat models the evolutionary history of currently living species in which\nleaves are labeled by species. In this paper, we investigate the problem of\nfinding the maximum consensus evolutionary tree from a set of given rooted\ntriplets. A rooted triplet is a phylogenetic tree on three leaves and shows the\nevolutionary relationship of the corresponding three species. The mentioned\nproblem is known to be APX-hard. We present two new heuristic algorithms. For a\ngiven set of m triplets on n species, the FastTree algorithm runs in O(mn^2)\nwhich is faster than any other previously known algorithms, although, the\noutcome is less satisfactory. The BPMTR algorithm runs in O(mn^3) and in\naverage performs better than any other previously known approximation\nalgorithms for this problem.\n",
        "  Dark-field images are formed by small-angle scattering of x-ray photons. The\nsmall-angle scattering signal is particularly sensitive to structural\nvariations and density fluctuation on a length scale of several ten to hundred\nnanometers, offering a new contrast mechanism to reveal subtle structural\nvariation of object. In this paper, we derive a novel physical model to\ndescribe x-ray absorption and small-angle scattering, and use the proposed\nmodel to reconstruct the volumetric small-angle scattering images. The\nnumerical experiments and test experiments demonstrate that the reconstructed\nscattering images reveal unique features with a high contrast resolution. The\nproposed approach has great potential in biomedical imaging, nondestructive\ndetections, and other applications.\n",
        "  A manifestation of a novel type of hysteresis related to the parametric\nresonance in the system of coupled Josephson junctions is demonstrated.\nOpposite to McCumber and Steward hysteresis, we find that the width of this\nhysteresis is inversely proportional to the McCumber parameter and depends also\non coupling between junctions and the boundary conditions. An investigation of\ntime dependence of the electric charge in superconducting layers allow us to\nexplain the origin of this hysteresis by different charge dynamics for\nincreasing and decreasing bias current processes. The effect of wavelength of\nthe longitudinal plasma waves created at the resonance on the charging of\nsuperconducting layers is demonstrated. We found a strong effect of the\ndissipation in the system on the amplitude of the charge oscillations at the\nresonance.\n",
        "  This paper discusses Centre for Development of Advanced Computing Mumbai's\n(CDACM) submission to the NLP Tools Contest on Statistical Machine Translation\nin Indian Languages (ILSMT) 2014 (collocated with ICON 2014). The objective of\nthe contest was to explore the effectiveness of Statistical Machine Translation\n(SMT) for Indian language to Indian language and English-Hindi machine\ntranslation. In this paper, we have proposed that suffix separation and word\nsplitting for SMT from agglutinative languages to Hindi significantly improves\nover the baseline (BL). We have also shown that the factored model with\nreordering outperforms the phrase-based SMT for English-Hindi (\\enhi). We\nreport our work on all five pairs of languages, namely Bengali-Hindi (\\bnhi),\nMarathi-Hindi (\\mrhi), Tamil-Hindi (\\tahi), Telugu-Hindi (\\tehi), and \\enhi for\nHealth, Tourism, and General domains.\n",
        "  Tumor growth relies heavily on the continuous blood and nutrients supply.\nTheoretically, it is an ideal therapeutic way of killing tumor by only vascular\nembolization. However, most of the existing vascular embolic agents are still\nrather insufficient to fulfill the real clinical need due to the reasons like:\nincomplete filling of target vasculature, being easily washed away by blood or\nbody solution, or just producing toxicity to tissues. Here from an alternative\nway, the body temperature liquid metal, a kind of soft and highly compliant\nmaterial, was proposed for the first time as blood vessel embolization agent\nfor tumor physical therapy. With its unique capability of easy phase transition\nbetween liquid and solid state and sub-cooling behavior, such material can be\nfluently injected into the tiny vessels including ending capillaries and fully\nblock them. The in vitro cytotoxicity experiments were performed which showed\nthat treating localized diseased tissues through liquid metal embolic agent is\nacceptable. Endowed with a high density, the liquid metal-filled vessels are\nhighly visible under the CT scan, which offers the potential of\ndiagnosis-treatment integration. To further demonstrate the new conceptual\nliquid metal vascular embolization therapy, several experiments on in vivo\nvasculatures of rabbit ears and mouse tails were performed to provide evidences\nof destroying the targeted tissues. To interpret the liquid metal starvation\ntherapy effects, a theoretical model was established to simulate the tumor\ngrowth with zero, partial or complete filling of the metal agent inside the\nvessels. All the results support that, given appropriate administration, the\nliquid metal embolization is able to destruct the target regions and might\nstarve the tumors to death through a relatively easy way. This study lays the\nfoundation of a promising tumor starvation therapy in the coming time.\n",
        "  In this work, we report on a novel application of Locality Sensitive Hashing\n(LSH) to seismic data at scale. Based on the high waveform similarity between\nreoccurring earthquakes, our application identifies potential earthquakes by\nsearching for similar time series segments via LSH. However, a straightforward\nimplementation of this LSH-enabled application has difficulty scaling beyond 3\nmonths of continuous time series data measured at a single seismic station. As\na case study of a data-driven science workflow, we illustrate how domain\nknowledge can be incorporated into the workload to improve both the efficiency\nand result quality. We describe several end-to-end optimizations of the\nanalysis pipeline from pre-processing to post-processing, which allow the\napplication to scale to time series data measured at multiple seismic stations.\nOur optimizations enable an over 100$\\times$ speedup in the end-to-end analysis\npipeline. This improved scalability enabled seismologists to perform seismic\nanalysis on more than ten years of continuous time series data from over ten\nseismic stations, and has directly enabled the discovery of 597 new earthquakes\nnear the Diablo Canyon nuclear power plant in California and 6123 new\nearthquakes in New Zealand.\n",
        "  With the evolution of neural network based methods, automatic speech\nrecognition (ASR) field has been advanced to a level where building an\napplication with speech interface is a reality. In spite of these advances,\nbuilding a real-time speech recogniser faces several problems such as low\nrecognition accuracy, domain constraint, and out-of-vocabulary words. The low\nrecognition accuracy problem is addressed by improving the acoustic model,\nlanguage model, decoder and by rescoring the N-best list at the output of the\ndecoder. We are considering the N-best list rescoring approach to improve the\nrecognition accuracy. Most of the methods in the literature use the\ngrammatical, lexical, syntactic and semantic connection between the words in a\nrecognised sentence as a feature to rescore. In this paper, we have tried to\nsee the semantic relatedness between the words in a sentence to rescore the\nN-best list. Semantic relatedness is computed using\nTransE~\\cite{bordes2013translating}, a method for low dimensional embedding of\na triple in a knowledge graph. The novelty of the paper is the application of\nsemantic web to automatic speech recognition.\n",
        "  We present an analysis of high-resolution ALMA interferometry of CO(4-3) line\nemission and dust continuum in the \"Ruby\" (PLCK_G244.8+54.9), a bright,\ngravitationally lensed galaxy at z = 3.0 discovered with the Planck all-sky\nsurvey. The Ruby is the brightest of Planck's Dusty GEMS, a sample of 11 of the\nbrightest gravitationally lensed high-redshift galaxies on the extragalactic\nsub-mm sky. We resolve the high-surface-brightness continuum and CO line\nemission of the Ruby in several extended clumps along a partial, nearly\ncircular Einstein ring with 1.4\" diameter around a massive galaxy at z = 1.5.\nLocal star-formation intensities are up to 2000 M$_{\\odot}$ yr$^{-1}$\nkpc$^{-2}$, amongst the highest observed at high redshift, and clearly in the\nrange of maximal starbursts. Gas-mass surface densities are a few $\\times$\n10$^4$ M$_{\\odot}$ pc$^{-2}$. The Ruby lies at, and in part even above, the\nstarburst sequence in the Schmidt-Kennicutt diagram, and at the limit expected\nfor star formation that is self-regulated through the kinetic energy injection\nfrom radiation pressure, stellar winds, and supernovae. We show that these\nprocesses can also inject sufficient kinetic energy and momentum into the gas\nto explain the turbulent line widths, which are consistent with marginally\ngravitationally bound molecular clouds embedded in a critically Toomre-stable\ndisk. The star-formation efficiency is in the range 1-10% per free-fall time,\nconsistent with the notion that the pressure balance that sets the local\nstar-formation law in the Milky Way may well be universal out to the highest\nstar-formation intensities. AGN feedback is not necessary to regulate the star\nformation in the Ruby, in agreement with the absence of a bright AGN component\nin the infrared and radio regimes.\n",
        "  The study of intermediate-mass black holes (IMBHs) is a young and promising\nfield of research. Formed by runaway collisions of massive stars in young and\ndense stellar clusters, intermediate-mass black holes could still be present in\nthe centers of globular clusters, today. Our group investigated the presence of\nintermediate-mass black holes for a sample of 10 Galactic globular clusters. We\nmeasured the inner kinematic profiles with integral-field spectroscopy and\ndetermined masses or upper limits of central black holes in each cluster. In\ncombination with literature data we further studied the positions of our\nresults on known black-hole scaling relations (such as M_bh - sigma) and found\na similar but flatter correlation for IMBHs. Applying cluster evolution codes,\nthe change in the slope could be explained with the stellar mass loss occurring\nin clusters in a tidal field over its life time. Furthermore, we present\nresults from several numerical simulations on the topic of IMBHs and integral\nfield units (IFUs). We ran N-body simulations of globular clusters containing\nIMBHs in a tidal field and studied their effects on mass-loss rates and remnant\nfractions and showed that an IMBH in the center prevents core collapse and\nejects massive objects more rapidly. These simulations were further used to\nsimulate IFU data cubes. For the specific case of NGC 6388 we simulated two\ndifferent IFU techniques and found that velocity dispersion measurements from\nindividual velocities are strongly biased towards lower values due to blends of\nneighbouring stars and background light. In addition, we use the Astrophysical\nMultipurpose Software Environment (AMUSE) to combine gravitational physics,\nstellar evolution and hydrodynamics to simulate the accretion of stellar winds\nonto a black hole.\n",
        "  In positron emission tomography (PET) imaging, statistical iterative\nreconstruction (IR) techniques appear particularly promising since they can\nprovide accurate system model. The system model matrix which describes the\nrelationship between image space and projection space is important to the image\nquality. It contains some factors such as geometrical component and blurring\ncomponent. The blurring component is usually described by point spread function\n(PSF). A PSF matrix derived from the single photon incidence response function\nis studied. And then an IR method based on the system matrix containing the PSF\nis developed. More specifically, the gamma photon incidence on a crystal array\nis simulated by Monte Carlo (MC) simulation, and then the single photon\nincidence response functions are calculated. Subsequently, the single photon\nincidence response functions is used to compute the coincidence blurring factor\naccording to the physical process of PET coincidence detection. Through\nweighting the ordinary system matrix response by the coincidence blurring\nfactors, the IR system matrix containing PSF is finally established. Using this\nsystem matrix, the image is reconstructed by ordered subset expectation\nmaximization (OSEM) algorithm. The experimental results show that the proposed\nsystem matrix can obviously improve the image radial resolution, contrast and\nnoise property. Furthermore, the simulated single gamma-ray incidence response\nfunction only depends on the crystal configuration, so the method could be\nextended to any PET scanners with the same detector crystal configuration.\n",
        "  We show the observation of the coexistence of bulk superconductivity and\nferromagnetism in CeO1-xFxBiS2(x = 0 - 1.0) prepared by annealing under\nhigh-pressure. In CeO1-xFxBiS2 system, both superconductivity and two types of\nferromagnetism with respective magnetic transition temperatures of 4.5 K and\n7.5 K are induced upon systematic F substitution. This fact suggests that\ncarriers generated by the substitution of O by F are supplied to not only the\nBiS2 superconducting layers but also the CeO blocking layers. Furthermore, the\nhighest superconducting transition temperature is observed when the\nferromagnetism is also enhanced, which implies that superconductivity and\nferromagnetism are linked to each other in the CeO1-xFxBiS2 system.\n",
        "  Tracing molecular hydrogen content with carbon monoxide in low-metallicity\ngalaxies has been exceedingly difficult. Here we present a new effort, with\nIRAM 30-m observations of 12CO(1-0) of a sample of 8 dwarf galaxies having\noxygen abundances ranging from 12+logO/H=7.7 to 8.4. CO emission is detected in\nall galaxies, including the most metal-poor galaxy of our sample (0.1 Zsun); to\nour knowledge this is the largest number of 12CO(1-0) detections ever reported\nfor galaxies with 12+logO/H<=8 (0.2 Zsun) outside the Local Group. We calculate\nstellar masses (Mstar) and star-formation rates (SFRs), and analyze our results\nby combining our observations with galaxy samples from the literature.\nExtending previous results for a correlation of the molecular gas depletion\ntime, tau(dep), with Mstar and specific SFR (sSFR), we find a variation in\ntau(dep) of a factor of 200 or more (from <50 Myr to 10 Gyr) over a spread of\n1000 in sSFR and Mstar. We exploit the variation of tau(dep) to constrain the\nCO-to-H2 mass conversion factor alpha(CO) at low metallicity, and assuming a\npower-law variation find alpha(CO) \\propto (Z/Zsun)^1.9, similar to results\nbased on dust continuum measurements compared with gas mass. By including HI\nmeasurements, we show that the fraction of total gas mass relative to the\nbaryonic mass is higher in galaxies that are metal poor, of low mass, and of\nhigh sSFR. Finally, comparisons of the data with star-formation models of the\nmolecular gas phases suggest that, at metallicities Z/Zsun<=0.2, there are some\ndiscrepancies with model predictions.\n",
        "  In 2016, UNSCEAR published an attachment to its Fukushima 2015 White Paper,\nentitled \"Development of isodose maps representing annual external exposure in\nJapan as a function of time,\" in which the committee presented annual\nadditional 1 mSv effective dose ab extra isodose lines for 1, 3, 5, 10, 30, 50\nyears after the accident, based on the soil deposition data of radionuclides\nwithin 100 km from FDNPP. Meanwhile, the median of the ratio, c, between the\nexternal effective dose rates and the ambient dose equivalent rates at 1 m\nabove the ground obtained by the airborne monitoring has been established to be\nc~0.15. We here compare the UNSCEAR predictions with respect to estimates based\non the airborne monitoring. Although both methods and data used in the two\napproaches are di erent, the resultant contours show relatively good agreement.\nHowever, to improve the accuracy of long-term annual effective isodose lines,\nfeedback from continuous measurements such as airborne monitoring is important.\n",
        "  Group living animals form aggregations and flocks that remain cohesive in\nspite of internal movements of individuals. This is possible because individual\ngroup members repeatedly adjust their position and motion in response to the\nposition and motion of other group members. Here we develop a theoretical\napproach to address the question, what general features -- if any -- underlie\nthe interaction rules that mediate group stability in animals of all species?\nWe do so by considering how the spatial organisation of a group would change in\nthe complete absence of interactions. Without interactions, a group would\ndisperse in a way that can be easily characterised in terms of Fick's diffusion\nequations. We can hence address the inverse theoretical problem of finding the\nindividual-level interaction responses that are required to counterbalance\ndiffusion and to preserve group stability. We show that an individual-level\nresponse to neighbour densities in the form of Weber's law (a 'universal' law\ndescribing the functioning of the sensory systems of animals of all species)\nresults in an 'anti-diffusion' term at the group level. On short time scales,\nthis anti-diffusion restores the initial group configuration in a way which is\nreminiscent of methods for image deblurring in image processing. We also show\nthat any non-homogeneous, spatial density distribution can be preserved over\ntime if individual movement patterns have the form of a Weber's law response.\nWeber's law describes the fundamental functioning of perceptual systems. Our\nstudy indicates that it is also a necessary -- but not sufficient -- feature of\ncollective interactions in stable animal groups.\n",
        "  For a given set $\\mathcal{L}$ of species and a set $\\mathcal{T}$ of triplets\non $\\mathcal{L}$, one wants to construct a phylogenetic network which is\nconsistent with $\\mathcal{T}$, i.e which represents all triplets of\n$\\mathcal{T}$. The level of a network is defined as the maximum number of\nhybrid vertices in its biconnected components. When $\\mathcal{T}$ is dense,\nthere exist polynomial time algorithms to construct level-$0,1,2$ networks (Aho\net al. 81, Jansson et al. 04, Iersel et al. 08). For higher levels, partial\nanswers were obtained by Iersel et al. 2008 with a polynomial time algorithm\nfor simple networks. In this paper, we detail the first complete answer for the\ngeneral case, solving a problem proposed by Jansson et al. 2004: for any $k$\nfixed, it is possible to construct a minimum level-$k$ network consistent with\n$\\mathcal{T}$, if there is any, in time\n$O(|\\mathcal{T}|^{k+1}n^{\\lfloor\\frac{4k}{3}\\rfloor+1})$. This is an improved\nresult of our preliminary version presented at CPM'2009.\n",
        "  We publicly release a new large-scale dataset, called SearchQA, for machine\ncomprehension, or question-answering. Unlike recently released datasets, such\nas DeepMind CNN/DailyMail and SQuAD, the proposed SearchQA was constructed to\nreflect a full pipeline of general question-answering. That is, we start not\nfrom an existing article and generate a question-answer pair, but start from an\nexisting question-answer pair, crawled from J! Archive, and augment it with\ntext snippets retrieved by Google. Following this approach, we built SearchQA,\nwhich consists of more than 140k question-answer pairs with each pair having\n49.6 snippets on average. Each question-answer-context tuple of the SearchQA\ncomes with additional meta-data such as the snippet's URL, which we believe\nwill be valuable resources for future research. We conduct human evaluation as\nwell as test two baseline methods, one simple word selection and the other deep\nlearning based, on the SearchQA. We show that there is a meaningful gap between\nthe human and machine performances. This suggests that the proposed dataset\ncould well serve as a benchmark for question-answering.\n",
        "  Purpose. - Radiotherapy is an important treatment for prostate cancer.During\ntreatment sessions, bladder and rectal repletion is difficult to quantify and\ncannot be measured with a single and initial CT scan acquisition. Some methods,\nsuch as image-guided radiation therapy and dose-guided radiation therapy, aimto\ncompensate thismissing information through periodic CT acquisitions. The aimis\nto adapt patient's position, beam configuration or prescribed dose for a\ndosimetric compliance. Methods. -We evaluated organmotion (and repletion) for\n54 patients after having computed the original ballistic on a new CT scan\nacquisition. A new delineation was done on the prostate, bladder and rectum to\ndetermine the newdisplacements and define organ dosesmistakes (equivalent\nuniformdose, average dose and dose-volume histograms). Results. - The new CT\nacquisitions confirmed that bladder and rectal volumes were not constant during\nsessions. Some cases showed that previously validated treatment plan became\nunsuitable. A proposed solution is to correct dosimetries when bladder volume\nmodifications are significant. The result is an improvement for the stability\nof bladder doses, D50 error is reduced by 25.3%, mean dose error by 5.1% and\nequivalent uniform dose error by 2.6%. For the rectum this method decreases\nerrors by only 1%. This process can reduce the risk of mismatch between the\ninitial scan and following treatment sessions. Conclusion. - For the\nproposedmethod, the cone-beamCT is necessary to properly position the isocenter\nand to quantify bladder and rectal volume variation and deposited doses. The\ndosimetries are performed in the event that bladder (or rectum) volume\nmodification limits are exceeded. To identify these limits, we have calculated\nthat a tolerance of 10% for the equivalent uniformdose (compared to the initial\nvalue of the first dosimetry), this represents 11% of obsolete dosimetries for\nthe bladder, and 4% for the rectum.\n",
        "  Language segmentation consists in finding the boundaries where one language\nends and another language begins in a text written in more than one language.\nThis is important for all natural language processing tasks. The problem can be\nsolved by training language models on language data. However, in the case of\nlow- or no-resource languages, this is problematic. I therefore investigate\nwhether unsupervised methods perform better than supervised methods when it is\ndifficult or impossible to train supervised approaches. A special focus is\ngiven to difficult texts, i.e. texts that are rather short (one sentence),\ncontaining abbreviations, low-resource languages and non-standard language. I\ncompare three approaches: supervised n-gram language models, unsupervised\nclustering and weakly supervised n-gram language model induction. I devised the\nweakly supervised approach in order to deal with difficult text specifically.\nIn order to test the approach, I compiled a small corpus of different text\ntypes, ranging from one-sentence texts to texts of about 300 words. The weakly\nsupervised language model induction approach works well on short and difficult\ntexts, outperforming the clustering algorithm and reaching scores in the\nvicinity of the supervised approach. The results look promising, but there is\nroom for improvement and a more thorough investigation should be undertaken.\n",
        "  A realistic computer-simulation of a breast computed tomography (CT) system\nand subject is constructed. The model is used to investigate the optimal number\nof views for the scan given a fixed total X-ray fluence. The reconstruction\nalgorithm is based on accurate solution to a constrained, TV-minimization\nproblem, which has received much interest recently for sparse-view CT data.\n",
        "  Superconductors can support large dissipation-free electrical currents only\nif vortex lines are effectively immobilized by material defects. Macroscopic\ncritical currents depend on elemental interactions of vortices with individual\npinning centers. Pinning mechanisms are nontrivial for large-size defects such\nas self-assembled nanoparticles. We investigate the problem of a vortex system\ninteracting with an isolated defect using time-dependent Ginzburg-Landau\nsimulations. In particular, we study the instability-limited depinning process\nand extract the dependence of the pin-breaking force on inclusion size and\nanisotropy for an \\emph{isolated vortex line}. In the case of a \\emph{vortex\nlattice} interacting with a large isolated defect, we find a series of\nfirst-order phase transitions at well-defined magnetic fields, when the number\nof vortex lines occupying the inclusion changes. The pin-breaking force has\nsharp local minima at those fields. As a consequence, in the case of isolated\nidentical large-size defects, the field dependence of the critical current is\ncomposed of a series of peaks located in between the occupation-number\ntransition points.\n",
        "  Relational DBMSs continue to dominate the database market, and inference\nproblem on external schema of relational DBMS's is still an important issue in\nterms of data privacy.Especially for the last 10 years, external schema\nconstruction for application-specific database usage has increased its\nindependency from the conceptual schema, as the definitions and implementations\nof views and procedures have been optimized. This paper offers an optimized\ndecomposition strategy for the external schema, which concentrates on the\nprivacy policy and required associations of attributes for the intended user\nroles. The method proposed in this article performs a proactive decomposition\nof the external schema, in order to satisfy both the forbidden and required\nassociations of attributes.Functional dependency constraints of a database\nschema can be represented as a graph, in which vertices are attribute sets and\nedges are functional dependencies. In this representation, inference problem\ncan be defined as a process of searching a subtree in the dependency graph\ncontaining the attributes that need to be related. The optimized decomposition\nprocess aims to generate an external schema, which guarantees the prevention of\nthe inference of the forbidden attribute sets while guaranteeing the\nassociation of the required attribute sets with a minimal loss of possible\nassociation among other attributes, if the inhibited and required attribute\nsets are consistent with each other. Our technique is purely proactive, and can\nbe viewed as a normalization process. Due to the usage independency of external\nschema construction tools, it can be easily applied to any existing systems\nwithout rewriting data access layer of applications. Our extensive experimental\nanalysis shows the effectiveness of this optimized proactive strategy for a\nwide variety of logical schema volumes.\n",
        "  This manuscript provides a response to a recent report by Mazzone et al.\navailable online on arXiv that, in turn, tentatively aims at demonstrating the\ninefficacy of proton boron capture in hadrotherapy. We clarify that Mazzone et\nal. do not add any scientific or technical insights to the points extensively\ndiscussed in the original manuscript by Cirrone et al., and/or in the series of\niterations had with the Referee, which ultimately lead to the publication of\nour original and pioneering experimental work. Here we summarize some of the\nkey points of the long scientific debate we had during the review process of\npaper by Cirrone et al., which are very similar to the considerations presented\nby Mazzone et al.. In conclusion, no quantitative explanation of our robust\nexperimental achievements presented in Cirrone et al. is provided in Mazzone et\nal.\n",
        "  We consider the classical tree edit distance between ordered labeled trees,\nwhich is defined as the minimum-cost sequence of node edit operations that\ntransform one tree into another. The state-of-the-art solutions for the tree\nedit distance are not satisfactory. The main competitors in the field either\nhave optimal worst-case complexity, but the worst case happens frequently, or\nthey are very efficient for some tree shapes, but degenerate for others. This\nleads to unpredictable and often infeasible runtimes. There is no obvious way\nto choose between the algorithms. In this paper we present RTED, a robust tree\nedit distance algorithm. The asymptotic complexity of RTED is smaller or equal\nto the complexity of the best competitors for any input instance, i.e., RTED is\nboth efficient and worst-case optimal. We introduce the class of LRH\n(Left-Right-Heavy) algorithms, which includes RTED and the fastest tree edit\ndistance algorithms presented in literature. We prove that RTED outperforms all\npreviously proposed LRH algorithms in terms of runtime complexity. In our\nexperiments on synthetic and real world data we empirically evaluate our\nsolution and compare it to the state-of-the-art.\n",
        "  X-ray imaging dose from serial cone-beam CT (CBCT) scans raises a clinical\nconcern in most image guided radiation therapy procedures. It is the goal of\nthis paper to develop a fast GPU-based algorithm to reconstruct high quality\nCBCT images from undersampled and noisy projection data so as to lower the\nimaging dose. For this purpose, we have developed an iterative tight frame (TF)\nbased CBCT reconstruction algorithm. A condition that a real CBCT image has a\nsparse representation under a TF basis is imposed in the iteration process as\nregularization to the solution. To speed up the computation, a multi-grid\nmethod is employed. Our GPU implementation has achieved high computational\nefficiency and a CBCT image of resolution 512\\times512\\times70 can be\nreconstructed in ~5 min. We have tested our algorithm on a digital NCAT phantom\nand a physical Catphan phantom. It is found that our TF-based algorithm is able\nto reconstrct CBCT in the context of undersampling and low mAs levels. We have\nalso quantitatively analyzed the reconstructed CBCT image quality in terms of\nmodulation-transfer-function and contrast-to-noise ratio under various scanning\nconditions. The results confirm the high CBCT image quality obtained from our\nTF algorithm. Moreover, our algorithm has also been validated in a real\nclinical context using a head-and-neck patient case. Comparisons of the\ndeveloped TF algorithm and the current state-of-the-art TV algorithm have also\nbeen made in various cases studied in terms of reconstructed image quality and\ncomputation efficiency.\n",
        "  This survey paper contains an elementary exposition of Casson and Rivin's\ntechnique for finding the hyperbolic metric on a 3-manifold M with toroidal\nboundary. We also survey a number of applications of this technique.\n  The method involves subdividing M into ideal tetrahedra and solving a system\nof gluing equations to find hyperbolic shapes for the tetrahedra. The gluing\nequations decompose into a linear and non-linear part. The solutions to the\nlinear equations form a convex polytope A. The solution to the non-linear part\n(unique if it exists) is a critical point of a certain volume functional on\nthis polytope. The main contribution of this paper is an elementary proof of\nRivin's theorem that a critical point of the volume functional on A produces a\ncomplete hyperbolic structure on M.\n",
        "  We predict a complete TM-TE transformation of the polarization of terahertz\nelectromagnetic waves reflected from a strongly anisotropic boundary of a\nlayered superconductor. We consider the case when the wave is incident on the\nsuperconductor from a dielectric prism separated from the sample by a thin\nvacuum gap. The physical origin of the predicted phenomenon is similar to the\nWood anomalies known in optics, and is related to the resonance excitation of\nthe oblique surface waves. We also discuss the dispersion relation for these\nwaves, propagating along the boundary of the superconductor at some angle with\nrespect to the anisotropy axis, as well as their excitation by the\nattenuated-total-reflection method.\n",
        "  We argue the usefulness of Gaifman graphs of first-order relational\nstructures as an exploratory data analysis tool. We illustrate our approach\nwith cases where the modular decompositions of these graphs reveal interesting\nfacts about the data. Then, we introduce generalized notions of Gaifman graphs,\nenhanced with quantitative information, to which we can apply more general,\nexisting decomposition notions via 2-structures; thus enlarging the analytical\ncapabilities of the scheme. The very essence of Gaifman graphs makes this\napproach immediately appropriate for the multirelational data framework.\n",
        "  Quasi-alternating links are homologically thin for both Khovanov homology and\nknot Floer homology. We show that every quasi-alternating link gives rise to an\ninfinite family of quasi-alternating links obtained by replacing a crossing\nwith an alternating rational tangle. Consequently, we show that many pretzel\nlinks are quasi-alternating, and we determine the thickness of Khovanov\nhomology for \"most\" pretzel links with arbitrarily many strands.\n",
        "  An ultrathin superconducting bilayer creates a coreless fractional vortex\nwhen only the second layer has a hole. The quantization is broken by the hole,\nand the normal core disappears. The magnetic flux is no longer confined near\nthe normal core, and its density profile around the hole becomes similar to\nthat of a cr\\`eme caramel; the divergence of the magnetic flux density is\ntruncated around the center. We propose basic design of a practical device to\nrealize a coreless fractional vortex.\n",
        "  Spatial heterogeneity plays an important role in complex ecosystem dynamics,\nand therefore is also an important consideration in sustainable resource\nmanagement. However, little is known about how spatial effects can influence\nmanagement targets derived from a non-spatial harvest model. Here, we extended\nthe Schaefer model, a conventional non-spatial harvest model that is widely\nused in resource management, to a spatially-explicit harvest model by\nintegrating environmental heterogeneities, as well as species exchange between\npatches. By comparing the maximum sustainable yields (MSY), one of the central\nmanagement targets in resource management, obtained from the spatially extended\nmodel with that of the conventional model, we examined the effect of spatial\nheterogeneity. When spatial heterogeneity exists, we found that the Schaefer\nmodel tends to overestimate the MSY, implying potential for causing\noverharvesting. In addition, by assuming a well-mixed population in the\nheterogeneous environment, we showed analytically that the Schaefer model\nalways overestimate the MSY, regardless of the number of patches existing. The\ndegree of overestimation becomes significant when spatial heterogeneity is\nmarked. Collectively, these results highlight the importance of integrating the\nspatial structure to conduct sustainable resource management.\n",
        "  Published evidence for the human-mediated extinction of megafauna is examined\nand is found to be unsubstantiated. It is shown that the claimed evidence is\nnot based on data describing the growth of human population but on the\nfabricated data. However, even these fabricated data, which were claimed to\nsupport the human-induced extinction of megafauna, contradict this claim. The\nbelief in the human-induced extinction of megafauna appears to be so strong\nthat even contradicting evidence based on the fabricated data is interpreted as\nthe evidence supporting this belief.\n",
        "  Sarin explosive dispersion simulations indicate that the effects of military,\nterrorist and accidental explosions on Sarin storage areas could be devastating\nat large distances from ground zero as they would practically amount to\ngigantic lethal chemical weapon explosions. As a case study, the April 14, 2018\nmilitary strikes on the alleged Syrian chemical weapons sites are investigated\ndue to their high relevance and similarity to the Sarin releases occurred in\nthe US demolition operations at the Khamisiyah Pit in Iraq (1991) believed to\nhave been a possible source of the Gulf War Syndrome. The results show that\neven if a few kilograms of Sarin had been explosively released from the alleged\nchemical weapons sites targeted in Syria then hundreds to thousands of people\nwould have experienced lethal or serious irreversible health effects in Syria.\nThe prospect of the appearance of a Sarin-induced Syrian War Syndrome is also\ndiscussed for the first time.\n",
        "  Inelastic x-ray scattering (IXS) was used to study the Cu-O bond-stretching\nvibrations in the static stripe phase compound La1.48Nd0.4Sr0.12CuO4. It was\nfound that the intrinsic width in Q-space of the previously reported huge\nanomalous phonon softening and broadening is approximately 0.08r.l.u HWHM. A\ndetailed comparison was also made to inelastic neutron scattering (INS)\nstudies, which indicate a two-peak lineshape (with superimposed broad and\nnarrow peaks) in the vicinity of the anomaly. The high resolution IXS data show\nthat the narrow peak is mostly an artifact of the poor transverse Q-resolution\nof INS. Otherwise the agreement between the INS and IXS was excellent.\n",
        "  Integrating data is a basic concern in many accredited laboratories that\nperform a large variety of measurements. However, the present working style in\nengineering faculties does not focus much on this aspect. To deal with this\nchallenge, we developed an educational platform that allows characterization of\nacquisition ensembles, generation of Web pages for lessons, as well as\ntransformation of measured data and storage in a common format. As generally we\nhad to develop individual parsers for each instrument, we also added the\npossibility to integrate the LabVIEW workbench, often used for rapid\ndevelopment of applications in electrical engineering and automatic control.\nThis paper describes how we configure the platform for specific equipment, i.e.\nhow we model it, how we create the learning material and how we integrate the\nresults in a central database. It also introduces a case study for collecting\ndata from a thermocouple-based acquisition system based on LabVIEW, used by\nstudents for a laboratory of measurement technologies and transducers.\n",
        "  We present the Team Keck Redshift Survey 2 (TKRS2), a near-infrared spectral\nobserving program targeting selected galaxies within the CANDELS subsection of\nthe GOODS-North Field. The TKRS2 program exploits the unique capabilities of\nMOSFIRE, an infrared multi-object spectrometer which entered service on the\nKeck I telescope in 2012 and contributes substantially to the study of galaxy\nspectral features at redshifts inaccessible to optical spectrographs. The TKRS2\nproject targets 97 galaxies drawn from samples that include z~2 emission-line\ngalaxies with features observable in the JHK bands as well as lower-redshift\ntargets with features in the Y band. We present a detailed measurement of\nMOSFIRE's sensitivity as a function of wavelength, including the effects of\ntelluric features across the YJHK filters. The largest utility of our survey is\nin providing rest-frame-optical emission lines for z>1 galaxies, and we\ndemonstrate that the ratios of strong, optical emission lines of z~2 galaxies\nsuggest the presence of either higher N/O abundances than are found in z~0\ngalaxies or low-metallicity gas ionized by an active galactic nucleus. We have\nreleased all TKRS2 data products into the public domain to allow researchers\naccess to representative raw and reduced MOSFIRE spectra.\n",
        "  We give a simple criterion when a Gluck twisting an odd smooth 4-manifold\nalong a 2-sphere $S\\subset X$ does not change its diffeomorphism type. We\nobtain this by handlebody techniques and plug twisting operation, getting a\nslightly stronger version of the known fact that Gluck twisting of a 2-sphere\n$S\\subset X$ of a compact smooth 4-manifold with an odd spherical class, in the\ncomplement of S, does not change the diffeomorphism type of X. This is the best\npossible result on Gluck twisting manifolds with odd homology classes.\n",
        "  In this paper, the stability analysis of an anaerobic digestion process is\npresented. The analysis is performed using a simplified mathematical model,\nwhich includes explicit temperature and pH dependence on kinetic growth rates.\nA detailed nonlinear and bifurcation analyses are performed in order to study\nthe effects of temperature and pH parameters on the process behaviour. In\naddition, both safety and optimal operation regions of the bioreactor are\nestablished. Based on bifurcation diagrams, it is observed that the washout\ncondition occurs by combining a fold bifurcation and a transcritical\nbifurcation. Consequently, to prevent the washout and guarantee optimal\noperation conditions of the bioreactor, a risk criterion oriented to monitor\nthe bioprocess on-line is proposed, allowing to detect the system\ndestabilisation.\n",
        "  Thurston's Circle Pattern Theorem studies existence and rigidity of circle\npatterns of a given combinatorial type and the given non-obtuse exterior\nintersection angles. Using topological degree theory, variational principle,\nTeichmuller theory, and Sard's Theorem, this paper generalizes Circle Pattern\nTheorem to the case of obtuse exterior intersection angles.\n",
        "  Understanding the relationship between genomic variation and variation in\nphenotypes for quantitative traits such as physiology, yield, fitness or\nbehavior, will provide important insights for both predicting adaptive\nevolution and for breeding schemes. A particular question is whether the\ngenetic variation that influences quantitative phenotypes is typically the\nresult of one or two mutations of large effect, or multiple mutations of small\neffect. In this paper we explore this issue using the wild model legume\nMedicago truncatula. We show that phenotypes, such as quantitative disease\nresistance, can be well-predicted using genome-wide patterns of admixture, from\nwhich it follows that there must be many mutations of small effect. Our\nfindings prove the potential of our novel 'whole-genome modeling' -WhoGEM-\nmethod and experimentally validate, for the first time, the infinitesimal model\nas a mechanism for adaptation of quantitative phenotypes in plants. This\ninsight can accelerate breeding and biomedicine research programs.\n",
        "  We describe some properties of noncompact Euclidean cone manifolds with cone\nangles less than c less than 2pi and singular locus a submanifold. More\nprecisely, we describe its structure outside a compact set. As a corollary we\nclassify those with cone angles less than 3pi/2 and those with all cone angles\nequal to 3pi/2.\n",
        "  We introduce Rabbit, a combinator-based query language. Rabbit is designed to\nlet data analysts and other accidental programmers query complex structured\ndata.\n  We combine the functional data model and the categorical semantics of\ncomputations to develop denotational semantics of database queries. In Rabbit,\na query is modeled as a Kleisli arrow for a monadic container determined by the\nquery cardinality. In this model, monadic composition can be used to navigate\nthe database, while other query combinators can aggregate, filter, sort and\npaginate data; construct compound data; connect self-referential data; and\nreorganize data with grouping and data cube operations. A context-aware query\nmodel, with the input context represented as a comonadic container, can express\nquery parameters and window functions. Rabbit semantics enables pipeline\nnotation, encouraging its users to construct database queries as a series of\ndistinct steps, each individually crafted and tested. We believe that Rabbit\ncan serve as a practical tool for data analytics.\n",
        "  Most stars do not form in isolation, but as part of a star cluster or\nassociation. These young stars are initially surrounded by protoplanetary\ndiscs. In these cluster environments tidal interactions with other cluster\nmembers can alter the disc properties. Besides the disc frequency, its mass,\nangular momentum, and energy, in particular the disc's size is prone to being\nchanged by a passing star. So far the change in disc size was only investigated\nfor a small number of very specific encounters. Several studies investigated\nthe effect of the cluster environment on the sizes of planetary systems, like\nour own solar system, based on a generalisation of information from this\nlimited sample. We performed numerical simulations covering the wide parameter\nspace typical for young star clusters, to test the validity of this approach.\nHere the sizes of discs after encounters are presented, based on a size\ndefinition which is comparable to that one used in observational studies. We\nfind that, except for encounters between equal-mass stars, the usually applied\nestimates are insufficient. They tend to severely overestimate the remaining\ndisc size. We show that the disc size after an encounter can be described by a\nrelatively simple dependence on the periastron distance and the mass ratio of\nthe encounter partners. This knowledge allows, for example, to pin down the\ntypes of encounter possibly responsible for the structure of today's solar\nsystem.\n",
        "  Let (V,W;F) be a weakly reducible, unstabilized, genus three Heegaard\nsplitting in an orientable, irreducible 3-manifold M. In this article, we prove\nthat either the disk complex D(F) is contractible or F is critical. Hence, the\ntopological index of F is two if F is topologically minimal.\n",
        "  UGC 12281 has been classified as having a pure disk and being a low surface\nbrightness galaxy (LSBG), thus being an obvious member of the so-called\nsuperthin galaxies. At the same time it represents an extremely untypical type\nof LSBG due to its remarkable amount of current star formation and evidence for\nextraplanar ionized gas. This makes it become a perfect tool to investigate the\ntriggering of star formation in LSB galaxies, located in an alleged isolated\narea. By means of deep photometry and long-slit spectroscopy we analyse the\nH$\\alpha$ halo and verify the existence of a potential dwarf companion which we\nfound on processed SDSS images.\n",
        "  Crowdsourced entity extraction is often used to acquire data for many\napplications, including recommendation systems, construction of aggregated\nlistings and directories, and knowledge base construction. Current solutions\nfocus on entity extraction using a single query, e.g., only using \"give me\nanother restaurant\", when assembling a list of all restaurants. Due to the cost\nof human labor, solutions that focus on a single query can be highly\nimpractical.\n  In this paper, we leverage the fact that entity extraction often focuses on\n{\\em structured domains}, i.e., domains that are described by a collection of\nattributes, each potentially exhibiting hierarchical structure. Given such a\ndomain, we enable a richer space of queries, e.g., \"give me another Moroccan\nrestaurant in Manhattan that does takeout\". Naturally, enabling a richer space\nof queries comes with a host of issues, especially since many queries return\nempty answers. We develop new statistical tools that enable us to reason about\nthe gain of issuing {\\em additional queries} given little to no information,\nand show how we can exploit the overlaps across the results of queries for\ndifferent points of the data domain to obtain accurate estimates of the gain.\nWe cast the problem of {\\em budgeted entity extraction} over large domains as\nan adaptive optimization problem that seeks to maximize the number of extracted\nentities, while minimizing the overall extraction costs. We evaluate our\ntechniques with experiments on both synthetic and real-world datasets,\ndemonstrating a yield of up to 4X over competing approaches for the same\nbudget.\n",
        "  We present Spitzer images of the Taurus Complex (TC) and take advantage of\nthe sensitivity and spatial resolution of the observations to characterize the\ndiffuse IR emission across the cloud. This work highlights evidence of dust\nevolution within the translucent sections of the archetype reference for\nstudies of quiescent molecular clouds. We combine Spitzer 160 um and IRAS 100\num observations to produce a dust temperature map and a far-IR dust opacity map\nat 5' resolution. The average dust temperature is about 14.5K with a dispersion\nof +/-1K across the cloud. The far-IR dust opacity is a factor 2 larger than\nthe average value for the diffuse ISM. This opacity increase and the\nattenuation of the radiation field (RF) both contribute to account for the\nlower emission temperature of the large grains. The structure of the TC\nsignificantly changes in the mid-IR images that trace emission from PAHs and\nVSGs. We focus our analysis of the mid-IR emission to a range of ecliptic\nlatitudes where the zodiacal light residuals are small. Within this cloud area,\nthere are no 8 and 24 um counterparts to the brightest 160 um emission\nfeatures. Conversely, the 8 and 24 um images reveal filamentary structure that\nis strikingly inconspicuous in the 160 um and extinction maps. The IR colors\nvary over sub-parsec distances across this filamentary structure. We compare\nthe observed colors with model calculations quantifying the impact of the RF\nintensity and the abundance of stochastically heated particles on the dust SED.\nTo match the range of observed colors, we have to invoke variations by a factor\nof a few of both the interstellar RF and the abundance of PAHs and VSGs. We\nconclude that within this filamentary structure a significant fraction of the\ndust mass cycles in and out the small size end of the dust size distribution.\n",
        "  Photoacoustic computed tomography (PACT) is an emerging computed imaging\nmodality that exploits optical contrast and ultrasonic detection principles to\nform images of the absorbed optical energy density within tissue. If the object\npossesses spatially variant acoustic properties that are unaccounted for by the\nreconstruction method, the estimated image can contain distortions. While\nreconstruction methods have recently been developed to compensate for this\neffect, they generally require the object's acoustic properties to be known a\npriori. To circumvent the need for detailed information regarding an object's\nacoustic properties, we previously proposed a half-time reconstruction method\nfor PACT. A half-time reconstruction method estimates the PACT image from a\ndata set that has been temporally truncated to exclude the data components that\nhave been strongly aberrated. However, this method can be improved upon when\nthe approximate sizes and locations of isolated heterogeneous structures, such\nas bones or gas pockets, are known. To address this, we investigate PACT\nreconstruction methods that are based on a variable data truncation (VDT)\napproach. The VDT approach represents a generalization of the half-time\napproach, in which the degree of temporal truncation for each measurement is\ndetermined by the distance between the corresponding ultrasonic transducer\nlocation and the nearest known bone or gas void location. Computer-simulated\nand experimental data are employed to demonstrate the effectiveness of the\napproach in mitigating artifacts due to acoustic heterogeneities.\n",
        "  Decision making is challenging when there is more than one criterion to\nconsider. In such cases, it is common to assign a goodness score to each item\nas a weighted sum of its attribute values and rank them accordingly. Clearly,\nthe ranking obtained depends on the weights used for this summation. Ideally,\none would want the ranked order not to change if the weights are changed\nslightly. We call this property {\\em stability} of the ranking. A consumer of a\nranked list may trust the ranking more if it has high stability. A producer of\na ranked list prefers to choose weights that result in a stable ranking, both\nto earn the trust of potential consumers and because a stable ranking is\nintrinsically likely to be more meaningful. In this paper, we develop a\nframework that can be used to assess the stability of a provided ranking and to\nobtain a stable ranking within an \"acceptable\" range of weight values (called\n\"the region of interest\"). We address the case where the user cares about the\nrank order of the entire set of items, and also the case where the user cares\nonly about the top-$k$ items. Using a geometric interpretation, we propose\nalgorithms that produce stable rankings. In addition to theoretical analyses,\nwe conduct extensive experiments on real datasets that validate our proposal.\n",
        "  We explore the task of multi-source morphological reinflection, which\ngeneralizes the standard, single-source version. The input consists of (i) a\ntarget tag and (ii) multiple pairs of source form and source tag for a lemma.\nThe motivation is that it is beneficial to have access to more than one source\nform since different source forms can provide complementary information, e.g.,\ndifferent stems. We further present a novel extension to the encoder- decoder\nrecurrent neural architecture, consisting of multiple encoders, to better solve\nthe task. We show that our new architecture outperforms single-source\nreinflection models and publish our dataset for multi-source morphological\nreinflection to facilitate future research.\n",
        "  Whole genome duplication (WGD) is one of the most important events in the\nmolecular evolution of organisms. In fish species, a WGD is considered to have\noccurred in the ancestral lineage of teleosts. Recent comprehensive ortholog\ncomparisons among teleost genomes have provided useful data and insights into\nthe fate of redundant genes generated by WGD. Based on these data, a\nmathematical model is proposed to explain the evolutionary scenario of genes\nafter WGD. The model is parameterized taking into account an equilibrium\nbetween i) rapid loss of either of the duplicate genes and ii) moderate\nfunctional differentiation of each of duplicate genes, both of which are\nfollowed by slow gene loss under purifying selection. This model predicts that,\nin the teleost lineage, a maximum of about 3000 gene pairs may have\ndifferentiated functionally during 90 million years after WGD. Thus, the\npresent study provides a possibility that the whole impact of WGD can be\nquantitatively assessed according to the model parameters, before details of\ngenomic structural changes or functional differentiation are investigated. If\nthe equilibrium model is valid not only for teleosts but also for other\nlineages that have undergone WGDs, correlations between the assessment indices\nand evolutionarily significant events, such as the diversification of species\nor the occurrence of novel phenotypes, could be tested and compared among those\nlineages.\n",
        "  Model-based language specification has applications in the implementation of\nlanguage processors, the design of domain-specific languages, model-driven\nsoftware development, data integration, text mining, natural language\nprocessing, and corpus-based induction of models. Model-based language\nspecification decouples language design from language processing and, unlike\ntraditional grammar-driven approaches, which constrain language designers to\nspecific kinds of grammars, it needs general parser generators able to deal\nwith ambiguities. In this paper, we propose Fence, an efficient bottom-up\nparsing algorithm with lexical and syntactic ambiguity support that enables the\nuse of model-based language specification in practice.\n",
        "  X-ray phase-contrast imaging has experienced rapid development over the last\nfew decades, and in this technology, the phase modulation strategy of\nphase-stepping is used most widely to measure the sample's phase signal.\nHowever, because of its discontinuous nature, phase-stepping has the defects of\nworse mechanical stability and high exposure dose, which greatly hinder its\nwide application in dynamic phase measurement and potential clinical\napplications. In this manuscript, we demonstrate preliminary research on the\nuse of integrating-bucket phase modulation method to retrieve the phase\ninformation in grating-based X-ray phase-contrast imaging. Experimental results\nshowed that our proposed method can be well employed to extract the\ndifferential phase-contrast image, compared with the current mostly used\nphase-stepping strategy, advantage of integrating-bucket phase modulation\ntechnique is that fast measurement and low dose are promising.\n",
        "  This is a survey talk on one of the best known quantum knot invariants, the\ncolored Jones polynomial of a knot, and its relation to the algebraic/geometric\ntopology and hyperbolic geometry of the knot complement. We review several\naspects of the colored Jones polynomial, emphasizing modularity, stability and\neffective computations. The talk was given in the Mathematische Arbeitstagung\nJune 24-July 1, 2011. Updated the bibliography.\n",
        "  Most existing models for multilingual natural language processing (NLP) treat\nlanguage as a discrete category, and make predictions for either one language\nor the other. In contrast, we propose using continuous vector representations\nof language. We show that these can be learned efficiently with a\ncharacter-based neural language model, and used to improve inference about\nlanguage varieties not seen during training. In experiments with 1303 Bible\ntranslations into 990 different languages, we empirically explore the capacity\nof multilingual language models, and also show that the language vectors\ncapture genetic relationships between languages.\n",
        "  This paper describes the Hangulphabet, a new writing system that should prove\nuseful in a number of contexts. Using the Hangulphabet, a user can instantly\nsee voicing, manner and place of articulation of any phoneme found in human\nlanguage. The Hangulphabet places consonant graphemes on a grid with the x-axis\nrepresenting the place of articulation and the y-axis representing manner of\narticulation. Each individual grapheme contains radicals from both axes where\nthe points intersect. The top radical represents manner of articulation where\nthe bottom represents place of articulation. A horizontal line running through\nthe middle of the bottom radical represents voicing. For vowels, place of\narticulation is located on a grid that represents the position of the tongue in\nthe mouth. This grid is similar to that of the IPA vowel chart (International\nPhonetic Association, 1999). The difference with the Hangulphabet being the\ntrapezoid representing the vocal apparatus is on a slight tilt. Place of\narticulation for a vowel is represented by a breakout figure from the grid.\nThis system can be used as an alternative to the International Phonetic\nAlphabet (IPA) or as a complement to it. Beginning students of linguistics may\nfind it particularly useful. A Hangulphabet font has been created to facilitate\nswitching between the Hangulphabet and the IPA.\n",
        "  Date City in Fukushima Prefecture has conducted a population-wide individual\ndose monitoring program after the Fukushima Daiichi Nuclear Power Plant\nAccident, which provides a unique and comprehensive data set of the individual\ndoses of citizens. The purpose of this paper, the first in the series, is to\nestablish a method for estimating effective doses based on the available\nambient dose rate survey data. We thus examined the relationship between the\nindividual external doses and the corresponding ambient doses assessed from\nairborne surveys. The results show that the individual doses were about 0.15\ntimes the ambient doses, the coefficient of 0.15 being a factor of 4 smaller\nthan the value employed by the Japanese government, throughout the period of\nthe airborne surveys used. The method obtained in this study could aid in the\nprediction of individual doses in the early phase of future radiological\naccidents involving large-scale contamination.\n",
        "  We explore the long-term evolution of the anisotropy in the velocity space of\nstar clusters starting with different structural and kinematical properties. We\nshow that the evolution of the radial anisotropy strength and its radial\nvariation within a cluster contain distinct imprints of the cluster initial\nstructural properties, dynamical history, and of the external tidal field of\nits host galaxy. Initially isotropic and compact clusters with small initial\nvalues of the ratio of the half-mass to Jacobi radius, $r_h/r_J$, develop a\nstrong radial anisotropy during their long-term dynamical evolution. Many\nclusters, if formed with small values of $r_h/r_J$, should now be characterized\nby a significant radial anisotropy increasing with the distance from the\ncluster centre, reaching its maximum at a distance between 0.2 $r_J$ and 0.4\n$r_J$, and then becoming more isotropic or mildly tangentially anisotropic in\nthe outermost regions. A similar radial variation of the anisotropy can also\nresult from an early violent relaxation phase. In both cases, as a cluster\ncontinues its evolution and loses mass, the anisotropy eventually starts to\ndecrease and the system evolves toward an isotropic velocity distribution.\nHowever, in order to completely erase the strong anisotropy developed by these\ncompact systems during their evolution, they must be in the advanced stages of\ntheir evolution and lose a large fraction of their initial mass. Clusters that\nare initially isotropic and characterized by larger initial values of\n$r_h/r_J$, on the other hand, never develop a significant radial anisotropy.\n",
        "  Anstee, Przyticki and Rolfsen introduced the idea of rotants, pairs of links\nrelated by a generalised form of link mutation. We exhibit infinitely many\npairs of rotants which can be distinguished by Khovanov homology, but not by\nthe Jones polynomial.\n",
        "  The European Commission's (EC) Directorate General for Translation, together\nwith the EC's Joint Research Centre, is making available a large translation\nmemory (TM; i.e. sentences and their professionally produced translations)\ncovering twenty-two official European Union (EU) languages and their 231\nlanguage pairs. Such a resource is typically used by translation professionals\nin combination with TM software to improve speed and consistency of their\ntranslations. However, this resource has also many uses for translation studies\nand for language technology applications, including Statistical Machine\nTranslation (SMT), terminology extraction, Named Entity Recognition (NER),\nmultilingual classification and clustering, and many more. In this reference\npaper for DGT-TM, we introduce this new resource, provide statistics regarding\nits size, and explain how it was produced and how to use it.\n",
        "  No abstract; review only\n",
        "  We analyse the rotation curves and gravitational stability of a sample of six\nbulgeless galaxies for which detailed images reveal no evidence for strong\nbars. We explore two scenarios: Newtonian dark matter models and MOdified\nNewtonian Dynamics (MOND). By adjusting the stellar mass-to-light ratio, dark\nmatter models can match simultaneously both the rotation curve and\nbar-stability requirements in these galaxies. To be consistent with stability\nconstraints, in two of these galaxies, the stellar mass-to-light ratio is a\nfactor of ~1.5-2 lower than the values suggested from galaxy colours. In\ncontrast, MOND fits to the rotation curves are poor in three galaxies, perhaps\nbecause the gas tracer contains noncircular motions. The bar stability analysis\nprovides a new observational test to MOND. We find that most of the galaxies\nunder study require abnormally-high levels of random stellar motions to be bar\nstable in MOND. In particular, for the only galaxy in the sample for which the\nline-of-sight stellar velocity dispersion has been measured (NGC 6503), the\nobserved velocity dispersion is not consistent with MOND predictions because it\nis far below the required value to guarantee bar stability. Precise\nmeasurements of mass-weighted velocity dispersions in (unbarred and bulgeless)\nspiral galaxies are crucial to test the consistency of MOND.\n",
        "  Low luminosity galaxies may be the building blocks of more luminous systems.\nSouthern African Large Telescope (SALT) observations of the low luminosity,\nearly-type galaxy NGC59 are obtained and analysed. These data are used to\nmeasure the stellar population parameters in the centre and off-centre regions\nof this galaxy, in order to uncover its likely star formation history. We find\nevidence of older stars, in addition to young stars in the emission line\nregions. The metallicity of the stellar population is constrained to be [Z/H] ~\n-1.1 to -1.6, which is extremely low, even for this low luminosity galaxy,\nsince it is not classed as a dwarf spheroidal galaxy. The measured [alpha/Fe]\nratio is sub-solar, which indicates an extended star formation history in\nNGC59. If such objects formed the building blocks of more massive, early-type\ngalaxies, then they must have been gaseous mergers, rather than dry mergers, in\norder to increase the metals to observed levels in luminous, early-type\ngalaxies.\n",
        "  For a nontrivial knot $K$, Negami found an upper bound on the stick number\n$s(K)$ in terms of its crossing number $c(K)$ which is $s(K) \\leq 2 c(K)$.\nLater, Huh and Oh utilized the arc index $\\alpha(K)$ to present a more precise\nupper bound $s(K) \\leq \\frac{3}{2} c(K) + \\frac{3}{2}$. Furthermore, Kim, No\nand Oh found an upper bound on the equilateral stick number $s_{=}(K)$ as\nfollows; $s_{=}(K) \\leq 2 c(K) +2$. As a sequel to this research program, we\nsimilarly define the stick number $s(G)$ and the equilateral stick number\n$s_{=}(G)$ of a spatial graph $G$, and present their upper bounds as follows;\n$$ s(G) \\leq \\frac{3}{2} c(G) + 2e + \\frac{3b}{2} -\\frac{v}{2}, $$ $$ s_{=}(G)\n\\leq 2 c(G) + 2e + 2b - k, $$ where $e$ and $v$ are the number of edges and\nvertices of $G$, respectively, $b$ is the number of bouquet cut-components, and\n$k$ is the number of non-splittable components.\n",
        "  The effects of magnetic vortices and nonmagnetic impurities on the low energy\nquasiparticle excitations and the spin-lattice relaxation rate are examined in\nthe iron-based superconductors for the $s_{\\pm}$-, $s$- and d-wave pairing\nsymmetries, respectively. The main effect of the vortices is to enhance the\nquasiparticle excitations and the spin-lattice relaxation rate for all\nsymmetries, and leads to a $T^{3}$ dependence of the relaxation rate followed\nby a nearly $T$-linearity at lower temperatures. This enhancement can only be\nseen for the $s_{\\pm}$- and d-wave symmetries in the presence of nonmagnetic\nimpurities. These results suggest that the $s_{\\pm}$-wave and d-wave pairing\nstates behave similarly in response to the magnetic field and nonmagnetic\nimpurities, therefore it may be impossible to distinguish them on the basis of\nthe measurements of spin-lattice relaxation rates when a magnetic field and/or\nimpurity scatterings are present.\n",
        "  In \"A survey on the Turaev genus of knots,\" Champanerkar and Kofman propose\nseveral open questions. The first asks whether the polynomial whose\ncoefficients count the number of quasi-trees of the all-A ribbon graph obtained\nfrom a diagram with minimal Turaev genus is an invariant of the knot. We answer\nnegatively by showing a counterexample obtained from the two diagrams of\n$8_{21}$ on the KnotAtlas and KnotScape.\n",
        "  We use inelastic neutron scattering to show that the spin waves in the iron\nchalcogenide Fe$_{1.05}$Te display novel dispersion clearly different from\nthose in the related iron pnictide systems. By fitting the spin waves to a\nHeisenberg Hamiltonian, we extract magnetic exchange couplings that are\ndramatically different from both predictions by density functional calculations\nand measurements on the iron pnictide CaFe$_2$As$_2$. While the\nnearest-neighbor exchange couplings in CaFe$_2$As$_2$ and Fe$_{1.05}$Te are\nquite different, their next-nearest-neighbor exchange couplings are similar.\nThese results suggest that superconductivity in the pnictides and chalcogenides\nshare a common magnetic origin that is intimately associated with the\nnext-nearest-neighbor magnetic coupling between the irons.\n",
        "  Ensemble methods using multiple classifiers have proven to be the most\nsuccessful approach for the task of Native Language Identification (NLI),\nachieving the current state of the art. However, a systematic examination of\nensemble methods for NLI has yet to be conducted. Additionally, deeper ensemble\narchitectures such as classifier stacking have not been closely evaluated. We\npresent a set of experiments using three ensemble-based models, testing each\nwith multiple configurations and algorithms. This includes a rigorous\napplication of meta-classification models for NLI, achieving state-of-the-art\nresults on three datasets from different languages. We also present the first\nuse of statistical significance testing for comparing NLI systems, showing that\nour results are significantly better than the previous state of the art. We\nmake available a collection of test set predictions to facilitate future\nstatistical tests.\n",
        "  We succeed in enhancement of a superconducting transition temperature (Tc)\nfor NdO0.7F0.3BiS2 single crystal by partial substitution of Pb for Bi. The Tc\nincreases with increasing Pb concentration until 6%. The maximum Tczero is 5.6\nK, which is the highest value among BiS2 based superconductors synthesized\nunder an ambient pressure. Pb substitution for Bi induces lattice shrinkage\nalong the c axis. These results reflect that superconductivity in this system\nis responsive to the lattice strain.\n",
        "  Water is a key molecule in many astrophysical studies. Its high dipole moment\nmakes this molecule to be subthermally populated under the typical conditions\nof most astrophysical objects. This motivated the calculation of various sets\nof collisional rate coefficients (CRC) for H$_2$O (with He or H$_2$) which are\nnecessary to model its rotational excitation and line emission. We performed\naccurate non--local non--LTE radiative transfer calculations using different\nsets of CRC in order to predict the line intensities from transitions that\ninvolve the lowest energy levels of H$_2$O (E $<$ 900 K). The results obtained\nfrom the different CRC sets are then compared using line intensity ratio\nstatistics. For the whole range of physical conditions considered in this work,\nwe obtain that the intensities based on the quantum and QCT CRC are in good\nagreement. However, at relatively low H$_2$ volume density ($n$(H$_2$) $<$\n10$^7$ cm$^{-3}$) and low water abundance ($\\chi$(H$_2$O) $<$ 10$^{-6}$), these\nphysical conditions being relevant to describe most molecular clouds, we find\ndifferences in the predicted line intensities of up to a factor of $\\sim$ 3 for\nthe bulk of the lines. Most of the recent studies interpreting early Herschel\nSpace Observatory spectra used the QCT CRC. Our results show that although the\nglobal conclusions from those studies will not be drastically changed, each\ncase has to be considered individually, since depending on the physical\nconditions, the use of the QCT CRC may lead to a mis--estimate of the water\nvapour abundance of up to a factor of $\\sim$ 3.\n",
        "  We study microwave scattering spectra of metallic stents in open air. We show\nthat they behave like dipole antennas in terms of microwave scattering and they\nexhibit characteristic resonant frequencies for a given nominal size. We obtain\na fair agreement between measured frequencies and the values provided by a\ntheoretical model for dipole antennas. This fact opens the door to obtaining\nmethods to detect structural distortions of stents within in vitro conditions.\nFinally we discuss the in vivo applicability of the suggested method in terms\nof our theoretical model and the skin depth of microwaves in biological\ntissues.\n",
        "  We give a short elementary proof that a Khovanov-type link homology\nconstructed from a diagonalisable Frobenius algebra is degenerate.\n",
        "  We present a simple yet effective approach for learning word sense\nembeddings. In contrast to existing techniques, which either directly learn\nsense representations from corpora or rely on sense inventories from lexical\nresources, our approach can induce a sense inventory from existing word\nembeddings via clustering of ego-networks of related words. An integrated WSD\nmechanism enables labeling of words in context with learned sense vectors,\nwhich gives rise to downstream applications. Experiments show that the\nperformance of our method is comparable to state-of-the-art unsupervised WSD\nsystems.\n",
        "  The acoustic-to-word model based on the connectionist temporal classification\n(CTC) criterion was shown as a natural end-to-end (E2E) model directly\ntargeting words as output units. However, the word-based CTC model suffers from\nthe out-of-vocabulary (OOV) issue as it can only model limited number of words\nin the output layer and maps all the remaining words into an OOV output node.\nHence, such a word-based CTC model can only recognize the frequent words\nmodeled by the network output nodes. Our first attempt to improve the\nacoustic-to-word model is a hybrid CTC model which consults a letter-based CTC\nwhen the word-based CTC model emits OOV tokens during testing time. Then, we\npropose a much better solution by training a mixed-unit CTC model which\ndecomposes all the OOV words into sequences of frequent words and multi-letter\nunits. Evaluated on a 3400 hours Microsoft Cortana voice assistant task, the\nfinal acoustic-to-word solution improves the baseline word-based CTC by\nrelative 12.09% word error rate (WER) reduction when combined with our proposed\nattention CTC. Such an E2E model without using any language model (LM) or\ncomplex decoder outperforms the traditional context-dependent phoneme CTC which\nhas strong LM and decoder by relative 6.79%.\n",
        "  A study of the ionized and neutral gas kinematics near 23 WR stars in the Irr\ngalaxy IC10 are provided. For most of the stars sings of the WR winds impact on\nthe interstellar medium were detected. A rough estimate of the power of wind WR\nstars is about ~(0.01-0.84) 10^38 erg / sec.\n",
        "  While confirming the long held view that viruses do not closely imitate the\nuse of their host's codon catalogue, Esposito and coworkers nevertheless\nconsider it surprising that, despite having the ability to infect the same\nhost, many mycobacteriophages share little or no genetic similarity (i.e.\nsimilarity in their GC contents and codon utilization patterns). Arguing\ncorrectly that efficient translation of a phage's proteins within a host is\nlikely to be optimized by the phage's ability to match the host's codon usage\npattern, it is concluded that the preferred host of many mycobacteriophages is\nnot Mycobacterium smegmatis, despite their having been isolated on that\norganism. Thus, a virus and its elusive preferred hosts would have had similar\nGC percentages and codon usages, but the same virus could still infect a\nless-preferred host (Mycobacterium smegmatis), where the virus-host similarity\nwould be less evident. However, there is another evolutionary interpretation.\n",
        "  We have investigated the pressure effect on the newly discovered samarium\ndoped La1-xSmxO0.5F0.5BiS2 superconductors. More than threefold increase in Tc\n(10.3 K) is observed with external pressure (at ~1.74 GPa at a rate of 4.08\nK/GPa)) for x = 0.2 composition. There is a concomitant large improvement in\nthe quality of the superconducting transition. Beyond this pressure Tc\ndecreases monotonously at the rate of -2.09 K/GPa. In the x = 0.8 sample, we do\nnot observe any enhancement in Tc with application of pressure (up to 1.76\nGPa). The semiconducting behavior observed in the normal state resistivity of\nboth of the samples is significantly subdued with the application of pressure\nwhich, if interpreted invoking thermal activation process, implies that the\nactivation energy gap of the carriers is significantly reduced with pressure.\nWe believe these observations should generate further interest in the\nLa1-xSmxO0.5F0.5BiS2 superconductors.\n",
        "  The oriented framed Homfly skein C of the annulus provides the natural\nparameter space for the Homfly satellite invariants of a knot. It contains a\nsubmodule C+ isomorphic to the algebra of the symmetric functions.\n  We collect and expand formulae relating elements expressed in terms of\nsymmetric functions to Turaev's geometrical basis of C+. We reformulate the\nformulae of Rosso and Jones for quantum sl(N) invariants of cables in terms of\nplethysms of symmetric functions, and use the connection between quantum sl(N)\ninvariants and C+ to give a formula for the satellite of a cable as an element\nof C+. We then analyse the case where a cable is decorated by the pattern which\ncorresponds to a power sum in the symmetric function interpretation of C+ to\nget direct relations between the Homfly invariants of some diagrams decorated\nby power sums.\n",
        "  The MgB2 superconductor was doped with electroluminescent Y2O3:Eu, to\nsynthesise a superconducting metamaterial. The temperature dependence of the\nresistivity of the superconductor indicates that the critical temperature (TC)\nof samples decrease when increasing the amount of doped Y2O3 nanorods, due to\nimpurity (Y2O3, MgO and YB4). However, the TC of the samples increase with\nincreasing amount of doped Y2O3:Eu3+ nanorods, which are opposite to doped Y2O3\nnanorods. Moreover, the transition temperature of the sample doped with 8 wt. %\nY2O3:Eu3+ nanorods is higher than those of doped and pure MgB2. The TC of the\nsample doped with 8 wt. % Y2O3:Eu3+ nanorods is 1.15 K higher than that of the\nsample doped with 8 wt. % Y2O3. The TC of sample doped with 8 wt. % Y2O3:Eu3+\nis 0.4 K higher than that of pure MgB2. Results indicate that doping\nelectroluminescent materials into MgB2 increases the transition temperature;\nthis novel strategy may also be applicable to other superconductors.\n",
        "  Existing studies on differential privacy mainly consider aggregation on data\nsets where each entry corresponds to a particular participant to be protected.\nIn many situations, a user may pose a relational algebra query on a sensitive\ndatabase, and desires differentially private aggregation on the result of the\nquery. However, no known work is capable to release this kind of aggregation\nwhen the query contains unrestricted join operations. This severely limits the\napplications of existing differential privacy techniques because many data\nanalysis tasks require unrestricted joins. One example is subgraph counting on\na graph. Existing methods for differentially private subgraph counting address\nonly edge differential privacy and are subject to very simple subgraphs. Before\nthis work, whether any nontrivial graph statistics can be released with\nreasonable accuracy under node differential privacy is still an open problem.\n  In this paper, we propose a novel differentially private mechanism to release\nan approximation to a linear statistic of the result of some positive\nrelational algebra calculation over a sensitive database. Unrestricted joins\nare supported in our mechanism. The error bound of the approximate answer is\nroughly proportional to the \\emph{empirical sensitivity} of the query --- a new\nnotion that measures the maximum possible change to the query answer when a\nparticipant withdraws its data from the sensitive database. For subgraph\ncounting, our mechanism provides the first solution to achieve node\ndifferential privacy, for any kind of subgraphs.\n",
        "  In this letter, we firstly report one unique object SDSS J0832+0643 with\nparticular features of narrow balmer emission lines: double-peaked narrow\nH\\alpha but single-peaked narrow H\\beta. The particular features can not be\nexpected by currently proposed kinematic models for double-peaked narrow\nemission lines, because the proposed kinematic models lead to similar line\nprofiles of narrow balmer emission lines. However, due to radiative transfer\neffects, the non-kinematic model can be naturally applied to well explain the\nparticular features of narrow balmer emission lines: larger optical depth in\nH\\alpha than 10 leads to observed double-peaked narrow H\\alpha, but smaller\noptical depth in H\\beta around 2 leads to observed single-peaked narrow H\\beta.\nTherefore, SDSS J0832+0643 can be used as strong evidence to support the\nnon-kinematic model for double-peaked narrow emission lines.\n",
        "  We report on the presence of large amounts of million-degree gas in the Milky\nWay's interstellar and circum-galactic medium. This gas (1) permeates both the\nGalactic plane and the halo, (2) extends to distances larger than 60-200 kpc\nfrom the center, and (3) its mass is sufficient to close the Galaxy's baryon\ncensus.\n  Moreover, we show that a vast, $\\sim 6$ kpc radius, spherically-symmetric\ncentral region of the Milky Way above and below the 0.16 kpc thick plane, has\neither been emptied of hot gas or the density of this gas within the cavity has\na peculiar profile, increasing from the center up to a radius of $\\sim 6$ kpc,\nand then decreasing with a typical halo density profile. This, and several\nother converging pieces of evidence, suggest that the current surface of the\ncavity, at 6 kpc from the Galaxy's center, traces the distant echo of a period\nof strong nuclear activity of our super-massive black-hole, occurred about 6\nMyrs ago.\n",
        "  Big array analytics is becoming indispensable in answering important\nscientific and business questions. Most analysis tasks consist of multiple\nsteps, each making one or multiple passes over the arrays to be analyzed and\ngenerating intermediate results. In the big data setting, I/O optimization is a\nkey to efficient analytics. In this paper, we develop a framework and\ntechniques for capturing a broad range of analysis tasks expressible in\nnested-loop forms, representing them in a declarative way, and optimizing their\nI/O by identifying sharing opportunities. Experiment results show that our\noptimizer is capable of finding execution plans that exploit nontrivial I/O\nsharing opportunities with significant savings.\n",
        "  Purpose: To derive a clinically-practical margin formula between clinical\ntarget volume (CTV) and planning target volume (PTV) for single-fraction\nstereotactic radiosurgery (SRS).Methods: In previous publications on the margin\nbetween the CTV and the PTV, a Gaussian function with zero mean was assumed for\nthe systematic error and the ma-chine systematic error was completely ignored.\nIn this work we adopted a Dirac delta function for the machine system-atic\nerror for a given machine with nonzero mean systematic error. Mathematical\nformulas for calculating the CTV-PTV margin for single-fraction SRS treatments\nwere proposed. Results: Margins for single fraction treatments were derived\nsuch that the CTVs received the prescribed dose in 95% of the SRS patients. The\nmargin defined in this study was machine specific and accounted for nonzero\nmean systematic error. The differences between our formulas and a previously\npublished formula were discussed. Conclusion: Clinical margin formulas were\nproposed for deter-mining the margin between the CTV and the PTV in SRS\ntreatments. Previous margin's recipes, being derived specifi-cally for\nconventional treatments, may be inappropriate for single-fraction SRS and could\nresult in geometric miss of the target and even treatment failure for machines\npossessing of large systematic errors.\n",
        "  A dessin is a 2-cell embedding of a connected bipartite graph into an\norientable closed surface. An automorphism of a dessin is a permutation of the\nedges of the underlying graph which preserves the colouring of the vertices and\nextends to an orientation-preserving self-homeomorphism of the supporting\nsurface. A dessin is regular if its automorphism group is transitive on the\nedges, and a regular dessin is totally symmetric if it is invariant under all\ndessin operations. Thus totally symmetric dessins possesses the highest level\nof external symmetry. In this paper we present a classification of totally\nsymmetric dessins with a nilpotent automorphism group of class three\n",
        "  Here we report a new class of superconductors prepared by high pressure\nsynthesis in the quaternary family ReFeAsO1-delta (Re = Sm, Nd, Pr, Ce, La)\nwithout fluorine doping. The onset superconducting critical temperature (Tc) in\nthese compounds increases with the reduction of Re atom size, and the highest\nTc obtained so far is 55 K in SmFeAsO1-delta. For the NdFeAsO1-delta system\nwith different oxygen concentration a dome-shaped phase diagram was found.\n",
        "  Coordination is a challenging everyday task; just think of the last time you\norganized a party or a meeting involving several people. As a growing part of\nour social and professional life goes online, an opportunity for an improved\ncoordination process arises. Recently, Gupta et al. proposed entangled queries\nas a declarative abstraction for data-driven coordination, where the difficulty\nof the coordination task is shifted from the user to the database.\nUnfortunately, evaluating entangled queries is very hard, and thus previous\nwork considered only a restricted class of queries that satisfy safety (the\ncoordination partners are fixed) and uniqueness (all queries need to be\nsatisfied). In this paper we significantly extend the class of feasible\nentangled queries beyond uniqueness and safety. First, we show that we can\nsimply drop uniqueness and still efficiently evaluate a set of safe entangled\nqueries. Second, we show that as long as all users coordinate on the same set\nof attributes, we can give an efficient algorithm for coordination even if the\nset of queries does not satisfy safety. In an experimental evaluation we show\nthat our algorithms are feasible for a wide spectrum of coordination scenarios.\n",
        "  The lack of parallel data for many language pairs is an important challenge\nto statistical machine translation (SMT). One common solution is to pivot\nthrough a third language for which there exist parallel corpora with the source\nand target languages. Although pivoting is a robust technique, it introduces\nsome low quality translations especially when a poor morphology language is\nused as the pivot between rich morphology languages. In this paper, we examine\nthe use of synchronous morphology constraint features to improve the quality of\nphrase pivot SMT. We compare hand-crafted constraints to those learned from\nlimited parallel data between source and target languages. The learned\nmorphology constraints are based on projected align- ments between the source\nand target phrases in the pivot phrase table. We show positive results on\nHebrew-Arabic SMT (pivoting on English). We get 1.5 BLEU points over a phrase\npivot baseline and 0.8 BLEU points over a system combination baseline with a\ndirect model built from parallel data.\n",
        "  Some implants have approximately a lifetime of 15 years. The femoral stem,\nfor example, should be made of 316L/316LN stainless steel. Fretting corrosion,\nfriction under small displacements, should occur during human gait, due to\nrepeated loadings and un-loadings, between stainless steel and bone for\ninstance. Some experimental investigations of fretting corrosion have been\npracticed. As well known, metallic alloys and especially stainless steels are\ncovered with a passive film that prevents from the corrosion and degradation.\nThis passive layer of few nanometers, at ambient temperature, is the key of our\ncivilization according to some authors. This work is dedicated to predict the\npassive layer thicknesses of stainless steel under fretting corrosion with a\nspecific emphasis on the role of proteins. The model is based on the Point\nDefect Model (micro scale) and an update of the model on the friction process\n(micro-macro scale). Genetic algorithm was used for finding solution of the\nproblem. The major results are, as expected from experimental results, albumin\nprevents from degradation at the lowest concentration of chlorides; an\nincubation time is necessary for degrading the passive film; under fretting\ncorrosion and high concentration of chlorides the passive behavior is\nannihilated.\n",
        "  This work focuses on the rapid development of linguistic annotation tools for\nresource-poor languages. We experiment several cross-lingual annotation\nprojection methods using Recurrent Neural Networks (RNN) models. The\ndistinctive feature of our approach is that our multilingual word\nrepresentation requires only a parallel corpus between the source and target\nlanguage. More precisely, our method has the following characteristics: (a) it\ndoes not use word alignment information, (b) it does not assume any knowledge\nabout foreign languages, which makes it applicable to a wide range of\nresource-poor languages, (c) it provides truly multilingual taggers. We\ninvestigate both uni- and bi-directional RNN models and propose a method to\ninclude external information (for instance low level information from POS) in\nthe RNN to train higher level taggers (for instance, super sense taggers). We\ndemonstrate the validity and genericity of our model by using parallel corpora\n(obtained by manual or automatic translation). Our experiments are conducted to\ninduce cross-lingual POS and super sense taggers.\n",
        "  It has been suggested that a Random Tree Puzzle (RTP) process leads to a\nYule-Harding (YH) distribution, when the number of taxa becomes large. In this\nstudy, we formalize this conjecture, and we prove that the two tree\ndistributions converge for two particular properties, which suggests that the\nconjecture may be true. However, we present evidence that, while the two\ndistributions are close, the RTP appears to converge on a different\ndistribution than does the YH.\n",
        "  We report on the design and implementation of the AC/DC gradient descent\nsolver for a class of optimization problems over normalized databases. AC/DC\ndecomposes an optimization problem into a set of aggregates over the join of\nthe database relations. It then uses the answers to these aggregates to\niteratively improve the solution to the problem until it converges.\n  The challenges faced by AC/DC are the large database size, the mixture of\ncontinuous and categorical features, and the large number of aggregates to\ncompute. AC/DC addresses these challenges by employing a sparse data\nrepresentation, factorized computation, problem reparameterization under\nfunctional dependencies, and a data structure that supports shared computation\nof aggregates.\n  To train polynomial regression models and factorization machines of up to\n154K features over the natural join of all relations from a real-world dataset\nof up to 86M tuples, AC/DC needs up to 30 minutes on one core of a commodity\nmachine. This is up to three orders of magnitude faster than its competitors R,\nMadLib, libFM, and TensorFlow whenever they finish and thus do not exceed\nmemory limitation, 24-hour timeout, or internal design limitations.\n",
        "  In this work we are analyzing scalability of the heuristic algorithm we used\nin the past to discover knowledge from multi-valued symbolic attributes in\nfuzzy databases. The non-atomic descriptors, characterizing a single attribute\nof a database record, are commonly used in fuzzy databases to reflect\nuncertainty about the recorded observation. In this paper, we present\nimplementation details and scalability tests of the algorithm, which we\ndeveloped to precisely interpret such non-atomic values and to transfer (i.e.\ndefuzzify) the fuzzy tuples to the forms acceptable for many regular (i.e.\natomic values based) data mining algorithms. Important advantages of our\napproach are: (1) its linear scalability, and (2) its unique capability of\nincorporating background knowledge, implicitly stored in the fuzzy database\nmodels in the form of fuzzy similarity hierarchy, into the\ninterpretation/defuzzification process.\n",
        "  We consider a stochastic process for the generation of species which combines\na Yule process with a simple model for hybridization between pairs of\nco-existent species. We assume that the origin of the process, when there was\none species, occurred at an unknown time in the past, and we condition the\nprocess on producing n species via the Yule process and a single hybridization\nevent. We prove results about the distribution of the time of the hybridization\nevent. In particular we calculate a formula for all moments, and show that\nunder various conditions, the distribution tends to an exponential with rate\ntwice that of the birth rate for the Yule process.\n",
        "  In this paper, a method is given to calculate the Jones polynomial of the\n6-plat presentations of knots by using a representation of the braid group\n$\\mathbb{B}_6$ into a group of $5\\times 5$ matrices. We also can calculate the\nJones polynomial of the $2n$-plat presentations of knots by generalizing the\nmethod for the 6-plat presentations of knots.\n",
        "  We present preliminary results from the highest available signal-to-noise\nrest-frame 2-8um spectra of z~2 ULIRGs. Our 10 targets are selected for their\ndeep silicate absorption features based on previous shallower IRS spectra. The\ngoal of this follow-up program is: 1) allow for a more accurate analysis of\ninner/hot dust continuum, 2) detecting the 3.3um and 6.2um PAH features, and 3)\ndetecting molecular absorption features such as due to water ice and\nhydrocarbons (HACs). We find that the 3.4um HAC absorption feature is observed\nin four sources, while the 3.05um water ice feature is observed in three of the\nsources. The HAC detectability is higher and ice detectability lower than\nexpected from local ULIRGs, but consistent with a more AGN-dominated sample\nsuch as this one. Where ice is detected, the ice-to-silicate ratio is somewhat\nlower than many local ULIRGs implying on average thinner ice mantles. One\nsource shows the, to our knowledge, highest redshift reported detection of the\n3.3um PAH feature (along with a previously detected 6.2um feature). The\nstrength of the 3.3um feature is as expected for a starburst-dominated ULIRG.\n",
        "  The newly discovered superconductor FeSe1-x (x=0.08, Tconset=13.5 K at\nambient pressure rising to 27 K at 1.48 GPa) exhibits a structural phase\ntransition from tetragonal to orthorhombic below 70 K at ambient pressure - the\ncrystal structure in the superconducting state shows remarkable similarities to\nthat of the REFeAsO1-xFx (RE = rare earth) superconductors\n",
        "  Single-shot gradient recalled echo planar imaging (EPI) is the primary tool\nfor functional magnetic resonance imaging (fMRI). The image often suffers from\nsignal drop near the air-tissue interface, such as the amygdala and regions of\nthe orbitofrontal lobe. An effective way to correct for this type of artifact\nis by applying multi-shot EPI using different z-shimming. Unfortunately, the\nscanning efficiency is significantly lowered. More recently, a new technique\ncalled volume-selective z-shim was proposed to implement z-shim compensation to\nonly specific slices with large susceptibility effects. The high imaging\nefficiency of volume selective z-shim makes it possible to substitute\nconventional EPI for whole brain studies. In this study two fMRI experiments\nwere conducted to compare volume- selective z-shim and conventional EPI while\nsubjects performed tasks on face stimuli. The comparison was focused on three\nbrain regions: amygdala, hippocampus, and fusiform gyrus. Our results indicate\nthat despite fewer volumes collected during the same amount of scan time,\nvolume-selective z-shim showed statistically higher activation in brain regions\nsusceptible to signal loss while minimal differences were observed elsewhere.\n",
        "  We apply sequence-to-sequence model to mitigate the impact of speech\nrecognition errors on open domain end-to-end dialog generation. We cast the\ntask as a domain adaptation problem where ASR transcriptions and original text\nare in two different domains. In this paper, our proposed model includes two\nindividual encoders for each domain data and make their hidden states similar\nto ensure the decoder predict the same dialog text. The method shows that the\nsequence-to-sequence model can learn the ASR transcriptions and original text\npair having the same meaning and eliminate the speech recognition errors.\nExperimental results on Cornell movie dialog dataset demonstrate that the\ndomain adaption system help the spoken dialog system generate more similar\nresponses with the original text answers.\n",
        "  Bats are unique mammals. This note discusses some questions regarding bat\nevolution including why they are nocturnal and why they can echolocate. It is\nhypothesized that echolocation was necessary for bats to survive the period of\nlimited visibility that followed the Cretaceous-Paleogene (K-Pg) extinction\nevent.\n",
        "  We know very little about how neural language models (LM) use prior\nlinguistic context. In this paper, we investigate the role of context in an\nLSTM LM, through ablation studies. Specifically, we analyze the increase in\nperplexity when prior context words are shuffled, replaced, or dropped. On two\nstandard datasets, Penn Treebank and WikiText-2, we find that the model is\ncapable of using about 200 tokens of context on average, but sharply\ndistinguishes nearby context (recent 50 tokens) from the distant history. The\nmodel is highly sensitive to the order of words within the most recent\nsentence, but ignores word order in the long-range context (beyond 50 tokens),\nsuggesting the distant past is modeled only as a rough semantic field or topic.\nWe further find that the neural caching model (Grave et al., 2017b) especially\nhelps the LSTM to copy words from within this distant context. Overall, our\nanalysis not only provides a better understanding of how neural LMs use their\ncontext, but also sheds light on recent success from cache-based models.\n",
        "  In a previous paper, we defined an operation $\\mu$ that generalizes Turaev's\ncobracket for loops on a surface. We showed that, in contrast to the cobracket,\nthis operation gives a formula for the minimum number of self-intersections of\na loop in a given free homotopy class. In this paper we consider the\ncorresponding question for virtual strings. We show that $\\mu$ gives a bound on\nthe minimal self-intersection number of a virtual string which is stronger than\na bound given by Turaev's virtual string cobracket. We use Turaev's based\nmatrices to describe strings $\\alpha$ such that $\\mu$ gives a formula for the\nminimal self-intersection number $\\alpha$. We also construct an example that\nshows the bound on the minimal self-intersection number given by $\\mu$ is\nsometimes stronger than the bound $\\rho$ given by Turaev's based matrix\ninvariant.\n",
        "  Arabic morphological analysis is one of the essential stages in Arabic\nNatural Language Processing. In this paper we present an approach for Arabic\nmorphological analysis. This approach is based on Arabic morphological\nautomaton (AMAUT). The proposed technique uses a morphological database\nrealized using XMODEL language. Arabic morphology represents a special type of\nmorphological systems because it is based on the concept of scheme to represent\nArabic words. We use this concept to develop the Arabic morphological automata.\nThe proposed approach has development standardization aspect. It can be\nexploited by NLP applications such as syntactic and semantic analysis,\ninformation retrieval, machine translation and orthographical correction. The\nproposed approach is compared with Xerox Arabic Analyzer and Smrz Arabic\nAnalyzer.\n",
        "  Simulation of x-ray projection images plays an important role in cone beam CT\n(CBCT) related research projects. A projection image contains primary signal,\nscatter signal, and noise. It is computationally demanding to perform accurate\nand realistic computations for all of these components. In this work, we\ndevelop a package on GPU, called gDRR, for the accurate and efficient\ncomputations of x-ray projection images in CBCT under clinically realistic\nconditions. The primary signal is computed by a tri-linear ray-tracing\nalgorithm. A Monte Carlo (MC) simulation is then performed, yielding the\nprimary signal and the scatter signal, both with noise. A denoising process is\napplied to obtain a smooth scatter signal. The noise component is then obtained\nby combining the difference between the MC primary and the ray-tracing primary\nsignals, and the difference between the MC simulated scatter and the denoised\nscatter signals. Finally, a calibration step converts the calculated noise\nsignal into a realistic one by scaling its amplitude. For a typical CBCT\nprojection with a poly-energetic spectrum, the calculation time for the primary\nsignal is 1.2~2.3 sec, while the MC simulations take 28.1~95.3 sec. Computation\ntime for all other steps is negligible. The ray-tracing primary signal matches\nwell with the primary part of the MC simulation result. The MC simulated\nscatter signal using gDRR is in agreement with EGSnrc results with a relative\ndifference of 3.8%. A noise calibration process is conducted to calibrate gDRR\nagainst a real CBCT scanner. The calculated projections are accurate and\nrealistic, such that beam-hardening artifacts and scatter artifacts can be\nreproduced using the simulated projections. The noise amplitudes in the CBCT\nimages reconstructed from the simulated projections also agree with those in\nthe measured images at corresponding mAs levels.\n",
        "  Flexible tendon sheath mechanism is commonly used in NOTES systems because it\noffers high flexibility, light weight, and easy transmission. Due to the size\nconstraints and sterilization problems, traditional sensors like force/torque\nsensor are extremely difficult to place at the tool tips of surgical arms. In\naddition, nonlinear dynamic friction and backlash cause challenges to provide\nhaptic feedback to the surgeons when the robotic arms are inside the patient's\nbody. Hence, it is extremely difficult to provide the force information to\nhaptic devices and subsequently to the surgeons. To deal with these problems,\nin this paper we propose a new approach of friction model in the tendon-sheath\nmechanism to provide the force at distal end of endoscopic system. In\ncomparison with current approaches in the literature, the proposed model is\nable to provide force information at zero velocity and it is smooth. In\naddition, the model is independent configuration and able to capture friction\nforce with any complex sheath shapes. A suitable experimental setup is\nestablished to validate the proposed approach using the two degrees of freedom\nMaster-Slave system. The validity of the proposed approach is confirmed with a\ngood agreement between the estimated model and real experimental data. Finally,\na force feedback structure is also given for use in flexible endoscopic\nsystems.\n",
        "  One of the central questions of metacommunity theory is how dispersal of\norganisms affects species diversity. Here we show that the diversity-dispersal\nrelationship should not be studied in isolation of other abiotic and biotic\nflows in the metacommunity. We study a mechanistic metacommunity model in which\nconsumer species compete for an abiotic or biotic resource. We consider both\nconsumer species specialized to a habitat patch, and generalist species capable\nof using the resource throughout the metacommunity. We present analytical\nresults for different limiting values of consumer dispersal and resource\ndispersal, and complement these results with simulations for intermediate\ndispersal values. Our analysis reveals generic patterns for the combined\neffects of consumer and resource dispersal on the metacommunity diversity of\nconsumer species, and shows that hump-shaped relationships between local\ndiversity and dispersal are not universal. Diversity-dispersal relationships\ncan also be monotonically increasing or multimodal. Our work is a new step\ntowards a general theory of metacommunity diversity integrating dispersal at\nmultiple trophic levels.\n",
        "  Rotational setup errors are usually neglected in most clinical centers. An\nanalytical formula is developed to determine the extra margin between clinical\ntarget volume (CTV) and planning target volume (PTV) to account for setup\nerrors. The proposed formula corrects for both translational and rotational\nsetup errors and then incorporated into margin determination for PTV.\n",
        "  Real projective structures on $n$-orbifolds are useful in understanding the\nspace of representations of discrete groups into $\\mathrm{SL}(n+1, \\mathbb{R})$\nor $\\mathrm{PGL}(n+1, \\mathbb{R})$. A recent work shows that many hyperbolic\nmanifolds deform to manifolds with such structures not projectively equivalent\nto the original ones. The purpose of this paper is to understand the structures\nof ends of real projective $n$-dimensional orbifolds. In particular, these have\nthe radial or totally geodesic ends. In previous papers, we classified properly\nconvex or complete radial ends under suitable conditions. In this paper, we\nwill study radial ends that are convex but not properly convex nor complete\naffine. The main techniques are the theory of Fried and Goldman on affine\nmanifolds, and a generalization of the work on Riemannian foliations by Molino,\nCarri\\`ere, and so on. We will show that these are quasi-joins of horospheres\nand totally geodesic radial ends. These are deformations of joins of\nhorospheres and totally geodesic radial ends.\n",
        "  [Context] Two competing models describe the formation of massive stars in\nobjects like the Orion Trapezium. In the turbulent core accretion model, the\nresulting stellar masses are directly related to the mass distribution of the\ncloud condensations. In the competitive accretion model, the gravitational\npotential of the protocluster captures gas from the surrounding cloud for which\nthe individual cluster members compete. [Aims] With high resolution\nsubmillimeter observations of the structure, kinematics, and chemistry of the\nproto-Trapezium cluster W3 IRS5, we aim to determine which mode of star\nformation dominates. [Methods] We present 354 GHz Submillimeter Array\nobservations at resolutions of 1\"-3\" (1800-5400 AU) of W3 IRS5. ......\n[Results] The observations show five emission peaks (SMM1-5). SMM1 and SMM2\ncontain massive embedded stars (~20 Msun); SMM3-5 are starless or contain\nlow-mass stars (<8 Msun). The inferred densities are high, >= 10^7 cm^-3, but\nthe core masses are small, 0.2-0.6 Msun. The detected molecular emission\nreveals four different chemical zones. ...... [Conclusions] The proto-Trapezium\ncluster W3 IRS5 is an ideal test case to discriminate between models of massive\nstar formation. Either the massive stars accrete locally from their local\ncores; in this case the small core masses imply that W3 IRS5 is at the very end\nstages (1000 yr) of infall and accretion, or the stars are accreting from the\nglobal collapse of a massive, cluster forming core. We find that the observed\nmasses, densities and line widths observed toward W3 IRS 5 and the surrounding\ncluster forming core are consistent with the competitive accretion of gas at\nrates of Macc~10^-4 Msun yr^-1 by the massive young forming stars. ......\n",
        "  We establish the slice-ribbon conjecture for a large family of Montesinos'\nknots by means of Donaldson's theorem on the intersection forms of definite\n4-manifolds.\n",
        "  We show that bilinear cup products with local coefficients of closed\n3-manifolds recover some twisted pairings of infinite covers and the\nCasson-Gordon local signatures. As a result, we further give diagrammatic\ncomputations of the pairings and signatures.\n",
        "  The temperature dependences of AC magnetic susceptibility at different\nmagnetic field amplitudes and frequencies are investigated for underdoped and\noverdoped R(1-x)Ca(x)Ba(2)Cu(3)O(7-delta)(R=Y; Gd and x=0; 0.2) polycrystalline\nsamples. The activation energy, Ea, for thermally assisted flux flow (TAFF) in\nintergranular region is determined. It was established that a correlation\nexists between the intergranular critical current and flux pinning activation\nenergy. In underdoped samples the intergranular current shows S-I-S behaviour\nand the activation energy is small, while in overdoped samples the\nintergranular current is changed to S-N-S type and the activation energy\nincreases. 2D pancake vortices are characteristic of underdoped samples, while\n3D vortex system exists in overdoped samples. In fact we demonstrate that Ca\nsubstitution not only increases carrier concentration, but improves\nintergranular activation energy for TAFF.\n",
        "  Given a clover link, we construct a bottom tangle by using a disk/band\nsurface of the clover link. Since the Milnor number is already defined for a\nbottom tangle, we define the Milnor number for the clover link to be the Milnor\nnumber for the bottom tangle and show that for a clover link, if Milnor numbers\nof length k or less vanish, then Milnor numbers of length 2k+1 or less are\nwell-defined. Moreover we prove that two clover links whose Milnor numbers of\nlength k or less vanish are equivalent up to edge-homotopy and\n$C_{2k+1}$-equivalence if and only if those Milnor numbers of length 2k+1 or\nless are equal. In particular, we give an edge-homotopy classification of\n3-clover links by their Milnor numbers of length 3 or less.\n",
        "  In this work we focus on the task of automatically extracting bilingual\nlexicon for the language pair Spanish-Nahuatl. This is a low-resource setting\nwhere only a small amount of parallel corpus is available. Most of the\ndownstream methods do not work well under low-resources conditions. This is\nspecially true for the approaches that use vectorial representations like\nWord2Vec. Our proposal is to construct bilingual word vectors from a graph.\nThis graph is generated using translation pairs obtained from an unsupervised\nword alignment method.\n  We show that, in a low-resource setting, these type of vectors are successful\nin representing words in a bilingual semantic space. Moreover, when a linear\ntransformation is applied to translate words from one language to another, our\ngraph based representations considerably outperform the popular setting that\nuses Word2Vec.\n",
        "  Let $\\mathcal {M}$ be the space of all, including singular, long knots in\n3-space and for which a fixed projection into the plane is an immersion. Let\n$cl(\\Sigma^{(1)}_{iness})$ be the closure of the union of all singular knots in\n$\\mathcal {M}$ with exactly one ordinary double point and such that the two\nresolutions represent the same (non singular) knot type. We call\n$\\Sigma^{(1)}_{iness}$ the {\\em inessential walls} and we call $\\mathcal\n{M}_{ess} = \\mathcal {M} \\setminus cl(\\Sigma^{(1)}_{iness})$ the {\\em essential\ndiagram space}. We construct a non trivial class in $H^1(\\mathcal {M}_{ess};\n\\mathbb{Z}[A, A^{-1}])$ by an extension of the Kauffman bracket. This implies\nin particular that there are loops in $\\mathcal {M}_{ess}$ which consist of\nregular isotopies of knots together with crossing changings and which are not\ncontractible in $\\mathcal {M}_{ess}$ (leading to the title of the paper).\n  We conjecture that our construction gives rise to a new knot polynomial for\nknots of unknotting number one.\n",
        "  This paper consists of two parts.In the first part, the scatter components of\ncomputed tomograpahy dose profiles are modeled using various functions\nincluding the solution to Riccati's differential equation. These scatter\nfunctions are combined with primary components such as a trapezoidal function\nand a constructed function that uses the analytic continuation of Heaviside\nstep function. A mathematical theory is developed in Banach space. The modeled\nfunction is used to accurately fit data from the O-arm cone beam imaging\nsystem. In a second part of the paper, an approach to dosimtery is developed\nthat shows that the results obtained from the use of a pencil shaped ion\nchamber is equivalent to that from a farmer chamber. This result is verified by\npresenting some preliminary experimental data measured in a 64 slice Siemens\nSensation scanner.\n",
        "  Let K be a (2p,q)-torus knot and M_n is a 3-manifold obtained by 1/n-Dehn\nsurgery along K. We consider a polynomial whose zeros are the inverses of the\nReideimeister torsion of M_n for SL(2;C)-irreducible representations. Johnson\ngave a formula for the case of the (2,3)-torus knot under some modification and\nnormalization. We generalize this formula by using Tchebychev polynomials.\n",
        "  The Phase diagram of SmFeAsO1-xFx in terms of x is exhibited in this study.\nSmFeAsO1-xFx from x = 0 to x = 0.3 were prepared by low temperature sintering\nwith slow cooling. The low temperature sintering suppresses the formation of\nthe amorphous FeAs, which is inevitably produced as an impurity by using high\ntemperature sintering. Moreover, slow cooling is effective to obtain the high\nfluorine concentration. The compositional change from feedstock composition is\nquite small after this synthesis. We can reproducibly observe a record\nsuperconducting transition for an iron based superconductor at 58.1 K. This\nachievement of a high superconducting transition is due to the success in a\nlarge amount of fluorine substitution. A shrinking of the a lattice parameter\ncaused by fluorine substitution is observed and the substitutional rate of\nfluorine changes at x =0.16.\n",
        "  We have simulated the evolution of age structured populations whose\nindividuals represented by their diploid genomes were distributed on a square\nlattice. The environmental conditions on the whole territory changed\nsimultaneously in the same way by switching on or off some requirements.\nMutations accumulated in the genes dispensable during a given period of time\nwere neutral, but they could cause a genetic death of individuals if the\nenvironment required their functions again. Populations survived due to\nretaining some surplus of genetic information in the individual genomes. The\nchanges of the environment caused the fluctuations of the population size.\nSince the simulations were performed with individuals spatially distributed on\nthe lattice and the maximal distance between mating partners was set as a\nparameter of the model, the inbreeding coefficient in populations changed\nunevenly, following the fluctuation of population size and enhancing the\nspeciation phenomena.\n",
        "  Crowdsourcing employs human workers to solve computer-hard problems, such as\ndata cleaning, entity resolution, and sentiment analysis. When crowdsourcing\ntabular data, e.g., the attribute values of an entity set, a worker's answers\non the different attributes (e.g., the nationality and age of a celebrity star)\nare often treated independently. This assumption is not always true and can\nlead to suboptimal crowdsourcing performance. In this paper, we present the\nT-Crowd system, which takes into consideration the intricate relationships\namong tasks, in order to converge faster to their true values. Particularly,\nT-Crowd integrates each worker's answers on different attributes to effectively\nlearn his/her trustworthiness and the true data values. The attribute\nrelationship information is also used to guide task allocation to workers.\nFinally, T-Crowd seamlessly supports categorical and continuous attributes,\nwhich are the two main datatypes found in typical databases. Our extensive\nexperiments on real and synthetic datasets show that T-Crowd outperforms\nstate-of-the-art methods in terms of truth inference and reducing the cost of\ncrowdsourcing.\n",
        "  The beam transport system between accelerator and patient treatment location\nin a particle therapy facility is described. After some general layout aspects\nthe major beam handling tasks of this system are discussed. These are energy\nselection, an optimal transport of the particle beam to the beam delivery\ndevice and the gantry, a device that is able to rotate a beam delivery system\naround the patient, so that the tumour can be irradiated from almost any\ndirection. Also the method of pencil beam scanning is described and how this is\nimplemented within a gantry. Using this method the particle dose is spread over\nthe tumour volume to the prescribed dose distribution.\n",
        "  Children learning their first language face multiple problems of induction:\nhow to learn the meanings of words, and how to build meaningful phrases from\nthose words according to syntactic rules. We consider how children might solve\nthese problems efficiently by solving them jointly, via a computational model\nthat learns the syntax and semantics of multi-word utterances in a grounded\nreference game. We select a well-studied empirical case in which children are\naware of patterns linking the syntactic and semantic properties of words ---\nthat the properties picked out by base nouns tend to be related to shape, while\nprenominal adjectives tend to refer to other properties such as color. We show\nthat children applying such inductive biases are accurately reflecting the\nstatistics of child-directed speech, and that inducing similar biases in our\ncomputational model captures children's behavior in a classic adjective\nlearning experiment. Our model incorporating such biases also demonstrates a\nclear data efficiency in learning, relative to a baseline model that learns\nwithout forming syntax-sensitive overhypotheses of word meaning. Thus solving a\nmore complex joint inference problem may make the full problem of language\nacquisition easier, not harder.\n",
        "  Data warehousing and OLAP applications must nowadays handle complex data that\nare not only numerical or symbolic. The XML language is well-suited to\nlogically and physically represent complex data. However, its usage induces new\ntheoretical and practical challenges at the modeling, storage and analysis\nlevels, and a new trend toward XML warehousing has been emerging for a couple\nof years. Unfortunately, no standard XML data warehouse architecture emerges.\nIn this paper, we propose a unified XML warehouse reference model that\nsynthesizes and enhances related work, and fits into a global XML warehousing\nand analysis approach we have developed. We also present a software platform\nthat is based on this model, as well as a case study that illustrates its\nusage.\n",
        "  To elucidate the magnetic structure and the origin of the nematicity in FeSe,\nwe perform a high-pressure $^{77}$Se NMR study on FeSe single crystals. We find\na suppression of the structural transition temperature with pressure up to\nabout 2 GPa from the anisotropy of the Knight shift. Above 2 GPa, a\nstripe-order antiferromagnetism that breaks the spatial four-fold rotational\nsymmetry is determined by the NMR spectra under different field orientations\nand with temperatures down to 50 mK. The magnetic phase transition is revealed\nto be first-order type, implying the existence of a concomitant structural\ntransition via a spin-lattice coupling. Stripe-type spin fluctuations are\nobserved at high temperatures, and remain strong with pressure. These results\nprovide clear evidences for strong coupling between nematicity and magnetism in\nFeSe, and therefore support a universal scenario of magnetic driven nematicity\nin iron-based superconductors.\n",
        "  We present a deterministic mathematical model for malaria transmission with\nwaning immunity. The model consists of five non-linear system of differential\nequations. We used next generation matrix to derive the basic reproduction\nnumber $R_0$. The disease free equilibrium was computed and its local stability\nhas been shown by the virtue of the Jacobean matrix. Moreover, using Lyapunov\nfunction theory and LaSalle Invariance Principle we have proved that the\ndisease free equilibrium is globally asymptotically stable. Conditions for\nexistence of endemic equilibrium point have been established. A qualitative\nstudy based on bifurcation theory reveals that backward bifurcation occur in\nthe model. The stable disease free equilibrium of the model coexists with the\nstable endemic equilibrium when $R_0<1$. Furthermore, we have shown that\nbringing the number of disease (malaria) induced death rate below some\nthreshold is sufficient enough to eliminate backward bifurcation in the model.\n",
        "  We are exploring low-dose proton radiography and computed tomography (pCT) as\ntechniques to improve the accuracy of proton treatment planning and to provide\nartifact-free images for verification and adaptive therapy at the time of\ntreatment. Here we report on comprehensive beam test results with our prototype\npCT head scanner. The detector system and data acquisition attain a sustained\nrate of more than a million protons individually measured per second, allowing\na full CT scan to be completed in six minutes or less of beam time. In order to\nassess the performance of the scanner for proton radiography as well as\ncomputed tomography, we have performed numerous scans of phantoms at the\nNorthwestern Medicine Chicago Proton Center including a custom phantom designed\nto assess the spatial resolution, a phantom to assess the measurement of\nrelative stopping power, and a dosimetry phantom. Some images, performance, and\ndosimetry results from those phantom scans are presented together with a\ndescription of the instrument, the data acquisition system, and the calibration\nmethods.\n",
        "  We study ODE models of epidemic spreading with a preventive behavioral\nresponse that is triggered by awareness of the infection. Previous studies of\nsuch models have mostly focused on the impact of the response on the initial\ngrowth of an outbreak and the existence and location of endemic equilibria.\nHere we study the question whether this type of response is sufficient to\nprevent future flare-ups from low endemic levels if awareness is assumed to\ndecay over time. In the ODE context, such flare-ups would translate into\nsustained oscillations with significant amplitudes.\n  Our results show that such oscillations are ruled out in\nSusceptible-Aware-Infectious-Susceptible models with a single compartment of\naware hosts, but can occur if we consider two distinct compartments of aware\nhosts who differ in their willingness to alert other susceptible hosts.\n",
        "  Using a slitless spectroscopy method with (a) the 8.2 m Subaru telescope and\nits FOCAS Cassegrain spectrograph, and (b) the ESO Very Large Telescope (VLT)\nunit 1 (Antu) and its FORS2 Cassegrain spectrograph, we have detected 326\nplanetary nebulae (PNs) in the giant Virgo elliptical galaxy NGC 4649 (M 60),\nand we have measured their radial velocities. After rejecting some PNs more\nlikely to belong to the companion galaxy NGC 4647, we have built a catalog with\nkinematic information for 298 PNs in M 60. Using these radial velocities we\nhave concluded that they support the presence of a dark matter halo around M\n60. Based on an isotropic, two-component Hernquist model, we estimate the dark\nmatter halo mass within 3$R_{\\rm e}$ to be 4$\\times10^{11} M_{\\odot}$, which is\nalmost one half of the total mass of about $10^{12} M_{\\odot}$ within 3$R_{\\rm\ne}$. This total mass is similar to that estimated from globular cluster,\nXMM-Newton and Chandra observations. The dark matter becomes dominant outside.\nMore detailed dynamical modeling of the PN data is being published in a\ncompanion paper. We have also measured the $m$(5007) magnitudes of many of\nthese PNs, and built a statistically complete sample of 218 PNs. The resulting\nPN luminosity function (PNLF) was used to estimate a distance modulus of\n30.7$\\pm$0.2 mag, equivalent to 14$\\pm$1 Mpc. This confirms an earlier PNLF\ndistance measurement, based on a much smaller sample. The PNLF distance modulus\nremains smaller than the surface brightness fluctuation (SBF) distance modulus\nby 0.4 mag.\n",
        "  Most works on Web services has focused on discovery, composition and\nselection processes of these kinds of services. Other few works were interested\nin how to represent Web services search queries. However, these queries cannot\nbe processed by ensuring a high level of performance without being adequately\nrepresented first. To this end, different query languages were designed. Even\nso, in the absence of a standard, these languages are quite various. Their\ndiversity makes it difficult choosing the most suitable language. In fact, this\nlanguage should be able to cover all types of preferences or requirements of\nclients such as their functional, nonfunctional,temporal or even specific\nconstraints as is the case of geographical or spatial constraints and meet\ntheir needs and preferences helping to provide them the best answer. It must\nalso be mutually simple and imposes no restrictions or at least not too many\nconstraints in terms of prior knowledge to use and also provide a formal or\nsemi-formal queries presentation to support their automatic post-processing. A\ncomparative study is eventually established to allow to reveal the advantages\nand limitations of various existing languages in this context. It is a\nsynthesis of this category of languages discussing their performance level and\ntheir capability to respond to various needs related to the Web services\nresearch and discovery case. The criterions identified at this stage may, in\nour opinion, constitute then the main pre-requisite that a language should\nsatisfy to be called perfect or to be a future standard.\n",
        "  Cooper pairing between a conduction electron ($c$ electron) and an $f$\nelectron, referred to as the \"$c$-$f$ pairing,\" is examined to explain s-wave\nsuperconductivity in heavy-fermion systems. We first apply the Schrieffer-Wolff\ntransformation to the periodic Anderson model assuming deep $f$ level and\nstrong Coulomb repulsion. The resulting effective Hamiltonian contains direct\nand spin-exchange interactions between $c$ and $f$ electrons, which are\nresponsible for the formation of the $c$-$f$ Cooper pairs. The mean-field\nanalysis shows that the fully gapped $c$-$f$ pairing phase with anisotropic\ns-wave symmetry appears in a large region of the phase diagram. We also find\ntwo different types of exotic $c$-$f$ pairing phases, the Fulde-Ferrell and\nbreached pairing phases. The formation of the $c$-$f$ Cooper pairs is\nattributed to the fact that the strong Coulomb repulsion makes a quasiparticle\n$f$ band near the center of the conduction band.\n",
        "  Four-dimensional computed tomography (4DCT) has been widely used in cancer\nradiotherapy for accurate target delineation and motion measurement for tumors\nin thorax and upper abdomen areas. However, 4DCT simulation is associated with\nmuch higher imaging dose than conventional CT simulation, which is a major\nconcern in its clinical application. Conventionally, each phase of 4DCT is\nreconstructed independently using the filtered backprojection (FBP) algorithm.\nThe basic idea of our new algorithm is that, by utilizing the common\ninformation among different phases, the input information required to\nreconstruct image of high quality, and thus the imaging dose, can be reduced.\nWe proposed a temporal non-local means (TNLM) method to explore the inter-phase\nsimilarity. All phases of the 4DCT images are reconstructed simultaneously by\nminimizing a cost function consisting of a data fidelity term and a TNLM\nregularization term. We utilized a forward-backward splitting algorithm and a\nGauss-Jacobi iteration method to efficiently solve the minimization problem.\nThe algorithm was also implemented on graphics processing unit (GPU) to achieve\na high computational speed. Our reconstruction algorithm has been tested on a\ndigital NCAT thorax phantom in three low dose scenarios. Our new algorithm\ngenerates visually much better CT images containing less image noise and\nstreaking artifacts compared with the standard FBP algorithm. Quantitative\nanalysis shows that much higher contrast-to-noise ratio and signal-to-noise\nratio can be obtained using our algorithm. The total reconstruction time for\nall 10 phases of a slice ranges from 90 to 140 seconds on an NVIDIA Tesla C1060\nGPU card.\n",
        "  We extend to the long virtual knot case the constructions first presented by\nA. Henrich and later generalized by the author to the framed virtual knot case.\nThese consist of three Vassiliev invariants of order one, including a universal\none, as well as the notions of a based matrix and a singular based matrix and\ntheir relation to long flat virtual knots.\n",
        "  Using the first WBC unit installed in Fukushima Prefecture after the\naccident, the radiocesium body burdens of 566 high-risk residents of Minamisoma\ncity were measured in July 2011 at the Minamisoma Municipal General Hospital.\nThe analysis of the data was challenging because this chair-type WBC unit did\nnot have sufficient shielding against background gamma rays, and methods had to\nbe developed to reliably compensate for the body-attenuated background\nradiation. Fortunately, data for repeated tests of hospital staff members using\nboth the chair-type and well-shielded FASTSCAN WBC units, installed in\nSeptember 2011, were available, and could be used to check the validity of the\nanalysis. The CEDs of all subjects, estimated under the assumption of acute\ninhalation in March 2011, were found to be less than 1 mSv.\n",
        "  Word embeddings are widely used in Natural Language Processing, mainly due to\ntheir success in capturing semantic information from massive corpora. However,\ntheir creation process does not allow the different meanings of a word to be\nautomatically separated, as it conflates them into a single vector. We address\nthis issue by proposing a new model which learns word and sense embeddings\njointly. Our model exploits large corpora and knowledge from semantic networks\nin order to produce a unified vector space of word and sense embeddings. We\nevaluate the main features of our approach both qualitatively and\nquantitatively in a variety of tasks, highlighting the advantages of the\nproposed method in comparison to state-of-the-art word- and sense-based models.\n",
        "  We review the uncertainties in high-z star-formation rate (SFR) measures and\nthe constraints that one obtains from high-z gamma-ray burst (GRB) rates on\nthem. We show that at the present time, the GRB rates per unit star-formation\nat z>3 are higher than at lower redshift. We also compare metallicity\npredictions made using a hierarchical model of cosmic chemical evolution based\non two recently proposed SFRs, one based on the observed galaxy luminosity\nfunction at high redshift and one based on the GRB rate and find that within\nthe considerable scatter in metal abundance measures, they both are consistent\nwith the data. Analyzing the ensemble of different measurements together, we\nconclude that despite metallicity biases, GRBs may be a less biased probe of\nstar-formation at z>3 than at z<2. There is likely to be a common origin to the\nhigh GRB rate per unit star-formation and the high observed Lyman-continuum\nproduction rate in high redshift galaxies and that this may be due to a\nrelative overabundance of stars with mass $>$25Msun which are likely GRB\nprogenitors. We also find that to reconcile these measurements with the Thomson\nscattering cross section of cosmic microwave background (CMB) photons measured\nby Planck, the escape fraction of Lyman-continuum photons from galaxies must be\nlow, about 15 percent or less and that the clumping factor of the IGM is likely\nto be small, about 3. Finally, we demonstrate that GRBs are unique probes of\nmetallicity evolution in low-mass galaxy samples and that GRB hosts likely lost\na significant fraction of metals to the intergalactic medium (IGM) due to\nfeedback processes such as stellar winds and supernovae.\n",
        "  Intensity modulated proton therapy (IMPT) provides highly conformal dose\ndistributions through the application of multiple, angularly spaced fields,\neach applying an ad-hoc pattern of spatially varying particle fluences. In\nparticular, Bragg peaks are simultaneously optimized for all the fields. Once\nthe number and the direction of the fields are set, the dose distribution of\nthe IMPT plan is determined by the dose constraints assigned to specific\norgans-at-risk (OARs) surrounding the tumour. In this work, we introduce a new\nfeature for the OARs, called \"opacity\", aimed at improving the quality and the\nrobustness of IMPT plans, by modifying, in the pre-optimization stage, the\nfluence of the pencil beams, that intersect those structures. The proposed IMPT\nplanning strategy is addressed to those clinical cases, where metallic\nprothesis or fast density-varying organs can compromise the stability of the\ndose distribution. We show how the usage of this additional parameter can lead\nto more accurate IMPT plans with respect to the situation in which only dose\nconstraints are adopted.\n",
        "  A low concentration of columnar defects is reported to transform a\nfirst-order vortex lattice melting line in BSCCO crystals into alternating\nsegments of first-order and second-order transitions separated by two critical\npoints. As the density of CDs is increased, the critical points shift apart and\nthe range of the intermediate second-order transition expands. A third, low\ntemperature critical point was also observed in one sample. The measurement of\nequilibrium magnetization and the mapping of the melting line down to 27K was\nmade possible by employment of the shaking technique.\n",
        "  In this paper, we investigate abelian knot contact homology by using the\ntrace-free slice of a knot, which is the cross-section of the character variety\nof a knot cut by the hyperplanes defined by meridionally trace-free (traceless)\nconditions. We first give a set of equations whose common solutions coincide\nwith the trace-free slice. This gives us a strong tool to describe exactly the\nrelationship between the trace-free slice, degree 0 abelian knot contact\nhomology and the character variety of the 2-fold branched cover of the 3-sphere\nbranched along the knot. Using this relationship, we introduce a notion called\nghost characters of a knot to check the followings: (1) when the conjecture\ngiven by L. Ng concerned with the above relationship does not hold, (2) when\nthe map from the trace-free slice to the character variety of the 2-fold\nbranched cover, constructed by Y. Yamaguchi and the author, is not surjective.\nNote that in the recent paper of Christopher R. Cornwell, he has translated the\nresults on the above relationship into the language of \"reflective\naugmentation\".\n",
        "  The CIII] and CIV rest-frame UV emission lines are powerful probes of the\nionizations states of galaxies. They have furthermore been suggested as\nalternatives for spectroscopic redshift confirmation of objects at the epoch of\nreionization ($z>6$), where the most frequently used redshift indicator,\nLy$\\alpha$, is attenuated by the high fraction of neutral hydrogen in the\ninter-galactic medium. However, currently only very few confirmations of carbon\nUV lines at these high redshifts exist, making it challenging to quantify these\nclaims. Here, we present the detection of CIV$\\lambda\\lambda$1548,1551\\AA\\ in\n\\HST\\ slitless grism spectroscopy obtained by GLASS of a Ly$\\alpha$ emitter at\n$z=6.11$ multiply imaged by the massive foreground galaxy cluster RXJ2248. The\nCIV emission is detected at the 3--5$\\sigma$ level in two images of the source,\nwith marginal detection in two other images. We do not detect significant\nCIII]$\\lambda\\lambda$1907,1909\\AA\\ emission implying an equivalent width\nEW$_\\textrm{CIII]}<20$\\AA\\ (1$\\sigma$) and $\\textrm{CIV/CIII}>0.7$ (2$\\sigma$).\nCombined with limits on the rest-frame UV flux from the HeII$\\lambda$1640\\AA\\\nemission line and the OIII]$\\lambda\\lambda$1661,1666\\AA\\ doublet, we put\nconstraints on the metallicity and the ionization state of the galaxy. The\nestimated line ratios and equivalent widths do not support a scenario where an\nAGN is responsible for ionizing the carbon atoms. SED fits including nebular\nemission lines imply a source with a mass of log(M/M$_\\odot)\\sim9$, SFR of\naround 10M$_\\odot$/yr, and a young stellar population $<50$Myr old. The source\nshows a stronger ionizing radiation field than objects with detected CIV\nemission at $z<2$ and adds to the growing sample of low-mass\n(log(M/M$_\\odot)\\lesssim9$) galaxies at the epoch of reionization with strong\nradiation fields from star formation.\n",
        "  Mutations in a microbial population can increase the frequency of a genotype\nnot only by increasing its exponential growth rate, but also by decreasing its\nlag time or adjusting the yield (resource efficiency). The contribution of\nmultiple life-history traits to selection is a critical question for\nevolutionary biology as we seek to predict the evolutionary fates of mutations.\nHere we use a model of microbial growth to show there are two distinct\ncomponents of selection corresponding to the growth and lag phases, while the\nyield modulates their relative importance. The model predicts rich population\ndynamics when there are tradeoffs between phases: multiple strains can coexist\nor exhibit bistability due to frequency-dependent selection, and strains can\nengage in rock-paper-scissors interactions due to non-transitive selection. We\ncharacterize the environmental conditions and patterns of traits necessary to\nrealize these phenomena, which we show to be readily accessible to experiments.\nOur results provide a theoretical framework for analyzing high-throughput\nmeasurements of microbial growth traits, especially interpreting the pleiotropy\nand correlations between traits across mutants. This work also highlights the\nneed for more comprehensive measurements of selection in simple microbial\nsystems, where the concept of an ordinary fitness landscape breaks down.\n",
        "  $^{13}$CO(J=2--1) and C$^{18}$O(J=2--1) observations of the molecular cloud\nG285.90+4.53 (Cloud~16) in the Carina Flare supershell (GSH287+04-17) with the\nAPEX telescope are presented. With an algorithm DENDROFIND we identify 51\nfragments and compute their sizes and masses. We discuss their mass spectrum\nand interpret it as being the result of the shell fragmentation process\ndescribed by the pressure assisted gravitational instability - PAGI. We\nconclude that the explanation of the clump mass function needs a combination of\ngravity with pressure external to the shell.\n",
        "  We present a new Convolutional Neural Network (CNN) model for text\nclassification that jointly exploits labels on documents and their component\nsentences. Specifically, we consider scenarios in which annotators explicitly\nmark sentences (or snippets) that support their overall document\ncategorization, i.e., they provide rationales. Our model exploits such\nsupervision via a hierarchical approach in which each document is represented\nby a linear combination of the vector representations of its component\nsentences. We propose a sentence-level convolutional model that estimates the\nprobability that a given sentence is a rationale, and we then scale the\ncontribution of each sentence to the aggregate document representation in\nproportion to these estimates. Experiments on five classification datasets that\nhave document labels and associated rationales demonstrate that our approach\nconsistently outperforms strong baselines. Moreover, our model naturally\nprovides explanations for its predictions.\n",
        "  In this paper we introduce distinct approaches to loop braid groups, a\ngeneralisation of braid groups, and unify all the definitions that have\nappeared so far in literature, with a complete proof of the equivalence of\nthese definitions. These groups have in fact been an object of interest in\ndifferent domains of mathematics and mathematical physics, and have been\ncalled, in addition to loop braid groups, with several names such as of motion\ngroups, groups of permutation-conjugacy automorphisms, braid-permutation\ngroups, welded braid groups and untwisted ring groups. In parallel to this, we\nintroduce an extension of these groups that appears to be a more natural\ngeneralisation of braid groups from the topological point of view. Throughout\nthe text we motivate the interest in studying loop braid groups and give\nreferences to some of their applications.\n",
        "  We present a study of the lattice dynamical properties of superconducting\nSrPt$_3$P ($T_c = 8.4$ K) via high-resolution inelastic x-ray scattering (IXS)\nand ab initio calculations. Density functional perturbation theory including\nspin-orbit coupling (SOC) results in enhanced electron-phonon coupling (EPC)\nfor the optic phonon modes originating from the Pt(I) atoms, with energies\n$\\sim 5$ meV, resulting in a large EPC constant $\\lambda \\sim 2$. An overall\nsoftening of the IXS powder spectra occurs from room to low temperatures,\nconsistent with the predicted strong EPC and with recent specific-heat\nexperiments ($2\\Delta_0 / k_{\\mathrm{B}}T_c \\sim 5$). The low-lying phonon\nmodes observed in the experiments are approximately 1.5 meV harder than the\ncorresponding calculated phonon branch. Moreover, we do not find any changes in\nthe spectra upon entering the superconducting phase. We conclude that current\ntheoretical calculations underestimate the energy of the lowest band of phonon\nmodes indicating that the coupling of these modes to the electronic subsystem\nis overestimated.\n",
        "  Social media platforms have recently seen an increase in the occurrence of\nhate speech discourse which has led to calls for improved detection methods.\nMost of these rely on annotated data, keywords, and a classification technique.\nWhile this approach provides good coverage, it can fall short when dealing with\nnew terms produced by online extremist communities which act as original\nsources of words which have alternate hate speech meanings. These code words\n(which can be both created and adopted words) are designed to evade automatic\ndetection and often have benign meanings in regular discourse. As an example,\n\"skypes\", \"googles\", and \"yahoos\" are all instances of words which have an\nalternate meaning that can be used for hate speech. This overlap introduces\nadditional challenges when relying on keywords for both the collection of data\nthat is specific to hate speech, and downstream classification. In this work,\nwe develop a community detection approach for finding extremist hate speech\ncommunities and collecting data from their members. We also develop a word\nembedding model that learns the alternate hate speech meaning of words and\ndemonstrate the candidacy of our code words with several annotation\nexperiments, designed to determine if it is possible to recognize a word as\nbeing used for hate speech without knowing its alternate meaning. We report an\ninter-annotator agreement rate of K=0.871, and K=0.676 for data drawn from our\nextremist community and the keyword approach respectively, supporting our claim\nthat hate speech detection is a contextual task and does not depend on a fixed\nlist of keywords. Our goal is to advance the domain by providing a high quality\nhate speech dataset in addition to learned code words that can be fed into\nexisting classification approaches, thus improving the accuracy of automated\ndetection.\n",
        "  Given a knot, we ask how its Khovanov and Khovanov-Rozansky homologies change\nunder the operation of introducing twists in a pair of strands. We obtain long\nexact sequences in homology and further algebraic structure which is then used\nto derive topological and computational results. Two of our applications\ninclude giving a new way to generate arbitrary numbers of knots with isomorphic\nhomologies and finding an infinite number of mutant knot pairs with isomorphic\nreduced homologies.\n",
        "  Iterative image reconstruction (IIR) algorithms in Computed Tomography (CT)\nare based on algorithms for solving a particular optimization problem. Design\nof the IIR algorithm, therefore, is aided by knowledge of the solution to the\noptimization problem on which it is based. Often times, however, it is\nimpractical to achieve accurate solution to the optimization of interest, which\ncomplicates design of IIR algorithms. This issue is particularly acute for CT\nwith a limited angular-range scan, which leads to poorly conditioned system\nmatrices and difficult to solve optimization problems. In this article, we\ndevelop IIR algorithms which solve a certain type of optimization called convex\nfeasibility. The convex feasibility approach can provide alternatives to\nunconstrained optimization approaches and at the same time allow for efficient\nalgorithms for their solution -- thereby facilitating the IIR algorithm design\nprocess. An accelerated version of the Chambolle-Pock (CP) algorithm is adapted\nto various convex feasibility problems of potential interest to IIR in CT. One\nof the proposed problems is seen to be equivalent to least-squares\nminimization, and two other problems provide alternatives to penalized,\nleast-squares minimization.\n",
        "  In this paper, we propose a method of estimating individual micro data from\naggregated open data based on semi-supervised learning and conditional\nprobability. Firstly, the proposed method collects aggregated open data and\nsupport data, which are related to the individual micro data to be estimated.\nThen, we perform the locality sensitive hashing (LSH) algorithm to find a\nsubset of the support data that is similar to the aggregated open data and then\nclassify them by using the Ensemble classification model, which is learned by\nsemi-supervised learning. Finally, we use conditional probability to estimate\nthe individual micro data by finding the most suitable record for the\nprobability distribution of the individual micro data among the classification\nresults. To evaluate the performance of the proposed method, we estimated the\nindividual building data where the fire occurred using the aggregated fire open\ndata. According to the experimental results, the micro data estimation\nperformance of the proposed method is 59.41% on average in terms of accuracy.\n",
        "  Given a virtual link diagram $D$, we define its unknotting index $U(D)$ to be\nminimum among $(m, n)$ tuples, where $m$ stands for the number of crossings\nvirtualized and $n$ stands for the number of classical crossing changes, to\nobtain a trivial link diagram. By using span of a diagram and linking number of\na diagram we provide a lower bound for unknotting index of a virtual link. Then\nusing warping degree of a diagram, we obtain an upper bound. Both these bounds\nare applied to find unknotting index for virtual links obtained from pretzel\nlinks by virtualizing some crossings\n",
        "  Miniature Hall-probe arrays were used to measure the critical current\ndensities for the three main directions of vortex motion in the stoichiometric\nLiFeAs superconductor. These correspond to vortices oriented along the c-axis\nmoving parallel to the ab-plane, and to vortices in the ab-plane moving\nperpendicular to, and within the plane, respectively. The measurements were\ncarried out in the low-field regime of strong vortex pinning, in which the\ncritical current anisotropy is solely determined by the coherence length\nanisotropy parameter, {\\epsilon}_{\\xi}. This allows extraction of\n{\\epsilon}_{\\xi} at magnetic fields far below the upper critical field B_c2. We\nfind that increasing magnetic field decreases the anisotropy of the coherence\nlength.\n",
        "  The evolution and maintenance of cooperation fascinated researchers for\nseveral decades. Recently, theoretical models and experimental evidence show\nthat costly punishment may facilitate cooperation in human societies, but may\nnot be used by winners. The puzzle how the costly punishment behaviour evolves\ncan be solved under voluntary participation. Could the punishers emerge if\nparticipation is compulsory? Is the punishment inevitably a selfish behaviour\nor an altruistic behaviour? The motivations behind punishment are still an\nenigma. Based on public goods interactions, we present a model in which just a\ncertain portion of the public good is divided equally among all members. The\nother portion is distributed to contributors when paying a second cost.\nContributors who are willing to pay a second cost can be costly (and then\naltruistic) punishers, but they can also flourish or dominate the population,\nin this case we may call them \"advantageous punishers\". We argue that most of\nsuccessful cooperators and punishers in nature are advantageous punishers, and\ncostly punishment mostly happens in humans. This indicates a universal\nsurviving rule: contributing more and gaining more. Our models show\ntheoretically that the original motivation behind punishment is to retrieve\ndeserved payoff from their own contributions, a selfish incentive.\n",
        "  Good database design is crucial to obtain a sound, consistent database, and -\nin turn - good database design methodologies are the best way to achieve the\nright design. These methodologies are taught to most Computer Science\nundergraduates, as part of any Introduction to Database class. They can be\nconsidered part of the \"canon\", and indeed, the overall approach to database\ndesign has been unchanged for years. Moreover, none of the major database\nresearch assessments identify database design as a strategic research\ndirection.\n  Should we conclude that database design is a solved problem?\n  Our thesis is that database design remains a critical unsolved problem.\nHence, it should be the subject of more research. Our starting point is the\nobservation that traditional database design is not used in practice - and if\nit were used it would result in designs that are not well adapted to current\nenvironments. In short, database design has failed to keep up with the times.\nIn this paper, we put forth arguments to support our viewpoint, analyze the\nroot causes of this situation and suggest some avenues of research.\n",
        "  Bars in galaxies may form not only through instability, but also as a result\nof an interaction with another galaxy. In particular, they may appear in disky\ndwarf galaxies interacting with Milky Way-like galaxies. Here we report the\nresults of N-body/SPH simulations of such dwarfs orbiting in the static\npotential of a larger galaxy. We used several models of the dwarf galaxy, all\nof the same mass, but covering a large range of gas fractions: 0, 30 and 70%.\nWe also tested the impact of subgrid star formation processes. In all cases\nbars of similar length formed in the stellar disk of the dwarfs at the first\npericenter passage. However, unexpectedly, the gaseous component remained\napproximately axisymmetric and unaffected by the bar potential. The bar\nproperties did not change significantly between two consecutive pericenters.\nThe impact of the later encounters with the host depends strongly on the exact\norientation of the bar at the pericenter. When the bar is spun up by the tidal\nforce torque, it is also shortened. Conversely, if it is slowed down, it gets\nlonger. In the models with a low gas fraction the bars were more pronounced and\nsurvived until the end of the simulations, while in the dwarfs with a high gas\nfraction the bars were destroyed after the second or third pericenter passage.\nIn terms of the ratio of the corotation radius to the bar length, the bars are\nslow, and remain so independently of the encounters with the host.\n",
        "  The biomechanical studies have proposed several forward fall arresting\nstrategies to reduce the impact forces in a human-robot collaboration\nenvironment. A proposed strategy is using a compliant flooring for an\nenvironment because it can reduce the stiffness of the ground. In this study,\nwe proposed that if a rigid layer is mounted on a compliant flooring\n(double-layer flooring), the impact force is further reduced. In order to\ninvestigate this goal, we designed two subjective laboratory experiments. In\nexperiment 1, the subjects were instructed falling to the ground where was\ncovered by a single layer of compliant material (a foam pad). In experiment 2,\nthe subjects fell on double-layer flooring when one rigid layer (a wood\nsurface) was mounted on the compliant layer. The impact forces were measured\nfor two short forward fall heights onto the outstretched hand. The results\nshowed that the profile of the impact forces consists of two peaks. The first\npeak has a higher magnitude, and it is followed by the second peak with a lower\nmagnitude. Comparing the magnitude of the first peak between two experiments\nshows a reduction of the impact force in experiment 2. In contrast, the\nmagnitudes of second peaks are identical for both experiments. Therefore, we\nconcluded that using a double-layer flooring, i.e.one rigid layer and one\ncompliant layer can be an effective strategy for considerably reducing the\nimpact force, and preventing the wrist fractures during the forward fall.\n",
        "  The structure of ecological interactions is commonly understood through\nanalyses of interaction networks. However, these analyses may be sensitive to\nsampling biases in both the interactors (the nodes of the network) and\ninteractions (the links between nodes). These issues may affect the accuracy of\nempirically constructed ecological networks. We explore the properties of\nsampled ecological networks by simulating large-scale ecological networks with\npredetermined topologies, and sampling them with different mathematical\nprocedures. Several types of modular networks were generated, intended to\nrepresent a wide variety of communities that vary in size and types of\necological interactions. We sampled these networks with different sampling\ndesigns that may be encountered in field experiments. The observed networks\ngenerated by each sampling process were analyzed with respect to number and\nsize of components. We show that the sampling effort needed to estimate\nunderlying network properties depends both on the sampling design and on\nnetwork topology. Networks with random or scale-free modules require more\ncomplete sampling compared to networks whose modules are nested or bipartite.\nOverall, the structure of nested modules was the easiest to detect, regardless\nof sampling design. Sampling according to species degree was consistently found\nto be the most accurate strategy to estimate network structure. Conversely,\nsampling according to module results in an accurate view of certain modules,\nbut fails to provide a global picture of the underlying network. We recommend\nthat these findings are incorporated into the design of projects aiming to\ncharacterize large networks of species interactions in the field, to reduce\nsampling biases. The software scripts developed to construct and sample\nnetworks are provided for further explorations of network structure and\ncomparisons to real interaction data.\n",
        "  Cannibalism, which is the act of killing and at least partial consumption of\nconspecifics, is ubiquitous in nature. Mathematical models have considered\ncannibalism in the predator primarily, and show that predator cannibalism in\ntwo species ODE models provides a strong stabilizing effect. There is strong\necological evidence that cannibalism exists among prey as well, yet this\nphenomenon has been much less investigated. In the current manuscript, we\ninvestigate both the ODE and spatially explicit forms of a Holling-Tanner\nmodel, with ratio dependent functional response. We show that cannibalism in\nthe predator provides a stabilizing influence as expected. However, when\ncannibalism in the prey is considered, we show that it cannot stabilise the\nunstable interior equilibrium in the ODE case, but can destabilise the stable\ninterior equilibrium. In the spatially explicit case, we show that in certain\nparameter regime, prey cannibalism can lead to pattern forming Turing dynamics,\nwhich is an impossibility without it. Lastly we consider a stochastic prey\ncannibalism rate, and find that it can alter both spatial patterns, as well as\nlimit cycle dynamics.\n",
        "  For $g>3$, we give two proofs of the fact that the \\emph{Birman exact\nsequence} for the Torelli group \\[ 1\\to \\pi_1(S_g)\\to {\\cal I}_{g,1}\\to {\\cal\nI}_g\\to 1 \\] does not split. This result was claimed by G. Mess in\n\\cite{mess1990unit}, but his proof has a critical and unrepairable error which\nwill be discussed in the introduction. Let ${\\cal\nUI}_{g,n}\\xrightarrow{Tu'_{g,n}} {\\cal BI}_{g,n}$ (resp. ${\\cal\nUPI}_{g,n}\\xrightarrow{Tu_{g,n}}{\\cal BPI}_{g,n}$) denote the universal surface\nbundle over the Torelli space fixing $n$ points as a set (resp. pointwise). We\nalso deduce that $Tu'_{g,n}$ has no sections when $n>1$ and that $Tu_{g,n}$ has\nprecisely $n$ distinct sections for $n\\ge 0$ up to homotopy.\n",
        "  The recently increased focus on misinformation has stimulated research in\nfact checking, the task of assessing the truthfulness of a claim. Research in\nautomating this task has been conducted in a variety of disciplines including\nnatural language processing, machine learning, knowledge representation,\ndatabases, and journalism. While there has been substantial progress, relevant\npapers and articles have been published in research communities that are often\nunaware of each other and use inconsistent terminology, thus impeding\nunderstanding and further progress. In this paper we survey automated fact\nchecking research stemming from natural language processing and related\ndisciplines, unifying the task formulations and methodologies across papers and\nauthors. Furthermore, we highlight the use of evidence as an important\ndistinguishing factor among them cutting across task formulations and methods.\nWe conclude with proposing avenues for future NLP research on automated fact\nchecking.\n",
        "  We investigate the long-term dynamical evolution of star clusters in a steady\ntidal field produced by its parent galaxy. In this paper, we focus on the\ninfluence of mass profile of the parent galaxy. The previous studies were done\nwith the simplification where the parent galaxy was expressed by point mass. We\nexpress different mass profiles of the parent galaxy by the tidal fields in\nwhich the ratios of the epicyclic frequency to the angular velocity are\ndifferent. We compare the mass-loss timescale of star clusters whose tidal\nradii are identical but in parent galaxies with different mass profile, by\nmeans of orbits calculations in fixed cluster potential and N-body simulations.\nIn this situation, a cluster rotates around the parent galaxy more rapidly as\nthe parent galaxy has shallower mass profile. We found that the mass-loss\ntimescale increase 20% and 50% for the cases that the mass density profile of\nthe parent galaxies are proportional to R^-2 and R^-1.5 where R is the distance\nfrom the galaxy center, compared to the point-mass case, in moderately strong\ntidal field. Counterintuitively, a cluster which rotates around the parent\ngalaxy more rapidly has a longer lifetime. The increase of lifetime is due to\nthe fact that the fraction occupied by regular-like orbit increases in\nshallower profile. Finally, we derive an evaluation formula for the mass-loss\ntimescale of clusters. Our formula can explain a property of the population of\nthe observed galactic globular clusters that their half-mass radii become\nsmaller as their distances from the galactic center become smaller.\n",
        "  Machine reading comprehension (MRC) on real web data usually requires the\nmachine to answer a question by analyzing multiple passages retrieved by search\nengine. Compared with MRC on a single passage, multi-passage MRC is more\nchallenging, since we are likely to get multiple confusing answer candidates\nfrom different passages. To address this problem, we propose an end-to-end\nneural model that enables those answer candidates from different passages to\nverify each other based on their content representations. Specifically, we\njointly train three modules that can predict the final answer based on three\nfactors: the answer boundary, the answer content and the cross-passage answer\nverification. The experimental results show that our method outperforms the\nbaseline by a large margin and achieves the state-of-the-art performance on the\nEnglish MS-MARCO dataset and the Chinese DuReader dataset, both of which are\ndesigned for MRC in real-world settings.\n",
        "  We present ALMA CO(1-0) and CO(3-2) observations of the brightest cluster\ngalaxy (BCG) in the 2A 0335+096 galaxy cluster (z = 0.0346). The total\nmolecular gas mass of (1.13+/-0.15) x 10^9 M_sun is divided into two\ncomponents: a nuclear region and a 7 kpc long dusty filament. The central\nmolecular gas component accounts for (3.2+/-0.4) x 10^8 M_sun of the total\nsupply of cold gas. Instead of forming a rotationally-supported ring or disk,\nit is composed of two distinct, blueshifted clumps south of the nucleus and a\nseries of low-significance redshifted clumps extending toward a nearby\ncompanion galaxy. The velocity of the redshifted clouds increases with radius\nto a value consistent with the companion galaxy, suggesting that an interaction\nbetween these galaxies <20 Myr ago disrupted a pre-existing molecular gas\nreservoir within the BCG. Most of the molecular gas, (7.8+/-0.9) x 10^8 M_sun,\nis located in the filament. The CO emission is co-spatial with a 10^4 K\nemission-line nebula and soft X-rays from 0.5 keV gas, indicating that the\nmolecular gas has cooled out of the intracluster medium over a period of 25-100\nMyr. The filament trails an X-ray cavity, suggesting that the gas has cooled\nfrom low entropy gas that has been lifted out of the cluster core and become\nthermally unstable. We are unable to distinguish between inflow and outflow\nalong the filament with the present data. Cloud velocities along the filament\nare consistent with gravitational free-fall near the plane of the sky, although\ntheir increasing blueshifts with radius are consistent with outflow.\n",
        "  In this paper, we show the trivializing number of all minimal diagrams of\npositive 2-bridge knots and study the relation between the trivializing number\nand the unknotting number for a part of these knots.\n",
        "  Aggregation has been an important operation since the early days of\nrelational databases. Today's Big Data applications bring further challenges\nwhen processing aggregation queries, demanding adaptive aggregation algorithms\nthat can process large volumes of data relative to a potentially limited memory\nbudget (especially in multiuser settings). Despite its importance, the design\nand evaluation of aggregation algorithms has not received the same attention\nthat other basic operators, such as joins, have received in the literature. As\na result, when considering which aggregation algorithm(s) to implement in a new\nparallel Big Data processing platform (AsterixDB), we faced a lack of \"off the\nshelf\" answers that we could simply read about and then implement based on\nprior performance studies.\n  In this paper we revisit the engineering of efficient local aggregation\nalgorithms for use in Big Data platforms. We discuss the salient implementation\ndetails of several candidate algorithms and present an in-depth experimental\nperformance study to guide future Big Data engine developers. We show that the\nefficient implementation of the aggregation operator for a Big Data platform is\nnon-trivial and that many factors, including memory usage, spilling strategy,\nand I/O and CPU cost, should be considered. Further, we introduce precise cost\nmodels that can help in choosing an appropriate algorithm based on input\nparameters including memory budget, grouping key cardinality, and data skew.\n",
        "  In this paper we argue that biological fitness is a multi-objective concept\nhence the statement \"fittest\" is inappropriate. The following statement is\nproposed \"Survival is mostly for those with non-dominated fitness\". Also we use\nsome TV games to show that under the following conditions: i) There are no\ndominant players. ii) At each time step successful players may eliminate some\nof their less successful competitors, Then the ultimate winner may not be the\nfittest (but close).\n",
        "  We provide a comparative study between neural word representations and\ntraditional vector spaces based on co-occurrence counts, in a number of\ncompositional tasks. We use three different semantic spaces and implement seven\ntensor-based compositional models, which we then test (together with simpler\nadditive and multiplicative approaches) in tasks involving verb disambiguation\nand sentence similarity. To check their scalability, we additionally evaluate\nthe spaces using simple compositional methods on larger-scale tasks with less\nconstrained language: paraphrase detection and dialogue act tagging. In the\nmore constrained tasks, co-occurrence vectors are competitive, although choice\nof compositional method is important; on the larger-scale tasks, they are\noutperformed by neural word embeddings, which show robust, stable performance\nacross the tasks.\n",
        "  This paper discusses the approach taken by the UWaterloo team to arrive at a\nsolution for the Fine-Grained Sentiment Analysis problem posed by Task 5 of\nSemEval 2017. The paper describes the document vectorization and sentiment\nscore prediction techniques used, as well as the design and implementation\ndecisions taken while building the system for this task. The system uses text\nvectorization models, such as N-gram, TF-IDF and paragraph embeddings, coupled\nwith regression model variants to predict the sentiment scores. Amongst the\nmethods examined, unigrams and bigrams coupled with simple linear regression\nobtained the best baseline accuracy. The paper also explores data augmentation\nmethods to supplement the training dataset. This system was designed for\nSubtask 2 (News Statements and Headlines).\n",
        "  This article describes a method to build syntactical dependencies starting\nfrom the phrase structure parsing process. The goal is to obtain all the\ninformation needed for a detailled semantical analysis. Interaction Grammars\nare used for parsing; the saturation of polarities which is the core of this\nformalism can be mapped to dependency relation. Formally, graph patterns are\nused to express the set of constraints which control dependency creations.\n",
        "  The surface terminations of 122-type alkaline earth metal iron pnictides\nAEFe2As2 (AE = Ca, Ba) are investigated with scanning tunneling\nmicroscopy/spectroscopy (STM/STS). Cleaving these crystals at a cryogenic\ntemperature yields a large majority of terminations with atomically resolved\nsquare-root-two (rt2) or 1*2 lattice, as well as the very rare terminations\nwith 1*1 symmetry. By means of lattice alignment and chemical marking, we\nidentify these terminations as rt2-AE, 1*2-As, and rt2-Fe surfaces,\nrespectively. Layer-resolved spectroscopy on these terminating surfaces reveals\na well-defined superconducting gap on the As terminations, while the gap\nfeatures become weaker and absent on AE and Fe terminations respectively. The\nlocal gap features are hardly affected by the surface reconstruction on As or\nAE surface, whereas a suppression of them along with the in-gap states can be\ninduced by As vacancies. The emergence of two impurity resonance peaks at +-2\nmeV is consistent with the sign-reversal pairing symmetry. The definite\nidentification of surface terminations and their spectroscopic signatures shall\nprovide a more comprehensive understanding of the high-temperature\nsuperconductivity in multilayered iron pnictides.\n",
        "  We investigate the role of migration patterns on the spread of epidemics in\ncomplex networks. We enhance the SIS-diffusion model on metapopulations to a\nnonlinear diffusion. Specifically, individuals move randomly over the network\nbut at a rate depending on the population of the departure patch. In the\nabsence of epidemics, the migration-driven equilibrium is described by\nquantifying the total number of individuals living in heavily/lightly populated\nareas. Our analytical approach reveals that strengthening the migration from\npopulous areas contains the infection at the early stage of the epidemic.\nMoreover, depending on the exponent of the nonlinear diffusion rate, epidemic\noutbreaks do not always occur in the most populated areas as one might expect.\n"
    ]
}